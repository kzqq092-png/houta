# FactorWeave-Quant æ•°æ®æºåˆ†ç±»ä½¿ç”¨ä¸å­˜å‚¨æŸ¥è¯¢å®Œæ•´æ–¹æ¡ˆ

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°

åŸºäºFactorWeave-Quantç°æœ‰çš„DuckDB+SQLiteæ··åˆæ¶æ„ï¼Œæœ¬æ–¹æ¡ˆæä¾›æ•°æ®æºçš„åˆ†ç±»ä½¿ç”¨ç­–ç•¥ã€é«˜æ•ˆå­˜å‚¨æ–¹æ¡ˆå’Œå¿«é€ŸæŸ¥è¯¢æ¥å£è®¾è®¡ã€‚

## ğŸ“Š æ•°æ®æºåˆ†ç±»ç­–ç•¥

### 1. æŒ‰ä¸šåŠ¡ç”¨é€”åˆ†ç±»

#### ğŸ¢ **æ ¸å¿ƒäº¤æ˜“æ•°æ®æº** (å®æ—¶æ€§è¦æ±‚é«˜)
```python
CORE_TRADING_SOURCES = {
    'primary': 'tongdaxin_stock_plugin',    # ä¸»è¦æ•°æ®æº
    'backup': 'sina_source',                # å¤‡ç”¨æ•°æ®æº  
    'cross_validation': 'eastmoney_source', # äº¤å‰éªŒè¯
    'use_cases': ['å®æ—¶äº¤æ˜“', 'é«˜é¢‘ç­–ç•¥', 'é£é™©æ§åˆ¶'],
    'update_frequency': 'å®æ—¶',
    'storage': 'memory_cache + sqlite'
}
```

#### ğŸ“ˆ **å†å²åˆ†ææ•°æ®æº** (æ•°æ®é‡å¤§)
```python
HISTORICAL_ANALYSIS_SOURCES = {
    'primary': 'eastmoney_stock_plugin',     # æ•°æ®æœ€å…¨é¢
    'secondary': 'tongdaxin_stock_plugin',   # æ•°æ®ç²¾åº¦é«˜
    'use_cases': ['å›æµ‹åˆ†æ', 'æŠ€æœ¯æŒ‡æ ‡è®¡ç®—', 'æ¨¡å¼è¯†åˆ«'],
    'update_frequency': 'æ—¥/å‘¨æ›´æ–°',
    'storage': 'duckdb + compression'
}
```

#### ğŸ§  **æƒ…ç»ªåˆ†ææ•°æ®æº** (è¾…åŠ©å†³ç­–)
```python
SENTIMENT_SOURCES = {
    'aggregator': 'multi_source_sentiment_plugin',  # å¤šæºèšåˆ
    'news': 'news_sentiment_plugin',                # æ–°é—»æƒ…ç»ª
    'market': 'vix_sentiment_plugin',               # å¸‚åœºææ…Œ
    'use_cases': ['æƒ…ç»ªæŒ‡æ ‡', 'å¸‚åœºé¢„è­¦', 'ç­–ç•¥ä¼˜åŒ–'],
    'update_frequency': 'å°æ—¶çº§',
    'storage': 'sqlite + time_series'
}
```

#### ğŸŒ **å¤šèµ„äº§æ•°æ®æº** (èµ„äº§é…ç½®)
```python
MULTI_ASSET_SOURCES = {
    'crypto': 'binance_crypto_plugin',      # æ•°å­—è´§å¸
    'futures': 'futures_data_plugin',       # æœŸè´§
    'forex': 'forex_data_plugin',           # å¤–æ±‡
    'use_cases': ['èµ„äº§é…ç½®', 'å¥—åˆ©ç­–ç•¥', 'é£é™©å¯¹å†²'],
    'update_frequency': 'å®æ—¶/æ—¥æ›´æ–°',
    'storage': 'duckdb_partitioned'
}
```

### 2. æŒ‰æ•°æ®ç‰¹å¾åˆ†ç±»

#### âš¡ **é«˜é¢‘æ•°æ®** (æ¯«ç§’çº§æ›´æ–°)
- **æ•°æ®æº**: é€šè¾¾ä¿¡ã€æ–°æµªè´¢ç»
- **å­˜å‚¨ç­–ç•¥**: å†…å­˜ç¼“å­˜ + å¼‚æ­¥æŒä¹…åŒ–
- **æŸ¥è¯¢ä¼˜åŒ–**: é¢„èšåˆ + ç´¢å¼•ä¼˜åŒ–

#### ğŸ“… **æ—¥é¢‘æ•°æ®** (æ¯æ—¥æ›´æ–°)
- **æ•°æ®æº**: ä¸œæ–¹è´¢å¯Œã€åŒèŠ±é¡º
- **å­˜å‚¨ç­–ç•¥**: DuckDBåˆ†åŒºè¡¨
- **æŸ¥è¯¢ä¼˜åŒ–**: æ—¶é—´åˆ†åŒº + åˆ—å¼å­˜å‚¨

#### ğŸ“° **äº‹ä»¶æ•°æ®** (ä¸è§„åˆ™æ›´æ–°)
- **æ•°æ®æº**: æƒ…ç»ªåˆ†ææ’ä»¶
- **å­˜å‚¨ç­–ç•¥**: SQLiteäº‹åŠ¡æ—¥å¿—
- **æŸ¥è¯¢ä¼˜åŒ–**: æ—¶é—´ç´¢å¼• + å…¨æ–‡æœç´¢

## ğŸ—ï¸ æ•°æ®å­˜å‚¨æ¶æ„è®¾è®¡

### 1. æ··åˆæ•°æ®åº“æ¶æ„

```python
DATABASE_ARCHITECTURE = {
    # SQLite - è½»é‡çº§äº‹åŠ¡æ•°æ®
    'sqlite': {
        'path': 'db/factorweave_system.sqlite',
        'use_cases': [
            'ç³»ç»Ÿé…ç½®', 'ç”¨æˆ·è®¾ç½®', 'æ’ä»¶å…ƒæ•°æ®',
            'äº¤æ˜“è®°å½•', 'ç­–ç•¥å‚æ•°', 'å®æ—¶ç¼“å­˜'
        ],
        'advantages': ['äº‹åŠ¡å®‰å…¨', 'å¹¶å‘è¯»å–', 'è½»é‡çº§']
    },
    
    # DuckDB - å¤§æ•°æ®åˆ†æ
    'duckdb': {
        'path': 'db/factorweave_analytics.duckdb',
        'use_cases': [
            'Kçº¿æ•°æ®', 'æŠ€æœ¯æŒ‡æ ‡', 'å›æµ‹ç»“æœ',
            'è´¢åŠ¡æ•°æ®', 'å®è§‚æ•°æ®', 'å› å­æ•°æ®'
        ],
        'advantages': ['åˆ—å¼å­˜å‚¨', 'å‹ç¼©é«˜æ•ˆ', 'åˆ†ææ€§èƒ½å¼º']
    }
}
```

### 2. è¡¨ç»“æ„è®¾è®¡

#### ğŸ“Š **Kçº¿æ•°æ®è¡¨** (DuckDB)
```sql
-- æŒ‰æ•°æ®æºå’Œå‘¨æœŸåˆ†è¡¨
CREATE TABLE kline_data_tongdaxin_daily (
    symbol VARCHAR NOT NULL,
    datetime TIMESTAMP NOT NULL,
    open DECIMAL(10,3) NOT NULL,
    high DECIMAL(10,3) NOT NULL,
    low DECIMAL(10,3) NOT NULL,
    close DECIMAL(10,3) NOT NULL,
    volume BIGINT NOT NULL,
    amount DECIMAL(15,2),
    data_source VARCHAR DEFAULT 'tongdaxin',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (symbol, datetime)
) PARTITION BY RANGE (datetime);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_kline_symbol_date ON kline_data_tongdaxin_daily(symbol, datetime);
CREATE INDEX idx_kline_date ON kline_data_tongdaxin_daily(datetime);
```

#### ğŸ“ˆ **å®æ—¶è¡Œæƒ…è¡¨** (SQLite)
```sql
-- å®æ—¶æ•°æ®è¡¨
CREATE TABLE realtime_quotes (
    symbol VARCHAR(10) PRIMARY KEY,
    name VARCHAR(50),
    price DECIMAL(10,3) NOT NULL,
    change_amount DECIMAL(10,3),
    change_percent DECIMAL(5,2),
    volume BIGINT,
    amount DECIMAL(15,2),
    high DECIMAL(10,3),
    low DECIMAL(10,3),
    open DECIMAL(10,3),
    prev_close DECIMAL(10,3),
    data_source VARCHAR(20) DEFAULT 'sina',
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºæ›´æ–°è§¦å‘å™¨
CREATE TRIGGER update_realtime_quotes_timestamp 
    AFTER UPDATE ON realtime_quotes
    BEGIN
        UPDATE realtime_quotes SET update_time = CURRENT_TIMESTAMP 
        WHERE symbol = NEW.symbol;
    END;
```

#### ğŸ§  **æƒ…ç»ªæ•°æ®è¡¨** (SQLite)
```sql
CREATE TABLE sentiment_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    symbol VARCHAR(10),
    sentiment_score DECIMAL(3,2), -- -1.0 åˆ° 1.0
    confidence DECIMAL(3,2),      -- 0.0 åˆ° 1.0
    data_source VARCHAR(30),
    data_type VARCHAR(20),        -- news, social, technical
    raw_data TEXT,               -- JSONæ ¼å¼åŸå§‹æ•°æ®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_sentiment_symbol_time (symbol, created_at),
    INDEX idx_sentiment_source (data_source)
);
```

### 3. æ•°æ®åˆ†åŒºç­–ç•¥

```python
PARTITIONING_STRATEGY = {
    'time_based': {
        'kline_data': 'PARTITION BY RANGE(YEAR(datetime))',
        'tick_data': 'PARTITION BY RANGE(DATE(datetime))',
        'sentiment_data': 'PARTITION BY RANGE(MONTH(created_at))'
    },
    
    'asset_based': {
        'multi_asset_data': 'PARTITION BY LIST(asset_type)',
        'crypto_data': 'PARTITION BY HASH(symbol)'
    }
}
```

## ğŸ” æ•°æ®æŸ¥è¯¢æ¥å£è®¾è®¡

### 1. ç»Ÿä¸€æ•°æ®æŸ¥è¯¢æ¥å£

```python
class UnifiedDataQueryManager:
    """ç»Ÿä¸€æ•°æ®æŸ¥è¯¢ç®¡ç†å™¨"""
    
    def __init__(self):
        self.sqlite_conn = sqlite3.connect('db/factorweave_system.sqlite')
        self.duckdb_conn = duckdb.connect('db/factorweave_analytics.duckdb')
        self.cache_manager = MultiLevelCacheManager()
    
    async def get_kline_data(self, 
                           symbol: str, 
                           period: str = 'daily',
                           start_date: str = None, 
                           end_date: str = None,
                           data_source: str = 'auto') -> pd.DataFrame:
        """
        è·å–Kçº¿æ•°æ® - æ™ºèƒ½æ•°æ®æºé€‰æ‹©
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸ (1min, 5min, 15min, 30min, 60min, daily, weekly, monthly)
            start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD  
            data_source: æ•°æ®æº (auto, tongdaxin, eastmoney, sina)
        """
        
        # 1. ç¼“å­˜æ£€æŸ¥
        cache_key = f"kline_{symbol}_{period}_{start_date}_{end_date}_{data_source}"
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # 2. æ™ºèƒ½æ•°æ®æºé€‰æ‹©
        if data_source == 'auto':
            data_source = self._select_optimal_data_source(period, start_date, end_date)
        
        # 3. æ„å»ºæŸ¥è¯¢
        table_name = f"kline_data_{data_source}_{period}"
        
        query = f"""
        SELECT symbol, datetime, open, high, low, close, volume, amount
        FROM {table_name}
        WHERE symbol = ?
        """
        
        params = [symbol]
        
        if start_date:
            query += " AND datetime >= ?"
            params.append(start_date)
        
        if end_date:
            query += " AND datetime <= ?"
            params.append(end_date)
            
        query += " ORDER BY datetime"
        
        # 4. æ‰§è¡ŒæŸ¥è¯¢
        df = pd.read_sql_query(query, self.duckdb_conn, params=params)
        
        # 5. ç¼“å­˜ç»“æœ
        await self.cache_manager.set(cache_key, df, ttl_minutes=30)
        
        return df
    
    def _select_optimal_data_source(self, period: str, start_date: str, end_date: str) -> str:
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ•°æ®æº"""
        
        # é«˜é¢‘æ•°æ®ä¼˜é€‰é€šè¾¾ä¿¡
        if period in ['1min', '5min']:
            return 'tongdaxin'
        
        # å†å²æ•°æ®ä¼˜é€‰ä¸œæ–¹è´¢å¯Œ
        if start_date and (datetime.now() - pd.to_datetime(start_date)).days > 365:
            return 'eastmoney'
        
        # é»˜è®¤é€šè¾¾ä¿¡
        return 'tongdaxin'
    
    async def get_realtime_quotes(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…æ•°æ®"""
        
        placeholders = ','.join(['?' for _ in symbols])
        query = f"""
        SELECT symbol, name, price, change_amount, change_percent, 
               volume, amount, high, low, open, prev_close, 
               data_source, update_time
        FROM realtime_quotes 
        WHERE symbol IN ({placeholders})
        ORDER BY symbol
        """
        
        df = pd.read_sql_query(query, self.sqlite_conn, params=symbols)
        return df
    
    async def get_sentiment_data(self, 
                               symbol: str = None,
                               data_source: str = None,
                               hours_back: int = 24) -> pd.DataFrame:
        """è·å–æƒ…ç»ªæ•°æ®"""
        
        query = """
        SELECT symbol, sentiment_score, confidence, data_source, 
               data_type, created_at
        FROM sentiment_data 
        WHERE created_at >= datetime('now', '-{} hours')
        """.format(hours_back)
        
        params = []
        
        if symbol:
            query += " AND symbol = ?"
            params.append(symbol)
            
        if data_source:
            query += " AND data_source = ?"
            params.append(data_source)
            
        query += " ORDER BY created_at DESC"
        
        df = pd.read_sql_query(query, self.sqlite_conn, params=params)
        return df
```

### 2. é«˜æ€§èƒ½æŸ¥è¯¢ä¼˜åŒ–

```python
class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_kline_query(symbol: str, period: str, days: int) -> str:
        """Kçº¿æŸ¥è¯¢ä¼˜åŒ–"""
        
        # å°æ•°æ®é‡ç›´æ¥æŸ¥è¯¢
        if days <= 30:
            return f"SELECT * FROM kline_data_{period} WHERE symbol = '{symbol}'"
        
        # å¤§æ•°æ®é‡ä½¿ç”¨åˆ†åŒºæŸ¥è¯¢
        return f"""
        SELECT * FROM kline_data_{period} 
        WHERE symbol = '{symbol}' 
        AND datetime >= current_date - interval '{days} days'
        """
    
    @staticmethod
    def create_materialized_views():
        """åˆ›å»ºç‰©åŒ–è§†å›¾åŠ é€Ÿå¸¸ç”¨æŸ¥è¯¢"""
        
        views = {
            # æ—¥çº¿æ±‡æ€»è§†å›¾
            'daily_summary': """
            CREATE MATERIALIZED VIEW daily_summary AS
            SELECT 
                symbol,
                date_trunc('day', datetime) as trade_date,
                first(open) as open,
                max(high) as high,
                min(low) as low,
                last(close) as close,
                sum(volume) as volume,
                sum(amount) as amount
            FROM kline_data_1min
            GROUP BY symbol, date_trunc('day', datetime)
            """,
            
            # æŠ€æœ¯æŒ‡æ ‡è§†å›¾  
            'technical_indicators': """
            CREATE MATERIALIZED VIEW technical_indicators AS
            SELECT 
                symbol,
                datetime,
                close,
                avg(close) OVER (PARTITION BY symbol ORDER BY datetime ROWS 19 PRECEDING) as ma20,
                avg(close) OVER (PARTITION BY symbol ORDER BY datetime ROWS 4 PRECEDING) as ma5
            FROM kline_data_daily
            """
        }
        
        return views
```

### 3. æ•°æ®åŒæ­¥ä¸ä¸€è‡´æ€§

```python
class DataSynchronizer:
    """æ•°æ®åŒæ­¥å™¨ - ä¿è¯å¤šæ•°æ®æºä¸€è‡´æ€§"""
    
    def __init__(self):
        self.data_sources = [
            'tongdaxin_stock_plugin',
            'eastmoney_stock_plugin', 
            'sina_source'
        ]
    
    async def sync_and_validate(self, symbol: str, date: str):
        """åŒæ­¥å¹¶éªŒè¯å¤šæ•°æ®æºæ•°æ®"""
        
        results = {}
        
        # 1. å¹¶è¡Œè·å–å¤šæºæ•°æ®
        tasks = []
        for source in self.data_sources:
            task = self._fetch_data_from_source(source, symbol, date)
            tasks.append(task)
        
        source_data = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 2. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        for i, data in enumerate(source_data):
            if isinstance(data, Exception):
                logger.warning(f"æ•°æ®æº {self.data_sources[i]} è·å–å¤±è´¥: {data}")
                continue
            
            results[self.data_sources[i]] = data
        
        # 3. äº¤å‰éªŒè¯
        validated_data = self._cross_validate(results)
        
        # 4. å­˜å‚¨æœ€ä¼˜æ•°æ®
        await self._store_validated_data(validated_data, symbol, date)
        
        return validated_data
    
    def _cross_validate(self, results: Dict) -> Dict:
        """äº¤å‰éªŒè¯æ•°æ®è´¨é‡"""
        
        if len(results) < 2:
            return list(results.values())[0] if results else None
        
        # ä»·æ ¼å·®å¼‚æ£€æŸ¥
        price_tolerance = 0.01  # 1%è¯¯å·®å®¹å¿
        
        validated = {}
        for field in ['open', 'high', 'low', 'close']:
            values = [data[field] for data in results.values() if field in data]
            
            if len(values) >= 2:
                mean_val = sum(values) / len(values)
                # é€‰æ‹©æœ€æ¥è¿‘å‡å€¼çš„æ•°æ®
                best_val = min(values, key=lambda x: abs(x - mean_val))
                validated[field] = best_val
        
        return validated
```

## ğŸ“ˆ å®é™…ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æ•°æ®æŸ¥è¯¢

```python
# åˆå§‹åŒ–æ•°æ®æŸ¥è¯¢ç®¡ç†å™¨
query_manager = UnifiedDataQueryManager()

# è·å–è‚¡ç¥¨Kçº¿æ•°æ®
df = await query_manager.get_kline_data(
    symbol='000001',
    period='daily',
    start_date='2024-01-01',
    end_date='2024-12-01',
    data_source='auto'  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æº
)

# è·å–å®æ—¶è¡Œæƒ…
quotes = await query_manager.get_realtime_quotes(['000001', '000002', '600036'])

# è·å–æƒ…ç»ªæ•°æ®
sentiment = await query_manager.get_sentiment_data(
    symbol='000001',
    hours_back=48
)
```

### 2. é«˜çº§åˆ†ææŸ¥è¯¢

```python
class AdvancedQueryExamples:
    """é«˜çº§æŸ¥è¯¢ç¤ºä¾‹"""
    
    @staticmethod
    async def get_multi_timeframe_data(symbol: str):
        """è·å–å¤šæ—¶é—´æ¡†æ¶æ•°æ®"""
        
        query_manager = UnifiedDataQueryManager()
        
        # å¹¶è¡Œè·å–å¤šä¸ªæ—¶é—´æ¡†æ¶æ•°æ®
        tasks = [
            query_manager.get_kline_data(symbol, '1min'),
            query_manager.get_kline_data(symbol, '5min'),
            query_manager.get_kline_data(symbol, 'daily')
        ]
        
        min1_data, min5_data, daily_data = await asyncio.gather(*tasks)
        
        return {
            '1min': min1_data,
            '5min': min5_data,
            'daily': daily_data
        }
    
    @staticmethod
    async def get_cross_asset_correlation(symbols: List[str], days: int = 30):
        """è·å–è·¨èµ„äº§ç›¸å…³æ€§æ•°æ®"""
        
        query_manager = UnifiedDataQueryManager()
        
        # è·å–å¤šä¸ªèµ„äº§æ•°æ®
        asset_data = {}
        for symbol in symbols:
            df = await query_manager.get_kline_data(
                symbol=symbol,
                period='daily',
                start_date=(datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            )
            asset_data[symbol] = df['close']
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_df = pd.DataFrame(asset_data).corr()
        
        return correlation_df
    
    @staticmethod
    async def get_sentiment_enhanced_data(symbol: str):
        """è·å–æƒ…ç»ªå¢å¼ºçš„è‚¡ç¥¨æ•°æ®"""
        
        query_manager = UnifiedDataQueryManager()
        
        # å¹¶è¡Œè·å–ä»·æ ¼æ•°æ®å’Œæƒ…ç»ªæ•°æ®
        price_task = query_manager.get_kline_data(symbol, 'daily')
        sentiment_task = query_manager.get_sentiment_data(symbol, hours_back=24*30)
        
        price_data, sentiment_data = await asyncio.gather(price_task, sentiment_task)
        
        # åˆå¹¶æ•°æ®
        if not sentiment_data.empty:
            # æŒ‰æ—¥æœŸèšåˆæƒ…ç»ªæ•°æ®
            daily_sentiment = sentiment_data.groupby(
                sentiment_data['created_at'].dt.date
            ).agg({
                'sentiment_score': 'mean',
                'confidence': 'mean'
            }).reset_index()
            
            # åˆå¹¶åˆ°ä»·æ ¼æ•°æ®
            price_data['date'] = price_data['datetime'].dt.date
            enhanced_data = price_data.merge(
                daily_sentiment, 
                left_on='date', 
                right_on='created_at', 
                how='left'
            )
            
            return enhanced_data
        
        return price_data
```

### 3. æ•°æ®è´¨é‡ç›‘æ§

```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§"""
    
    @staticmethod
    async def check_data_completeness(symbol: str, start_date: str, end_date: str):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        
        query_manager = UnifiedDataQueryManager()
        
        # è·å–æ•°æ®
        df = await query_manager.get_kline_data(symbol, 'daily', start_date, end_date)
        
        # è®¡ç®—é¢„æœŸäº¤æ˜“æ—¥æ•°é‡
        expected_days = pd.bdate_range(start_date, end_date)
        actual_days = len(df)
        
        completeness = actual_days / len(expected_days) * 100
        
        return {
            'symbol': symbol,
            'expected_days': len(expected_days),
            'actual_days': actual_days,
            'completeness_rate': completeness,
            'missing_days': len(expected_days) - actual_days
        }
    
    @staticmethod
    async def validate_data_consistency():
        """éªŒè¯å¤šæ•°æ®æºä¸€è‡´æ€§"""
        
        # é€‰æ‹©æ ·æœ¬è‚¡ç¥¨
        sample_symbols = ['000001', '000002', '600036']
        
        consistency_report = {}
        
        for symbol in sample_symbols:
            # è·å–å¤šæºæ•°æ®è¿›è¡Œå¯¹æ¯”
            sources = ['tongdaxin', 'eastmoney', 'sina']
            source_data = {}
            
            for source in sources:
                try:
                    df = await query_manager.get_kline_data(
                        symbol=symbol,
                        period='daily',
                        data_source=source
                    )
                    source_data[source] = df.tail(5)  # æœ€è¿‘5å¤©æ•°æ®
                except Exception as e:
                    logger.warning(f"è·å–{source}æ•°æ®å¤±è´¥: {e}")
            
            # è®¡ç®—ä»·æ ¼å·®å¼‚
            if len(source_data) >= 2:
                price_diffs = []
                source_pairs = list(combinations(source_data.keys(), 2))
                
                for s1, s2 in source_pairs:
                    df1, df2 = source_data[s1], source_data[s2]
                    # è®¡ç®—æ”¶ç›˜ä»·å·®å¼‚
                    diff = abs(df1['close'] - df2['close']).mean()
                    price_diffs.append({
                        'sources': f"{s1}_vs_{s2}",
                        'avg_price_diff': diff
                    })
                
                consistency_report[symbol] = price_diffs
        
        return consistency_report
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¼“å­˜ç­–ç•¥
- **L1ç¼“å­˜**: å†…å­˜ç¼“å­˜çƒ­ç‚¹æ•°æ® (100MB)
- **L2ç¼“å­˜**: ç£ç›˜ç¼“å­˜å†å²æ•°æ® (500MB)
- **L3ç¼“å­˜**: æ•°æ®åº“æŸ¥è¯¢ç»“æœç¼“å­˜

### 2. æŸ¥è¯¢ä¼˜åŒ–
- **é¢„èšåˆ**: å¸¸ç”¨æŒ‡æ ‡æå‰è®¡ç®—
- **åˆ†åŒºæŸ¥è¯¢**: æŒ‰æ—¶é—´å’Œè‚¡ç¥¨åˆ†åŒº
- **ç´¢å¼•ä¼˜åŒ–**: å¤åˆç´¢å¼•è¦†ç›–å¸¸ç”¨æŸ¥è¯¢

### 3. å¹¶å‘æ§åˆ¶
- **è¯»å†™åˆ†ç¦»**: DuckDBè¯»å–ï¼ŒSQLiteå†™å…¥
- **è¿æ¥æ± **: æ•°æ®åº“è¿æ¥å¤ç”¨
- **å¼‚æ­¥å¤„ç†**: éé˜»å¡æ•°æ®è·å–

## ğŸ“‹ éƒ¨ç½²é…ç½®ç¤ºä¾‹

```python
# config/data_sources.json
{
    "data_source_config": {
        "classification": {
            "core_trading": ["tongdaxin_stock_plugin", "sina_source"],
            "historical_analysis": ["eastmoney_stock_plugin"],
            "sentiment": ["multi_source_sentiment_plugin"],
            "multi_asset": ["binance_crypto_plugin", "futures_data_plugin"]
        },
        "storage": {
            "sqlite": {
                "path": "db/factorweave_system.sqlite",
                "pool_size": 10,
                "timeout": 30
            },
            "duckdb": {
                "path": "db/factorweave_analytics.duckdb", 
                "memory_limit": "8GB",
                "threads": 4
            }
        },
        "cache": {
            "enable": true,
            "memory_size_mb": 100,
            "disk_size_mb": 500,
            "default_ttl_minutes": 30
        }
    }
}
```

---

**æ–¹æ¡ˆä¼˜åŠ¿**:
- âœ… **é«˜æ€§èƒ½**: DuckDBåˆ—å¼å­˜å‚¨ + SQLiteäº‹åŠ¡å®‰å…¨
- âœ… **å¯æ‰©å±•**: æ”¯æŒæ–°æ•°æ®æºå¿«é€Ÿæ¥å…¥
- âœ… **é«˜å¯ç”¨**: å¤šæ•°æ®æºå†—ä½™ + è‡ªåŠ¨æ•…éšœè½¬ç§»
- âœ… **æ˜“ç»´æŠ¤**: ç»Ÿä¸€æŸ¥è¯¢æ¥å£ + æ ‡å‡†åŒ–å­˜å‚¨

**é€‚ç”¨åœºæ™¯**: é‡åŒ–äº¤æ˜“ã€é‡‘èåˆ†æã€æŠ•èµ„ç ”ç©¶ã€é£é™©ç®¡ç†
