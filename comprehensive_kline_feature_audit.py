#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢Kçº¿åŠŸèƒ½å®¡è®¡å·¥å…·

æ·±åº¦åˆ†ææ•´ä¸ªé¡¹ç›®ä¸­çš„Kçº¿ç›¸å…³åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æŠ€æœ¯æŒ‡æ ‡æ¨¡å—
- å¯è§†åŒ–ç»„ä»¶
- æ•°æ®å¯¼å…¥å¯¼å‡º
- å›¾è¡¨æ¸²æŸ“
- å®æ—¶æ•°æ®åŠŸèƒ½
"""

import sys
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple
import re
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class KLineFeatureAuditor:
    """Kçº¿åŠŸèƒ½å®¡è®¡å™¨"""

    def __init__(self):
        self.project_root = Path(".")
        self.feature_modules = {
            'technical_indicators': [],
            'chart_rendering': [],
            'data_import_export': [],
            'ui_components': [],
            'visualization': [],
            'real_time_features': [],
            'data_validation': [],
            'analysis_tools': []
        }

    def audit_all_kline_features(self):
        """å®¡è®¡æ‰€æœ‰Kçº¿ç›¸å…³åŠŸèƒ½"""
        logger.info("=== å…¨é¢Kçº¿åŠŸèƒ½å®¡è®¡å¼€å§‹ ===")

        # 1. æ‰«ææ‰€æœ‰ç›¸å…³æ–‡ä»¶
        relevant_files = self._find_kline_related_files()
        logger.info(f"ğŸ“ å‘ç°Kçº¿ç›¸å…³æ–‡ä»¶: {len(relevant_files)} ä¸ª")

        # 2. åˆ†æå„ç±»åŠŸèƒ½æ¨¡å—
        feature_analysis = self._analyze_feature_modules(relevant_files)

        # 3. æ£€æŸ¥UIé›†æˆçŠ¶æ€
        ui_integration = self._check_ui_integration()

        # 4. ç”ŸæˆåŠŸèƒ½å®Œæ•´æ€§æŠ¥å‘Š
        self._generate_comprehensive_report(feature_analysis, ui_integration)

        return {
            'files': relevant_files,
            'features': feature_analysis,
            'ui_integration': ui_integration
        }

    def _find_kline_related_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰Kçº¿ç›¸å…³æ–‡ä»¶"""
        patterns = [
            r'.*kline.*\.py$',
            r'.*chart.*\.py$',
            r'.*indicator.*\.py$',
            r'.*technical.*\.py$',
            r'.*candlestick.*\.py$',
            r'.*ohlc.*\.py$',
            r'.*analysis.*\.py$'
        ]

        relevant_files = []

        # æœç´¢æ‰€æœ‰Pythonæ–‡ä»¶
        for py_file in self.project_root.glob("**/*.py"):
            if any(re.match(pattern, str(py_file), re.IGNORECASE) for pattern in patterns):
                relevant_files.append(py_file)
                continue

            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«Kçº¿ç›¸å…³å…³é”®å­—
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                kline_keywords = [
                    'kline', 'candlestick', 'ohlc', 'technical.*indicator',
                    'ma|ema|macd|rsi|bollinger|kdj', 'chart.*render',
                    'data.*import.*kline', 'price.*visualization'
                ]

                if any(re.search(keyword, content, re.IGNORECASE) for keyword in kline_keywords):
                    relevant_files.append(py_file)

            except Exception as e:
                continue

        return relevant_files

    def _analyze_feature_modules(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æåŠŸèƒ½æ¨¡å—"""
        feature_analysis = {
            'technical_indicators': self._analyze_technical_indicators(files),
            'chart_rendering': self._analyze_chart_rendering(files),
            'data_processing': self._analyze_data_processing(files),
            'ui_components': self._analyze_ui_components(files),
            'visualization': self._analyze_visualization(files),
            'real_time_features': self._analyze_real_time_features(files),
            'export_features': self._analyze_export_features(files),
            'advanced_analysis': self._analyze_advanced_analysis(files)
        }

        return feature_analysis

    def _analyze_technical_indicators(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯æŒ‡æ ‡åŠŸèƒ½"""
        indicators = {
            'MA': [], 'EMA': [], 'MACD': [], 'RSI': [],
            'Bollinger': [], 'KDJ': [], 'Volume': [], 'BOLL': [],
            'STOCH': [], 'Williams': [], 'CCI': [], 'ATR': []
        }

        implementations = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡
                for indicator in indicators.keys():
                    patterns = [
                        rf'def.*{indicator.lower()}.*\(',
                        rf'class.*{indicator}.*:',
                        rf'{indicator.upper()}.*=',
                        rf'calculate.*{indicator.lower()}'
                    ]

                    for pattern in patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            indicators[indicator].append({
                                'file': str(file_path),
                                'type': 'implementation'
                            })
                            break

                # æŸ¥æ‰¾æŠ€æœ¯æŒ‡æ ‡å®ç°
                impl_patterns = [
                    r'def\s+calculate_(\w+)',
                    r'class\s+(\w+Indicator)',
                    r'def\s+(\w+_indicator)'
                ]

                for pattern in impl_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        implementations.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'supported_indicators': indicators,
            'implementations': implementations,
            'total_indicators': len([ind for ind, files in indicators.items() if files]),
            'total_implementations': len(implementations)
        }

    def _analyze_chart_rendering(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æå›¾è¡¨æ¸²æŸ“åŠŸèƒ½"""
        chart_types = {
            'Candlestick': [],
            'OHLC': [],
            'Line': [],
            'Area': [],
            'Volume': []
        }

        renderers = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥å›¾è¡¨ç±»å‹
                for chart_type in chart_types.keys():
                    patterns = [
                        rf'render.*{chart_type.lower()}',
                        rf'{chart_type}.*chart',
                        rf'plot.*{chart_type.lower()}'
                    ]

                    for pattern in patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            chart_types[chart_type].append(str(file_path))
                            break

                # æŸ¥æ‰¾æ¸²æŸ“å™¨
                renderer_patterns = [
                    r'class\s+(\w+Renderer)',
                    r'def\s+render_(\w+)',
                    r'class\s+(\w+Chart)'
                ]

                for pattern in renderer_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        renderers.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'chart_types': chart_types,
            'renderers': renderers,
            'supported_charts': len([ct for ct, files in chart_types.items() if files]),
            'total_renderers': len(renderers)
        }

    def _analyze_data_processing(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®å¤„ç†åŠŸèƒ½"""
        data_formats = {
            'CSV': [], 'Excel': [], 'JSON': [], 'Parquet': [], 'HDF5': [], 'Feather': []
        }

        processors = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥æ•°æ®æ ¼å¼æ”¯æŒ
                for format_name in data_formats.keys():
                    patterns = [
                        rf'\.to_{format_name.lower()}\(',
                        rf'read_{format_name.lower()}\(',
                        rf'{format_name.lower()}.*import',
                        rf'export.*{format_name.lower()}'
                    ]

                    for pattern in patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            data_formats[format_name].append(str(file_path))
                            break

                # æŸ¥æ‰¾æ•°æ®å¤„ç†å™¨
                processor_patterns = [
                    r'class\s+(\w+Processor)',
                    r'def\s+process_(\w+)',
                    r'class\s+(\w+Importer)'
                ]

                for pattern in processor_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        processors.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'data_formats': data_formats,
            'processors': processors,
            'supported_formats': len([fmt for fmt, files in data_formats.items() if files]),
            'total_processors': len(processors)
        }

    def _analyze_ui_components(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æUIç»„ä»¶"""
        ui_components = {
            'Dialogs': [],
            'Widgets': [],
            'Charts': [],
            'Panels': [],
            'Tabs': []
        }

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥UIç»„ä»¶ç±»å‹
                if 'QDialog' in content or 'dialog' in str(file_path).lower():
                    ui_components['Dialogs'].append(str(file_path))

                if 'QWidget' in content or 'widget' in str(file_path).lower():
                    ui_components['Widgets'].append(str(file_path))

                if any(chart_word in content.lower() for chart_word in ['chart', 'plot', 'graph']):
                    ui_components['Charts'].append(str(file_path))

                if 'panel' in str(file_path).lower() or 'QFrame' in content:
                    ui_components['Panels'].append(str(file_path))

                if 'QTabWidget' in content or 'tab' in str(file_path).lower():
                    ui_components['Tabs'].append(str(file_path))

            except Exception:
                continue

        return {
            'components': ui_components,
            'total_components': sum(len(files) for files in ui_components.values())
        }

    def _analyze_visualization(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æå¯è§†åŒ–åŠŸèƒ½"""
        viz_libraries = {
            'Matplotlib': [], 'PyQt5': [], 'Plotly': [], 'Bokeh': [], 'WebGL': []
        }

        viz_features = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥å¯è§†åŒ–åº“
                for lib in viz_libraries.keys():
                    if lib.lower() in content.lower():
                        viz_libraries[lib].append(str(file_path))

                # æŸ¥æ‰¾å¯è§†åŒ–ç‰¹æ€§
                viz_patterns = [
                    r'def\s+plot_(\w+)',
                    r'def\s+draw_(\w+)',
                    r'def\s+render_(\w+)',
                    r'class\s+(\w+Visualizer)'
                ]

                for pattern in viz_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        viz_features.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'libraries': viz_libraries,
            'features': viz_features,
            'supported_libraries': len([lib for lib, files in viz_libraries.items() if files]),
            'total_features': len(viz_features)
        }

    def _analyze_real_time_features(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æå®æ—¶åŠŸèƒ½"""
        realtime_keywords = [
            'real.*time', 'live.*data', 'streaming', 'websocket',
            'update.*timer', 'auto.*refresh', 'real.*time.*update'
        ]

        realtime_files = []
        features = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                for keyword in realtime_keywords:
                    if re.search(keyword, content, re.IGNORECASE):
                        realtime_files.append(str(file_path))

                        # æå–å…·ä½“åŠŸèƒ½
                        feature_patterns = [
                            r'def\s+(\w*update\w*)',
                            r'def\s+(\w*refresh\w*)',
                            r'class\s+(\w*Timer\w*)',
                            r'QTimer'
                        ]

                        for pattern in feature_patterns:
                            matches = re.findall(pattern, content)
                            for match in matches:
                                if match:  # æ’é™¤ç©ºåŒ¹é…
                                    features.append({
                                        'name': match,
                                        'file': str(file_path)
                                    })
                        break

            except Exception:
                continue

        return {
            'realtime_files': list(set(realtime_files)),
            'features': features,
            'total_files': len(set(realtime_files)),
            'total_features': len(features)
        }

    def _analyze_export_features(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æå¯¼å‡ºåŠŸèƒ½"""
        export_formats = {
            'PDF': [], 'PNG': [], 'SVG': [], 'CSV': [], 'Excel': []
        }

        export_functions = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥å¯¼å‡ºæ ¼å¼
                for format_name in export_formats.keys():
                    patterns = [
                        rf'export.*{format_name.lower()}',
                        rf'save.*{format_name.lower()}',
                        rf'\.{format_name.lower()}\(',
                        rf'to_{format_name.lower()}'
                    ]

                    for pattern in patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            export_formats[format_name].append(str(file_path))
                            break

                # æŸ¥æ‰¾å¯¼å‡ºå‡½æ•°
                export_patterns = [
                    r'def\s+(export_\w+)',
                    r'def\s+(save_\w+)',
                    r'class\s+(\w+Exporter)'
                ]

                for pattern in export_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        export_functions.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'formats': export_formats,
            'functions': export_functions,
            'supported_formats': len([fmt for fmt, files in export_formats.items() if files]),
            'total_functions': len(export_functions)
        }

    def _analyze_advanced_analysis(self, files: List[Path]) -> Dict[str, Any]:
        """åˆ†æé«˜çº§åˆ†æåŠŸèƒ½"""
        analysis_features = {
            'Pattern Recognition': [],
            'Trend Analysis': [],
            'Support/Resistance': [],
            'Fibonacci': [],
            'Volume Analysis': []
        }

        analysis_tools = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # æ£€æŸ¥åˆ†æåŠŸèƒ½
                for feature in analysis_features.keys():
                    keywords = feature.lower().split('/')
                    if any(re.search(keyword.strip(), content, re.IGNORECASE) for keyword in keywords):
                        analysis_features[feature].append(str(file_path))

                # æŸ¥æ‰¾åˆ†æå·¥å…·
                tool_patterns = [
                    r'def\s+(analyze_\w+)',
                    r'def\s+(detect_\w+)',
                    r'class\s+(\w+Analyzer)',
                    r'class\s+(\w+Detector)'
                ]

                for pattern in tool_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        analysis_tools.append({
                            'name': match,
                            'file': str(file_path)
                        })

            except Exception:
                continue

        return {
            'features': analysis_features,
            'tools': analysis_tools,
            'supported_features': len([feat for feat, files in analysis_features.items() if files]),
            'total_tools': len(analysis_tools)
        }

    def _check_ui_integration(self) -> Dict[str, Any]:
        """æ£€æŸ¥UIé›†æˆçŠ¶æ€"""
        main_ui_files = [
            "gui/dialogs/unified_duckdb_import_dialog.py",
            "gui/widgets/data_import_dashboard.py"
        ]

        integration_status = {}

        for ui_file in main_ui_files:
            ui_path = Path(ui_file)
            if ui_path.exists():
                try:
                    with open(ui_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    integration_status[ui_file] = {
                        'has_chart_import': 'Chart' in content,
                        'has_indicator_import': any(ind in content for ind in ['MA', 'EMA', 'MACD', 'RSI']),
                        'has_visualization': any(viz in content for viz in ['plot', 'render', 'draw']),
                        'has_export_functions': any(exp in content for exp in ['export', 'save']),
                        'has_realtime_features': any(rt in content for rt in ['timer', 'update', 'refresh']),
                        'imports_count': len(re.findall(r'^from .* import', content, re.MULTILINE)),
                        'classes_count': len(re.findall(r'^class ', content, re.MULTILINE)),
                        'methods_count': len(re.findall(r'def ', content))
                    }
                except Exception as e:
                    integration_status[ui_file] = {'error': str(e)}
            else:
                integration_status[ui_file] = {'error': 'File not found'}

        return integration_status

    def _generate_comprehensive_report(self, feature_analysis: Dict[str, Any], ui_integration: Dict[str, Any]):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ Kçº¿åŠŸèƒ½å…¨é¢å®¡è®¡æŠ¥å‘Š")
        logger.info("=" * 80)

        # æŠ€æœ¯æŒ‡æ ‡åˆ†æ
        indicators = feature_analysis['technical_indicators']
        logger.info(f"\nğŸ”§ æŠ€æœ¯æŒ‡æ ‡åŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„æŒ‡æ ‡: {indicators['total_indicators']}/12")
        logger.info(f"  ğŸ”¨ å®ç°æ¨¡å—: {indicators['total_implementations']} ä¸ª")

        supported_indicators = [ind for ind, files in indicators['supported_indicators'].items() if files]
        if supported_indicators:
            logger.info(f"  âœ… å·²æ”¯æŒ: {', '.join(supported_indicators)}")

        # å›¾è¡¨æ¸²æŸ“åˆ†æ
        charts = feature_analysis['chart_rendering']
        logger.info(f"\nğŸ“ˆ å›¾è¡¨æ¸²æŸ“åŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„å›¾è¡¨ç±»å‹: {charts['supported_charts']}/5")
        logger.info(f"  ğŸ¨ æ¸²æŸ“å™¨: {charts['total_renderers']} ä¸ª")

        # æ•°æ®å¤„ç†åˆ†æ
        data_proc = feature_analysis['data_processing']
        logger.info(f"\nğŸ’¾ æ•°æ®å¤„ç†åŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„æ ¼å¼: {data_proc['supported_formats']}/6")
        logger.info(f"  å¤„ç†å™¨: {data_proc['total_processors']} ä¸ª")

        # UIç»„ä»¶åˆ†æ
        ui_comp = feature_analysis['ui_components']
        logger.info(f"\nğŸ–¥ï¸ UIç»„ä»¶:")
        logger.info(f"  ğŸ“Š æ€»ç»„ä»¶æ•°: {ui_comp['total_components']}")
        for comp_type, files in ui_comp['components'].items():
            logger.info(f"    â€¢ {comp_type}: {len(files)} ä¸ª")

        # å¯è§†åŒ–åˆ†æ
        viz = feature_analysis['visualization']
        logger.info(f"\nğŸ¨ å¯è§†åŒ–åŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„åº“: {viz['supported_libraries']}/5")
        logger.info(f"  ğŸ”§ å¯è§†åŒ–åŠŸèƒ½: {viz['total_features']} ä¸ª")

        # å®æ—¶åŠŸèƒ½åˆ†æ
        realtime = feature_analysis['real_time_features']
        logger.info(f"\nâš¡ å®æ—¶åŠŸèƒ½:")
        logger.info(f"  ğŸ“Š ç›¸å…³æ–‡ä»¶: {realtime['total_files']} ä¸ª")
        logger.info(f"  ğŸ”§ å®æ—¶åŠŸèƒ½: {realtime['total_features']} ä¸ª")

        # å¯¼å‡ºåŠŸèƒ½åˆ†æ
        export = feature_analysis['export_features']
        logger.info(f"\nğŸ“¤ å¯¼å‡ºåŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„æ ¼å¼: {export['supported_formats']}/5")
        logger.info(f"  ğŸ”§ å¯¼å‡ºåŠŸèƒ½: {export['total_functions']} ä¸ª")

        # é«˜çº§åˆ†æåŠŸèƒ½
        advanced = feature_analysis['advanced_analysis']
        logger.info(f"\nğŸ§  é«˜çº§åˆ†æåŠŸèƒ½:")
        logger.info(f"  ğŸ“Š æ”¯æŒçš„åŠŸèƒ½: {advanced['supported_features']}/5")
        logger.info(f"  ğŸ”§ åˆ†æå·¥å…·: {advanced['total_tools']} ä¸ª")

        # UIé›†æˆçŠ¶æ€
        logger.info(f"\nğŸ”— UIé›†æˆçŠ¶æ€:")
        for ui_file, status in ui_integration.items():
            logger.info(f"  ğŸ“„ {ui_file}:")
            if 'error' in status:
                logger.error(f"    âŒ é”™è¯¯: {status['error']}")
            else:
                for feature, has_feature in status.items():
                    if isinstance(has_feature, bool):
                        status_icon = "âœ…" if has_feature else "âŒ"
                        logger.info(f"    {status_icon} {feature}: {has_feature}")
                    elif isinstance(has_feature, int):
                        logger.info(f"    ğŸ“Š {feature}: {has_feature}")

        # æ€»ä½“è¯„åˆ†
        self._calculate_overall_score(feature_analysis, ui_integration)

    def _calculate_overall_score(self, feature_analysis: Dict[str, Any], ui_integration: Dict[str, Any]):
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        logger.info(f"\nğŸ¯ æ€»ä½“åŠŸèƒ½è¯„åˆ†:")

        # è®¡ç®—å„æ¨¡å—å¾—åˆ†
        scores = {}

        # æŠ€æœ¯æŒ‡æ ‡å¾—åˆ† (æ»¡åˆ†25åˆ†)
        indicators = feature_analysis['technical_indicators']
        scores['indicators'] = (indicators['total_indicators'] / 12) * 25

        # å›¾è¡¨æ¸²æŸ“å¾—åˆ† (æ»¡åˆ†20åˆ†)
        charts = feature_analysis['chart_rendering']
        scores['charts'] = (charts['supported_charts'] / 5) * 20

        # æ•°æ®å¤„ç†å¾—åˆ† (æ»¡åˆ†15åˆ†)
        data_proc = feature_analysis['data_processing']
        scores['data_processing'] = (data_proc['supported_formats'] / 6) * 15

        # å¯è§†åŒ–å¾—åˆ† (æ»¡åˆ†15åˆ†)
        viz = feature_analysis['visualization']
        scores['visualization'] = (viz['supported_libraries'] / 5) * 15

        # å®æ—¶åŠŸèƒ½å¾—åˆ† (æ»¡åˆ†10åˆ†)
        realtime = feature_analysis['real_time_features']
        scores['realtime'] = min((realtime['total_files'] / 5) * 10, 10)

        # å¯¼å‡ºåŠŸèƒ½å¾—åˆ† (æ»¡åˆ†10åˆ†)
        export = feature_analysis['export_features']
        scores['export'] = (export['supported_formats'] / 5) * 10

        # é«˜çº§åˆ†æå¾—åˆ† (æ»¡åˆ†5åˆ†)
        advanced = feature_analysis['advanced_analysis']
        scores['advanced'] = (advanced['supported_features'] / 5) * 5

        # æ˜¾ç¤ºå¾—åˆ†
        total_score = sum(scores.values())
        for module, score in scores.items():
            logger.info(f"  ğŸ“Š {module}: {score:.1f}åˆ†")

        logger.info(f"\nğŸ† æ€»åˆ†: {total_score:.1f}/100")

        if total_score >= 80:
            logger.info("âœ… åŠŸèƒ½å®Œå–„åº¦: ä¼˜ç§€")
        elif total_score >= 60:
            logger.info("ğŸ“ˆ åŠŸèƒ½å®Œå–„åº¦: è‰¯å¥½")
        elif total_score >= 40:
            logger.info("âš ï¸ åŠŸèƒ½å®Œå–„åº¦: ä¸­ç­‰ï¼Œéœ€è¦æ”¹è¿›")
        else:
            logger.warning("ğŸš¨ åŠŸèƒ½å®Œå–„åº¦: è¾ƒä½ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("Kçº¿åŠŸèƒ½å…¨é¢å®¡è®¡å·¥å…·å¯åŠ¨")

    auditor = KLineFeatureAuditor()
    results = auditor.audit_all_kline_features()

    logger.info("\nâœ… å®¡è®¡å®Œæˆï¼")


if __name__ == "__main__":
    main()
