#!/usr/bin/env python3
"""
å¹¶å‘å®‰å…¨æ€§å’Œèµ„æºç®¡ç†åˆ†æå™¨
========================

åˆ†æå›æµ‹ç³»ç»Ÿçš„å¹¶å‘å®‰å…¨æ€§ã€çº¿ç¨‹å®‰å…¨ã€å†…å­˜ç®¡ç†å’Œèµ„æºæ³„æ¼é—®é¢˜
"""

import sys
import ast
import re
import logging
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ConcurrencyIssue:
    """å¹¶å‘é—®é¢˜"""
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW
    category: str
    method_name: str
    file_path: str
    line_number: int
    issue_type: str
    description: str
    code_snippet: str
    recommendation: str


@dataclass
class ResourceIssue:
    """èµ„æºé—®é¢˜"""
    severity: str
    category: str
    method_name: str
    file_path: str
    line_number: int
    resource_type: str
    description: str
    recommendation: str


class ConcurrencyResourceAnalyzer:
    """å¹¶å‘å’Œèµ„æºåˆ†æå™¨"""

    def __init__(self):
        self.concurrency_issues = []
        self.resource_issues = []
        self.thread_safety_analysis = {}
        self.resource_usage_analysis = {}

    def analyze_concurrency_and_resources(self) -> Dict[str, Any]:
        """åˆ†æå¹¶å‘å®‰å…¨æ€§å’Œèµ„æºç®¡ç†"""
        logger.info("ğŸ”’ å¼€å§‹åˆ†æå¹¶å‘å®‰å…¨æ€§å’Œèµ„æºç®¡ç†...")

        results = {
            'thread_safety_analysis': {},
            'resource_management_analysis': {},
            'concurrency_issues': [],
            'resource_leaks': [],
            'memory_management_issues': [],
            'performance_impact': {},
            'safety_recommendations': []
        }

        try:
            # 1. åˆ†æçº¿ç¨‹å®‰å…¨æ€§
            logger.info("ğŸ§µ åˆ†æçº¿ç¨‹å®‰å…¨æ€§...")
            results['thread_safety_analysis'] = self._analyze_thread_safety()

            # 2. åˆ†æèµ„æºç®¡ç†
            logger.info("ğŸ’¾ åˆ†æèµ„æºç®¡ç†...")
            results['resource_management_analysis'] = self._analyze_resource_management()

            # 3. æ£€æµ‹å¹¶å‘é—®é¢˜
            logger.info("âš ï¸ æ£€æµ‹å¹¶å‘é—®é¢˜...")
            results['concurrency_issues'] = self._detect_concurrency_issues()

            # 4. æ£€æµ‹èµ„æºæ³„æ¼
            logger.info("ğŸ” æ£€æµ‹èµ„æºæ³„æ¼...")
            results['resource_leaks'] = self._detect_resource_leaks()

            # 5. åˆ†æå†…å­˜ç®¡ç†
            logger.info("ğŸ§  åˆ†æå†…å­˜ç®¡ç†...")
            results['memory_management_issues'] = self._analyze_memory_management()

            # 6. è¯„ä¼°æ€§èƒ½å½±å“
            logger.info("ğŸ“ˆ è¯„ä¼°æ€§èƒ½å½±å“...")
            results['performance_impact'] = self._assess_performance_impact()

            # 7. ç”Ÿæˆå®‰å…¨å»ºè®®
            logger.info("ğŸ’¡ ç”Ÿæˆå®‰å…¨å»ºè®®...")
            results['safety_recommendations'] = self._generate_safety_recommendations()

            logger.info("âœ… å¹¶å‘å’Œèµ„æºåˆ†æå®Œæˆ")
            return results

        except Exception as e:
            logger.error(f"å¹¶å‘å’Œèµ„æºåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return results

    def _analyze_thread_safety(self) -> Dict[str, Any]:
        """åˆ†æçº¿ç¨‹å®‰å…¨æ€§"""
        thread_safety = {
            'shared_variables': [],
            'thread_unsafe_operations': [],
            'synchronization_mechanisms': [],
            'race_condition_risks': [],
            'thread_safety_score': 0
        }

        # åˆ†æä¸»è¦æ–‡ä»¶
        target_files = [
            'backtest/unified_backtest_engine.py',
            'backtest/real_time_backtest_monitor.py',
            'core/metrics/repository.py',
            'gui/widgets/backtest_widget.py'
        ]

        for file_path in target_files:
            full_path = project_root / file_path
            if full_path.exists():
                file_analysis = self._analyze_file_thread_safety(full_path)
                thread_safety['shared_variables'].extend(file_analysis['shared_vars'])
                thread_safety['thread_unsafe_operations'].extend(file_analysis['unsafe_ops'])
                thread_safety['synchronization_mechanisms'].extend(file_analysis['sync_mechanisms'])
                thread_safety['race_condition_risks'].extend(file_analysis['race_risks'])

        # è®¡ç®—çº¿ç¨‹å®‰å…¨è¯„åˆ†
        total_issues = (len(thread_safety['thread_unsafe_operations']) +
                        len(thread_safety['race_condition_risks']))
        sync_mechanisms = len(thread_safety['synchronization_mechanisms'])

        if total_issues == 0:
            thread_safety['thread_safety_score'] = 100
        else:
            # åŸºç¡€åˆ†æ•°å‡å»é—®é¢˜åˆ†æ•°ï¼ŒåŠ ä¸ŠåŒæ­¥æœºåˆ¶åˆ†æ•°
            thread_safety['thread_safety_score'] = max(0, 100 - (total_issues * 10) + (sync_mechanisms * 5))

        return thread_safety

    def _analyze_file_thread_safety(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶çš„çº¿ç¨‹å®‰å…¨æ€§"""
        analysis = {
            'shared_vars': [],
            'unsafe_ops': [],
            'sync_mechanisms': [],
            'race_risks': []
        }

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.splitlines()

            # æ£€æŸ¥å…±äº«å˜é‡
            class_vars = []
            in_class = False
            current_class = ""

            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()

                # æ£€æµ‹ç±»å®šä¹‰
                if line_stripped.startswith('class '):
                    in_class = True
                    current_class = line_stripped.split()[1].split('(')[0]
                elif line_stripped.startswith('def ') and in_class:
                    in_class = False

                # æ£€æµ‹å…±äº«å˜é‡
                if in_class and '=' in line_stripped and not line_stripped.startswith('def'):
                    if not line_stripped.startswith('#'):
                        var_name = line_stripped.split('=')[0].strip()
                        if not var_name.startswith('_'):  # å…¬å…±å˜é‡
                            analysis['shared_vars'].append({
                                'variable': var_name,
                                'class': current_class,
                                'file': str(file_path),
                                'line': i
                            })

                # æ£€æµ‹çº¿ç¨‹ä¸å®‰å…¨æ“ä½œ
                unsafe_patterns = [
                    'append(', 'extend(', 'pop(', 'remove(',  # åˆ—è¡¨æ“ä½œ
                    '+=', '-=', '*=', '/=',  # åŸåœ°æ“ä½œ
                    'dict[', 'dict.update(',  # å­—å…¸æ“ä½œ
                    'global ', 'nonlocal '  # å…¨å±€å˜é‡
                ]

                for pattern in unsafe_patterns:
                    if pattern in line_stripped:
                        analysis['unsafe_ops'].append({
                            'operation': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹åŒæ­¥æœºåˆ¶
                sync_patterns = [
                    'threading.Lock()', 'threading.RLock()',
                    'threading.Semaphore()', 'threading.Event()',
                    'with.*lock:', 'acquire()', 'release()',
                    'queue.Queue()', 'multiprocessing.'
                ]

                for pattern in sync_patterns:
                    if re.search(pattern, line_stripped, re.IGNORECASE):
                        analysis['sync_mechanisms'].append({
                            'mechanism': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹ç«æ€æ¡ä»¶é£é™©
                race_patterns = [
                    r'if.*len\(.*\).*:',  # æ£€æŸ¥é•¿åº¦åæ“ä½œ
                    r'if.*in.*:',  # æ£€æŸ¥å­˜åœ¨åæ“ä½œ
                    r'if.*is None:',  # æ£€æŸ¥Noneåæ“ä½œ
                ]

                for pattern in race_patterns:
                    if re.search(pattern, line_stripped):
                        # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰ä¿®æ”¹æ“ä½œ
                        if i < len(lines):
                            next_line = lines[i].strip()
                            if any(op in next_line for op in ['=', 'append', 'pop', 'del ']):
                                analysis['race_risks'].append({
                                    'risk_type': 'check_then_act',
                                    'file': str(file_path),
                                    'line': i,
                                    'code': f"{line_stripped} -> {next_line}"
                                })

        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶çº¿ç¨‹å®‰å…¨æ€§å¤±è´¥ {file_path}: {e}")

        return analysis

    def _analyze_resource_management(self) -> Dict[str, Any]:
        """åˆ†æèµ„æºç®¡ç†"""
        resource_analysis = {
            'file_operations': [],
            'database_connections': [],
            'memory_allocations': [],
            'network_connections': [],
            'resource_cleanup': [],
            'context_managers': [],
            'resource_management_score': 0
        }

        # åˆ†æèµ„æºç®¡ç†ç›¸å…³æ–‡ä»¶
        target_files = [
            'backtest/unified_backtest_engine.py',
            'core/metrics/repository.py',
            'core/database/factorweave_analytics_db.py',
            'gui/widgets/backtest_widget.py'
        ]

        for file_path in target_files:
            full_path = project_root / file_path
            if full_path.exists():
                file_analysis = self._analyze_file_resources(full_path)
                resource_analysis['file_operations'].extend(file_analysis['file_ops'])
                resource_analysis['database_connections'].extend(file_analysis['db_connections'])
                resource_analysis['memory_allocations'].extend(file_analysis['memory_allocs'])
                resource_analysis['resource_cleanup'].extend(file_analysis['cleanup'])
                resource_analysis['context_managers'].extend(file_analysis['context_mgrs'])

        # è®¡ç®—èµ„æºç®¡ç†è¯„åˆ†
        total_resources = (len(resource_analysis['file_operations']) +
                           len(resource_analysis['database_connections']) +
                           len(resource_analysis['memory_allocations']))

        cleanup_mechanisms = (len(resource_analysis['resource_cleanup']) +
                              len(resource_analysis['context_managers']))

        if total_resources == 0:
            resource_analysis['resource_management_score'] = 100
        else:
            # æ¸…ç†æœºåˆ¶è¦†ç›–ç‡
            coverage = min(100, (cleanup_mechanisms / total_resources) * 100)
            resource_analysis['resource_management_score'] = int(coverage)

        return resource_analysis

    def _analyze_file_resources(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶çš„èµ„æºä½¿ç”¨"""
        analysis = {
            'file_ops': [],
            'db_connections': [],
            'memory_allocs': [],
            'cleanup': [],
            'context_mgrs': []
        }

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.splitlines()

            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()

                # æ£€æµ‹æ–‡ä»¶æ“ä½œ
                file_patterns = ['open(', 'file(', 'codecs.open(', 'io.open(']
                for pattern in file_patterns:
                    if pattern in line_stripped:
                        analysis['file_ops'].append({
                            'operation': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹æ•°æ®åº“è¿æ¥
                db_patterns = ['connect(', 'sqlite3.', 'duckdb.', 'cursor()', 'execute(']
                for pattern in db_patterns:
                    if pattern in line_stripped:
                        analysis['db_connections'].append({
                            'operation': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹å¤§å†…å­˜åˆ†é…
                memory_patterns = ['pandas.', 'numpy.', 'DataFrame(', 'array(', 'zeros(', 'ones(']
                for pattern in memory_patterns:
                    if pattern in line_stripped:
                        analysis['memory_allocs'].append({
                            'operation': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹èµ„æºæ¸…ç†
                cleanup_patterns = ['close()', 'commit()', 'rollback()', 'finally:', 'del ', '__del__']
                for pattern in cleanup_patterns:
                    if pattern in line_stripped:
                        analysis['cleanup'].append({
                            'cleanup_type': pattern,
                            'file': str(file_path),
                            'line': i,
                            'code': line_stripped
                        })

                # æ£€æµ‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                if 'with ' in line_stripped and ':' in line_stripped:
                    analysis['context_mgrs'].append({
                        'context_manager': 'with statement',
                        'file': str(file_path),
                        'line': i,
                        'code': line_stripped
                    })

        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶èµ„æºä½¿ç”¨å¤±è´¥ {file_path}: {e}")

        return analysis

    def _detect_concurrency_issues(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¹¶å‘é—®é¢˜"""
        issues = []

        # åŸºäºçº¿ç¨‹å®‰å…¨åˆ†æç»“æœæ£€æµ‹é—®é¢˜
        thread_analysis = self.thread_safety_analysis

        # æ£€æµ‹å…±äº«å˜é‡æ— ä¿æŠ¤è®¿é—®
        for var in thread_analysis.get('shared_variables', []):
            issues.append({
                'severity': 'HIGH',
                'category': 'çº¿ç¨‹å®‰å…¨',
                'issue_type': 'unprotected_shared_variable',
                'description': f"å…±äº«å˜é‡ {var['variable']} å¯èƒ½å­˜åœ¨å¹¶å‘è®¿é—®é£é™©",
                'file': var['file'],
                'line': var['line'],
                'recommendation': 'ä½¿ç”¨é”æˆ–å…¶ä»–åŒæ­¥æœºåˆ¶ä¿æŠ¤å…±äº«å˜é‡è®¿é—®'
            })

        # æ£€æµ‹çº¿ç¨‹ä¸å®‰å…¨æ“ä½œ
        for op in thread_analysis.get('thread_unsafe_operations', []):
            issues.append({
                'severity': 'MEDIUM',
                'category': 'çº¿ç¨‹å®‰å…¨',
                'issue_type': 'thread_unsafe_operation',
                'description': f"çº¿ç¨‹ä¸å®‰å…¨æ“ä½œ: {op['operation']}",
                'file': op['file'],
                'line': op['line'],
                'code': op['code'],
                'recommendation': 'åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆ'
            })

        # æ£€æµ‹ç«æ€æ¡ä»¶
        for risk in thread_analysis.get('race_condition_risks', []):
            issues.append({
                'severity': 'HIGH',
                'category': 'ç«æ€æ¡ä»¶',
                'issue_type': 'race_condition',
                'description': f"æ½œåœ¨ç«æ€æ¡ä»¶: {risk['risk_type']}",
                'file': risk['file'],
                'line': risk['line'],
                'code': risk['code'],
                'recommendation': 'ä½¿ç”¨åŸå­æ“ä½œæˆ–é€‚å½“çš„åŒæ­¥æœºåˆ¶'
            })

        return issues

    def _detect_resource_leaks(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹èµ„æºæ³„æ¼"""
        leaks = []

        resource_analysis = self.resource_usage_analysis

        # æ£€æµ‹æ–‡ä»¶æœªå…³é—­
        file_ops = resource_analysis.get('file_operations', [])
        cleanup_ops = resource_analysis.get('resource_cleanup', [])

        file_lines = {op['line'] for op in file_ops}
        close_lines = {op['line'] for op in cleanup_ops if 'close' in op.get('cleanup_type', '')}

        for file_op in file_ops:
            # ç®€å•æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶æ“ä½œé™„è¿‘æ²¡æœ‰closeæ“ä½œ
            nearby_closes = any(abs(file_op['line'] - close_line) < 20 for close_line in close_lines)
            if not nearby_closes:
                leaks.append({
                    'severity': 'MEDIUM',
                    'category': 'èµ„æºæ³„æ¼',
                    'resource_type': 'file',
                    'description': 'æ–‡ä»¶å¯èƒ½æœªæ­£ç¡®å…³é—­',
                    'file': file_op['file'],
                    'line': file_op['line'],
                    'recommendation': 'ä½¿ç”¨ with è¯­å¥æˆ–ç¡®ä¿åœ¨ finally å—ä¸­å…³é—­æ–‡ä»¶'
                })

        # æ£€æµ‹æ•°æ®åº“è¿æ¥æœªå…³é—­
        db_connections = resource_analysis.get('database_connections', [])
        for db_op in db_connections:
            if 'connect' in db_op['operation']:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å…³é—­æ“ä½œ
                nearby_closes = any(abs(db_op['line'] - close_line) < 30 for close_line in close_lines)
                if not nearby_closes:
                    leaks.append({
                        'severity': 'HIGH',
                        'category': 'èµ„æºæ³„æ¼',
                        'resource_type': 'database_connection',
                        'description': 'æ•°æ®åº“è¿æ¥å¯èƒ½æœªæ­£ç¡®å…³é—­',
                        'file': db_op['file'],
                        'line': db_op['line'],
                        'recommendation': 'ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æˆ–ç¡®ä¿è¿æ¥åœ¨ä½¿ç”¨åå…³é—­'
                    })

        return leaks

    def _analyze_memory_management(self) -> List[Dict[str, Any]]:
        """åˆ†æå†…å­˜ç®¡ç†é—®é¢˜"""
        memory_issues = []

        # åˆ†æå¤§å¯¹è±¡åˆ›å»ºå’Œå†…å­˜ä½¿ç”¨
        target_files = [
            'backtest/unified_backtest_engine.py',
            'core/metrics/repository.py'
        ]

        for file_path in target_files:
            full_path = project_root / file_path
            if full_path.exists():
                file_issues = self._analyze_file_memory(full_path)
                memory_issues.extend(file_issues)

        return memory_issues

    def _analyze_file_memory(self, file_path: Path) -> List[Dict[str, Any]]:
        """åˆ†ææ–‡ä»¶çš„å†…å­˜ä½¿ç”¨"""
        issues = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.splitlines()

            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()

                # æ£€æµ‹å¤§æ•°æ®ç»“æ„åˆ›å»º
                large_data_patterns = [
                    'pd.DataFrame(',
                    'np.array(',
                    'np.zeros(',
                    'np.ones(',
                    'list(' + '.*' + 'range(',
                    'dict(' + '.*' + 'zip('
                ]

                for pattern in large_data_patterns:
                    if re.search(pattern, line_stripped):
                        issues.append({
                            'severity': 'MEDIUM',
                            'category': 'å†…å­˜ç®¡ç†',
                            'issue_type': 'large_object_creation',
                            'description': f'åˆ›å»ºå¤§æ•°æ®ç»“æ„: {pattern}',
                            'file': str(file_path),
                            'line': i,
                            'recommendation': 'è€ƒè™‘ä½¿ç”¨ç”Ÿæˆå™¨æˆ–åˆ†æ‰¹å¤„ç†å¤§æ•°æ®'
                        })

                # æ£€æµ‹å†…å­˜æ³„æ¼é£é™©
                if 'global ' in line_stripped or 'self.' in line_stripped:
                    if any(keyword in line_stripped for keyword in ['list', 'dict', 'DataFrame']):
                        issues.append({
                            'severity': 'LOW',
                            'category': 'å†…å­˜ç®¡ç†',
                            'issue_type': 'potential_memory_leak',
                            'description': 'å…¨å±€æˆ–å®ä¾‹å˜é‡å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼',
                            'file': str(file_path),
                            'line': i,
                            'recommendation': 'å®šæœŸæ¸…ç†ä¸éœ€è¦çš„æ•°æ®ï¼Œä½¿ç”¨å¼±å¼•ç”¨'
                        })

        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶å†…å­˜ä½¿ç”¨å¤±è´¥ {file_path}: {e}")

        return issues

    def _assess_performance_impact(self) -> Dict[str, Any]:
        """è¯„ä¼°æ€§èƒ½å½±å“"""
        impact = {
            'thread_safety_impact': 'LOW',
            'resource_management_impact': 'MEDIUM',
            'memory_impact': 'MEDIUM',
            'overall_impact': 'MEDIUM',
            'bottleneck_areas': []
        }

        # åŸºäºå‘ç°çš„é—®é¢˜è¯„ä¼°å½±å“
        concurrency_issues = len(self.concurrency_issues)
        resource_leaks = len(self.resource_issues)

        if concurrency_issues > 10:
            impact['thread_safety_impact'] = 'HIGH'
        elif concurrency_issues > 5:
            impact['thread_safety_impact'] = 'MEDIUM'

        if resource_leaks > 5:
            impact['resource_management_impact'] = 'HIGH'
        elif resource_leaks > 2:
            impact['resource_management_impact'] = 'MEDIUM'

        # è¯†åˆ«ç“¶é¢ˆåŒºåŸŸ
        if concurrency_issues > 0:
            impact['bottleneck_areas'].append('å¹¶å‘æ§åˆ¶')

        if resource_leaks > 0:
            impact['bottleneck_areas'].append('èµ„æºç®¡ç†')

        # è®¡ç®—æ€»ä½“å½±å“
        high_impacts = sum(1 for imp in [impact['thread_safety_impact'],
                                         impact['resource_management_impact'],
                                         impact['memory_impact']] if imp == 'HIGH')

        if high_impacts >= 2:
            impact['overall_impact'] = 'HIGH'
        elif high_impacts >= 1:
            impact['overall_impact'] = 'MEDIUM'
        else:
            impact['overall_impact'] = 'LOW'

        return impact

    def _generate_safety_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        recommendations = [
            {
                'category': 'çº¿ç¨‹å®‰å…¨',
                'priority': 'HIGH',
                'title': 'å®ç°çº¿ç¨‹å®‰å…¨æœºåˆ¶',
                'description': 'ä¸ºå…±äº«èµ„æºæ·»åŠ é€‚å½“çš„åŒæ­¥æœºåˆ¶',
                'implementation': [
                    'è¯†åˆ«æ‰€æœ‰å…±äº«å˜é‡å’Œèµ„æº',
                    'ä½¿ç”¨ threading.Lock() ä¿æŠ¤å…³é”®åŒºåŸŸ',
                    'è€ƒè™‘ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ•°æ®ç»“æ„',
                    'å®ç°åŸå­æ“ä½œé¿å…ç«æ€æ¡ä»¶'
                ],
                'benefits': ['é¿å…æ•°æ®ç«äº‰', 'ç¡®ä¿æ•°æ®ä¸€è‡´æ€§', 'æé«˜ç³»ç»Ÿç¨³å®šæ€§']
            },
            {
                'category': 'èµ„æºç®¡ç†',
                'priority': 'HIGH',
                'title': 'å®Œå–„èµ„æºæ¸…ç†æœºåˆ¶',
                'description': 'ç¡®ä¿æ‰€æœ‰èµ„æºéƒ½èƒ½æ­£ç¡®é‡Šæ”¾',
                'implementation': [
                    'ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (with è¯­å¥)',
                    'åœ¨ finally å—ä¸­é‡Šæ”¾èµ„æº',
                    'å®ç° __del__ æ–¹æ³•ä½œä¸ºå¤‡ç”¨æ¸…ç†',
                    'å®šæœŸæ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ'
                ],
                'benefits': ['é˜²æ­¢èµ„æºæ³„æ¼', 'æé«˜ç³»ç»Ÿæ€§èƒ½', 'é¿å…ç³»ç»Ÿå´©æºƒ']
            },
            {
                'category': 'å†…å­˜ç®¡ç†',
                'priority': 'MEDIUM',
                'title': 'ä¼˜åŒ–å†…å­˜ä½¿ç”¨',
                'description': 'å‡å°‘å†…å­˜å ç”¨å’Œé¿å…å†…å­˜æ³„æ¼',
                'implementation': [
                    'ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ•°æ®é›†',
                    'åŠæ—¶æ¸…ç†ä¸éœ€è¦çš„å˜é‡',
                    'ä½¿ç”¨å¼±å¼•ç”¨é¿å…å¾ªç¯å¼•ç”¨',
                    'ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ'
                ],
                'benefits': ['é™ä½å†…å­˜å ç”¨', 'æé«˜è¿è¡Œæ•ˆç‡', 'é¿å…å†…å­˜æº¢å‡º']
            },
            {
                'category': 'å¹¶å‘æ§åˆ¶',
                'priority': 'MEDIUM',
                'title': 'æ”¹è¿›å¹¶å‘è®¾è®¡',
                'description': 'ä¼˜åŒ–å¤šçº¿ç¨‹å’Œå¼‚æ­¥å¤„ç†',
                'implementation': [
                    'ä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†çº¿ç¨‹',
                    'å®ç°å¼‚æ­¥I/Oæ“ä½œ',
                    'é¿å…è¿‡åº¦å¹¶å‘',
                    'ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡'
                ],
                'benefits': ['æé«˜å¹¶å‘æ€§èƒ½', 'å‡å°‘èµ„æºç«äº‰', 'æé«˜å“åº”é€Ÿåº¦']
            }
        ]

        return recommendations

    def generate_concurrency_resource_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¹¶å‘å’Œèµ„æºåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# å¹¶å‘å®‰å…¨æ€§å’Œèµ„æºç®¡ç†åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)

        # æ‰§è¡Œæ‘˜è¦
        report.append(f"\n## ğŸ“Š æ‰§è¡Œæ‘˜è¦")

        thread_safety = results.get('thread_safety_analysis', {})
        resource_mgmt = results.get('resource_management_analysis', {})
        concurrency_issues = results.get('concurrency_issues', [])
        resource_leaks = results.get('resource_leaks', [])
        memory_issues = results.get('memory_management_issues', [])
        performance_impact = results.get('performance_impact', {})

        report.append(f"- çº¿ç¨‹å®‰å…¨è¯„åˆ†: {thread_safety.get('thread_safety_score', 0)}/100")
        report.append(f"- èµ„æºç®¡ç†è¯„åˆ†: {resource_mgmt.get('resource_management_score', 0)}/100")
        report.append(f"- å¹¶å‘é—®é¢˜æ•°é‡: {len(concurrency_issues)}")
        report.append(f"- èµ„æºæ³„æ¼é£é™©: {len(resource_leaks)}")
        report.append(f"- å†…å­˜ç®¡ç†é—®é¢˜: {len(memory_issues)}")
        report.append(f"- æ€»ä½“æ€§èƒ½å½±å“: {performance_impact.get('overall_impact', 'UNKNOWN')}")

        # çº¿ç¨‹å®‰å…¨åˆ†æ
        if thread_safety:
            report.append(f"\n## ğŸ§µ çº¿ç¨‹å®‰å…¨åˆ†æ")

            shared_vars = thread_safety.get('shared_variables', [])
            if shared_vars:
                report.append(f"\n### å…±äº«å˜é‡ ({len(shared_vars)} ä¸ª)")
                for i, var in enumerate(shared_vars[:10], 1):
                    report.append(f"{i}. **{var['variable']}** (ç±»: {var['class']})")
                    report.append(f"   - æ–‡ä»¶: {Path(var['file']).name}:{var['line']}")

            unsafe_ops = thread_safety.get('thread_unsafe_operations', [])
            if unsafe_ops:
                report.append(f"\n### çº¿ç¨‹ä¸å®‰å…¨æ“ä½œ ({len(unsafe_ops)} ä¸ª)")
                for i, op in enumerate(unsafe_ops[:10], 1):
                    report.append(f"{i}. **{op['operation']}** ({Path(op['file']).name}:{op['line']})")
                    report.append(f"   - ä»£ç : `{op['code']}`")

            sync_mechanisms = thread_safety.get('synchronization_mechanisms', [])
            if sync_mechanisms:
                report.append(f"\n### åŒæ­¥æœºåˆ¶ ({len(sync_mechanisms)} ä¸ª)")
                for i, sync in enumerate(sync_mechanisms[:5], 1):
                    report.append(f"{i}. **{sync['mechanism']}** ({Path(sync['file']).name}:{sync['line']})")

        # èµ„æºç®¡ç†åˆ†æ
        if resource_mgmt:
            report.append(f"\n## ğŸ’¾ èµ„æºç®¡ç†åˆ†æ")

            file_ops = resource_mgmt.get('file_operations', [])
            if file_ops:
                report.append(f"\n### æ–‡ä»¶æ“ä½œ ({len(file_ops)} ä¸ª)")
                for i, op in enumerate(file_ops[:5], 1):
                    report.append(f"{i}. **{op['operation']}** ({Path(op['file']).name}:{op['line']})")

            db_connections = resource_mgmt.get('database_connections', [])
            if db_connections:
                report.append(f"\n### æ•°æ®åº“è¿æ¥ ({len(db_connections)} ä¸ª)")
                for i, conn in enumerate(db_connections[:5], 1):
                    report.append(f"{i}. **{conn['operation']}** ({Path(conn['file']).name}:{conn['line']})")

            context_mgrs = resource_mgmt.get('context_managers', [])
            if context_mgrs:
                report.append(f"\n### ä¸Šä¸‹æ–‡ç®¡ç†å™¨ ({len(context_mgrs)} ä¸ª)")
                report.append(f"âœ… å‘ç° {len(context_mgrs)} ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæœ‰åŠ©äºèµ„æºç®¡ç†")

        # å¹¶å‘é—®é¢˜
        if concurrency_issues:
            report.append(f"\n## âš ï¸ å¹¶å‘é—®é¢˜")

            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            high_issues = [issue for issue in concurrency_issues if issue.get('severity') == 'HIGH']
            medium_issues = [issue for issue in concurrency_issues if issue.get('severity') == 'MEDIUM']

            if high_issues:
                report.append(f"\n### ğŸ”´ é«˜ä¸¥é‡æ€§é—®é¢˜")
                for i, issue in enumerate(high_issues[:10], 1):
                    report.append(f"{i}. **{issue['issue_type']}** ({Path(issue['file']).name}:{issue['line']})")
                    report.append(f"   - æè¿°: {issue['description']}")
                    report.append(f"   - å»ºè®®: {issue['recommendation']}")

            if medium_issues:
                report.append(f"\n### ğŸŸ¡ ä¸­ä¸¥é‡æ€§é—®é¢˜")
                for i, issue in enumerate(medium_issues[:5], 1):
                    report.append(f"{i}. **{issue['issue_type']}** ({Path(issue['file']).name}:{issue['line']})")
                    report.append(f"   - æè¿°: {issue['description']}")

        # èµ„æºæ³„æ¼
        if resource_leaks:
            report.append(f"\n## ğŸ” èµ„æºæ³„æ¼é£é™©")

            for i, leak in enumerate(resource_leaks[:10], 1):
                report.append(f"{i}. **{leak['resource_type']}** ({Path(leak['file']).name}:{leak['line']})")
                report.append(f"   - æè¿°: {leak['description']}")
                report.append(f"   - å»ºè®®: {leak['recommendation']}")

        # å†…å­˜ç®¡ç†é—®é¢˜
        if memory_issues:
            report.append(f"\n## ğŸ§  å†…å­˜ç®¡ç†é—®é¢˜")

            for i, issue in enumerate(memory_issues[:10], 1):
                report.append(f"{i}. **{issue['issue_type']}** ({Path(issue['file']).name}:{issue['line']})")
                report.append(f"   - æè¿°: {issue['description']}")
                report.append(f"   - å»ºè®®: {issue['recommendation']}")

        # æ€§èƒ½å½±å“è¯„ä¼°
        if performance_impact:
            report.append(f"\n## ğŸ“ˆ æ€§èƒ½å½±å“è¯„ä¼°")
            report.append(f"- çº¿ç¨‹å®‰å…¨å½±å“: {performance_impact.get('thread_safety_impact', 'UNKNOWN')}")
            report.append(f"- èµ„æºç®¡ç†å½±å“: {performance_impact.get('resource_management_impact', 'UNKNOWN')}")
            report.append(f"- å†…å­˜å½±å“: {performance_impact.get('memory_impact', 'UNKNOWN')}")
            report.append(f"- æ€»ä½“å½±å“: {performance_impact.get('overall_impact', 'UNKNOWN')}")

            bottlenecks = performance_impact.get('bottleneck_areas', [])
            if bottlenecks:
                report.append(f"\n### ç“¶é¢ˆåŒºåŸŸ")
                for bottleneck in bottlenecks:
                    report.append(f"- {bottleneck}")

        # å®‰å…¨å»ºè®®
        safety_recommendations = results.get('safety_recommendations', [])
        if safety_recommendations:
            report.append(f"\n## ğŸ’¡ å®‰å…¨å»ºè®®")

            for rec in safety_recommendations:
                report.append(f"\n### {rec['priority']} - {rec['title']}")
                report.append(f"**ç±»åˆ«**: {rec['category']}")
                report.append(f"**æè¿°**: {rec['description']}")

                if rec.get('implementation'):
                    report.append("**å®æ–½æ­¥éª¤**:")
                    for step in rec['implementation']:
                        report.append(f"- {step}")

                if rec.get('benefits'):
                    report.append("**é¢„æœŸæ”¶ç›Š**:")
                    for benefit in rec['benefits']:
                        report.append(f"- {benefit}")

        return "\n".join(report)

    def run_analysis_and_save_report(self):
        """è¿è¡Œåˆ†æå¹¶ä¿å­˜æŠ¥å‘Š"""
        try:
            # è¿è¡Œå¹¶å‘å’Œèµ„æºåˆ†æ
            results = self.analyze_concurrency_and_resources()

            # ä¿å­˜åˆ†æç»“æœåˆ°å®ä¾‹å˜é‡
            self.thread_safety_analysis = results.get('thread_safety_analysis', {})
            self.resource_usage_analysis = results.get('resource_management_analysis', {})
            self.concurrency_issues = results.get('concurrency_issues', [])
            self.resource_issues = results.get('resource_leaks', [])

            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_concurrency_resource_report(results)

            # ä¿å­˜æŠ¥å‘Š
            with open('concurrency_resource_analysis.md', 'w', encoding='utf-8') as f:
                f.write(report)

            # ä¿å­˜åŸå§‹æ•°æ®
            with open('concurrency_resource_data.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            # æ˜¾ç¤ºæ‘˜è¦
            print("\n" + "="*80)
            print("ğŸ”’ å¹¶å‘å®‰å…¨æ€§å’Œèµ„æºç®¡ç†åˆ†æç»“æœ")
            print("="*80)

            thread_safety = results.get('thread_safety_analysis', {})
            resource_mgmt = results.get('resource_management_analysis', {})
            concurrency_issues = results.get('concurrency_issues', [])
            resource_leaks = results.get('resource_leaks', [])
            memory_issues = results.get('memory_management_issues', [])
            performance_impact = results.get('performance_impact', {})

            print(f"ğŸ“Š åˆ†æç»“æœ:")
            print(f"   çº¿ç¨‹å®‰å…¨è¯„åˆ†: {thread_safety.get('thread_safety_score', 0)}/100")
            print(f"   èµ„æºç®¡ç†è¯„åˆ†: {resource_mgmt.get('resource_management_score', 0)}/100")
            print(f"   å¹¶å‘é—®é¢˜: {len(concurrency_issues)} ä¸ª")
            print(f"   èµ„æºæ³„æ¼é£é™©: {len(resource_leaks)} ä¸ª")
            print(f"   å†…å­˜ç®¡ç†é—®é¢˜: {len(memory_issues)} ä¸ª")

            print(f"\nâš¡ æ€§èƒ½å½±å“: {performance_impact.get('overall_impact', 'UNKNOWN')}")

            bottlenecks = performance_impact.get('bottleneck_areas', [])
            if bottlenecks:
                print(f"\nğŸ”¥ ç“¶é¢ˆåŒºåŸŸ:")
                for bottleneck in bottlenecks:
                    print(f"   - {bottleneck}")

            logger.info("ğŸ“„ å¹¶å‘å’Œèµ„æºåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° concurrency_resource_analysis.md")
            logger.info("ğŸ“„ åŸå§‹åˆ†ææ•°æ®å·²ä¿å­˜åˆ° concurrency_resource_data.json")

            return results

        except Exception as e:
            logger.error(f"å¹¶å‘å’Œèµ„æºåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ä¸»å‡½æ•°"""
    analyzer = ConcurrencyResourceAnalyzer()
    return analyzer.run_analysis_and_save_report()


if __name__ == "__main__":
    main()
