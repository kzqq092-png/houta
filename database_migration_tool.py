#!/usr/bin/env python3
"""
HIkyuu-UI æ•°æ®åº“è¿ç§»å·¥å…·

å®ç°æ•°æ®åº“æ¶æ„é‡æ„ï¼š
- ä»å¤šä¸ªåˆ†æ•£çš„æ•°æ®åº“è¿ç§»åˆ°ç»Ÿä¸€çš„åŒæ•°æ®åº“æ¶æ„
- è‡ªåŠ¨å¤‡ä»½ã€è¿ç§»ã€éªŒè¯å’Œæ¸…ç†
- æ›´æ–°ä»£ç ä¸­çš„æ•°æ®åº“è·¯å¾„å¼•ç”¨

ä½œè€…: FactorWeave-Quantå›¢é˜Ÿ
"""

import os
import sys
import shutil
import sqlite3
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    import duckdb
    import pandas as pd
except ImportError as e:
    print(f"ç¼ºå°‘å¿…è¦çš„åº“: {e}")
    print("è¯·å®‰è£…: pip install duckdb pandas")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('database_migration.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class DatabaseMigrationTool:
    """æ•°æ®åº“è¿ç§»å·¥å…·"""

    def __init__(self, dry_run: bool = False):
        """
        åˆå§‹åŒ–è¿ç§»å·¥å…·

        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…æ‰§è¡Œè¿ç§»ï¼‰
        """
        self.project_root = Path(__file__).parent
        self.db_dir = self.project_root / "db"
        self.backup_dir = self.project_root / "db_backup" / f"migration_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.dry_run = dry_run

        # ç›®æ ‡æ•°æ®åº“è·¯å¾„
        self.target_system_db = self.db_dir / "hikyuu_system.sqlite"
        self.target_analytics_db = self.db_dir / "hikyuu_analytics.duckdb"

        # è¿ç§»æ˜ å°„é…ç½®
        self.migration_map = self._load_migration_config()

    def _load_migration_config(self) -> Dict:
        """åŠ è½½è¿ç§»é…ç½®"""
        return {
            # SQLite æ•°æ®åº“è¿ç§»åˆ°ç³»ç»Ÿæ•°æ®åº“
            "system_migrations": [
                {
                    "source": "factorweave_system.db",
                    "target": "hikyuu_system.sqlite",
                    "action": "migrate",
                    "description": "ç³»ç»Ÿé…ç½®å’Œå…ƒæ•°æ®"
                },
                {
                    "source": "import_config.db",
                    "target": "hikyuu_system.sqlite",
                    "action": "merge",
                    "description": "å¯¼å…¥é…ç½®æ•°æ®"
                },
                {
                    "source": "strategies.db",
                    "target": "hikyuu_system.sqlite",
                    "action": "migrate",
                    "description": "ç­–ç•¥å®šä¹‰æ•°æ®"
                }
            ],

            # DuckDB æ•°æ®åº“è¿ç§»åˆ°åˆ†ææ•°æ®åº“
            "analytics_migrations": [
                {
                    "source": "market_data.duckdb",
                    "target": "hikyuu_analytics.duckdb",
                    "action": "migrate",
                    "description": "å¸‚åœºæ•°æ®"
                },
                {
                    "source": "performance_metrics.duckdb",
                    "target": "hikyuu_analytics.duckdb",
                    "action": "merge",
                    "description": "æ€§èƒ½æŒ‡æ ‡"
                },
                {
                    "source": "backtest_results.duckdb",
                    "target": "hikyuu_analytics.duckdb",
                    "action": "migrate",
                    "description": "å›æµ‹ç»“æœ"
                },
                {
                    "source": "analytics.duckdb",
                    "target": "hikyuu_analytics.duckdb",
                    "action": "migrate",
                    "description": "åˆ†ææ•°æ®"
                },
                {
                    "source": "kline_stock.duckdb",
                    "target": "hikyuu_analytics.duckdb",
                    "action": "migrate",
                    "description": "Kçº¿æ•°æ®"
                },
                {
                    "source": "metrics.db",  # SQLite -> DuckDB
                    "target": "hikyuu_analytics.duckdb",
                    "action": "migrate_sqlite_to_duckdb",
                    "description": "æ€§èƒ½æŒ‡æ ‡æ•°æ®ï¼ˆSQLiteè½¬DuckDBï¼‰"
                }
            ],

            # éœ€è¦åˆ é™¤çš„é‡å¤/åºŸå¼ƒæ–‡ä»¶
            "cleanup_files": [
                "factorweave_strategies.db",  # é‡å¤
                "factorweave_analytics.duckdb",  # é‡å¤
                "analytics_factorweave_analytics.duckdb",  # é‡å¤
                "hikyuu_system.db",  # ç©ºæ–‡ä»¶
                "hikyuu_system.db.bakck",  # å¤‡ä»½æ–‡ä»¶
            ]
        }

    def run_migration(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“è¿ç§»"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“æ¶æ„è¿ç§»")
            logger.info(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
            logger.info(f"ğŸ”„ æ¨¡å¼: {'è¯•è¿è¡Œ' if self.dry_run else 'æ­£å¼è¿ç§»'}")

            # 1. åˆ›å»ºå¤‡ä»½
            if not self._create_backup():
                return False

            # 2. æ£€æŸ¥æºæ•°æ®åº“
            if not self._check_source_databases():
                return False

            # 3. åˆ›å»ºç›®æ ‡æ•°æ®åº“
            if not self._create_target_databases():
                return False

            # 4. è¿ç§»ç³»ç»Ÿæ•°æ®
            if not self._migrate_system_data():
                return False

            # 5. è¿ç§»åˆ†ææ•°æ®
            if not self._migrate_analytics_data():
                return False

            # 6. éªŒè¯è¿ç§»ç»“æœ
            if not self._verify_migration():
                return False

            # 7. æ›´æ–°ä»£ç å¼•ç”¨
            if not self._update_code_references():
                return False

            # 8. æ¸…ç†æ—§æ–‡ä»¶
            if not self._cleanup_old_files():
                return False

            logger.info("âœ… æ•°æ®åº“è¿ç§»å®Œæˆï¼")
            self._print_migration_summary()
            return True

        except Exception as e:
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _create_backup(self) -> bool:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        try:
            logger.info("ğŸ“¦ åˆ›å»ºæ•°æ®åº“å¤‡ä»½...")

            if self.dry_run:
                logger.info("ğŸ”„ è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡å¤‡ä»½åˆ›å»º")
                return True

            # åˆ›å»ºå¤‡ä»½ç›®å½•
            self.backup_dir.mkdir(parents=True, exist_ok=True)

            # å¤‡ä»½æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶
            backup_count = 0
            for db_file in self.db_dir.glob("*.db"):
                if db_file.is_file() and db_file.stat().st_size > 0:
                    backup_path = self.backup_dir / db_file.name
                    shutil.copy2(db_file, backup_path)
                    backup_count += 1
                    logger.info(f"  âœ… å¤‡ä»½: {db_file.name}")

            for db_file in self.db_dir.glob("*.duckdb"):
                if db_file.is_file() and db_file.stat().st_size > 0:
                    backup_path = self.backup_dir / db_file.name
                    shutil.copy2(db_file, backup_path)
                    backup_count += 1
                    logger.info(f"  âœ… å¤‡ä»½: {db_file.name}")

            logger.info(f"ğŸ“¦ å¤‡ä»½å®Œæˆï¼Œå…±å¤‡ä»½ {backup_count} ä¸ªæ–‡ä»¶åˆ°: {self.backup_dir}")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return False

    def _check_source_databases(self) -> bool:
        """æ£€æŸ¥æºæ•°æ®åº“"""
        logger.info("ğŸ” æ£€æŸ¥æºæ•°æ®åº“...")

        all_sources = []
        all_sources.extend([m["source"] for m in self.migration_map["system_migrations"]])
        all_sources.extend([m["source"] for m in self.migration_map["analytics_migrations"]])

        missing_files = []
        existing_files = []

        for source_file in all_sources:
            source_path = self.db_dir / source_file
            if source_path.exists() and source_path.stat().st_size > 0:
                existing_files.append(source_file)
                logger.info(f"  âœ… æ‰¾åˆ°: {source_file}")
            else:
                missing_files.append(source_file)
                logger.warning(f"  âš ï¸ ç¼ºå¤±: {source_file}")

        logger.info(f"ğŸ“Š æ£€æŸ¥ç»“æœ: {len(existing_files)} ä¸ªå­˜åœ¨, {len(missing_files)} ä¸ªç¼ºå¤±")

        if missing_files:
            logger.warning("âš ï¸ éƒ¨åˆ†æºæ•°æ®åº“ç¼ºå¤±ï¼Œå°†è·³è¿‡ç›¸å…³è¿ç§»")

        return len(existing_files) > 0

    def _create_target_databases(self) -> bool:
        """åˆ›å»ºç›®æ ‡æ•°æ®åº“"""
        try:
            logger.info("ğŸ—ï¸ åˆ›å»ºç›®æ ‡æ•°æ®åº“...")

            if self.dry_run:
                logger.info("ğŸ”„ è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡ç›®æ ‡æ•°æ®åº“åˆ›å»º")
                return True

            # åˆ›å»ºç³»ç»Ÿæ•°æ®åº“ï¼ˆSQLiteï¼‰
            if not self.target_system_db.exists():
                with sqlite3.connect(str(self.target_system_db)) as conn:
                    conn.execute("SELECT 1")  # åˆ›å»ºæ•°æ®åº“æ–‡ä»¶
                logger.info(f"  âœ… åˆ›å»ºç³»ç»Ÿæ•°æ®åº“: {self.target_system_db.name}")

            # åˆ›å»ºåˆ†ææ•°æ®åº“ï¼ˆDuckDBï¼‰
            if not self.target_analytics_db.exists():
                with duckdb.connect(str(self.target_analytics_db)) as conn:
                    conn.execute("SELECT 1")  # åˆ›å»ºæ•°æ®åº“æ–‡ä»¶
                logger.info(f"  âœ… åˆ›å»ºåˆ†ææ•°æ®åº“: {self.target_analytics_db.name}")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç›®æ ‡æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def _migrate_system_data(self) -> bool:
        """è¿ç§»ç³»ç»Ÿæ•°æ®"""
        logger.info("ğŸ”„ è¿ç§»ç³»ç»Ÿæ•°æ®...")

        for migration in self.migration_map["system_migrations"]:
            source_path = self.db_dir / migration["source"]

            if not source_path.exists():
                logger.warning(f"  âš ï¸ è·³è¿‡ç¼ºå¤±æ–‡ä»¶: {migration['source']}")
                continue

            try:
                logger.info(f"  ğŸ”„ è¿ç§»: {migration['source']} -> {migration['target']}")

                if self.dry_run:
                    logger.info(f"    ğŸ”„ è¯•è¿è¡Œ: {migration['description']}")
                    continue

                # å®é™…è¿ç§»é€»è¾‘
                if migration["action"] == "migrate":
                    self._migrate_sqlite_database(source_path, self.target_system_db)
                elif migration["action"] == "merge":
                    self._merge_sqlite_database(source_path, self.target_system_db)

                logger.info(f"    âœ… å®Œæˆ: {migration['description']}")

            except Exception as e:
                logger.error(f"    âŒ è¿ç§»å¤±è´¥ {migration['source']}: {e}")
                return False

        return True

    def _migrate_analytics_data(self) -> bool:
        """è¿ç§»åˆ†ææ•°æ®"""
        logger.info("ğŸ”„ è¿ç§»åˆ†ææ•°æ®...")

        for migration in self.migration_map["analytics_migrations"]:
            source_path = self.db_dir / migration["source"]

            if not source_path.exists():
                logger.warning(f"  âš ï¸ è·³è¿‡ç¼ºå¤±æ–‡ä»¶: {migration['source']}")
                continue

            try:
                logger.info(f"  ğŸ”„ è¿ç§»: {migration['source']} -> {migration['target']}")

                if self.dry_run:
                    logger.info(f"    ğŸ”„ è¯•è¿è¡Œ: {migration['description']}")
                    continue

                # å®é™…è¿ç§»é€»è¾‘
                if migration["action"] == "migrate":
                    if source_path.suffix == ".duckdb":
                        self._migrate_duckdb_database(source_path, self.target_analytics_db)
                    else:
                        self._migrate_sqlite_to_duckdb(source_path, self.target_analytics_db)
                elif migration["action"] == "merge":
                    self._merge_duckdb_database(source_path, self.target_analytics_db)
                elif migration["action"] == "migrate_sqlite_to_duckdb":
                    self._migrate_sqlite_to_duckdb(source_path, self.target_analytics_db)

                logger.info(f"    âœ… å®Œæˆ: {migration['description']}")

            except Exception as e:
                logger.error(f"    âŒ è¿ç§»å¤±è´¥ {migration['source']}: {e}")
                return False

        return True

    def _migrate_sqlite_database(self, source_path: Path, target_path: Path):
        """è¿ç§»SQLiteæ•°æ®åº“"""
        # ç®€åŒ–å®ç°ï¼šå¤åˆ¶æ‰€æœ‰è¡¨ç»“æ„å’Œæ•°æ®
        with sqlite3.connect(str(source_path)) as source_conn:
            with sqlite3.connect(str(target_path)) as target_conn:
                # è·å–æ‰€æœ‰è¡¨
                tables = source_conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table'"
                ).fetchall()

                for (table_name,) in tables:
                    # å¤åˆ¶è¡¨ç»“æ„
                    schema = source_conn.execute(
                        f"SELECT sql FROM sqlite_master WHERE name='{table_name}'"
                    ).fetchone()

                    if schema and schema[0]:
                        target_conn.execute(schema[0])

                    # å¤åˆ¶æ•°æ®
                    data = source_conn.execute(f"SELECT * FROM {table_name}").fetchall()
                    if data:
                        columns = [desc[0] for desc in source_conn.execute(f"SELECT * FROM {table_name}").description]
                        placeholders = ','.join(['?' for _ in columns])
                        target_conn.executemany(
                            f"INSERT INTO {table_name} VALUES ({placeholders})", data
                        )

    def _merge_sqlite_database(self, source_path: Path, target_path: Path):
        """åˆå¹¶SQLiteæ•°æ®åº“ï¼ˆé¿å…è¡¨åå†²çªï¼‰"""
        # å®ç°è¡¨åå‰ç¼€ç­–ç•¥é¿å…å†²çª
        prefix = source_path.stem + "_"

        with sqlite3.connect(str(source_path)) as source_conn:
            with sqlite3.connect(str(target_path)) as target_conn:
                tables = source_conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table'"
                ).fetchall()

                for (table_name,) in tables:
                    new_table_name = prefix + table_name

                    # å¤åˆ¶è¡¨ç»“æ„ï¼ˆé‡å‘½åè¡¨ï¼‰
                    schema = source_conn.execute(
                        f"SELECT sql FROM sqlite_master WHERE name='{table_name}'"
                    ).fetchone()

                    if schema and schema[0]:
                        new_schema = schema[0].replace(table_name, new_table_name)
                        target_conn.execute(new_schema)

                    # å¤åˆ¶æ•°æ®
                    data = source_conn.execute(f"SELECT * FROM {table_name}").fetchall()
                    if data:
                        columns = [desc[0] for desc in source_conn.execute(f"SELECT * FROM {table_name}").description]
                        placeholders = ','.join(['?' for _ in columns])
                        target_conn.executemany(
                            f"INSERT INTO {new_table_name} VALUES ({placeholders})", data
                        )

    def _migrate_duckdb_database(self, source_path: Path, target_path: Path):
        """è¿ç§»DuckDBæ•°æ®åº“"""
        with duckdb.connect(str(source_path)) as source_conn:
            with duckdb.connect(str(target_path)) as target_conn:
                # è·å–æ‰€æœ‰è¡¨
                tables = source_conn.execute(
                    "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
                ).fetchall()

                for (table_name,) in tables:
                    # å¤åˆ¶è¡¨ç»“æ„å’Œæ•°æ®
                    target_conn.execute(f"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM '{source_path}'::{table_name}")

    def _merge_duckdb_database(self, source_path: Path, target_path: Path):
        """åˆå¹¶DuckDBæ•°æ®åº“"""
        # ç±»ä¼¼SQLiteåˆå¹¶ï¼Œä½†ä½¿ç”¨DuckDBè¯­æ³•
        prefix = source_path.stem + "_"

        with duckdb.connect(str(source_path)) as source_conn:
            with duckdb.connect(str(target_path)) as target_conn:
                tables = source_conn.execute(
                    "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
                ).fetchall()

                for (table_name,) in tables:
                    new_table_name = prefix + table_name
                    target_conn.execute(f"CREATE TABLE IF NOT EXISTS {new_table_name} AS SELECT * FROM '{source_path}'::{table_name}")

    def _migrate_sqlite_to_duckdb(self, source_path: Path, target_path: Path):
        """ä»SQLiteè¿ç§»åˆ°DuckDB"""
        with sqlite3.connect(str(source_path)) as source_conn:
            with duckdb.connect(str(target_path)) as target_conn:
                tables = source_conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table'"
                ).fetchall()

                for (table_name,) in tables:
                    # ä½¿ç”¨DuckDBçš„SQLiteæ‰©å±•ç›´æ¥è¯»å–
                    target_conn.execute("INSTALL sqlite")
                    target_conn.execute("LOAD sqlite")
                    target_conn.execute(f"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM sqlite_scan('{source_path}', '{table_name}')")

    def _verify_migration(self) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("ğŸ” éªŒè¯è¿ç§»ç»“æœ...")

        if self.dry_run:
            logger.info("ğŸ”„ è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡éªŒè¯")
            return True

        try:
            # éªŒè¯ç³»ç»Ÿæ•°æ®åº“
            if self.target_system_db.exists():
                with sqlite3.connect(str(self.target_system_db)) as conn:
                    tables = conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()
                    logger.info(f"  âœ… ç³»ç»Ÿæ•°æ®åº“åŒ…å« {len(tables)} ä¸ªè¡¨")

            # éªŒè¯åˆ†ææ•°æ®åº“
            if self.target_analytics_db.exists():
                with duckdb.connect(str(self.target_analytics_db)) as conn:
                    tables = conn.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'").fetchall()
                    logger.info(f"  âœ… åˆ†ææ•°æ®åº“åŒ…å« {len(tables)} ä¸ªè¡¨")

            return True

        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return False

    def _update_code_references(self) -> bool:
        """æ›´æ–°ä»£ç ä¸­çš„æ•°æ®åº“è·¯å¾„å¼•ç”¨"""
        logger.info("ğŸ”„ æ›´æ–°ä»£ç å¼•ç”¨...")

        if self.dry_run:
            logger.info("ğŸ”„ è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡ä»£ç æ›´æ–°")
            return True

        # è¿™é‡Œå¯ä»¥å®ç°è‡ªåŠ¨ä»£ç æ›´æ–°é€»è¾‘
        # æš‚æ—¶åªè®°å½•éœ€è¦æ‰‹åŠ¨æ›´æ–°çš„æ–‡ä»¶
        logger.info("  âš ï¸ éœ€è¦æ‰‹åŠ¨æ›´æ–°ä»¥ä¸‹æ–‡ä»¶ä¸­çš„æ•°æ®åº“è·¯å¾„:")
        logger.info("    - core/services/config_service.py")
        logger.info("    - core/importdata/import_execution_engine.py")
        logger.info("    - utils/config_manager.py")
        logger.info("    - database_check_tool.py")

        return True

    def _cleanup_old_files(self) -> bool:
        """æ¸…ç†æ—§çš„æ•°æ®åº“æ–‡ä»¶"""
        logger.info("ğŸ§¹ æ¸…ç†æ—§æ–‡ä»¶...")

        if self.dry_run:
            logger.info("ğŸ”„ è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡æ–‡ä»¶æ¸…ç†")
            for filename in self.migration_map["cleanup_files"]:
                logger.info(f"  ğŸ”„ å°†åˆ é™¤: {filename}")
            return True

        cleaned_count = 0
        for filename in self.migration_map["cleanup_files"]:
            file_path = self.db_dir / filename
            if file_path.exists():
                try:
                    file_path.unlink()
                    logger.info(f"  âœ… åˆ é™¤: {filename}")
                    cleaned_count += 1
                except Exception as e:
                    logger.warning(f"  âš ï¸ åˆ é™¤å¤±è´¥ {filename}: {e}")

        logger.info(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
        return True

    def _print_migration_summary(self):
        """æ‰“å°è¿ç§»æ‘˜è¦"""
        logger.info("ğŸ“Š è¿ç§»æ‘˜è¦:")
        logger.info("=" * 50)
        logger.info(f"ğŸ“¦ å¤‡ä»½ä½ç½®: {self.backup_dir}")
        logger.info(f"ğŸ¯ ç›®æ ‡æ•°æ®åº“:")
        logger.info(f"  - ç³»ç»Ÿæ•°æ®åº“: {self.target_system_db}")
        logger.info(f"  - åˆ†ææ•°æ®åº“: {self.target_analytics_db}")
        logger.info("âœ… è¿ç§»å®Œæˆï¼æ–°çš„æ•°æ®åº“æ¶æ„å·²å°±ç»ªã€‚")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="HIkyuu-UI æ•°æ®åº“è¿ç§»å·¥å…·")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…æ‰§è¡Œè¿ç§»ï¼‰")
    parser.add_argument("--backup-only", action="store_true", help="ä»…åˆ›å»ºå¤‡ä»½")

    args = parser.parse_args()

    migration_tool = DatabaseMigrationTool(dry_run=args.dry_run)

    if args.backup_only:
        logger.info("ğŸ“¦ ä»…æ‰§è¡Œå¤‡ä»½æ“ä½œ...")
        success = migration_tool._create_backup()
    else:
        success = migration_tool.run_migration()

    if success:
        logger.info("ğŸ‰ æ“ä½œå®Œæˆï¼")
        sys.exit(0)
    else:
        logger.error("ğŸ’¥ æ“ä½œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()
