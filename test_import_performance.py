#!/usr/bin/env python3
"""
æ•°æ®å¯¼å…¥æ€§èƒ½æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•ä¼˜åŒ–åçš„å¹¶å‘ä¸‹è½½å’Œæ‰¹é‡ä¿å­˜åŠŸèƒ½
"""

import sys
import time
import logging
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s [%(name)s::%(funcName)s]'
)
logger = logging.getLogger(__name__)


def test_import_performance():
    """æµ‹è¯•å¯¼å…¥æ€§èƒ½"""
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from core.importdata.import_execution_engine import DataImportExecutionEngine
        from core.importdata.import_config_manager import ImportTaskConfig, ImportMode, DataFrequency
        from core.services.unified_data_manager import get_unified_data_manager

        logger.info("ğŸš€ å¼€å§‹æ•°æ®å¯¼å…¥æ€§èƒ½æµ‹è¯•")

        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡é…ç½®
        test_symbols = [
            "000001",  # å¹³å®‰é“¶è¡Œ
            "000002",  # ä¸‡ç§‘A
            "000858",  # äº”ç²®æ¶²
            "002415",  # æµ·åº·å¨è§†
            "600036",  # æ‹›å•†é“¶è¡Œ
        ]

        task_config = ImportTaskConfig(
            task_id="test_performance_001",
            data_source="examples.akshare_stock_plugin",
            asset_type="è‚¡ç¥¨",
            data_type="Kçº¿æ•°æ®",
            symbols=test_symbols,
            start_date="2024-01-01",
            end_date="2024-08-30",
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH,
            max_workers=4  # æµ‹è¯•å¹¶å‘æ•°
        )

        logger.info(f"ğŸ“Š æµ‹è¯•é…ç½®:")
        logger.info(f"  - è‚¡ç¥¨æ•°é‡: {len(test_symbols)}")
        logger.info(f"  - æ—¶é—´èŒƒå›´: {task_config.start_date} åˆ° {task_config.end_date}")
        logger.info(f"  - å¹¶å‘çº¿ç¨‹: {task_config.max_workers}")
        logger.info(f"  - æ•°æ®é¢‘ç‡: {task_config.frequency.value}")

        # åˆ›å»ºå¯¼å…¥å¼•æ“
        data_manager = get_unified_data_manager()
        import_engine = DataImportExecutionEngine(data_manager=data_manager)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œå¯¼å…¥ä»»åŠ¡
        logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œå¯¼å…¥ä»»åŠ¡...")
        task_id = import_engine.start_task(task_config.task_id)

        if task_id:
            logger.info(f"âœ… ä»»åŠ¡å¯åŠ¨æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")

            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            max_wait_time = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            wait_start = time.time()

            while time.time() - wait_start < max_wait_time:
                status = import_engine.get_task_status(task_id)
                logger.info(f"ğŸ“Š ä»»åŠ¡çŠ¶æ€: {status}")

                if status and status.get('status') in ['completed', 'failed', 'cancelled']:
                    break

                time.sleep(2)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡

            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            total_time = end_time - start_time

            # è·å–æœ€ç»ˆçŠ¶æ€
            final_status = import_engine.get_task_status(task_id)

            logger.info("ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
            logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f} ç§’")
            logger.info(f"  - å¹³å‡æ¯åªè‚¡ç¥¨: {total_time/len(test_symbols):.2f} ç§’")
            logger.info(f"  - æœ€ç»ˆçŠ¶æ€: {final_status}")

            # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®
            logger.info("ğŸ” æ£€æŸ¥å¯¼å…¥çš„æ•°æ®...")
            check_imported_data()

        else:
            logger.error("âŒ ä»»åŠ¡å¯åŠ¨å¤±è´¥")

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())


def check_imported_data():
    """æ£€æŸ¥å¯¼å…¥çš„æ•°æ®"""
    try:
        import duckdb

        db_path = "db/kline_stock.duckdb"
        if not Path(db_path).exists():
            logger.warning(f"âš ï¸ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            return

        with duckdb.connect(db_path) as conn:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            tables_query = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'main' AND table_name LIKE '%kline%'
            """
            tables_result = conn.execute(tables_query).fetchall()

            if not tables_result:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°Kçº¿æ•°æ®è¡¨")
                return

            for table_row in tables_result:
                table_name = table_row[0]
                logger.info(f"ğŸ“Š æ£€æŸ¥è¡¨: {table_name}")

                # ç»Ÿè®¡æ•°æ®
                stats_query = f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as stock_count,
                    MIN(datetime) as min_date,
                    MAX(datetime) as max_date
                FROM {table_name}
                """
                stats_result = conn.execute(stats_query).fetchone()

                if stats_result:
                    total_records, stock_count, min_date, max_date = stats_result
                    logger.info(f"  âœ… æ€»è®°å½•æ•°: {total_records:,}")
                    logger.info(f"  ğŸ“ˆ è‚¡ç¥¨æ•°é‡: {stock_count}")
                    logger.info(f"  ğŸ“… æ—¥æœŸèŒƒå›´: {min_date} åˆ° {max_date}")

                    # æ˜¾ç¤ºæ¯åªè‚¡ç¥¨çš„è®°å½•æ•°
                    stock_stats_query = f"""
                    SELECT symbol, COUNT(*) as records
                    FROM {table_name}
                    GROUP BY symbol
                    ORDER BY symbol
                    """
                    stock_stats = conn.execute(stock_stats_query).fetchall()

                    logger.info("  ğŸ“Š å„è‚¡ç¥¨è®°å½•æ•°:")
                    for symbol, records in stock_stats:
                        logger.info(f"    - {symbol}: {records:,} æ¡")

    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥å¯¼å…¥æ•°æ®å¤±è´¥: {e}")


if __name__ == "__main__":
    test_import_performance()
