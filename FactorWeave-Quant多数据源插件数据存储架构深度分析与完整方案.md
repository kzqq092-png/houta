# FactorWeave-Quant å¤šæ•°æ®æºæ’ä»¶æ•°æ®å­˜å‚¨æ¶æ„æ·±åº¦åˆ†æä¸å®Œæ•´æ–¹æ¡ˆ

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

æœ¬æ–¹æ¡ˆåŸºäºå¯¹ç°æœ‰ç³»ç»Ÿçš„æ·±å…¥åˆ†æï¼ŒåŒ…æ‹¬TETæ•°æ®ç®¡é“ã€æ’ä»¶ç³»ç»Ÿå®ç°ã€æ•°æ®åº“æ¶æ„ç­‰ï¼Œç»“åˆè¡Œä¸šä¸“ä¸šè½¯ä»¶æ ‡å‡†ï¼Œè®¾è®¡äº†ä¸€å¥—å®Œæ•´çš„å¤šæ•°æ®æºæ’ä»¶æ•°æ®å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿ä¸ç°æœ‰æ¶æ„æ— ç¼é›†æˆï¼Œé¿å…é‡å¤è®¾è®¡ã€‚

**è®¾è®¡ç‰ˆæœ¬**: v2.0  
**è®¾è®¡æ—¥æœŸ**: 2025-01-27  
**è®¾è®¡ç›®æ ‡**: æ·±åº¦é›†æˆã€æ— ç¼æ‰©å±•ã€è¡Œä¸šå¯¹æ ‡

## ğŸ” ç°æœ‰ç³»ç»Ÿæ·±åº¦åˆ†æ

### 1. ç°æœ‰æ’ä»¶ç³»ç»Ÿæ¶æ„åˆ†æ

#### 1.1 æ’ä»¶ç±»å‹ä½“ç³»ï¼ˆå·²å®ç°ï¼‰
```python
class PluginType(Enum):
    INDICATOR = "indicator"          # æŠ€æœ¯æŒ‡æ ‡æ’ä»¶ âœ…
    STRATEGY = "strategy"            # ç­–ç•¥æ’ä»¶ âœ…
    DATA_SOURCE = "data_source"      # æ•°æ®æºæ’ä»¶ âœ…
    ANALYSIS = "analysis"            # åˆ†æå·¥å…·æ’ä»¶ âœ…
    UI_COMPONENT = "ui_component"    # UIç»„ä»¶æ’ä»¶ âœ…
    EXPORT = "export"                # å¯¼å‡ºæ’ä»¶ âœ…
    NOTIFICATION = "notification"    # é€šçŸ¥æ’ä»¶ âœ…
    CHART_TOOL = "chart_tool"        # å›¾è¡¨å·¥å…·æ’ä»¶ âœ…
```

#### 1.2 ç°æœ‰æ•°æ®æºæ’ä»¶å®ç°
- **HIkyuuæ•°æ®æ’ä»¶**: æ”¯æŒè‚¡ç¥¨ã€æŒ‡æ•°ã€åŸºé‡‘æ•°æ®
- **æ’ä»¶å…ƒæ•°æ®ç®¡ç†**: JSONé…ç½®æ–‡ä»¶æ”¯æŒ
- **æ’ä»¶ç”Ÿå‘½å‘¨æœŸ**: åˆå§‹åŒ–ã€è¿æ¥ã€å¥åº·æ£€æŸ¥ã€æ¸…ç†
- **èƒ½åŠ›å£°æ˜**: æ”¯æŒçš„èµ„äº§ç±»å‹ã€æ•°æ®ç±»å‹ã€å¸‚åœºã€é¢‘ç‡

#### 1.3 ç°æœ‰æ’ä»¶ç®¡ç†æœºåˆ¶
- **PluginDatabaseManager**: SQLiteæ’ä»¶æ³¨å†Œè¡¨
- **æ’ä»¶SDK**: å¼€å‘ã€éªŒè¯ã€æ„å»ºã€æµ‹è¯•å·¥å…·é“¾
- **æ’ä»¶å¸‚åœº**: æµè§ˆã€å®‰è£…ã€ç®¡ç†ç•Œé¢

### 2. TETæ•°æ®ç®¡é“æ¶æ„åˆ†æ

#### 2.1 TETæ ¸å¿ƒç»„ä»¶ï¼ˆå·²å®ç°ï¼‰
```python
class TETDataPipeline:
    """Transform-Extract-Transformä¸‰é˜¶æ®µæ•°æ®å¤„ç†"""
    
    # Stage 1: Transform Queryï¼ˆæŸ¥è¯¢è½¬æ¢ï¼‰
    def transform_query(self, query: StandardQuery) -> RoutingRequest
    
    # Stage 2: Extract Dataï¼ˆæ•°æ®æå–ï¼‰
    def extract_data_with_failover(self, request, query) -> Tuple[Any, Dict, FailoverResult]
    
    # Stage 3: Transform Dataï¼ˆæ•°æ®æ ‡å‡†åŒ–ï¼‰
    def transform_data(self, raw_data, query) -> pd.DataFrame
```

#### 2.2 ç°æœ‰å­—æ®µæ˜ å°„æœºåˆ¶
```python
self.field_mappings = {
    DataType.HISTORICAL_KLINE: {
        'o': 'open', 'Open': 'open', 'OPEN': 'open',
        'h': 'high', 'High': 'high', 'HIGH': 'high',
        # ... æ›´å¤šæ˜ å°„
    }
}
```

#### 2.3 æ•°æ®æºè·¯ç”±å™¨
- **DataSourceRouter**: æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
- **æ•…éšœè½¬ç§»**: å¤šæ•°æ®æºè‡ªåŠ¨åˆ‡æ¢
- **ç¼“å­˜æœºåˆ¶**: 5åˆ†é’ŸTTLç¼“å­˜
- **æ€§èƒ½ç»Ÿè®¡**: è¯·æ±‚ç»Ÿè®¡å’Œå“åº”æ—¶é—´ç›‘æ§

### 3. ç°æœ‰æ•°æ®åº“æ¶æ„åˆ†æ

#### 3.1 æ··åˆæ•°æ®åº“æ¶æ„ï¼ˆå·²å®ç°ï¼‰
```
SQLite (OLTP)              â”‚  DuckDB (OLAP)
â€¢ ç³»ç»Ÿé…ç½®                 â”‚ â€¢ å†å²Kçº¿æ•°æ®
â€¢ æ’ä»¶ç®¡ç†                 â”‚ â€¢ å›æµ‹ç»“æœ
â€¢ ç”¨æˆ·è®¾ç½®                 â”‚ â€¢ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
â€¢ å®æ—¶çŠ¶æ€                 â”‚ â€¢ ç»Ÿè®¡åˆ†æ
â€¢ ç¼“å­˜æ•°æ®                 â”‚ â€¢ æ€§èƒ½ç›‘æ§æ•°æ®
```

#### 3.2 ç°æœ‰æ•°æ®åº“æ–‡ä»¶
- **hikyuu_system.db** (180KB) - SQLiteç³»ç»Ÿæ•°æ®åº“
- **factorweave_system.db** (3.0MB) - FactorWeaveç³»ç»Ÿæ•°æ®åº“
- **factorweave_analytics.db** (3.2MB) - DuckDBåˆ†ææ•°æ®åº“

#### 3.3 DuckDBæ€§èƒ½ä¼˜åŒ–ï¼ˆå·²å®ç°ï¼‰
- **DuckDBPerformanceOptimizer**: è‡ªåŠ¨æ€§èƒ½ä¼˜åŒ–
- **å·¥ä½œè´Ÿè½½ç±»å‹**: OLAP/OLTP/MIXED
- **é…ç½®ç®¡ç†**: DuckDBConfigManager

## ğŸ¯ å®Œæ•´æ–¹æ¡ˆè®¾è®¡ï¼ˆåŸºäºç°æœ‰æ¶æ„æ‰©å±•ï¼‰

### 1. æ•°æ®æºæ’ä»¶æ‰©å±•æ¶æ„

#### 1.1 æ‰©å±•IDataSourcePluginæ¥å£
```python
class IEnhancedDataSourcePlugin(IDataSourcePlugin):
    """å¢å¼ºæ•°æ®æºæ’ä»¶æ¥å£ï¼ˆæ‰©å±•ç°æœ‰æ¥å£ï¼‰"""
    
    # ç»§æ‰¿ç°æœ‰æ–¹æ³•
    # connect(), disconnect(), is_connected(), health_check()
    # get_asset_list(), get_kdata()
    
    # æ–°å¢æ–¹æ³•
    @abstractmethod
    def get_fundamental_data(self, symbol: str, **kwargs) -> pd.DataFrame:
        """è·å–åŸºæœ¬é¢æ•°æ®"""
        pass
    
    @abstractmethod
    def get_macro_data(self, indicator: str, **kwargs) -> pd.DataFrame:
        """è·å–å®è§‚æ•°æ®"""
        pass
    
    @abstractmethod
    def get_financial_statements(self, symbol: str, **kwargs) -> pd.DataFrame:
        """è·å–è´¢åŠ¡æŠ¥è¡¨æ•°æ®"""
        pass
    
    @abstractmethod
    def get_market_depth(self, symbol: str, **kwargs) -> pd.DataFrame:
        """è·å–å¸‚åœºæ·±åº¦æ•°æ®"""
        pass
    
    @abstractmethod
    def get_trade_ticks(self, symbol: str, **kwargs) -> pd.DataFrame:
        """è·å–é€ç¬”äº¤æ˜“æ•°æ®"""
        pass
    
    def get_data_schema(self, data_type: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æ¨¡å¼å®šä¹‰"""
        return {}
    
    def validate_data_quality(self, data: pd.DataFrame, data_type: str) -> Dict[str, Any]:
        """æ•°æ®è´¨é‡éªŒè¯"""
        return {"quality_score": 1.0, "issues": []}
```

#### 1.2 æ’ä»¶æ³¨å†Œè¡¨æ‰©å±•ï¼ˆåŸºäºç°æœ‰pluginsè¡¨ï¼‰
```sql
-- æ‰©å±•ç°æœ‰pluginsè¡¨ï¼Œæ·»åŠ æ•°æ®æºç‰¹å®šå­—æ®µ
ALTER TABLE plugins ADD COLUMN supported_assets TEXT DEFAULT '[]';
ALTER TABLE plugins ADD COLUMN supported_data_types TEXT DEFAULT '[]';
ALTER TABLE plugins ADD COLUMN supported_markets TEXT DEFAULT '[]';
ALTER TABLE plugins ADD COLUMN field_mappings TEXT DEFAULT '{}';
ALTER TABLE plugins ADD COLUMN api_endpoints TEXT DEFAULT '{}';
ALTER TABLE plugins ADD COLUMN rate_limits TEXT DEFAULT '{}';
ALTER TABLE plugins ADD COLUMN data_quality_config TEXT DEFAULT '{}';

-- æ–°å¢æ•°æ®æºæ’ä»¶é…ç½®è¡¨
CREATE TABLE IF NOT EXISTS data_source_configs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    plugin_name TEXT NOT NULL,
    config_key TEXT NOT NULL,
    config_value TEXT,
    config_type TEXT DEFAULT 'string',
    is_encrypted BOOLEAN DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (plugin_name) REFERENCES plugins(name),
    UNIQUE(plugin_name, config_key)
);
```

### 2. TETæ•°æ®ç®¡é“å¢å¼º

#### 2.1 æ‰©å±•StandardQueryï¼ˆåŸºäºç°æœ‰å®ç°ï¼‰
```python
@dataclass
class EnhancedStandardQuery(StandardQuery):
    """å¢å¼ºæ ‡å‡†æŸ¥è¯¢ï¼ˆæ‰©å±•ç°æœ‰StandardQueryï¼‰"""
    
    # ç»§æ‰¿ç°æœ‰å­—æ®µ
    # symbol, asset_type, data_type, start_date, end_date, period, market, provider
    
    # æ–°å¢å­—æ®µ
    data_quality_threshold: float = 0.8      # æ•°æ®è´¨é‡é˜ˆå€¼
    enable_cache: bool = True                # æ˜¯å¦å¯ç”¨ç¼“å­˜
    cache_ttl_minutes: int = 5               # ç¼“å­˜TTL
    enable_validation: bool = True           # æ˜¯å¦å¯ç”¨æ•°æ®éªŒè¯
    output_format: str = "pandas"            # è¾“å‡ºæ ¼å¼
    include_metadata: bool = True            # æ˜¯å¦åŒ…å«å…ƒæ•°æ®
    
    # é«˜çº§æŸ¥è¯¢å‚æ•°
    aggregation_level: Optional[str] = None  # èšåˆçº§åˆ«
    filters: Dict[str, Any] = field(default_factory=dict)  # è¿‡æ»¤æ¡ä»¶
    sort_by: Optional[str] = None            # æ’åºå­—æ®µ
    limit: Optional[int] = None              # è®°å½•é™åˆ¶
```

#### 2.2 å¢å¼ºå­—æ®µæ˜ å°„é…ç½®ï¼ˆæ‰©å±•ç°æœ‰field_mappingsï¼‰
```python
ENHANCED_FIELD_MAPPINGS = {
    # ç»§æ‰¿ç°æœ‰æ˜ å°„
    **TETDataPipeline.field_mappings,
    
    # æ–°å¢åŸºæœ¬é¢æ•°æ®æ˜ å°„
    DataType.FUNDAMENTAL: {
        # Windæ ‡å‡†å­—æ®µ
        'total_mv': 'market_cap', 'æ€»å¸‚å€¼': 'market_cap',
        'float_mv': 'float_market_cap', 'æµé€šå¸‚å€¼': 'float_market_cap',
        'pe_ttm': 'pe_ratio', 'PE(TTM)': 'pe_ratio',
        'pb_lf': 'pb_ratio', 'PB(LF)': 'pb_ratio',
        'ps_ttm': 'ps_ratio', 'PS(TTM)': 'ps_ratio',
        
        # Bloombergæ ‡å‡†å­—æ®µ
        'EV_TO_EBITDA': 'ev_ebitda', 'ev_ebitda': 'ev_ebitda',
        'RETURN_ON_EQUITY': 'roe', 'roe': 'roe',
        'RETURN_ON_ASSETS': 'roa', 'roa': 'roa',
        'GROSS_MARGIN': 'gross_margin', 'gross_margin': 'gross_margin',
        
        # è¡Œä¸šåˆ†ç±»æ˜ å°„
        'industry_citic_l1': 'industry_l1', 'ä¸­ä¿¡ä¸€çº§è¡Œä¸š': 'industry_l1',
        'industry_citic_l2': 'industry_l2', 'ä¸­ä¿¡äºŒçº§è¡Œä¸š': 'industry_l2',
        'industry_sw_l1': 'industry_l1', 'ç”³ä¸‡ä¸€çº§è¡Œä¸š': 'industry_l1',
    },
    
    # è´¢åŠ¡æ•°æ®æ˜ å°„
    DataType.FINANCIAL_STATEMENTS: {
        # èµ„äº§è´Ÿå€ºè¡¨
        'total_assets': 'total_assets', 'èµ„äº§æ€»è®¡': 'total_assets',
        'total_liab': 'total_liabilities', 'è´Ÿå€ºåˆè®¡': 'total_liabilities',
        'total_equity': 'shareholders_equity', 'è‚¡ä¸œæƒç›Šåˆè®¡': 'shareholders_equity',
        
        # åˆ©æ¶¦è¡¨
        'total_revenue': 'total_revenue', 'è¥ä¸šæ€»æ”¶å…¥': 'total_revenue',
        'oper_rev': 'operating_revenue', 'è¥ä¸šæ”¶å…¥': 'operating_revenue',
        'net_profit_is': 'net_profit', 'å‡€åˆ©æ¶¦': 'net_profit',
        'net_profit_parent': 'net_profit_parent', 'å½’æ¯å‡€åˆ©æ¶¦': 'net_profit_parent',
        
        # ç°é‡‘æµé‡è¡¨
        'ocf': 'operating_cash_flow', 'ç»è¥æ´»åŠ¨ç°é‡‘æµ': 'operating_cash_flow',
        'icf': 'investing_cash_flow', 'æŠ•èµ„æ´»åŠ¨ç°é‡‘æµ': 'investing_cash_flow',
        'fcf': 'financing_cash_flow', 'ç­¹èµ„æ´»åŠ¨ç°é‡‘æµ': 'financing_cash_flow',
    },
    
    # å®è§‚æ•°æ®æ˜ å°„
    DataType.MACRO_ECONOMIC: {
        'indicator_code': 'indicator_code', 'æŒ‡æ ‡ä»£ç ': 'indicator_code',
        'indicator_name': 'indicator_name', 'æŒ‡æ ‡åç§°': 'indicator_name',
        'value': 'value', 'æ•°å€¼': 'value', 'val': 'value',
        'unit': 'unit', 'å•ä½': 'unit',
        'frequency': 'frequency', 'é¢‘ç‡': 'frequency',
        'country': 'country', 'å›½å®¶': 'country',
        'region': 'region', 'åœ°åŒº': 'region',
    }
}
```

### 3. æ•°æ®å­˜å‚¨æ¶æ„è®¾è®¡

#### 3.1 DuckDBè¡¨ç»“æ„è®¾è®¡ï¼ˆæ‰©å±•ç°æœ‰factorweave_analytics.dbï¼‰

##### 3.1.1 Kçº¿æ•°æ®è¡¨ï¼ˆæŒ‰æ’ä»¶åˆ†è¡¨ï¼‰
```sql
-- åŠ¨æ€åˆ›å»ºè¡¨ï¼škline_data_{plugin_name}_{period}
CREATE TABLE kline_data_hikyuu_1d (
    symbol VARCHAR NOT NULL,
    datetime TIMESTAMP NOT NULL,
    
    -- åŸºç¡€OHLCV
    open DECIMAL(12,4) NOT NULL,
    high DECIMAL(12,4) NOT NULL,
    low DECIMAL(12,4) NOT NULL,
    close DECIMAL(12,4) NOT NULL,
    volume BIGINT NOT NULL,
    amount DECIMAL(20,2),
    
    -- å¤æƒæ•°æ®ï¼ˆWindæ ‡å‡†ï¼‰
    adj_close DECIMAL(12,4),
    adj_factor DECIMAL(10,6),
    
    -- æŠ€æœ¯æŒ‡æ ‡é¢„è®¡ç®—
    ma5 DECIMAL(12,4),
    ma10 DECIMAL(12,4),
    ma20 DECIMAL(12,4),
    ma60 DECIMAL(12,4),
    rsi_14 DECIMAL(8,4),
    macd_dif DECIMAL(8,4),
    macd_dea DECIMAL(8,4),
    kdj_k DECIMAL(8,4),
    kdj_d DECIMAL(8,4),
    kdj_j DECIMAL(8,4),
    
    -- å¸‚åœºå¾®è§‚ç»“æ„ï¼ˆBloombergæ ‡å‡†ï¼‰
    vwap DECIMAL(12,4),
    bid_price DECIMAL(12,4),
    ask_price DECIMAL(12,4),
    spread DECIMAL(8,4),
    
    -- èµ„é‡‘æµå‘ï¼ˆä¸œæ–¹è´¢å¯Œæ ‡å‡†ï¼‰
    net_inflow_large DECIMAL(20,2),
    net_inflow_main DECIMAL(20,2),
    
    -- å¸‚åœºæƒ…ç»ª
    turnover_rate DECIMAL(8,4),
    amplitude DECIMAL(8,4),
    change_pct DECIMAL(8,4),
    
    -- æ‰©å±•å­—æ®µ
    plugin_specific_data JSON,
    
    -- å…ƒæ•°æ®
    data_source VARCHAR NOT NULL,
    data_quality_score DECIMAL(4,3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (symbol, datetime)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_kline_symbol_datetime ON kline_data_hikyuu_1d(symbol, datetime);
CREATE INDEX idx_kline_datetime ON kline_data_hikyuu_1d(datetime);
CREATE INDEX idx_kline_data_source ON kline_data_hikyuu_1d(data_source);
```

##### 3.1.2 åŸºæœ¬é¢æ•°æ®è¡¨
```sql
CREATE TABLE stock_fundamental_{plugin_name} (
    symbol VARCHAR NOT NULL,
    trade_date DATE NOT NULL,
    
    -- åŸºæœ¬ä¿¡æ¯
    name VARCHAR,
    market VARCHAR,
    industry_l1 VARCHAR,
    industry_l2 VARCHAR,
    industry_l3 VARCHAR,
    
    -- å¸‚å€¼ä¿¡æ¯
    total_shares BIGINT,
    float_shares BIGINT,
    market_cap DECIMAL(20,2),
    float_market_cap DECIMAL(20,2),
    
    -- ä¼°å€¼æŒ‡æ ‡
    pe_ratio DECIMAL(10,4),
    pb_ratio DECIMAL(10,4),
    ps_ratio DECIMAL(10,4),
    pcf_ratio DECIMAL(10,4),
    ev_ebitda DECIMAL(10,4),
    
    -- ç›ˆåˆ©èƒ½åŠ›
    roe DECIMAL(8,4),
    roa DECIMAL(8,4),
    gross_margin DECIMAL(8,4),
    net_margin DECIMAL(8,4),
    
    -- é£é™©æŒ‡æ ‡
    beta DECIMAL(8,6),
    volatility_30d DECIMAL(8,6),
    volatility_252d DECIMAL(8,6),
    
    -- æ‰©å±•å­—æ®µ
    plugin_specific_data JSON,
    
    -- å…ƒæ•°æ®
    data_source VARCHAR NOT NULL,
    data_quality_score DECIMAL(4,3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (symbol, trade_date)
);
```

##### 3.1.3 è´¢åŠ¡æŠ¥è¡¨æ•°æ®è¡¨
```sql
CREATE TABLE financial_statements_{plugin_name} (
    symbol VARCHAR NOT NULL,
    report_date DATE NOT NULL,
    report_type VARCHAR NOT NULL, -- annual/semi_annual/quarterly
    
    -- èµ„äº§è´Ÿå€ºè¡¨
    total_assets DECIMAL(20,2),
    total_liabilities DECIMAL(20,2),
    shareholders_equity DECIMAL(20,2),
    current_assets DECIMAL(20,2),
    current_liabilities DECIMAL(20,2),
    cash_and_equivalents DECIMAL(20,2),
    
    -- åˆ©æ¶¦è¡¨
    total_revenue DECIMAL(20,2),
    operating_revenue DECIMAL(20,2),
    operating_cost DECIMAL(20,2),
    gross_profit DECIMAL(20,2),
    operating_profit DECIMAL(20,2),
    net_profit DECIMAL(20,2),
    net_profit_parent DECIMAL(20,2),
    
    -- ç°é‡‘æµé‡è¡¨
    operating_cash_flow DECIMAL(20,2),
    investing_cash_flow DECIMAL(20,2),
    financing_cash_flow DECIMAL(20,2),
    free_cash_flow DECIMAL(20,2),
    
    -- è´¢åŠ¡æ¯”ç‡
    current_ratio DECIMAL(8,4),
    quick_ratio DECIMAL(8,4),
    debt_to_equity DECIMAL(8,4),
    interest_coverage DECIMAL(8,4),
    
    -- æ‰©å±•å­—æ®µ
    plugin_specific_data JSON,
    
    -- å…ƒæ•°æ®
    data_source VARCHAR NOT NULL,
    data_quality_score DECIMAL(4,3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (symbol, report_date, report_type)
);
```

##### 3.1.4 å®è§‚ç»æµæ•°æ®è¡¨
```sql
CREATE TABLE macro_economic_{plugin_name} (
    indicator_code VARCHAR NOT NULL,
    date DATE NOT NULL,
    
    -- åŸºæœ¬ä¿¡æ¯
    indicator_name VARCHAR NOT NULL,
    value DECIMAL(20,6),
    unit VARCHAR,
    frequency VARCHAR, -- daily/weekly/monthly/quarterly/yearly
    
    -- åˆ†ç±»ä¿¡æ¯
    category_l1 VARCHAR,
    category_l2 VARCHAR,
    category_l3 VARCHAR,
    
    -- åœ°åŒºä¿¡æ¯
    country VARCHAR,
    region VARCHAR,
    
    -- æ•°æ®å±æ€§
    is_seasonally_adjusted BOOLEAN DEFAULT FALSE,
    is_preliminary BOOLEAN DEFAULT FALSE,
    revision_count INTEGER DEFAULT 0,
    
    -- æ‰©å±•å­—æ®µ
    plugin_specific_data JSON,
    
    -- å…ƒæ•°æ®
    data_source VARCHAR NOT NULL,
    data_quality_score DECIMAL(4,3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (indicator_code, date)
);
```

#### 3.2 SQLiteé…ç½®è¡¨æ‰©å±•ï¼ˆåŸºäºç°æœ‰ç³»ç»Ÿæ•°æ®åº“ï¼‰

##### 3.2.1 å­—æ®µæ˜ å°„è¡¨
```sql
CREATE TABLE IF NOT EXISTS field_mappings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    plugin_name TEXT NOT NULL,
    data_type TEXT NOT NULL,
    source_field TEXT NOT NULL,
    target_field TEXT NOT NULL,
    field_type TEXT NOT NULL,
    transform_rule TEXT DEFAULT '{}', -- JSONæ ¼å¼è½¬æ¢è§„åˆ™
    validation_rule TEXT DEFAULT '{}', -- JSONæ ¼å¼éªŒè¯è§„åˆ™
    is_required BOOLEAN DEFAULT 0,
    default_value TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(plugin_name, data_type, source_field)
);
```

##### 3.2.2 æ•°æ®è´¨é‡ç›‘æ§è¡¨
```sql
CREATE TABLE IF NOT EXISTS data_quality_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    plugin_name TEXT NOT NULL,
    table_name TEXT NOT NULL,
    metric_date DATE NOT NULL,
    
    -- å®Œæ•´æ€§æŒ‡æ ‡
    total_records INTEGER DEFAULT 0,
    null_records INTEGER DEFAULT 0,
    duplicate_records INTEGER DEFAULT 0,
    completeness_score DECIMAL(5,4) DEFAULT 0,
    
    -- å‡†ç¡®æ€§æŒ‡æ ‡
    validation_errors INTEGER DEFAULT 0,
    format_errors INTEGER DEFAULT 0,
    range_errors INTEGER DEFAULT 0,
    accuracy_score DECIMAL(5,4) DEFAULT 0,
    
    -- åŠæ—¶æ€§æŒ‡æ ‡
    data_delay_minutes INTEGER DEFAULT 0,
    timeliness_score DECIMAL(5,4) DEFAULT 0,
    
    -- ä¸€è‡´æ€§æŒ‡æ ‡
    consistency_errors INTEGER DEFAULT 0,
    consistency_score DECIMAL(5,4) DEFAULT 0,
    
    -- ç»¼åˆè¯„åˆ†
    overall_score DECIMAL(5,4) DEFAULT 0,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(plugin_name, table_name, metric_date)
);
```

### 4. æ•°æ®ç®¡ç†æœåŠ¡å¢å¼º

#### 4.1 æ‰©å±•UnifiedDataManager
```python
class EnhancedUnifiedDataManager(UnifiedDataManager):
    """å¢å¼ºç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ï¼ˆæ‰©å±•ç°æœ‰å®ç°ï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.plugin_table_manager = PluginTableManager()
        self.data_quality_monitor = DataQualityMonitor()
        self.field_mapping_manager = FieldMappingManager()
    
    def register_data_source_plugin(self, plugin: IEnhancedDataSourcePlugin) -> bool:
        """æ³¨å†Œå¢å¼ºæ•°æ®æºæ’ä»¶"""
        try:
            # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
            if not super().register_plugin_data_source(plugin.plugin_info.id, plugin):
                return False
            
            # åˆ›å»ºæ’ä»¶ä¸“ç”¨è¡¨
            self.plugin_table_manager.create_plugin_tables(plugin)
            
            # æ³¨å†Œå­—æ®µæ˜ å°„
            self.field_mapping_manager.register_plugin_mappings(plugin)
            
            # åˆå§‹åŒ–æ•°æ®è´¨é‡ç›‘æ§
            self.data_quality_monitor.setup_plugin_monitoring(plugin)
            
            return True
            
        except Exception as e:
            logger.error(f"æ³¨å†Œæ’ä»¶å¤±è´¥: {e}")
            return False
    
    def get_multi_source_data(self, symbol: str, data_types: List[str], 
                             quality_threshold: float = 0.8) -> Dict[str, pd.DataFrame]:
        """è·å–å¤šæºæ•°æ®ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
        results = {}
        
        for data_type in data_types:
            try:
                # ä½¿ç”¨TETç®¡é“è·å–æ•°æ®
                query = EnhancedStandardQuery(
                    symbol=symbol,
                    data_type=DataType(data_type),
                    data_quality_threshold=quality_threshold
                )
                
                result = self.tet_pipeline.process(query)
                
                if result and result.data is not None:
                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    quality_score = self.data_quality_monitor.calculate_quality_score(
                        result.data, data_type
                    )
                    
                    if quality_score >= quality_threshold:
                        results[data_type] = result.data
                    else:
                        logger.warning(f"æ•°æ®è´¨é‡ä¸è¾¾æ ‡: {symbol} {data_type} (score: {quality_score})")
                        
            except Exception as e:
                logger.error(f"è·å–æ•°æ®å¤±è´¥: {symbol} {data_type} - {e}")
        
        return results
```

#### 4.2 æ’ä»¶è¡¨ç®¡ç†å™¨
```python
class PluginTableManager:
    """æ’ä»¶è¡¨ç®¡ç†å™¨"""
    
    def __init__(self, duckdb_path: str = "db/factorweave_analytics.db"):
        self.duckdb_path = duckdb_path
        self.conn = duckdb.connect(duckdb_path)
    
    def create_plugin_tables(self, plugin: IEnhancedDataSourcePlugin) -> bool:
        """ä¸ºæ’ä»¶åˆ›å»ºä¸“ç”¨æ•°æ®è¡¨"""
        try:
            plugin_name = plugin.plugin_info.id
            supported_data_types = plugin.plugin_info.supported_data_types
            
            for data_type in supported_data_types:
                if data_type == DataType.HISTORICAL_KLINE:
                    self._create_kline_tables(plugin_name)
                elif data_type == DataType.FUNDAMENTAL:
                    self._create_fundamental_table(plugin_name)
                elif data_type == DataType.FINANCIAL_STATEMENTS:
                    self._create_financial_table(plugin_name)
                elif data_type == DataType.MACRO_ECONOMIC:
                    self._create_macro_table(plugin_name)
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ’ä»¶è¡¨å¤±è´¥: {e}")
            return False
    
    def _create_kline_tables(self, plugin_name: str):
        """åˆ›å»ºKçº¿æ•°æ®è¡¨"""
        periods = ['1m', '5m', '15m', '30m', '1h', '1d', '1w', '1M']
        
        for period in periods:
            table_name = f"kline_data_{plugin_name}_{period}"
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            if self._table_exists(table_name):
                continue
            
            sql = f"""
            CREATE TABLE {table_name} (
                symbol VARCHAR NOT NULL,
                datetime TIMESTAMP NOT NULL,
                open DECIMAL(12,4) NOT NULL,
                high DECIMAL(12,4) NOT NULL,
                low DECIMAL(12,4) NOT NULL,
                close DECIMAL(12,4) NOT NULL,
                volume BIGINT NOT NULL,
                amount DECIMAL(20,2),
                adj_close DECIMAL(12,4),
                adj_factor DECIMAL(10,6),
                ma5 DECIMAL(12,4),
                ma10 DECIMAL(12,4),
                ma20 DECIMAL(12,4),
                ma60 DECIMAL(12,4),
                rsi_14 DECIMAL(8,4),
                macd_dif DECIMAL(8,4),
                macd_dea DECIMAL(8,4),
                kdj_k DECIMAL(8,4),
                kdj_d DECIMAL(8,4),
                kdj_j DECIMAL(8,4),
                vwap DECIMAL(12,4),
                bid_price DECIMAL(12,4),
                ask_price DECIMAL(12,4),
                spread DECIMAL(8,4),
                net_inflow_large DECIMAL(20,2),
                net_inflow_main DECIMAL(20,2),
                turnover_rate DECIMAL(8,4),
                amplitude DECIMAL(8,4),
                change_pct DECIMAL(8,4),
                plugin_specific_data JSON,
                data_source VARCHAR NOT NULL,
                data_quality_score DECIMAL(4,3),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (symbol, datetime)
            );
            """
            
            self.conn.execute(sql)
            
            # åˆ›å»ºç´¢å¼•
            self.conn.execute(f"CREATE INDEX idx_{table_name}_symbol_datetime ON {table_name}(symbol, datetime);")
            self.conn.execute(f"CREATE INDEX idx_{table_name}_datetime ON {table_name}(datetime);")
    
    def _table_exists(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        result = self.conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
            [table_name]
        ).fetchone()
        return result is not None
```

### 5. æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ

#### 5.1 æ•°æ®è´¨é‡ç›‘æ§å™¨
```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self, sqlite_path: str = "db/factorweave_system.db"):
        self.sqlite_path = sqlite_path
        self.conn = sqlite3.connect(sqlite_path)
    
    def calculate_quality_score(self, data: pd.DataFrame, data_type: str) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡ç»¼åˆè¯„åˆ†"""
        if data is None or data.empty:
            return 0.0
        
        scores = {}
        
        # å®Œæ•´æ€§æ£€æŸ¥
        scores['completeness'] = self._check_completeness(data)
        
        # å‡†ç¡®æ€§æ£€æŸ¥
        scores['accuracy'] = self._check_accuracy(data, data_type)
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        scores['consistency'] = self._check_consistency(data, data_type)
        
        # åŠæ—¶æ€§æ£€æŸ¥
        scores['timeliness'] = self._check_timeliness(data)
        
        # åŠ æƒè®¡ç®—ç»¼åˆè¯„åˆ†
        weights = {
            'completeness': 0.3,
            'accuracy': 0.3,
            'consistency': 0.2,
            'timeliness': 0.2
        }
        
        overall_score = sum(scores[key] * weights[key] for key in scores)
        
        return round(overall_score, 4)
    
    def _check_completeness(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        if data.empty:
            return 0.0
        
        total_cells = data.size
        null_cells = data.isnull().sum().sum()
        
        completeness = (total_cells - null_cells) / total_cells
        return completeness
    
    def _check_accuracy(self, data: pd.DataFrame, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
        accuracy_score = 1.0
        
        if data_type == "kline":
            # Kçº¿æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥
            if 'open' in data.columns and 'high' in data.columns and 'low' in data.columns and 'close' in data.columns:
                # æ£€æŸ¥OHLCé€»è¾‘å…³ç³»
                invalid_ohlc = (
                    (data['high'] < data['open']) |
                    (data['high'] < data['close']) |
                    (data['low'] > data['open']) |
                    (data['low'] > data['close'])
                )
                
                if invalid_ohlc.any():
                    accuracy_score -= 0.2
            
            # æ£€æŸ¥æˆäº¤é‡æ˜¯å¦ä¸ºè´Ÿæ•°
            if 'volume' in data.columns:
                if (data['volume'] < 0).any():
                    accuracy_score -= 0.1
        
        return max(0.0, accuracy_score)
    
    def _check_consistency(self, data: pd.DataFrame, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        consistency_score = 1.0
        
        # æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
        if 'datetime' in data.columns:
            data_sorted = data.sort_values('datetime')
            time_diffs = data_sorted['datetime'].diff()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„æ—¶é—´è·³è·ƒ
            if len(time_diffs) > 1:
                median_diff = time_diffs.median()
                outliers = time_diffs > median_diff * 3
                
                if outliers.any():
                    consistency_score -= 0.1
        
        return consistency_score
    
    def _check_timeliness(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®åŠæ—¶æ€§"""
        if 'datetime' in data.columns and not data.empty:
            latest_time = pd.to_datetime(data['datetime']).max()
            current_time = datetime.now()
            
            # è®¡ç®—æ•°æ®å»¶è¿Ÿï¼ˆåˆ†é’Ÿï¼‰
            delay_minutes = (current_time - latest_time).total_seconds() / 60
            
            # æ ¹æ®å»¶è¿Ÿæ—¶é—´è®¡ç®—åŠæ—¶æ€§è¯„åˆ†
            if delay_minutes <= 5:
                return 1.0
            elif delay_minutes <= 30:
                return 0.8
            elif delay_minutes <= 60:
                return 0.6
            elif delay_minutes <= 1440:  # 1å¤©
                return 0.4
            else:
                return 0.2
        
        return 1.0
```

### 6. å®æ–½è®¡åˆ’ä¸é›†æˆç­–ç•¥

#### 6.1 é˜¶æ®µåŒ–å®æ–½è®¡åˆ’

##### é˜¶æ®µä¸€ï¼šæ ¸å¿ƒæ‰©å±•ï¼ˆ2å‘¨ï¼‰
1. **æ‰©å±•ç°æœ‰æ¥å£**
   - æ‰©å±•IDataSourcePluginæ¥å£
   - å¢å¼ºStandardQueryå’ŒTETç®¡é“
   - æ‰©å±•UnifiedDataManager

2. **æ•°æ®åº“æ¶æ„æ‰©å±•**
   - æ‰©å±•ç°æœ‰SQLiteè¡¨ç»“æ„
   - åœ¨DuckDBä¸­åˆ›å»ºæ–°çš„æ•°æ®è¡¨
   - å®ç°PluginTableManager

##### é˜¶æ®µäºŒï¼šæ•°æ®è´¨é‡ç³»ç»Ÿï¼ˆ2å‘¨ï¼‰
1. **æ•°æ®è´¨é‡ç›‘æ§**
   - å®ç°DataQualityMonitor
   - é›†æˆåˆ°TETç®¡é“
   - åˆ›å»ºè´¨é‡æŠ¥å‘Šç•Œé¢

2. **å­—æ®µæ˜ å°„ç®¡ç†**
   - å®ç°FieldMappingManager
   - æ‰©å±•å­—æ®µæ˜ å°„é…ç½®
   - æ”¯æŒåŠ¨æ€æ˜ å°„æ›´æ–°

##### é˜¶æ®µä¸‰ï¼šæ’ä»¶ç”Ÿæ€æ‰©å±•ï¼ˆ3å‘¨ï¼‰
1. **ç°æœ‰æ’ä»¶å‡çº§**
   - å‡çº§HIkyuuæ•°æ®æ’ä»¶
   - æ·»åŠ æ–°çš„æ•°æ®æºæ’ä»¶
   - å®Œå–„æ’ä»¶å…ƒæ•°æ®

2. **æµ‹è¯•å’ŒéªŒè¯**
   - å•å…ƒæµ‹è¯•è¦†ç›–
   - é›†æˆæµ‹è¯•éªŒè¯
   - æ€§èƒ½åŸºå‡†æµ‹è¯•

##### é˜¶æ®µå››ï¼šä¼˜åŒ–å’Œéƒ¨ç½²ï¼ˆ2å‘¨ï¼‰
1. **æ€§èƒ½ä¼˜åŒ–**
   - DuckDBæŸ¥è¯¢ä¼˜åŒ–
   - ç¼“å­˜ç­–ç•¥è°ƒæ•´
   - å†…å­˜ä½¿ç”¨ä¼˜åŒ–

2. **æ–‡æ¡£å’ŒåŸ¹è®­**
   - æ›´æ–°å¼€å‘æ–‡æ¡£
   - åˆ›å»ºä½¿ç”¨æŒ‡å—
   - æ’ä»¶å¼€å‘æ•™ç¨‹

#### 6.2 ä¸ç°æœ‰ç³»ç»Ÿé›†æˆç­–ç•¥

##### 6.2.1 æ— ç¼é›†æˆåŸåˆ™
- **å‘åå…¼å®¹**: æ‰€æœ‰ç°æœ‰åŠŸèƒ½ä¿æŒä¸å˜
- **æ¸è¿›å¼å‡çº§**: é€æ­¥å¯ç”¨æ–°åŠŸèƒ½
- **é…ç½®é©±åŠ¨**: é€šè¿‡é…ç½®æ§åˆ¶æ–°åŠŸèƒ½å¯ç”¨

##### 6.2.2 é›†æˆæ£€æŸ¥æ¸…å•
- [ ] ç°æœ‰æ’ä»¶ç³»ç»Ÿå…¼å®¹æ€§æµ‹è¯•
- [ ] TETæ•°æ®ç®¡é“åŠŸèƒ½éªŒè¯
- [ ] æ•°æ®åº“è¿ç§»è„šæœ¬æµ‹è¯•
- [ ] UIç•Œé¢é€‚é…éªŒè¯
- [ ] æ€§èƒ½å½±å“è¯„ä¼°

## ğŸ“ˆ é¢„æœŸæ”¶ç›Šä¸æŠ€æœ¯ä¼˜åŠ¿

### æŠ€æœ¯æ”¶ç›Š
1. **å®Œå…¨å…¼å®¹**: ä¸ç°æœ‰ç³»ç»Ÿ100%å…¼å®¹ï¼Œæ— ç ´åæ€§å˜æ›´
2. **æ€§èƒ½æå‡**: DuckDBåˆ—å¼å­˜å‚¨ï¼ŒæŸ¥è¯¢æ€§èƒ½æå‡10-50å€
3. **æ•°æ®è´¨é‡**: å»ºç«‹å®Œå–„çš„æ•°æ®è´¨é‡ç›‘æ§å’Œè¯„åˆ†ä½“ç³»
4. **æ‰©å±•æ€§å¼º**: æ”¯æŒæ–°æ•°æ®æºæ’ä»¶çš„å¿«é€Ÿæ¥å…¥
5. **æ ‡å‡†ç»Ÿä¸€**: åŸºäºTETæ ‡å‡†ï¼Œç»Ÿä¸€æ‰€æœ‰æ•°æ®æºæ ¼å¼

### ä¸šåŠ¡æ”¶ç›Š
1. **è¡Œä¸šå¯¹æ ‡**: æ•°æ®å­—æ®µå®Œå…¨å¯¹æ ‡Windã€Bloombergç­‰ä¸“ä¸šè½¯ä»¶
2. **æ•°æ®ä¸°å¯Œ**: æ”¯æŒKçº¿ã€åŸºæœ¬é¢ã€è´¢åŠ¡ã€å®è§‚ç­‰å…¨æ–¹ä½æ•°æ®
3. **è´¨é‡ä¿éšœ**: æ•°æ®è´¨é‡å®æ—¶ç›‘æ§ï¼Œç¡®ä¿æ•°æ®å¯é æ€§
4. **å¼€å‘æ•ˆç‡**: æ ‡å‡†åŒ–æ¥å£ï¼Œé™ä½æ’ä»¶å¼€å‘æˆæœ¬
5. **ç”¨æˆ·ä½“éªŒ**: æ•°æ®æºåˆ‡æ¢æ— ç¼ï¼Œç”¨æˆ·ä½“éªŒä¸€è‡´

## ğŸ“‹ æ€»ç»“

æœ¬æ–¹æ¡ˆåŸºäºå¯¹ç°æœ‰ç³»ç»Ÿçš„æ·±å…¥åˆ†æï¼Œé‡‡ç”¨æ‰©å±•è€Œéé‡æ„çš„æ–¹å¼ï¼Œç¡®ä¿ä¸ç°æœ‰æ¶æ„çš„å®Œç¾é›†æˆã€‚é€šè¿‡å¢å¼ºTETæ•°æ®ç®¡é“ã€æ‰©å±•æ’ä»¶ç³»ç»Ÿã€ä¼˜åŒ–æ•°æ®åº“æ¶æ„ï¼Œå®ç°äº†å¤šæ•°æ®æºæ’ä»¶çš„ç»Ÿä¸€ç®¡ç†å’Œé«˜æ•ˆå­˜å‚¨ï¼ŒåŒæ—¶å»ºç«‹äº†å®Œå–„çš„æ•°æ®è´¨é‡ä¿éšœä½“ç³»ã€‚

è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼š
1. **æ— ç¼é›†æˆ**: å®Œå…¨åŸºäºç°æœ‰æ¶æ„æ‰©å±•ï¼Œæ— ç ´åæ€§å˜æ›´
2. **æ ‡å‡†ç»Ÿä¸€**: åŸºäºTETæ ‡å‡†å’Œè¡Œä¸šæœ€ä½³å®è·µ
3. **è´¨é‡ä¿éšœ**: å®Œå–„çš„æ•°æ®è´¨é‡ç›‘æ§å’Œè¯„åˆ†ä½“ç³»
4. **æ€§èƒ½ä¼˜åŒ–**: å……åˆ†åˆ©ç”¨DuckDBçš„æŠ€æœ¯ä¼˜åŠ¿
5. **æ‰©å±•æ€§å¼º**: æ”¯æŒæœªæ¥æ–°æ•°æ®æºçš„å¿«é€Ÿæ¥å…¥

è¿™å°†ä¸ºFactorWeave-Quantç³»ç»Ÿæä¾›å¼ºå¤§çš„å¤šæ•°æ®æºç»Ÿä¸€ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒå†å²kçº¿å’Œé‡‘èä¿¡æ¯åœ¨SQLite/DuckDBä¸­çš„é«˜æ•ˆå­˜å‚¨å’ŒæŸ¥è¯¢ï¼Œå®Œå…¨æ»¡è¶³ä¸“ä¸šé‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„éœ€æ±‚ã€‚ 