#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç®—æ³•ä¼˜åŒ–å™¨
ä½¿ç”¨é—ä¼ ç®—æ³•ã€è´å¶æ–¯ä¼˜åŒ–ç­‰æ–¹æ³•è‡ªåŠ¨ä¼˜åŒ–å½¢æ€è¯†åˆ«ç®—æ³•
"""

from analysis.pattern_manager import PatternManager
from optimization.version_manager import VersionManager
from core.performance import UnifiedPerformanceMonitor as PerformanceEvaluator, PerformanceMetric as PerformanceMetrics
import random
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional, Callable
from datetime import datetime
import json
import re
from dataclasses import dataclass

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    method: str = "genetic"  # genetic, bayesian, random, gradient
    max_iterations: int = 50
    population_size: int = 20
    mutation_rate: float = 0.1
    crossover_rate: float = 0.8
    target_metric: str = "overall_score"
    min_improvement: float = 0.05
    timeout_minutes: int = 30
    parallel_workers: int = 4


class AlgorithmOptimizer:
    """æ™ºèƒ½ç®—æ³•ä¼˜åŒ–å™¨"""

    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.evaluator = PerformanceEvaluator(debug_mode)
        self.version_manager = VersionManager()
        self.pattern_manager = PatternManager()

        # ä¼˜åŒ–å†å²
        self.optimization_history = []

    def optimize_algorithm(self, pattern_name: str,
                           config: OptimizationConfig = None,
                           test_datasets: List[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æŒ‡å®šå½¢æ€çš„ç®—æ³•

        Args:
            pattern_name: å½¢æ€åç§°
            config: ä¼˜åŒ–é…ç½®
            test_datasets: æµ‹è¯•æ•°æ®é›†

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        if config is None:
            config = OptimizationConfig()

        print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–ç®—æ³•: {pattern_name}")
        print(f"ğŸ“‹ ä¼˜åŒ–æ–¹æ³•: {config.method}")
        print(f"ç›®æ ‡æŒ‡æ ‡: {config.target_metric}")

        # è·å–å½“å‰ç®—æ³•é…ç½®
        pattern_config = self.pattern_manager.get_pattern_by_name(pattern_name)
        if not pattern_config:
            raise ValueError(f"æœªæ‰¾åˆ°å½¢æ€é…ç½®: {pattern_name}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if test_datasets is None:
            test_datasets = self.evaluator.create_test_datasets(
                pattern_name, count=5)

        # è¯„ä¼°åŸºå‡†æ€§èƒ½
        baseline_metrics = self.evaluator.evaluate_algorithm(
            pattern_name, test_datasets
        )

        print(f"åŸºå‡†æ€§èƒ½: {baseline_metrics.overall_score:.3f}")

        # å¼€å§‹ä¼˜åŒ–æ—¥å¿—
        session_id = self.version_manager.db_manager.start_optimization_log(
            pattern_name=pattern_name,
            optimization_method=config.method,
            initial_version_id=0,  # éœ€è¦è·å–å½“å‰ç‰ˆæœ¬ID
            config=config.__dict__
        )

        try:
            # æ ¹æ®ä¼˜åŒ–æ–¹æ³•é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
            if config.method == "genetic":
                result = self._genetic_optimization(
                    pattern_name, pattern_config, config, test_datasets, baseline_metrics
                )
            elif config.method == "bayesian":
                result = self._bayesian_optimization(
                    pattern_name, pattern_config, config, test_datasets, baseline_metrics
                )
            elif config.method == "random":
                result = self._random_optimization(
                    pattern_name, pattern_config, config, test_datasets, baseline_metrics
                )
            elif config.method == "gradient":
                result = self._gradient_optimization(
                    pattern_name, pattern_config, config, test_datasets, baseline_metrics
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–æ–¹æ³•: {config.method}")

            # æ›´æ–°ä¼˜åŒ–æ—¥å¿—
            self.version_manager.db_manager.update_optimization_log(
                session_id=session_id,
                status="completed",
                final_version_id=result.get('best_version_id'),
                iterations=result.get('iterations', 0),
                best_score=result.get('best_score', 0),
                improvement_percentage=result.get('improvement_percentage', 0),
                optimization_log=json.dumps(result.get('optimization_log', []))
            )

            print(f"âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"â†‘ æ€§èƒ½æå‡: {result.get('improvement_percentage', 0):.3f}%")

            return result

        except Exception as e:
            # æ›´æ–°ä¼˜åŒ–æ—¥å¿—ä¸ºå¤±è´¥çŠ¶æ€
            self.version_manager.db_manager.update_optimization_log(
                session_id=session_id,
                status="failed",
                error_message=str(e)
            )
            raise e

    def _genetic_optimization(self, pattern_name: str, pattern_config,
                              config: OptimizationConfig, test_datasets: List[pd.DataFrame],
                              baseline_metrics: PerformanceMetrics) -> Dict[str, Any]:
        """é—ä¼ ç®—æ³•ä¼˜åŒ–"""
        print("ğŸ§¬ ä½¿ç”¨é—ä¼ ç®—æ³•ä¼˜åŒ–...")

        # åˆå§‹åŒ–ç§ç¾¤
        population = self._initialize_population(
            pattern_config, config.population_size)

        best_individual = None
        best_score = baseline_metrics.overall_score
        optimization_log = []

        for generation in range(config.max_iterations):
            print(f"  ç¬¬ {generation + 1}/{config.max_iterations} ä»£")

            # è¯„ä¼°ç§ç¾¤
            fitness_scores = []
            for individual in population:
                try:
                    # åˆ›å»ºä¸´æ—¶ç®—æ³•é…ç½®
                    temp_config = self._create_temp_config(
                        pattern_config, individual)

                    # è¯„ä¼°æ€§èƒ½
                    metrics = self._evaluate_individual(
                        temp_config, test_datasets)
                    score = getattr(metrics, config.target_metric, 0)
                    fitness_scores.append(score)

                    # æ›´æ–°æœ€ä½³ä¸ªä½“
                    if score > best_score:
                        best_score = score
                        best_individual = individual.copy()

                        print(f"   å‘ç°æ›´å¥½çš„è§£: {score:.3f}")

                except Exception as e:
                    if self.debug_mode:
                        print(f"    âŒ ä¸ªä½“è¯„ä¼°å¤±è´¥: {e}")
                    fitness_scores.append(0.0)

            # è®°å½•å½“ä»£æœ€ä½³
            generation_best = max(fitness_scores) if fitness_scores else 0
            optimization_log.append({
                "generation": generation + 1,
                "best_score": generation_best,
                "avg_score": np.mean(fitness_scores) if fitness_scores else 0,
                "population_diversity": self._calculate_diversity(population)
            })

            # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
            if generation_best - baseline_metrics.overall_score < config.min_improvement:
                if generation > 10:  # è‡³å°‘è¿è¡Œ10ä»£
                    print(f"    ğŸ›‘ æ”¶æ•›ï¼Œæå‰åœæ­¢")
                    break

            # é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
            population = self._evolve_population(
                population, fitness_scores, config
            )

        # ä¿å­˜æœ€ä½³ç‰ˆæœ¬
        best_version_id = None
        if best_individual:
            best_version_id = self._save_optimized_version(
                pattern_name, pattern_config, best_individual,
                f"é—ä¼ ç®—æ³•ä¼˜åŒ– - ç¬¬{len(optimization_log)}ä»£", best_score
            )

        improvement_percentage = (
            best_score - baseline_metrics.overall_score) / baseline_metrics.overall_score * 100

        return {
            "method": "genetic",
            "best_score": best_score,
            "baseline_score": baseline_metrics.overall_score,
            "improvement_percentage": improvement_percentage,
            "iterations": len(optimization_log),
            "best_version_id": best_version_id,
            "optimization_log": optimization_log
        }

    def _bayesian_optimization(self, pattern_name: str, pattern_config,
                               config: OptimizationConfig, test_datasets: List[pd.DataFrame],
                               baseline_metrics: PerformanceMetrics) -> Dict[str, Any]:
        """è´å¶æ–¯ä¼˜åŒ–"""
        print("ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–...")

        # ç®€åŒ–çš„è´å¶æ–¯ä¼˜åŒ–å®ç°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨scikit-optimizeç­‰åº“

        best_individual = None
        best_score = baseline_metrics.overall_score
        optimization_log = []

        # å‚æ•°ç©ºé—´å®šä¹‰
        param_space = self._define_parameter_space(pattern_config)

        for iteration in range(config.max_iterations):
            print(f"  ç¬¬ {iteration + 1}/{config.max_iterations} æ¬¡è¿­ä»£")

            # é€‰æ‹©ä¸‹ä¸€ä¸ªå‚æ•°ç»„åˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if iteration < 5:
                # å‰å‡ æ¬¡éšæœºé‡‡æ ·
                individual = self._random_sample_parameters(param_space)
            else:
                # åŸºäºå†å²ç»“æœé€‰æ‹©æœ‰å¸Œæœ›çš„åŒºåŸŸ
                individual = self._bayesian_sample_parameters(
                    param_space, optimization_log)

            try:
                # è¯„ä¼°å‚æ•°ç»„åˆ
                temp_config = self._create_temp_config(
                    pattern_config, individual)
                metrics = self._evaluate_individual(temp_config, test_datasets)
                score = getattr(metrics, config.target_metric, 0)

                # è®°å½•ç»“æœ
                optimization_log.append({
                    "iteration": iteration + 1,
                    "parameters": individual,
                    "score": score
                })

                # æ›´æ–°æœ€ä½³ç»“æœ
                if score > best_score:
                    best_score = score
                    best_individual = individual.copy()
                    print(f"   å‘ç°æ›´å¥½çš„è§£: {score:.3f}")

            except Exception as e:
                if self.debug_mode:
                    print(f"    âŒ å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                optimization_log.append({
                    "iteration": iteration + 1,
                    "parameters": individual,
                    "score": 0.0,
                    "error": str(e)
                })

        # ä¿å­˜æœ€ä½³ç‰ˆæœ¬
        best_version_id = None
        if best_individual:
            best_version_id = self._save_optimized_version(
                pattern_name, pattern_config, best_individual,
                f"è´å¶æ–¯ä¼˜åŒ– - {len(optimization_log)}æ¬¡è¿­ä»£", best_score
            )

        improvement_percentage = (
            best_score - baseline_metrics.overall_score) / baseline_metrics.overall_score * 100

        return {
            "method": "bayesian",
            "best_score": best_score,
            "baseline_score": baseline_metrics.overall_score,
            "improvement_percentage": improvement_percentage,
            "iterations": len(optimization_log),
            "best_version_id": best_version_id,
            "optimization_log": optimization_log
        }

    def _random_optimization(self, pattern_name: str, pattern_config,
                             config: OptimizationConfig, test_datasets: List[pd.DataFrame],
                             baseline_metrics: PerformanceMetrics) -> Dict[str, Any]:
        """éšæœºæœç´¢ä¼˜åŒ–"""
        print("ğŸ² ä½¿ç”¨éšæœºæœç´¢ä¼˜åŒ–...")

        best_individual = None
        best_score = baseline_metrics.overall_score
        optimization_log = []

        param_space = self._define_parameter_space(pattern_config)

        for iteration in range(config.max_iterations):
            print(f"  ç¬¬ {iteration + 1}/{config.max_iterations} æ¬¡å°è¯•")

            # éšæœºé‡‡æ ·å‚æ•°
            individual = self._random_sample_parameters(param_space)

            try:
                temp_config = self._create_temp_config(
                    pattern_config, individual)
                metrics = self._evaluate_individual(temp_config, test_datasets)
                score = getattr(metrics, config.target_metric, 0)

                optimization_log.append({
                    "iteration": iteration + 1,
                    "parameters": individual,
                    "score": score
                })

                if score > best_score:
                    best_score = score
                    best_individual = individual.copy()
                    print(f"   å‘ç°æ›´å¥½çš„è§£: {score:.3f}")

            except Exception as e:
                if self.debug_mode:
                    print(f"    âŒ å‚æ•°è¯„ä¼°å¤±è´¥: {e}")

        # ä¿å­˜æœ€ä½³ç‰ˆæœ¬
        best_version_id = None
        if best_individual:
            best_version_id = self._save_optimized_version(
                pattern_name, pattern_config, best_individual,
                f"éšæœºæœç´¢ä¼˜åŒ– - {len(optimization_log)}æ¬¡å°è¯•", best_score
            )

        improvement_percentage = (
            best_score - baseline_metrics.overall_score) / baseline_metrics.overall_score * 100

        return {
            "method": "random",
            "best_score": best_score,
            "baseline_score": baseline_metrics.overall_score,
            "improvement_percentage": improvement_percentage,
            "iterations": len(optimization_log),
            "best_version_id": best_version_id,
            "optimization_log": optimization_log
        }

    def _gradient_optimization(self, pattern_name: str, pattern_config,
                               config: OptimizationConfig, test_datasets: List[pd.DataFrame],
                               baseline_metrics: PerformanceMetrics) -> Dict[str, Any]:
        """æ¢¯åº¦ä¼˜åŒ–ï¼ˆæ•°å€¼æ¢¯åº¦ï¼‰"""
        print("â†‘ ä½¿ç”¨æ¢¯åº¦ä¼˜åŒ–...")

        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ•°å€¼æ¢¯åº¦å®ç°
        # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ¢¯åº¦è®¡ç®—

        current_params = self._extract_numeric_parameters(pattern_config)
        best_score = baseline_metrics.overall_score
        optimization_log = []

        learning_rate = 0.01

        for iteration in range(config.max_iterations):
            print(f"  ç¬¬ {iteration + 1}/{config.max_iterations} æ¬¡è¿­ä»£")

            # è®¡ç®—æ•°å€¼æ¢¯åº¦
            gradients = {}
            for param_name, param_value in current_params.items():
                if isinstance(param_value, (int, float)):
                    # è®¡ç®—åå¯¼æ•°
                    epsilon = abs(param_value) * 0.01 + 1e-6

                    # æ­£å‘æ‰°åŠ¨
                    params_plus = current_params.copy()
                    params_plus[param_name] = param_value + epsilon
                    score_plus = self._evaluate_params(
                        pattern_config, params_plus, test_datasets, config.target_metric)

                    # è´Ÿå‘æ‰°åŠ¨
                    params_minus = current_params.copy()
                    params_minus[param_name] = param_value - epsilon
                    score_minus = self._evaluate_params(
                        pattern_config, params_minus, test_datasets, config.target_metric)

                    # è®¡ç®—æ¢¯åº¦
                    gradient = (score_plus - score_minus) / (2 * epsilon)
                    gradients[param_name] = gradient

            # æ›´æ–°å‚æ•°
            for param_name, gradient in gradients.items():
                current_params[param_name] += learning_rate * gradient

                # å‚æ•°çº¦æŸ
                current_params[param_name] = max(
                    0.01, min(10.0, current_params[param_name]))

            # è¯„ä¼°å½“å‰å‚æ•°
            current_score = self._evaluate_params(
                pattern_config, current_params, test_datasets, config.target_metric)

            optimization_log.append({
                "iteration": iteration + 1,
                "parameters": current_params.copy(),
                "score": current_score,
                "gradients": gradients.copy()
            })

            if current_score > best_score:
                best_score = current_score
                print(f"   å‘ç°æ›´å¥½çš„è§£: {current_score:.3f}")

        # ä¿å­˜æœ€ä½³ç‰ˆæœ¬
        best_version_id = self._save_optimized_version(
            pattern_name, pattern_config, current_params,
            f"æ¢¯åº¦ä¼˜åŒ– - {len(optimization_log)}æ¬¡è¿­ä»£", best_score
        )

        improvement_percentage = (
            best_score - baseline_metrics.overall_score) / baseline_metrics.overall_score * 100

        return {
            "method": "gradient",
            "best_score": best_score,
            "baseline_score": baseline_metrics.overall_score,
            "improvement_percentage": improvement_percentage,
            "iterations": len(optimization_log),
            "best_version_id": best_version_id,
            "optimization_log": optimization_log
        }

    def _initialize_population(self, pattern_config, population_size: int) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–é—ä¼ ç®—æ³•ç§ç¾¤"""
        population = []
        param_space = self._define_parameter_space(pattern_config)

        for _ in range(population_size):
            individual = self._random_sample_parameters(param_space)
            population.append(individual)

        return population

    def _define_parameter_space(self, pattern_config) -> Dict[str, Dict[str, Any]]:
        """å®šä¹‰å‚æ•°æœç´¢ç©ºé—´"""
        param_space = {}

        # ä»ç®—æ³•ä»£ç ä¸­æå–å¯ä¼˜åŒ–çš„å‚æ•°
        if pattern_config.parameters:
            for param_name, param_value in pattern_config.parameters.items():
                if isinstance(param_value, (int, float)):
                    param_space[param_name] = {
                        "type": "numeric",
                        "min": param_value * 0.5,
                        "max": param_value * 2.0,
                        "current": param_value
                    }
                elif isinstance(param_value, bool):
                    param_space[param_name] = {
                        "type": "boolean",
                        "current": param_value
                    }

        # æ·»åŠ é€šç”¨çš„å½¢æ€è¯†åˆ«å‚æ•°
        param_space.update({
            "confidence_threshold": {
                "type": "numeric",
                "min": 0.1,
                "max": 0.9,
                "current": pattern_config.confidence_threshold
            },
            "min_body_ratio": {
                "type": "numeric",
                "min": 0.1,
                "max": 0.8,
                "current": 0.3
            },
            "shadow_ratio_threshold": {
                "type": "numeric",
                "min": 1.5,
                "max": 4.0,
                "current": 2.0
            }
        })

        return param_space

    def _random_sample_parameters(self, param_space: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """éšæœºé‡‡æ ·å‚æ•°"""
        individual = {}

        for param_name, param_info in param_space.items():
            if param_info["type"] == "numeric":
                value = random.uniform(param_info["min"], param_info["max"])
                individual[param_name] = value
            elif param_info["type"] == "boolean":
                individual[param_name] = random.choice([True, False])

        return individual

    def _bayesian_sample_parameters(self, param_space: Dict[str, Dict[str, Any]],
                                    history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºäºè´å¶æ–¯ä¼˜åŒ–é‡‡æ ·å‚æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨ä¸“ä¸šçš„è´å¶æ–¯ä¼˜åŒ–åº“

        if len(history) < 3:
            return self._random_sample_parameters(param_space)

        # æ‰¾åˆ°å†å²æœ€ä½³å‚æ•°
        best_entry = max(history, key=lambda x: x.get("score", 0))
        best_params = best_entry.get("parameters", {})

        # åœ¨æœ€ä½³å‚æ•°é™„è¿‘é‡‡æ ·
        individual = {}
        for param_name, param_info in param_space.items():
            if param_info["type"] == "numeric":
                if param_name in best_params:
                    # åœ¨æœ€ä½³å€¼é™„è¿‘æ·»åŠ å™ªå£°
                    best_value = best_params[param_name]
                    noise_scale = (param_info["max"] - param_info["min"]) * 0.1
                    value = best_value + random.gauss(0, noise_scale)
                    value = max(param_info["min"], min(
                        param_info["max"], value))
                else:
                    value = random.uniform(
                        param_info["min"], param_info["max"])
                individual[param_name] = value
            elif param_info["type"] == "boolean":
                if param_name in best_params:
                    # 80%æ¦‚ç‡ä¿æŒæœ€ä½³å€¼
                    if random.random() < 0.8:
                        individual[param_name] = best_params[param_name]
                    else:
                        individual[param_name] = not best_params[param_name]
                else:
                    individual[param_name] = random.choice([True, False])

        return individual

    def _create_temp_config(self, base_config, parameters: Dict[str, Any]):
        """åˆ›å»ºä¸´æ—¶é…ç½®"""
        temp_config = base_config
        temp_config.parameters = parameters
        return temp_config

    def _evaluate_individual(self, config, test_datasets: List[pd.DataFrame]) -> PerformanceMetrics:
        """è¯„ä¼°ä¸ªä½“æ€§èƒ½"""
        # è¿™é‡Œéœ€è¦å®ç°ä¸´æ—¶ç®—æ³•çš„åˆ›å»ºå’Œè¯„ä¼°
        # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°å™¨
        return self.evaluator.evaluate_algorithm(
            config.english_name, test_datasets
        )

    def _evaluate_params(self, pattern_config, parameters: Dict[str, Any],
                         test_datasets: List[pd.DataFrame], target_metric: str) -> float:
        """è¯„ä¼°å‚æ•°ç»„åˆ"""
        try:
            temp_config = self._create_temp_config(pattern_config, parameters)
            metrics = self._evaluate_individual(temp_config, test_datasets)
            return getattr(metrics, target_metric, 0)
        except:
            return 0.0

    def _evolve_population(self, population: List[Dict[str, Any]],
                           fitness_scores: List[float],
                           config: OptimizationConfig) -> List[Dict[str, Any]]:
        """è¿›åŒ–ç§ç¾¤"""
        new_population = []

        # ç²¾è‹±ä¿ç•™
        elite_count = max(1, int(config.population_size * 0.1))
        elite_indices = sorted(range(len(fitness_scores)),
                               key=lambda i: fitness_scores[i], reverse=True)[:elite_count]

        for idx in elite_indices:
            new_population.append(population[idx].copy())

        # ç”Ÿæˆæ–°ä¸ªä½“
        while len(new_population) < config.population_size:
            # é€‰æ‹©çˆ¶æ¯
            parent1 = self._tournament_selection(population, fitness_scores)
            parent2 = self._tournament_selection(population, fitness_scores)

            # äº¤å‰
            if random.random() < config.crossover_rate:
                child1, child2 = self._crossover(parent1, parent2)
            else:
                child1, child2 = parent1.copy(), parent2.copy()

            # å˜å¼‚
            if random.random() < config.mutation_rate:
                child1 = self._mutate(child1)
            if random.random() < config.mutation_rate:
                child2 = self._mutate(child2)

            new_population.extend([child1, child2])

        return new_population[:config.population_size]

    def _tournament_selection(self, population: List[Dict[str, Any]],
                              fitness_scores: List[float], tournament_size: int = 3) -> Dict[str, Any]:
        """é”¦æ ‡èµ›é€‰æ‹©"""
        tournament_indices = random.sample(range(len(population)),
                                           min(tournament_size, len(population)))
        best_idx = max(tournament_indices, key=lambda i: fitness_scores[i])
        return population[best_idx].copy()

    def _crossover(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """äº¤å‰æ“ä½œ"""
        child1 = parent1.copy()
        child2 = parent2.copy()

        # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œäº¤å‰
        for key in parent1.keys():
            if random.random() < 0.5:
                child1[key], child2[key] = child2[key], child1[key]

        return child1, child2

    def _mutate(self, individual: Dict[str, Any]) -> Dict[str, Any]:
        """å˜å¼‚æ“ä½œ"""
        mutated = individual.copy()

        for key, value in mutated.items():
            if random.random() < 0.1:  # 10%çš„å‚æ•°å‘ç”Ÿå˜å¼‚
                if isinstance(value, float):
                    # é«˜æ–¯å˜å¼‚
                    noise = random.gauss(0, abs(value) * 0.1 + 0.01)
                    mutated[key] = max(0.01, value + noise)
                elif isinstance(value, bool):
                    mutated[key] = not value

        return mutated

    def _calculate_diversity(self, population: List[Dict[str, Any]]) -> float:
        """è®¡ç®—ç§ç¾¤å¤šæ ·æ€§"""
        if len(population) < 2:
            return 0.0

        # ç®€åŒ–çš„å¤šæ ·æ€§è®¡ç®—
        total_distance = 0
        count = 0

        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                distance = self._calculate_individual_distance(
                    population[i], population[j])
                total_distance += distance
                count += 1

        return total_distance / count if count > 0 else 0.0

    def _calculate_individual_distance(self, ind1: Dict[str, Any], ind2: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸ªä½“é—´è·ç¦»"""
        distance = 0.0
        common_keys = set(ind1.keys()) & set(ind2.keys())

        for key in common_keys:
            val1, val2 = ind1[key], ind2[key]
            if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                distance += abs(val1 - val2)
            elif isinstance(val1, bool) and isinstance(val2, bool):
                distance += 0 if val1 == val2 else 1

        return distance

    def _extract_numeric_parameters(self, pattern_config) -> Dict[str, float]:
        """æå–æ•°å€¼å‚æ•°"""
        numeric_params = {}

        if pattern_config.parameters:
            for key, value in pattern_config.parameters.items():
                if isinstance(value, (int, float)):
                    numeric_params[key] = float(value)

        # æ·»åŠ é»˜è®¤å‚æ•°
        numeric_params.update({
            "confidence_threshold": pattern_config.confidence_threshold,
            "min_body_ratio": 0.3,
            "shadow_ratio_threshold": 2.0
        })

        return numeric_params

    def _save_optimized_version(self, pattern_name: str, base_config,
                                optimized_params: Dict[str, Any],
                                description: str, score: float) -> int:
        """ä¿å­˜ä¼˜åŒ–åçš„ç‰ˆæœ¬"""
        # ç”Ÿæˆä¼˜åŒ–åçš„ç®—æ³•ä»£ç 
        optimized_code = self._generate_optimized_code(
            base_config, optimized_params)

        # ä¿å­˜ç‰ˆæœ¬
        version_id = self.version_manager.save_version(
            pattern_id=0,  # éœ€è¦è·å–æ­£ç¡®çš„pattern_id
            pattern_name=pattern_name,
            algorithm_code=optimized_code,
            parameters=optimized_params,
            description=description,
            optimization_method="auto_optimization"
        )

        return version_id

    def _generate_optimized_code(self, base_config, optimized_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¼˜åŒ–åçš„ç®—æ³•ä»£ç """
        # è¿™é‡Œéœ€è¦å®ç°ä»£ç ç”Ÿæˆé€»è¾‘
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›åŸå§‹ä»£ç å¹¶æ›´æ–°å‚æ•°

        original_code = base_config.algorithm_code

        # æ›¿æ¢å‚æ•°å€¼
        optimized_code = original_code
        for param_name, param_value in optimized_params.items():
            # ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ASTè§£æï¼‰
            pattern = f"{param_name}\\s*=\\s*[\\d\\.]+|{param_name}\\s*=\\s*True|{param_name}\\s*=\\s*False"
            replacement = f"{param_name} = {param_value}"
            optimized_code = re.sub(pattern, replacement, optimized_code)

        return optimized_code


def create_algorithm_optimizer(debug_mode: bool = False) -> AlgorithmOptimizer:
    """åˆ›å»ºç®—æ³•ä¼˜åŒ–å™¨å®ä¾‹"""
    return AlgorithmOptimizer(debug_mode=debug_mode)


if __name__ == "__main__":
    # æµ‹è¯•ç®—æ³•ä¼˜åŒ–å™¨
    optimizer = create_algorithm_optimizer(debug_mode=True)

    # åˆ›å»ºä¼˜åŒ–é…ç½®
    config = OptimizationConfig(
        method="genetic",
        max_iterations=10,
        population_size=5
    )

    # ä¼˜åŒ–é”¤å¤´çº¿ç®—æ³•
    result = optimizer.optimize_algorithm("hammer", config)

    print(f"\nä¼˜åŒ–ç»“æœ:")
    print(f"  æ–¹æ³•: {result['method']}")
    print(f"  æœ€ä½³è¯„åˆ†: {result['best_score']:.3f}")
    print(f"  åŸºå‡†è¯„åˆ†: {result['baseline_score']:.3f}")
    print(f"  æ€§èƒ½æå‡: {result['improvement_percentage']:.3f}%")
    print(f"  è¿­ä»£æ¬¡æ•°: {result['iterations']}")
    print(f"  æœ€ä½³ç‰ˆæœ¬ID: {result['best_version_id']}")
