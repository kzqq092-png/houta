#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®—æ³•æ€§èƒ½è¯„ä¼°å™¨
æä¾›å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡è¯„ä¼°ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€æ€§èƒ½ã€ä¸šåŠ¡å’Œç¨³å®šæ€§æŒ‡æ ‡
"""

from analysis.pattern_base import PatternAlgorithmFactory, PatternResult
from analysis.pattern_manager import PatternManager
import time
import psutil
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
import tracemalloc
from dataclasses import dataclass

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    # å‡†ç¡®æ€§æŒ‡æ ‡
    true_positives: int = 0
    false_positives: int = 0
    true_negatives: int = 0
    false_negatives: int = 0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    accuracy: float = 0.0

    # æ€§èƒ½æŒ‡æ ‡
    execution_time: float = 0.0
    memory_usage: float = 0.0
    cpu_usage: float = 0.0

    # ä¸šåŠ¡æŒ‡æ ‡
    signal_quality: float = 0.0
    confidence_avg: float = 0.0
    confidence_std: float = 0.0
    patterns_found: int = 0

    # ç¨³å®šæ€§æŒ‡æ ‡
    robustness_score: float = 0.0
    parameter_sensitivity: float = 0.0

    # ç»¼åˆè¯„åˆ†
    overall_score: float = 0.0


class PerformanceEvaluator:
    """ç®—æ³•æ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.manager = PatternManager()

    def evaluate_algorithm(self, pattern_name: str, test_datasets: List[pd.DataFrame],
                           ground_truth: Optional[List[List[Dict]]] = None,
                           test_conditions: Dict[str, Any] = None) -> PerformanceMetrics:
        """
        è¯„ä¼°ç®—æ³•æ€§èƒ½

        Args:
            pattern_name: å½¢æ€åç§°
            test_datasets: æµ‹è¯•æ•°æ®é›†åˆ—è¡¨
            ground_truth: çœŸå®æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            test_conditions: æµ‹è¯•æ¡ä»¶

        Returns:
            æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ğŸ” å¼€å§‹è¯„ä¼°ç®—æ³•: {pattern_name}")

        # è·å–å½¢æ€é…ç½®
        config = self.manager.get_pattern_by_name(pattern_name)
        if not config:
            raise ValueError(f"æœªæ‰¾åˆ°å½¢æ€é…ç½®: {pattern_name}")

        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = PatternAlgorithmFactory.create(config)

        metrics = PerformanceMetrics()
        all_results = []
        execution_times = []
        memory_usages = []
        cpu_usages = []

        # å¯¹æ¯ä¸ªæµ‹è¯•æ•°æ®é›†è¿›è¡Œè¯„ä¼°
        for i, dataset in enumerate(test_datasets):
            print(f"  æµ‹è¯•æ•°æ®é›† {i+1}/{len(test_datasets)}")

            # æ€§èƒ½ç›‘æ§
            start_time = time.time()
            tracemalloc.start()
            cpu_start = psutil.cpu_percent()

            try:
                # æ‰§è¡Œè¯†åˆ«
                results = recognizer.recognize(dataset)
                all_results.extend(results)

                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                end_time = time.time()
                execution_time = end_time - start_time
                execution_times.append(execution_time)

                # å†…å­˜ä½¿ç”¨
                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)  # MB
                tracemalloc.stop()

                # CPUä½¿ç”¨
                cpu_end = psutil.cpu_percent()
                cpu_usages.append(max(0, cpu_end - cpu_start))

                if self.debug_mode:
                    print(f"    â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
                    print(f"    ğŸ’¾ å†…å­˜ä½¿ç”¨: {peak/1024/1024:.3f}MB")
                    print(f"    ğŸ”¢ è¯†åˆ«ç»“æœ: {len(results)}ä¸ªå½¢æ€")

            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                if self.debug_mode:
                    traceback.print_exc()
                continue

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        metrics.execution_time = np.mean(execution_times) if execution_times else 0
        metrics.memory_usage = np.mean(memory_usages) if memory_usages else 0
        metrics.cpu_usage = np.mean(cpu_usages) if cpu_usages else 0
        metrics.patterns_found = len(all_results)

        # è®¡ç®—ä¸šåŠ¡æŒ‡æ ‡
        if all_results:
            confidences = [r.confidence for r in all_results]
            metrics.confidence_avg = np.mean(confidences)
            metrics.confidence_std = np.std(confidences)
            metrics.signal_quality = self._calculate_signal_quality(all_results)

        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼‰
        if ground_truth:
            accuracy_metrics = self._calculate_accuracy_metrics(all_results, ground_truth)
            metrics.true_positives = accuracy_metrics['tp']
            metrics.false_positives = accuracy_metrics['fp']
            metrics.true_negatives = accuracy_metrics['tn']
            metrics.false_negatives = accuracy_metrics['fn']
            metrics.precision = accuracy_metrics['precision']
            metrics.recall = accuracy_metrics['recall']
            metrics.f1_score = accuracy_metrics['f1_score']
            metrics.accuracy = accuracy_metrics['accuracy']

        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        metrics.robustness_score = self._calculate_robustness_score(
            pattern_name, test_datasets, recognizer
        )

        # è®¡ç®—å‚æ•°æ•æ„Ÿæ€§
        metrics.parameter_sensitivity = self._calculate_parameter_sensitivity(
            pattern_name, test_datasets[0] if test_datasets else None
        )

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        metrics.overall_score = self._calculate_overall_score(metrics)

        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")
        return metrics

    def _calculate_signal_quality(self, results: List[PatternResult]) -> float:
        """è®¡ç®—ä¿¡å·è´¨é‡"""
        if not results:
            return 0.0

        # åŸºäºç½®ä¿¡åº¦åˆ†å¸ƒå’Œä¿¡å·ä¸€è‡´æ€§è®¡ç®—è´¨é‡
        confidences = [r.confidence for r in results]

        # é«˜ç½®ä¿¡åº¦ç»“æœçš„æ¯”ä¾‹
        high_confidence_ratio = sum(1 for c in confidences if c > 0.7) / len(confidences)

        # ç½®ä¿¡åº¦çš„ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå¥½ï¼‰
        confidence_stability = 1.0 - min(1.0, np.std(confidences))

        # ä¿¡å·å¼ºåº¦ï¼ˆå¹³å‡ç½®ä¿¡åº¦ï¼‰
        signal_strength = np.mean(confidences)

        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality = (high_confidence_ratio * 0.4 +
                   confidence_stability * 0.3 +
                   signal_strength * 0.3)

        return quality

    def _calculate_accuracy_metrics(self, results: List[PatternResult],
                                    ground_truth: List[List[Dict]]) -> Dict[str, float]:
        """è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡"""
        # è¿™é‡Œéœ€è¦å®ç°ä¸çœŸå®æ ‡ç­¾çš„æ¯”è¾ƒé€»è¾‘
        # ç”±äºæ²¡æœ‰æ ‡å‡†çš„çœŸå®æ ‡ç­¾ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶

        tp = fp = tn = fn = 0

        # TODO: å®ç°å…·ä½“çš„å‡†ç¡®æ€§è®¡ç®—é€»è¾‘
        # è¿™éœ€è¦æ ¹æ®å…·ä½“çš„æ ‡æ³¨æ•°æ®æ ¼å¼æ¥å®ç°

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0

        return {
            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
            'precision': precision, 'recall': recall,
            'f1_score': f1_score, 'accuracy': accuracy
        }

    def _calculate_robustness_score(self, pattern_name: str,
                                    test_datasets: List[pd.DataFrame],
                                    recognizer) -> float:
        """è®¡ç®—é²æ£’æ€§è¯„åˆ†"""
        if len(test_datasets) < 2:
            return 0.5  # é»˜è®¤ä¸­ç­‰é²æ£’æ€§

        results_consistency = []

        # åœ¨ä¸åŒæ•°æ®é›†ä¸Šæµ‹è¯•ä¸€è‡´æ€§
        for dataset in test_datasets:
            try:
                results = recognizer.recognize(dataset)
                # è®¡ç®—ç»“æœçš„ä¸€è‡´æ€§æŒ‡æ ‡
                if results:
                    avg_confidence = np.mean([r.confidence for r in results])
                    results_consistency.append(avg_confidence)
                else:
                    results_consistency.append(0.0)
            except:
                results_consistency.append(0.0)

        if not results_consistency:
            return 0.0

        # ä¸€è‡´æ€§è¶Šé«˜ï¼Œé²æ£’æ€§è¶Šå¥½
        consistency_score = 1.0 - np.std(results_consistency) / (np.mean(results_consistency) + 1e-6)
        return max(0.0, min(1.0, consistency_score))

    def _calculate_parameter_sensitivity(self, pattern_name: str,
                                         test_dataset: Optional[pd.DataFrame]) -> float:
        """è®¡ç®—å‚æ•°æ•æ„Ÿæ€§"""
        if test_dataset is None:
            return 0.5

        try:
            config = self.manager.get_pattern_by_name(pattern_name)
            if not config or not config.parameters:
                return 0.5

            # è·å–åŸºå‡†ç»“æœ
            base_recognizer = PatternAlgorithmFactory.create(config)
            base_results = base_recognizer.recognize(test_dataset)
            base_score = np.mean([r.confidence for r in base_results]) if base_results else 0

            # æµ‹è¯•å‚æ•°å˜åŒ–å¯¹ç»“æœçš„å½±å“
            sensitivity_scores = []

            for param_name, param_value in config.parameters.items():
                if isinstance(param_value, (int, float)):
                    # æµ‹è¯•å‚æ•°å˜åŒ–Â±20%
                    for factor in [0.8, 1.2]:
                        try:
                            modified_params = config.parameters.copy()
                            modified_params[param_name] = param_value * factor

                            # åˆ›å»ºä¿®æ”¹å‚æ•°çš„é…ç½®
                            modified_config = config
                            modified_config.parameters = modified_params

                            modified_recognizer = PatternAlgorithmFactory.create(modified_config)
                            modified_results = modified_recognizer.recognize(test_dataset)
                            modified_score = np.mean([r.confidence for r in modified_results]) if modified_results else 0

                            # è®¡ç®—æ•æ„Ÿæ€§
                            if base_score > 0:
                                sensitivity = abs(modified_score - base_score) / base_score
                                sensitivity_scores.append(sensitivity)
                        except:
                            continue

            # æ•æ„Ÿæ€§è¶Šä½è¶Šå¥½
            if sensitivity_scores:
                avg_sensitivity = np.mean(sensitivity_scores)
                return max(0.0, min(1.0, 1.0 - avg_sensitivity))
            else:
                return 0.5

        except Exception as e:
            if self.debug_mode:
                print(f"å‚æ•°æ•æ„Ÿæ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    def _calculate_overall_score(self, metrics: PerformanceMetrics) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        scores = []
        weights = []

        # ä¸šåŠ¡æŒ‡æ ‡æƒé‡æœ€é«˜
        if metrics.signal_quality > 0:
            scores.append(metrics.signal_quality)
            weights.append(0.3)

        if metrics.confidence_avg > 0:
            scores.append(metrics.confidence_avg)
            weights.append(0.2)

        # æ€§èƒ½æŒ‡æ ‡
        if metrics.execution_time > 0:
            # æ‰§è¡Œæ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¯„åˆ†
            time_score = max(0, min(1.0, 1.0 - metrics.execution_time / 10.0))
            scores.append(time_score)
            weights.append(0.15)

        # ç¨³å®šæ€§æŒ‡æ ‡
        if metrics.robustness_score > 0:
            scores.append(metrics.robustness_score)
            weights.append(0.15)

        if metrics.parameter_sensitivity > 0:
            scores.append(metrics.parameter_sensitivity)
            weights.append(0.1)

        # å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if metrics.f1_score > 0:
            scores.append(metrics.f1_score)
            weights.append(0.1)

        if not scores:
            return 0.5  # é»˜è®¤è¯„åˆ†

        # åŠ æƒå¹³å‡
        weighted_score = sum(s * w for s, w in zip(scores, weights)) / sum(weights)
        return max(0.0, min(1.0, weighted_score))

    def create_test_datasets(self, pattern_name: str, count: int = 5) -> List[pd.DataFrame]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        print(f"ä¸º {pattern_name} åˆ›å»º {count} ä¸ªæµ‹è¯•æ•°æ®é›†")

        datasets = []

        for i in range(count):
            # åˆ›å»ºä¸åŒå¸‚åœºæ¡ä»¶çš„æµ‹è¯•æ•°æ®
            dataset = self._generate_test_data(
                periods=200,
                volatility=0.02 + i * 0.01,  # ä¸åŒæ³¢åŠ¨ç‡
                trend=0.001 * (i - 2),       # ä¸åŒè¶‹åŠ¿
                pattern_injection=True
            )
            datasets.append(dataset)

        return datasets

    def _generate_test_data(self, periods: int = 200, volatility: float = 0.02,
                            trend: float = 0.0, pattern_injection: bool = True) -> pd.DataFrame:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        dates = pd.date_range(start='2023-01-01', periods=periods, freq='D')
        data = []

        base_price = 100.0

        for i, date in enumerate(dates):
            # è¶‹åŠ¿å’Œéšæœºæ³¢åŠ¨
            price_change = trend + np.random.normal(0, volatility)
            base_price *= (1 + price_change)

            # ç”ŸæˆOHLC
            open_price = base_price
            close_price = base_price * (1 + np.random.normal(0, volatility * 0.5))
            high_price = max(open_price, close_price) * (1 + abs(np.random.normal(0, volatility * 0.3)))
            low_price = min(open_price, close_price) * (1 - abs(np.random.normal(0, volatility * 0.3)))

            data.append({
                'datetime': date,
                'open': round(open_price, 2),
                'high': round(high_price, 2),
                'low': round(low_price, 2),
                'close': round(close_price, 2),
                'volume': np.random.randint(800000, 1500000)
            })

        return pd.DataFrame(data)

    def benchmark_against_baseline(self, pattern_name: str,
                                   current_metrics: PerformanceMetrics,
                                   baseline_metrics: Optional[PerformanceMetrics] = None) -> Dict[str, float]:
        """ä¸åŸºå‡†è¿›è¡Œå¯¹æ¯”"""
        if baseline_metrics is None:
            # ä½¿ç”¨é»˜è®¤åŸºå‡†
            baseline_metrics = PerformanceMetrics(
                signal_quality=0.5,
                confidence_avg=0.5,
                execution_time=1.0,
                robustness_score=0.5,
                overall_score=0.5
            )

        improvements = {}

        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„æ”¹è¿›ç™¾åˆ†æ¯”
        metrics_to_compare = [
            'signal_quality', 'confidence_avg', 'robustness_score',
            'parameter_sensitivity', 'overall_score'
        ]

        for metric in metrics_to_compare:
            current_value = getattr(current_metrics, metric)
            baseline_value = getattr(baseline_metrics, metric)

            if baseline_value > 0:
                improvement = (current_value - baseline_value) / baseline_value * 100
                improvements[metric] = improvement
            else:
                improvements[metric] = 0.0

        # æ‰§è¡Œæ—¶é—´æ”¹è¿›ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        if baseline_metrics.execution_time > 0:
            time_improvement = (baseline_metrics.execution_time - current_metrics.execution_time) / baseline_metrics.execution_time * 100
            improvements['execution_time'] = time_improvement

        return improvements


def create_performance_evaluator(debug_mode: bool = False) -> PerformanceEvaluator:
    """åˆ›å»ºæ€§èƒ½è¯„ä¼°å™¨å®ä¾‹"""
    return PerformanceEvaluator(debug_mode=debug_mode)


if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨
    evaluator = create_performance_evaluator(debug_mode=True)

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_datasets = evaluator.create_test_datasets("hammer", count=3)

    # è¯„ä¼°é”¤å¤´çº¿ç®—æ³•
    metrics = evaluator.evaluate_algorithm("hammer", test_datasets)

    print(f"\næ€§èƒ½è¯„ä¼°ç»“æœ:")
    print(f"  ç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")
    print(f"  ä¿¡å·è´¨é‡: {metrics.signal_quality:.3f}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {metrics.confidence_avg:.3f}")
    print(f"  æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.3f}ç§’")
    print(f"  é²æ£’æ€§: {metrics.robustness_score:.3f}")
    print(f"  è¯†åˆ«å½¢æ€æ•°: {metrics.patterns_found}")
