#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨è°ƒä¼˜å™¨
æä¾›ä¸€é”®ä¼˜åŒ–ã€æ‰¹é‡ä¼˜åŒ–å’Œæ™ºèƒ½è°ƒåº¦åŠŸèƒ½
"""

from analysis.pattern_manager import PatternManager
from optimization.database_schema import OptimizationDatabaseManager
from optimization.version_manager import VersionManager
from core.performance import UnifiedPerformanceMonitor as PerformanceEvaluator
from optimization.algorithm_optimizer import AlgorithmOptimizer, OptimizationConfig
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
import json

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


@dataclass
class TuningTask:
    """è°ƒä¼˜ä»»åŠ¡"""
    pattern_name: str
    priority: int = 5  # 1-10ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
    config: OptimizationConfig = None
    status: str = "pending"  # pending, running, completed, failed
    progress: float = 0.0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None


class AutoTuner:
    """è‡ªåŠ¨è°ƒä¼˜å™¨"""

    def __init__(self, max_workers: int = os.cpu_count() * 2, debug_mode: bool = False):
        self.max_workers = max_workers
        self.debug_mode = debug_mode

        # æ ¸å¿ƒç»„ä»¶
        self.optimizer = AlgorithmOptimizer(debug_mode)
        self.evaluator = PerformanceEvaluator(debug_mode)
        self.version_manager = VersionManager()
        self.pattern_manager = PatternManager()
        self.db_manager = OptimizationDatabaseManager()

        # ä»»åŠ¡ç®¡ç†
        self.task_queue: List[TuningTask] = []
        self.running_tasks: Dict[str, TuningTask] = {}
        self.completed_tasks: List[TuningTask] = []

        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.is_running = False

        # å›è°ƒå‡½æ•°
        self.progress_callback: Optional[Callable] = None
        self.completion_callback: Optional[Callable] = None

    def one_click_optimize(self, pattern_names: List[str] = None,
                           optimization_method: str = "genetic",
                           max_iterations: int = 20) -> Dict[str, Any]:
        """
        ä¸€é”®ä¼˜åŒ–æŒ‡å®šå½¢æ€æˆ–æ‰€æœ‰å½¢æ€

        Args:
            pattern_names: è¦ä¼˜åŒ–çš„å½¢æ€åç§°åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä¼˜åŒ–æ‰€æœ‰å½¢æ€
            optimization_method: ä¼˜åŒ–æ–¹æ³•
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

        Returns:
            ä¼˜åŒ–ç»“æœæ‘˜è¦
        """
        print("ğŸš€ å¯åŠ¨ä¸€é”®ä¼˜åŒ–...")

        # è·å–è¦ä¼˜åŒ–çš„å½¢æ€åˆ—è¡¨
        if pattern_names is None:
            pattern_names = self._get_all_pattern_names()

        print(f"ğŸ“‹ å¾…ä¼˜åŒ–å½¢æ€: {len(pattern_names)}ä¸ª")

        # åˆ›å»ºä¼˜åŒ–é…ç½®
        config = OptimizationConfig(
            method=optimization_method,
            max_iterations=max_iterations,
            population_size=10,  # ä¸€é”®ä¼˜åŒ–ä½¿ç”¨è¾ƒå°çš„ç§ç¾¤
            timeout_minutes=15   # æ¯ä¸ªå½¢æ€æœ€å¤š15åˆ†é’Ÿ
        )

        # æ·»åŠ ä¼˜åŒ–ä»»åŠ¡
        tasks = []
        for i, pattern_name in enumerate(pattern_names):
            task = TuningTask(
                pattern_name=pattern_name,
                priority=i + 1,  # æŒ‰é¡ºåºè®¾ç½®ä¼˜å…ˆçº§
                config=config
            )
            tasks.append(task)
            self.add_task(task)

        # å¼€å§‹æ‰¹é‡ä¼˜åŒ–
        results = self.run_batch_optimization()

        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        report = self._generate_optimization_report(results)

        print("âœ… ä¸€é”®ä¼˜åŒ–å®Œæˆï¼")
        return report

    def smart_optimize(self, performance_threshold: float = 0.7,
                       improvement_target: float = 0.1) -> Dict[str, Any]:
        """
        æ™ºèƒ½ä¼˜åŒ–ï¼šè‡ªåŠ¨è¯†åˆ«éœ€è¦ä¼˜åŒ–çš„å½¢æ€å¹¶è¿›è¡Œä¼˜åŒ–

        Args:
            performance_threshold: æ€§èƒ½é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„å½¢æ€å°†è¢«ä¼˜åŒ–
            improvement_target: æ”¹è¿›ç›®æ ‡ï¼ŒæœŸæœ›çš„æ€§èƒ½æå‡æ¯”ä¾‹

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        print("ğŸ§  å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–...")

        # è¯„ä¼°æ‰€æœ‰å½¢æ€çš„å½“å‰æ€§èƒ½
        pattern_names = self._get_all_pattern_names()
        performance_scores = {}

        print("è¯„ä¼°å½“å‰æ€§èƒ½...")
        for pattern_name in pattern_names:
            try:
                test_datasets = self.evaluator.create_test_datasets(
                    pattern_name, count=3)
                metrics = self.evaluator.evaluate_algorithm(
                    pattern_name, test_datasets)
                performance_scores[pattern_name] = metrics.overall_score

                if self.debug_mode:
                    print(f"  {pattern_name}: {metrics.overall_score:.3f}")

            except Exception as e:
                if self.debug_mode:
                    print(f"  {pattern_name}: è¯„ä¼°å¤±è´¥ - {e}")
                performance_scores[pattern_name] = 0.0

        # è¯†åˆ«éœ€è¦ä¼˜åŒ–çš„å½¢æ€
        patterns_to_optimize = [
            name for name, score in performance_scores.items()
            if score < performance_threshold
        ]

        print(f"è¯†åˆ«åˆ° {len(patterns_to_optimize)} ä¸ªéœ€è¦ä¼˜åŒ–çš„å½¢æ€")

        if not patterns_to_optimize:
            return {
                "status": "no_optimization_needed",
                "message": "æ‰€æœ‰å½¢æ€æ€§èƒ½éƒ½è¾¾åˆ°é˜ˆå€¼è¦æ±‚",
                "performance_scores": performance_scores
            }

        # æŒ‰æ€§èƒ½åˆ†æ•°æ’åºï¼Œä¼˜å…ˆä¼˜åŒ–æ€§èƒ½æœ€å·®çš„
        patterns_to_optimize.sort(key=lambda x: performance_scores[x])

        # åˆ›å»ºæ™ºèƒ½ä¼˜åŒ–é…ç½®
        smart_configs = self._create_smart_configs(
            patterns_to_optimize, performance_scores)

        # æ·»åŠ ä¼˜åŒ–ä»»åŠ¡
        for pattern_name, config in smart_configs.items():
            task = TuningTask(
                pattern_name=pattern_name,
                priority=1,  # æ™ºèƒ½ä¼˜åŒ–é«˜ä¼˜å…ˆçº§
                config=config
            )
            self.add_task(task)

        # æ‰§è¡Œä¼˜åŒ–
        results = self.run_batch_optimization()

        # ç”Ÿæˆæ™ºèƒ½ä¼˜åŒ–æŠ¥å‘Š
        report = self._generate_smart_optimization_report(
            results, performance_scores, improvement_target
        )

        return report

    def add_task(self, task: TuningTask):
        """æ·»åŠ ä¼˜åŒ–ä»»åŠ¡"""
        self.task_queue.append(task)
        self.task_queue.sort(key=lambda t: t.priority)  # æŒ‰ä¼˜å…ˆçº§æ’åº

        if self.debug_mode:
            print(f"ğŸ“ æ·»åŠ ä»»åŠ¡: {task.pattern_name} (ä¼˜å…ˆçº§: {task.priority})")

    def run_batch_optimization(self) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰¹é‡ä¼˜åŒ–"""
        if not self.task_queue:
            return []

        print(f"å¼€å§‹æ‰¹é‡ä¼˜åŒ–ï¼Œå…± {len(self.task_queue)} ä¸ªä»»åŠ¡")

        self.is_running = True
        results = []

        try:
            # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            future_to_task = {}

            for task in self.task_queue[:self.max_workers]:  # é™åˆ¶å¹¶å‘æ•°
                future = self.executor.submit(self._run_single_task, task)
                future_to_task[future] = task
                task.status = "running"
                task.start_time = datetime.now()
                self.running_tasks[task.pattern_name] = task

            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_task):
                task = future_to_task[future]

                try:
                    result = future.result()
                    task.result = result
                    task.status = "completed"
                    task.end_time = datetime.now()
                    results.append(result)

                    if self.debug_mode:
                        print(f"âœ… ä»»åŠ¡å®Œæˆ: {task.pattern_name}")

                    # è°ƒç”¨å®Œæˆå›è°ƒ
                    if self.completion_callback:
                        self.completion_callback(task)

                except Exception as e:
                    task.error_message = str(e)
                    task.status = "failed"
                    task.end_time = datetime.now()

                    if self.debug_mode:
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {task.pattern_name} - {e}")

                # ç§»é™¤è¿è¡Œä¸­çš„ä»»åŠ¡
                if task.pattern_name in self.running_tasks:
                    del self.running_tasks[task.pattern_name]

                # æ·»åŠ åˆ°å®Œæˆåˆ—è¡¨
                self.completed_tasks.append(task)

                # å¯åŠ¨ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼ˆå¦‚æœæœ‰ï¼‰
                remaining_tasks = [
                    t for t in self.task_queue if t.status == "pending"]
                if remaining_tasks and len(self.running_tasks) < self.max_workers:
                    next_task = remaining_tasks[0]
                    future = self.executor.submit(
                        self._run_single_task, next_task)
                    future_to_task[future] = next_task
                    next_task.status = "running"
                    next_task.start_time = datetime.now()
                    self.running_tasks[next_task.pattern_name] = next_task

        finally:
            self.is_running = False
            self.task_queue.clear()

        print(f"ğŸ‰ æ‰¹é‡ä¼˜åŒ–å®Œæˆï¼ŒæˆåŠŸ {len(results)} ä¸ªä»»åŠ¡")
        return results

    def _run_single_task(self, task: TuningTask) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªä¼˜åŒ–ä»»åŠ¡"""
        try:
            # æ›´æ–°è¿›åº¦
            task.progress = 0.1
            if self.progress_callback:
                self.progress_callback(task)

            # æ‰§è¡Œä¼˜åŒ–
            result = self.optimizer.optimize_algorithm(
                pattern_name=task.pattern_name,
                config=task.config
            )

            # æ›´æ–°è¿›åº¦
            task.progress = 1.0
            if self.progress_callback:
                self.progress_callback(task)

            return result

        except Exception as e:
            task.error_message = str(e)
            raise e

    def schedule_optimization(self, pattern_name: str,
                              schedule_time: datetime,
                              config: OptimizationConfig = None) -> bool:
        """è°ƒåº¦ä¼˜åŒ–ä»»åŠ¡"""
        if schedule_time <= datetime.now():
            raise ValueError("è°ƒåº¦æ—¶é—´å¿…é¡»åœ¨æœªæ¥")

        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = self.db_manager.db_path
        # è¿™é‡Œå¯ä»¥å®ç°å®šæ—¶ä»»åŠ¡çš„æ•°æ®åº“å­˜å‚¨

        print(f"â° å·²è°ƒåº¦ä¼˜åŒ–ä»»åŠ¡: {pattern_name} åœ¨ {schedule_time}")
        return True

    def get_optimization_status(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–çŠ¶æ€"""
        try:
            # è·å–æ´»è·ƒçš„ä¼˜åŒ–ä»»åŠ¡
            active_count = 0
            completed_count = 0
            failed_count = 0

            # æ£€æŸ¥çº¿ç¨‹æ± çŠ¶æ€
            if hasattr(self, 'executor') and self.executor:
                # ç»Ÿè®¡æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
                active_count = len(
                    [f for f in self.optimization_futures if not f.done()])
                completed_count = len(
                    [f for f in self.optimization_futures if f.done() and not f.exception()])
                failed_count = len(
                    [f for f in self.optimization_futures if f.done() and f.exception()])

            # è·å–æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
            db_stats = self.db_manager.get_optimization_statistics()

            return {
                "active_optimizations": active_count,
                "completed_optimizations": completed_count,
                "failed_optimizations": failed_count,
                "total_versions": db_stats.get("total_versions", 0),
                "active_versions": db_stats.get("active_versions", 0),
                "avg_improvement": db_stats.get("avg_improvement", 0),
                "system_status": "running" if active_count > 0 else "idle",
                "last_update": datetime.now().isoformat()
            }

        except Exception as e:
            if self.debug_mode:
                print(f"âŒ è·å–ä¼˜åŒ–çŠ¶æ€å¤±è´¥: {e}")

            return {
                "active_optimizations": 0,
                "completed_optimizations": 0,
                "failed_optimizations": 0,
                "total_versions": 0,
                "active_versions": 0,
                "avg_improvement": 0,
                "system_status": "error",
                "error": str(e),
                "last_update": datetime.now().isoformat()
            }

    def cancel_task(self, pattern_name: str) -> bool:
        """å–æ¶ˆä¼˜åŒ–ä»»åŠ¡"""
        # å–æ¶ˆå¾…æ‰§è¡Œçš„ä»»åŠ¡
        for task in self.task_queue:
            if task.pattern_name == pattern_name and task.status == "pending":
                task.status = "cancelled"
                self.task_queue.remove(task)
                return True

        # å¯¹äºæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œè¿™é‡Œéœ€è¦å®ç°ä¸­æ–­æœºåˆ¶
        if pattern_name in self.running_tasks:
            # å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„ä¸­æ–­é€»è¾‘
            print(f"âš ï¸  æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ— æ³•ç«‹å³å–æ¶ˆ: {pattern_name}")
            return False

        return False

    def _get_all_pattern_names(self) -> List[str]:
        """è·å–æ‰€æœ‰å½¢æ€åç§°"""
        try:
            patterns = self.pattern_manager.get_all_patterns()
            return [p.english_name for p in patterns if p.is_active]
        except Exception as e:
            if self.debug_mode:
                print(f"è·å–å½¢æ€åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _create_smart_configs(self, patterns: List[str],
                              performance_scores: Dict[str, float]) -> Dict[str, OptimizationConfig]:
        """ä¸ºä¸åŒå½¢æ€åˆ›å»ºæ™ºèƒ½ä¼˜åŒ–é…ç½®"""
        configs = {}

        for pattern_name in patterns:
            score = performance_scores.get(pattern_name, 0.5)

            # æ ¹æ®å½“å‰æ€§èƒ½è°ƒæ•´ä¼˜åŒ–å¼ºåº¦
            if score < 0.3:
                # æ€§èƒ½å¾ˆå·®ï¼Œä½¿ç”¨å¼ºåŒ–ä¼˜åŒ–
                config = OptimizationConfig(
                    method="genetic",
                    max_iterations=50,
                    population_size=30,
                    mutation_rate=0.15,
                    timeout_minutes=45
                )
            elif score < 0.5:
                # æ€§èƒ½ä¸€èˆ¬ï¼Œä½¿ç”¨æ ‡å‡†ä¼˜åŒ–
                config = OptimizationConfig(
                    method="genetic",
                    max_iterations=30,
                    population_size=20,
                    mutation_rate=0.1,
                    timeout_minutes=30
                )
            else:
                # æ€§èƒ½å°šå¯ï¼Œä½¿ç”¨è½»åº¦ä¼˜åŒ–
                config = OptimizationConfig(
                    method="bayesian",
                    max_iterations=20,
                    population_size=10,
                    timeout_minutes=20
                )

            configs[pattern_name] = config

        return configs

    def _generate_optimization_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        if not results:
            return {"status": "no_results", "message": "æ²¡æœ‰ä¼˜åŒ–ç»“æœ"}

        total_tasks = len(results)
        successful_tasks = len(
            [r for r in results if r.get("improvement_percentage", 0) > 0])

        improvements = [r.get("improvement_percentage", 0) for r in results]
        avg_improvement = sum(improvements) / \
            len(improvements) if improvements else 0

        best_improvement = max(improvements) if improvements else 0
        best_pattern = None
        for result in results:
            if result.get("improvement_percentage", 0) == best_improvement:
                best_pattern = result.get("pattern_name", "unknown")
                break

        report = {
            "summary": {
                "total_tasks": total_tasks,
                "successful_tasks": successful_tasks,
                "success_rate": successful_tasks / total_tasks * 100,
                "average_improvement": avg_improvement,
                "best_improvement": best_improvement,
                "best_pattern": best_pattern
            },
            "details": results,
            "recommendations": self._generate_recommendations(results)
        }

        return report

    def _generate_smart_optimization_report(self, results: List[Dict[str, Any]],
                                            baseline_scores: Dict[str, float],
                                            improvement_target: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½ä¼˜åŒ–æŠ¥å‘Š"""
        report = self._generate_optimization_report(results)

        # æ·»åŠ æ™ºèƒ½åˆ†æ
        achieved_targets = 0
        for result in results:
            pattern_name = result.get("pattern_name", "")
            improvement = result.get("improvement_percentage", 0) / 100
            if improvement >= improvement_target:
                achieved_targets += 1

        report["smart_analysis"] = {
            "baseline_performance": baseline_scores,
            "improvement_target": improvement_target * 100,
            "targets_achieved": achieved_targets,
            "target_achievement_rate": achieved_targets / len(results) * 100 if results else 0
        }

        return report

    def _generate_recommendations(self, results: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if not results:
            return ["æ²¡æœ‰ä¼˜åŒ–ç»“æœå¯ä¾›åˆ†æ"]

        # åˆ†æä¼˜åŒ–æ•ˆæœ
        improvements = [r.get("improvement_percentage", 0) for r in results]
        avg_improvement = sum(improvements) / len(improvements)

        if avg_improvement < 5:
            recommendations.append("æ•´ä½“ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œå»ºè®®æ£€æŸ¥ç®—æ³•åŸºç¡€é€»è¾‘æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
        elif avg_improvement > 20:
            recommendations.append("ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®å°†ä¼˜åŒ–åçš„ç‰ˆæœ¬éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")

        # åˆ†æä¼˜åŒ–æ–¹æ³•æ•ˆæœ
        methods = {}
        for result in results:
            method = result.get("method", "unknown")
            improvement = result.get("improvement_percentage", 0)
            if method not in methods:
                methods[method] = []
            methods[method].append(improvement)

        best_method = max(methods.keys(), key=lambda m: sum(
            methods[m])/len(methods[m])) if methods else None
        if best_method:
            recommendations.append(f"'{best_method}' æ–¹æ³•æ•ˆæœæœ€ä½³ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨")

        # è¯†åˆ«é—®é¢˜å½¢æ€
        poor_performers = [
            r.get("pattern_name", "unknown")
            for r in results
            if r.get("improvement_percentage", 0) < 2
        ]

        if poor_performers:
            recommendations.append(
                f"ä»¥ä¸‹å½¢æ€ä¼˜åŒ–æ•ˆæœä¸ä½³ï¼Œéœ€è¦äººå·¥æ£€æŸ¥: {', '.join(poor_performers[:3])}")

        return recommendations

    def set_progress_callback(self, callback: Callable[[TuningTask], None]):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback

    def set_completion_callback(self, callback: Callable[[TuningTask], None]):
        """è®¾ç½®å®Œæˆå›è°ƒå‡½æ•°"""
        self.completion_callback = callback

    def export_optimization_history(self, export_path: str) -> bool:
        """å¯¼å‡ºä¼˜åŒ–å†å²"""
        try:
            history = {
                "export_time": datetime.now().isoformat(),
                "completed_tasks": [
                    {
                        "pattern_name": task.pattern_name,
                        "start_time": task.start_time.isoformat() if task.start_time else None,
                        "end_time": task.end_time.isoformat() if task.end_time else None,
                        "status": task.status,
                        "result": task.result,
                        "error_message": task.error_message
                    }
                    for task in self.completed_tasks
                ]
            }

            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)

            print(f"âœ… ä¼˜åŒ–å†å²å·²å¯¼å‡ºåˆ°: {export_path}")
            return True

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return False


def create_auto_tuner(max_workers: int = 4, debug_mode: bool = False) -> AutoTuner:
    """åˆ›å»ºè‡ªåŠ¨è°ƒä¼˜å™¨å®ä¾‹"""
    return AutoTuner(max_workers=max_workers, debug_mode=debug_mode)


if __name__ == "__main__":
    # æµ‹è¯•è‡ªåŠ¨è°ƒä¼˜å™¨
    tuner = create_auto_tuner(max_workers=2, debug_mode=True)

    # æµ‹è¯•ä¸€é”®ä¼˜åŒ–ï¼ˆä»…ä¼˜åŒ–å‡ ä¸ªå½¢æ€ï¼‰
    test_patterns = ["hammer", "doji"]

    print("ğŸ§ª æµ‹è¯•ä¸€é”®ä¼˜åŒ–...")
    result = tuner.one_click_optimize(
        pattern_names=test_patterns,
        optimization_method="random",  # ä½¿ç”¨å¿«é€Ÿçš„éšæœºä¼˜åŒ–è¿›è¡Œæµ‹è¯•
        max_iterations=5
    )

    print(f"\nä¼˜åŒ–æŠ¥å‘Š:")
    print(f"  æ€»ä»»åŠ¡æ•°: {result['summary']['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡æ•°: {result['summary']['successful_tasks']}")
    print(f"  å¹³å‡æ”¹è¿›: {result['summary']['average_improvement']:.3f}%")
    print(f"  æœ€ä½³æ”¹è¿›: {result['summary']['best_improvement']:.3f}%")
    print(f"  æœ€ä½³å½¢æ€: {result['summary']['best_pattern']}")

    print(f"\nğŸ’¡ å»ºè®®:")
    for rec in result['recommendations']:
        print(f"  - {rec}")
