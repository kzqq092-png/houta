# AIæ¨¡å‹è®­ç»ƒç³»ç»Ÿ_ç»¼åˆç‰ˆ

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ•´åˆäº†AIæ¨¡å‹è®­ç»ƒå·¥å…·å’Œé¢„æµ‹å‡†ç¡®æ€§è·Ÿè¸ªç³»ç»Ÿçš„è®¾è®¡æ–¹æ¡ˆã€å®¡æŸ¥æŠ¥å‘Šå’Œå¼€å‘è®¡åˆ’ï¼Œä¸ºäº¤æ˜“ç³»ç»Ÿæä¾›å®Œæ•´çš„AIæ¨¡å‹è®­ç»ƒã€é¢„æµ‹è·Ÿè¸ªå’Œè´¨é‡è¯„ä¼°è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼šå»ºç«‹å®Œæ•´çš„AIæ¨¡å‹è®­ç»ƒç”Ÿæ€ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå®ç°æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œé¢„æµ‹å‡†ç¡®æ€§è·Ÿè¸ªã€‚

**ç³»ç»Ÿæ¶æ„**ï¼šåŸºäºç°æœ‰æœåŠ¡å®¹å™¨æ¶æ„ï¼Œé‡‡ç”¨äº‹ä»¶é©±åŠ¨è®¾è®¡ï¼Œç¡®ä¿ä¸ç°æœ‰ç³»ç»Ÿçš„æ— ç¼é›†æˆã€‚

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1. æ•´ä½“æ¶æ„

#### æœåŠ¡å±‚æ¶æ„
```python
class UnifiedAIServiceContainer:
    """ç»Ÿä¸€AIæœåŠ¡å®¹å™¨"""
    
    def __init__(self):
        # æ ¸å¿ƒæœåŠ¡
        self.model_training_service = ModelTrainingService()
        self.prediction_tracking_service = PredictionTrackingService()
        self.quality_assessment_service = QualityAssessmentService()
        self.model_version_service = ModelVersionService()
        
        # é›†æˆæœåŠ¡
        self.event_bus = EventBus()
        self.database_service = DatabaseService()
        self.cache_service = CacheService()
        self.monitoring_service = MonitoringService()
```

#### æ ¸å¿ƒæœåŠ¡è®¾è®¡

**1. ModelTrainingService - æ¨¡å‹è®­ç»ƒæœåŠ¡**
```python
class ModelTrainingService(BaseService):
    """æ¨¡å‹è®­ç»ƒæœåŠ¡"""
    
    async def create_training_task(self, task_config: TrainingTaskConfig) -> str:
        """åˆ›å»ºè®­ç»ƒä»»åŠ¡"""
        # å‚æ•°éªŒè¯
        self._validate_task_config(task_config)
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"train_{uuid.uuid4().hex[:8]}"
        
        # ä¿å­˜ä»»åŠ¡é…ç½®
        await self._save_training_task(task_id, task_config)
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        await self._init_task_status(task_id)
        
        logger.info(f"åˆ›å»ºè®­ç»ƒä»»åŠ¡æˆåŠŸ: {task_id}")
        return task_id
    
    async def execute_training_task(self, task_id: str):
        """æ‰§è¡Œè®­ç»ƒä»»åŠ¡"""
        # è·å–ä»»åŠ¡é…ç½®
        task_config = await self._get_training_task(task_id)
        if not task_config:
            raise ValueError(f"è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        await self._update_task_status(task_id, TaskStatus.RUNNING)
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            train_data, eval_data = await self._prepare_training_data(task_config)
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = self._initialize_model(task_config.model_type, task_config.model_params)
            
            # åˆ›å»ºè®­ç»ƒçº¿ç¨‹
            training_thread = TrainingThread(
                model=model,
                train_data=train_data,
                eval_data=eval_data,
                params=task_config.training_params,
                callback=self._training_progress_callback(task_id)
            )
            
            # å¯åŠ¨è®­ç»ƒ
            training_thread.start()
            
            # ç­‰å¾…è®­ç»ƒå®Œæˆ
            while training_thread.is_alive():
                await asyncio.sleep(1)
            
            # è·å–è®­ç»ƒç»“æœ
            training_result = training_thread.get_result()
            
            # è¯„ä¼°æ¨¡å‹
            eval_metrics = await self._evaluate_model(model, eval_data)
            
            # ä¿å­˜æ¨¡å‹ç‰ˆæœ¬
            model_version = await self._save_model_version(
                task_id, model, training_result, eval_metrics
            )
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            await self._update_task_status(
                task_id, 
                TaskStatus.COMPLETED,
                result={
                    "model_version": model_version,
                    "metrics": eval_metrics
                }
            )
            
        except Exception as e:
            logger.error(f"è®­ç»ƒä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            await self._update_task_status(
                task_id, 
                TaskStatus.FAILED,
                error=str(e)
            )
```

**2. PredictionTrackingService - é¢„æµ‹è·Ÿè¸ªæœåŠ¡**
```python
class PredictionTrackingService(BaseService):
    """é¢„æµ‹è·Ÿè¸ªæœåŠ¡"""
    
    def __init__(self):
        self.prediction_buffer = {}
        self.tracking_metrics = {}
        self.performance_monitor = PerformanceMonitor()
    
    async def track_prediction(self, 
                             model_version: str,
                             prediction_input: Any,
                             prediction_output: Any,
                             actual_result: Any = None):
        """è·Ÿè¸ªé¢„æµ‹ç»“æœ"""
        
        # åˆ›å»ºè·Ÿè¸ªè®°å½•
        tracking_record = PredictionRecord(
            model_version=model_version,
            timestamp=datetime.now(),
            input_data=prediction_input,
            predicted_value=prediction_output,
            actual_value=actual_result,
            tracking_id=str(uuid.uuid4())
        )
        
        # ä¿å­˜åˆ°ç¼“å†²åŒº
        await self._save_to_buffer(tracking_record)
        
        # å¦‚æœæœ‰å®é™…ç»“æœï¼Œæ›´æ–°å‡†ç¡®æ€§æŒ‡æ ‡
        if actual_result is not None:
            await self._update_accuracy_metrics(tracking_record)
        
        # å‘å¸ƒé¢„æµ‹è·Ÿè¸ªäº‹ä»¶
        self.event_bus.publish(
            PredictionTrackedEvent(
                model_version=model_version,
                tracking_id=tracking_record.tracking_id,
                has_actual=actual_result is not None
            )
        )
        
        return tracking_record.tracking_id
    
    async def generate_performance_report(self, 
                                        model_version: str,
                                        time_range: TimeRange) -> PerformanceReport:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„è·Ÿè¸ªè®°å½•
        records = await self._get_tracking_records(model_version, time_range)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        accuracy_metrics = await self._calculate_accuracy_metrics(records)
        drift_metrics = await self._calculate_drift_metrics(records)
        performance_trends = await self._analyze_performance_trends(records)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = PerformanceReport(
            model_version=model_version,
            time_range=time_range,
            total_predictions=len(records),
            accuracy_metrics=accuracy_metrics,
            drift_metrics=drift_metrics,
            performance_trends=performance_trends,
            recommendations=self._generate_recommendations(
                accuracy_metrics, drift_metrics, performance_trends
            ),
            generated_at=datetime.now()
        )
        
        return report
```

**3. QualityAssessmentService - è´¨é‡è¯„ä¼°æœåŠ¡**
```python
class QualityAssessmentService(BaseService):
    """è´¨é‡è¯„ä¼°æœåŠ¡"""
    
    async def assess_model_quality(self, 
                                 model_version: str,
                                 assessment_config: QualityAssessmentConfig) -> QualityAssessmentResult:
        """è¯„ä¼°æ¨¡å‹è´¨é‡"""
        
        # è·å–æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯
        model_info = await self.model_version_service.get_model_info(model_version)
        if not model_info:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬ä¸å­˜åœ¨: {model_version}")
        
        # æ”¶é›†è¯„ä¼°æ•°æ®
        training_metrics = model_info.get('training_metrics', {})
        validation_metrics = model_info.get('validation_metrics', {})
        
        # è·å–é¢„æµ‹è·Ÿè¸ªæ•°æ®
        tracking_data = await self.prediction_tracking_service.get_recent_tracking_data(
            model_version, days=30
        )
        
        # è®¡ç®—è´¨é‡ç»´åº¦è¯„åˆ†
        accuracy_score = await self._calculate_accuracy_score(
            validation_metrics, tracking_data
        )
        
        stability_score = await self._calculate_stability_score(
            training_metrics, validation_metrics
        )
        
        robustness_score = await self._calculate_robustness_score(
            model_info, tracking_data
        )
        
        efficiency_score = await self._calculate_efficiency_score(
            model_info, tracking_data
        )
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        overall_score = self._calculate_overall_quality_score(
            accuracy_score, stability_score, robustness_score, efficiency_score,
            assessment_config.weights
        )
        
        # ç”Ÿæˆè¯„ä¼°ç»“æœ
        assessment_result = QualityAssessmentResult(
            model_version=model_version,
            assessment_time=datetime.now(),
            overall_score=overall_score,
            dimension_scores={
                'accuracy': accuracy_score,
                'stability': stability_score,
                'robustness': robustness_score,
                'efficiency': efficiency_score
            },
            detailed_metrics={
                'training_metrics': training_metrics,
                'validation_metrics': validation_metrics,
                'prediction_metrics': await self._calculate_prediction_metrics(tracking_data)
            },
            recommendations=self._generate_quality_recommendations(
                accuracy_score, stability_score, robustness_score, efficiency_score
            )
        )
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        await self._save_assessment_result(assessment_result)
        
        # å‘å¸ƒè´¨é‡è¯„ä¼°äº‹ä»¶
        self.event_bus.publish(
            ModelQualityAssessedEvent(
                model_version=model_version,
                overall_score=overall_score,
                assessment_result=assessment_result
            )
        )
        
        return assessment_result
```

### 2. æ•°æ®æµæ¶æ„

#### è®­ç»ƒæ•°æ®æµ
```python
# æ•°æ®å‡†å¤‡æµç¨‹
async def _prepare_training_data(self, task_config: TrainingTaskConfig):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    
    # 1. è·å–åŸå§‹æ•°æ®
    raw_data = await self.data_service.get_historical_data(
        symbols=task_config.symbols,
        start_date=task_config.start_date,
        end_date=task_config.end_date,
        data_types=task_config.data_types
    )
    
    # 2. æ•°æ®é¢„å¤„ç†
    processed_data = await self._preprocess_data(raw_data, task_config.preprocessing_params)
    
    # 3. ç‰¹å¾å·¥ç¨‹
    features = await self._engineer_features(processed_data, task_config.feature_params)
    
    # 4. æ•°æ®åˆ†å‰²
    train_data, eval_data, test_data = await self._split_data(
        features, task_config.split_ratios
    )
    
    # 5. æ•°æ®æ ‡å‡†åŒ–
    normalized_train, normalized_eval, normalized_test = await self._normalize_data(
        train_data, eval_data, test_data, task_config.normalization_params
    )
    
    return (normalized_train, normalized_eval, normalized_test)
```

#### é¢„æµ‹è·Ÿè¸ªæ•°æ®æµ
```python
# é¢„æµ‹è·Ÿè¸ªæµç¨‹
async def track_prediction_flow(self, prediction_request: PredictionRequest):
    """é¢„æµ‹è·Ÿè¸ªæµç¨‹"""
    
    # 1. è·å–æ¨¡å‹ç‰ˆæœ¬
    model_version = prediction_request.model_version
    
    # 2. æ‰§è¡Œé¢„æµ‹
    prediction_result = await self._execute_prediction(
        model_version, prediction_request.input_data
    )
    
    # 3. è®°å½•é¢„æµ‹
    tracking_id = await self.prediction_tracking_service.track_prediction(
        model_version=model_version,
        prediction_input=prediction_request.input_data,
        prediction_output=prediction_result.output,
        actual_result=None  # å®é™…ç»“æœç¨åæ›´æ–°
    )
    
    # 4. æ›´æ–°é¢„æµ‹çŠ¶æ€
    await self._update_prediction_status(tracking_id, 'pending_actual')
    
    return tracking_id
```

---

## ğŸ§  æ¨¡å‹è®­ç»ƒå®ç°

### 1. æ”¯æŒçš„æ¨¡å‹ç±»å‹

#### æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
```python
class TimeSeriesModelFactory:
    """æ—¶é—´åºåˆ—æ¨¡å‹å·¥å‚"""
    
    @staticmethod
    def create_model(model_type: str, **params):
        """åˆ›å»ºæ—¶é—´åºåˆ—æ¨¡å‹"""
        
        if model_type == "lstm":
            return LSTMModel(**params)
        elif model_type == "gru":
            return GRUModel(**params)
        elif model_type == "transformer":
            return TransformerModel(**params)
        elif model_type == "prophet":
            return ProphetModel(**params)
        elif model_type == "xgboost":
            return XGBoostModel(**params)
        elif model_type == "lightgbm":
            return LightGBMModel(**params)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

class LSTMModel(nn.Module):
    """LSTMæ¨¡å‹å®ç°"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, 
                 output_size: int, dropout: float = 0.2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        return out

class XGBoostModel:
    """XGBoostæ¨¡å‹å®ç°"""
    
    def __init__(self, **params):
        self.params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 100,
            **params
        }
        self.model = None
    
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒæ¨¡å‹"""
        dtrain = xgb.DMatrix(X_train, label=y_train)
        
        eval_list = []
        if X_val is not None and y_val is not None:
            dval = xgb.DMatrix(X_val, label=y_val)
            eval_list = [(dtrain, 'train'), (dval, 'eval')]
        
        self.model = xgb.train(
            params=self.params,
            dtrain=dtrain,
            num_boost_round=1000,
            evals=eval_list,
            early_stopping_rounds=50,
            verbose_eval=100
        )
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        dtest = xgb.DMatrix(X)
        return self.model.predict(dtest)
```

### 2. è®­ç»ƒä»»åŠ¡ç®¡ç†

#### è®­ç»ƒä»»åŠ¡é…ç½®
```python
@dataclass
class TrainingTaskConfig:
    """è®­ç»ƒä»»åŠ¡é…ç½®"""
    
    task_id: str
    task_name: str
    description: str
    
    # æ¨¡å‹é…ç½®
    model_type: str
    model_params: Dict[str, Any]
    
    # æ•°æ®é…ç½®
    symbols: List[str]
    start_date: str
    end_date: str
    data_types: List[str]
    
    # è®­ç»ƒé…ç½®
    training_params: Dict[str, Any]
    split_ratios: Dict[str, float] = field(default_factory=lambda: {
        'train': 0.7, 'val': 0.2, 'test': 0.1
    })
    
    # ç‰¹å¾å·¥ç¨‹é…ç½®
    feature_params: Dict[str, Any] = field(default_factory=dict)
    preprocessing_params: Dict[str, Any] = field(default_factory=dict)
    
    # è¯„ä¼°é…ç½®
    evaluation_metrics: List[str] = field(default_factory=lambda: [
        'mse', 'mae', 'rmse', 'r2', 'directional_accuracy'
    ])
    
    # é«˜çº§é…ç½®
    hyperparameter_tuning: bool = False
    cross_validation: bool = False
    ensemble_method: str = None
    
    created_at: datetime = field(default_factory=datetime.now)
    created_by: str = ""
```

#### è®­ç»ƒæ‰§è¡Œå¼•æ“
```python
class TrainingEngine:
    """è®­ç»ƒæ‰§è¡Œå¼•æ“"""
    
    def __init__(self, config: TrainingTaskConfig):
        self.config = config
        self.model = None
        self.training_history = []
        self.evaluation_results = {}
    
    async def execute(self) -> TrainingResult:
        """æ‰§è¡Œè®­ç»ƒ"""
        
        try:
            # 1. æ•°æ®å‡†å¤‡
            train_data, val_data, test_data = await self._prepare_data()
            
            # 2. æ¨¡å‹åˆå§‹åŒ–
            self.model = self._initialize_model()
            
            # 3. è®­ç»ƒå¾ªç¯
            if self.config.cross_validation:
                result = await self._cross_validation_training(train_data)
            else:
                result = await self._standard_training(train_data, val_data)
            
            # 4. æ¨¡å‹è¯„ä¼°
            if test_data is not None:
                self.evaluation_results = await self._evaluate_model(test_data)
            
            # 5. ç”Ÿæˆè®­ç»ƒç»“æœ
            training_result = TrainingResult(
                task_id=self.config.task_id,
                model_version=result.model_version,
                training_metrics=result.training_metrics,
                evaluation_metrics=self.evaluation_results,
                model_artifacts=result.model_artifacts,
                training_time=result.training_time,
                success=True
            )
            
            return training_result
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            return TrainingResult(
                task_id=self.config.task_id,
                success=False,
                error=str(e)
            )
    
    async def _standard_training(self, train_data, val_data) -> TrainingResult:
        """æ ‡å‡†è®­ç»ƒæµç¨‹"""
        
        start_time = time.time()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.training_params['epochs']):
            epoch_start = time.time()
            
            # å‰å‘ä¼ æ’­
            train_loss = self._train_epoch(train_data)
            
            # éªŒè¯
            val_loss = self._validate_epoch(val_data)
            
            # è®°å½•å†å²
            self.training_history.append({
                'epoch': epoch,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'epoch_time': time.time() - epoch_start
            })
            
            # æ—©åœæ£€æŸ¥
            if self._should_early_stop():
                logger.info(f"æ—©åœè§¦å‘ï¼Œè®­ç»ƒåœ¨ç¬¬{epoch}è½®åœæ­¢")
                break
            
            # è¿›åº¦æŠ¥å‘Š
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
        
        training_time = time.time() - start_time
        
        # ä¿å­˜æ¨¡å‹ç‰ˆæœ¬
        model_version = await self._save_model_version()
        
        return TrainingResult(
            task_id=self.config.task_id,
            model_version=model_version,
            training_metrics=self._calculate_final_metrics(),
            training_time=training_time,
            model_artifacts=self._get_model_artifacts()
        )
```

### 3. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

#### ç‰ˆæœ¬ç®¡ç†ç­–ç•¥
```python
class ModelVersionService(BaseService):
    """æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æœåŠ¡"""
    
    async def _save_model_version(self, task_id: str, model, training_result, eval_metrics):
        """ä¿å­˜æ¨¡å‹ç‰ˆæœ¬"""
        
        # ç”Ÿæˆç‰ˆæœ¬å·
        version = await self._generate_model_version()
        
        # ä¿å­˜æ¨¡å‹æ–‡ä»¶
        model_path = await self._save_model_file(model, version)
        
        # è®°å½•ç‰ˆæœ¬ä¿¡æ¯
        version_info = {
            "version": version,
            "task_id": task_id,
            "model_type": model.__class__.__name__,
            "metrics": eval_metrics,
            "training_time": training_result["training_time"],
            "params": training_result["params"],
            "file_path": model_path,
            "created_at": datetime.now(),
            "status": "active"
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        version_id = await self._db_service.insert(
            "model_versions", 
            version_info
        )
        
        # å‘å¸ƒæ¨¡å‹ç‰ˆæœ¬åˆ›å»ºäº‹ä»¶
        self.event_bus.publish(
            ModelVersionCreatedEvent(
                version_id=version_id,
                version=version,
                model_type=version_info["model_type"]
            )
        )
        
        return version
    
    async def _generate_model_version(self) -> str:
        """ç”Ÿæˆæ¨¡å‹ç‰ˆæœ¬å·"""
        
        # è·å–å½“å‰æœ€æ–°ç‰ˆæœ¬
        latest_version = await self._get_latest_version()
        
        if latest_version:
            # è§£æç‰ˆæœ¬å·å¹¶é€’å¢
            version_parts = latest_version.split('.')
            major = int(version_parts[0])
            minor = int(version_parts[1])
            patch = int(version_parts[2]) + 1
            
            # å¦‚æœpatchè¶…è¿‡99ï¼Œé‡ç½®å¹¶é€’å¢minor
            if patch > 99:
                patch = 0
                minor += 1
                
            # å¦‚æœminorè¶…è¿‡99ï¼Œé‡ç½®å¹¶é€’å¢major
            if minor > 99:
                minor = 0
                major += 1
                
            return f"{major}.{minor:02d}.{patch:02d}"
        else:
            # é¦–æ¬¡ç‰ˆæœ¬
            return "1.00.00"
```

---

## ğŸ“Š é¢„æµ‹è·Ÿè¸ªç³»ç»Ÿ

### 1. è·Ÿè¸ªæ•°æ®æ¨¡å‹

#### é¢„æµ‹è®°å½•ç»“æ„
```python
@dataclass
class PredictionRecord:
    """é¢„æµ‹è®°å½•"""
    
    tracking_id: str
    model_version: str
    timestamp: datetime
    
    # è¾“å…¥è¾“å‡ºæ•°æ®
    input_data: Any
    predicted_value: Any
    actual_value: Optional[Any] = None
    
    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    symbol: Optional[str] = None
    prediction_horizon: Optional[int] = None
    confidence_score: Optional[float] = None
    
    # è´¨é‡æŒ‡æ ‡
    prediction_error: Optional[float] = None
    absolute_error: Optional[float] = None
    percentage_error: Optional[float] = None
    
    # å…ƒæ•°æ®
    execution_time: Optional[float] = None
    data_quality_score: Optional[float] = None
    feature_drift_score: Optional[float] = None
    
    def update_actual_value(self, actual_value: Any):
        """æ›´æ–°å®é™…å€¼å¹¶è®¡ç®—è¯¯å·®æŒ‡æ ‡"""
        self.actual_value = actual_value
        
        if self.predicted_value is not None and actual_value is not None:
            # è®¡ç®—é¢„æµ‹è¯¯å·®
            self.prediction_error = self.predicted_value - actual_value
            self.absolute_error = abs(self.prediction_error)
            
            # è®¡ç®—ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆé¿å…é™¤é›¶ï¼‰
            if actual_value != 0:
                self.percentage_error = abs(self.prediction_error / actual_value) * 100
            else:
                self.percentage_error = 0.0
```

#### æ€§èƒ½æŒ‡æ ‡è®¡ç®—
```python
class PerformanceMetricsCalculator:
    """æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_accuracy_metrics(records: List[PredictionRecord]) -> AccuracyMetrics:
        """è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡"""
        
        valid_records = [r for r in records if r.actual_value is not None]
        
        if not valid_records:
            return AccuracyMetrics()
        
        # è®¡ç®—å„ç§è¯¯å·®æŒ‡æ ‡
        errors = [r.prediction_error for r in valid_records if r.prediction_error is not None]
        abs_errors = [r.absolute_error for r in valid_records if r.absolute_error is not None]
        pct_errors = [r.percentage_error for r in valid_records if r.percentage_error is not None]
        
        # æ–¹å‘å‡†ç¡®æ€§ï¼ˆå¯¹äºä»·æ ¼é¢„æµ‹ï¼‰
        directional_accuracy = PerformanceMetricsCalculator._calculate_directional_accuracy(
            valid_records
        )
        
        return AccuracyMetrics(
            mse=np.mean([e**2 for e in errors]) if errors else 0.0,
            rmse=np.sqrt(np.mean([e**2 for e in errors])) if errors else 0.0,
            mae=np.mean(abs_errors) if abs_errors else 0.0,
            mape=np.mean(pct_errors) if pct_errors else 0.0,
            directional_accuracy=directional_accuracy,
            total_predictions=len(valid_records),
            valid_predictions=len([r for r in valid_records if r.prediction_error is not None])
        )
    
    @staticmethod
    def _calculate_directional_accuracy(records: List[PredictionRecord]) -> float:
        """è®¡ç®—æ–¹å‘å‡†ç¡®æ€§"""
        
        directional_correct = 0
        total_directional = 0
        
        for i in range(1, len(records)):
            curr_record = records[i]
            prev_record = records[i-1]
            
            if (curr_record.actual_value is not None and 
                prev_record.actual_value is not None and
                curr_record.predicted_value is not None and
                prev_record.predicted_value is not None):
                
                # è®¡ç®—å®é™…æ–¹å‘å˜åŒ–
                actual_direction = np.sign(curr_record.actual_value - prev_record.actual_value)
                predicted_direction = np.sign(curr_record.predicted_value - prev_record.predicted_value)
                
                if actual_direction != 0:  # å¿½ç•¥æ— å˜åŒ–çš„æƒ…å†µ
                    if actual_direction == predicted_direction:
                        directional_correct += 1
                    total_directional += 1
        
        return directional_correct / total_directional if total_directional > 0 else 0.0
```

### 2. æ¨¡å‹æ¼‚ç§»æ£€æµ‹

#### æ•°æ®æ¼‚ç§»æ£€æµ‹
```python
class DataDriftDetector:
    """æ•°æ®æ¼‚ç§»æ£€æµ‹å™¨"""
    
    def __init__(self, reference_data: pd.DataFrame):
        self.reference_data = reference_data
        self.reference_stats = self._calculate_reference_stats()
    
    def detect_drift(self, current_data: pd.DataFrame, threshold: float = 0.05) -> DriftAnalysis:
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""
        
        current_stats = self._calculate_current_stats(current_data)
        drift_scores = {}
        drift_alerts = []
        
        for column in self.reference_data.columns:
            if column in current_data.columns:
                # è®¡ç®—ç»Ÿè®¡æ¼‚ç§»
                stat_drift = self._calculate_statistical_drift(
                    self.reference_data[column], current_data[column]
                )
                
                # è®¡ç®—åˆ†å¸ƒæ¼‚ç§»
                distribution_drift = self._calculate_distribution_drift(
                    self.reference_data[column], current_data[column]
                )
                
                # è®¡ç®—ç»¼åˆæ¼‚ç§»åˆ†æ•°
                combined_drift = (stat_drift + distribution_drift) / 2
                drift_scores[column] = combined_drift
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                if combined_drift > threshold:
                    drift_alerts.append(DriftAlert(
                        feature=column,
                        drift_score=combined_drift,
                        threshold=threshold,
                        severity='high' if combined_drift > threshold * 2 else 'medium'
                    ))
        
        return DriftAnalysis(
            drift_scores=drift_scores,
            alerts=drift_alerts,
            overall_drift_score=np.mean(list(drift_scores.values())),
            analysis_timestamp=datetime.now()
        )
    
    def _calculate_statistical_drift(self, reference: pd.Series, current: pd.Series) -> float:
        """è®¡ç®—ç»Ÿè®¡æ¼‚ç§»"""
        
        # ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒ
        ks_statistic, p_value = ks_2samp(reference.dropna(), current.dropna())
        
        # è½¬æ¢ä¸ºæ¼‚ç§»åˆ†æ•° (0-1)
        drift_score = ks_statistic
        
        return drift_score
    
    def _calculate_distribution_drift(self, reference: pd.Series, current: pd.Series) -> float:
        """è®¡ç®—åˆ†å¸ƒæ¼‚ç§»"""
        
        # ä½¿ç”¨Jensen-Shannonæ•£åº¦
        ref_dist = reference.dropna().values
        curr_dist = current.dropna().values
        
        # åˆ›å»ºç›´æ–¹å›¾
        bins = np.linspace(min(min(ref_dist), min(curr_dist)), 
                          max(max(ref_dist), max(curr_dist)), 50)
        
        ref_hist, _ = np.histogram(ref_dist, bins=bins, density=True)
        curr_hist, _ = np.histogram(curr_dist, bins=bins, density=True)
        
        # å½’ä¸€åŒ–
        ref_hist = ref_hist / np.sum(ref_hist)
        curr_hist = curr_hist / np.sum(curr_hist)
        
        # è®¡ç®—JSæ•£åº¦
        js_divergence = distance.jensenshannon(ref_hist, curr_hist)
        
        return js_divergence
```

### 3. æ€§èƒ½ç›‘æ§

#### å®æ—¶æ€§èƒ½ç›‘æ§
```python
class RealTimePerformanceMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_buffer = defaultdict(list)
        self.alert_thresholds = {
            'accuracy_drop': 0.1,  # å‡†ç¡®æ€§ä¸‹é™è¶…è¿‡10%
            'prediction_delay': 5.0,  # é¢„æµ‹å»¶è¿Ÿè¶…è¿‡5ç§’
            'error_rate': 0.05,  # é”™è¯¯ç‡è¶…è¿‡5%
            'drift_score': 0.3  # æ¼‚ç§»åˆ†æ•°è¶…è¿‡0.3
        }
        self.monitoring_active = True
    
    async def start_monitoring(self, model_versions: List[str]):
        """å¯åŠ¨ç›‘æ§"""
        
        self.monitoring_active = True
        
        for model_version in model_versions:
            # ä¸ºæ¯ä¸ªæ¨¡å‹ç‰ˆæœ¬å¯åŠ¨ç›‘æ§ä»»åŠ¡
            asyncio.create_task(self._monitor_model_performance(model_version))
    
    async def _monitor_model_performance(self, model_version: str):
        """ç›‘æ§å•ä¸ªæ¨¡å‹æ€§èƒ½"""
        
        while self.monitoring_active:
            try:
                # è·å–æœ€è¿‘çš„é¢„æµ‹è®°å½•
                recent_records = await self._get_recent_predictions(model_version, minutes=30)
                
                if recent_records:
                    # è®¡ç®—å½“å‰æ€§èƒ½æŒ‡æ ‡
                    current_metrics = await self._calculate_current_metrics(recent_records)
                    
                    # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
                    await self._check_alert_conditions(model_version, current_metrics)
                    
                    # å­˜å‚¨æŒ‡æ ‡
                    self.metrics_buffer[f"{model_version}_metrics"].append({
                        'timestamp': datetime.now(),
                        'metrics': current_metrics
                    })
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥ï¼ˆ5åˆ†é’Ÿé—´éš”ï¼‰
                await asyncio.sleep(300)
                
            except Exception as e:
                logger.error(f"ç›‘æ§æ¨¡å‹ {model_version} æ—¶å‡ºé”™: {str(e)}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•
    
    async def _check_alert_conditions(self, model_version: str, current_metrics: Dict):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        
        alerts = []
        
        # æ£€æŸ¥å‡†ç¡®æ€§ä¸‹é™
        if 'accuracy' in current_metrics:
            baseline_accuracy = await self._get_baseline_accuracy(model_version)
            accuracy_drop = (baseline_accuracy - current_metrics['accuracy']) / baseline_accuracy
            
            if accuracy_drop > self.alert_thresholds['accuracy_drop']:
                alerts.append(PerformanceAlert(
                    model_version=model_version,
                    alert_type='accuracy_drop',
                    message=f"æ¨¡å‹å‡†ç¡®æ€§ä¸‹é™ {accuracy_drop:.2%}",
                    severity='high',
                    timestamp=datetime.now()
                ))
        
        # æ£€æŸ¥é¢„æµ‹å»¶è¿Ÿ
        if 'avg_prediction_time' in current_metrics:
            if current_metrics['avg_prediction_time'] > self.alert_thresholds['prediction_delay']:
                alerts.append(PerformanceAlert(
                    model_version=model_version,
                    alert_type='prediction_delay',
                    message=f"é¢„æµ‹å»¶è¿Ÿè¿‡é«˜: {current_metrics['avg_prediction_time']:.2f}ç§’",
                    severity='medium',
                    timestamp=datetime.now()
                ))
        
        # å‘é€å‘Šè­¦
        for alert in alerts:
            await self._send_alert(alert)
```

---

## ğŸ” è´¨é‡è¯„ä¼°ç³»ç»Ÿ

### 1. å¤šç»´åº¦è´¨é‡è¯„ä¼°

#### è´¨é‡è¯„ä¼°æ¡†æ¶
```python
class QualityAssessmentFramework:
    """è´¨é‡è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self):
        self.evaluation_dimensions = {
            'accuracy': AccuracyEvaluator(),
            'stability': StabilityEvaluator(),
            'robustness': RobustnessEvaluator(),
            'efficiency': EfficiencyEvaluator(),
            'interpretability': InterpretabilityEvaluator(),
            'fairness': FairnessEvaluator()
        }
    
    async def comprehensive_assessment(self, 
                                     model_version: str,
                                     assessment_scope: AssessmentScope) -> ComprehensiveAssessment:
        """ç»¼åˆè´¨é‡è¯„ä¼°"""
        
        # è·å–è¯„ä¼°æ‰€éœ€æ•°æ®
        model_data = await self._collect_model_data(model_version, assessment_scope)
        prediction_data = await self._collect_prediction_data(model_version, assessment_scope)
        training_data = await self._collect_training_data(model_version, assessment_scope)
        
        # å„ç»´åº¦è¯„ä¼°
        dimension_results = {}
        for dimension_name, evaluator in self.evaluation_dimensions.items():
            if dimension_name in assessment_scope.dimensions:
                try:
                    result = await evaluator.evaluate(
                        model_data, prediction_data, training_data
                    )
                    dimension_results[dimension_name] = result
                except Exception as e:
                    logger.error(f"è¯„ä¼°ç»´åº¦ {dimension_name} æ—¶å‡ºé”™: {str(e)}")
                    dimension_results[dimension_name] = DimensionResult(
                        score=0.0, error=str(e), status='failed'
                    )
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score = self._calculate_overall_score(dimension_results, assessment_scope.weights)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = await self._generate_recommendations(dimension_results)
        
        return ComprehensiveAssessment(
            model_version=model_version,
            overall_score=overall_score,
            dimension_results=dimension_results,
            recommendations=recommendations,
            assessment_timestamp=datetime.now(),
            assessment_scope=assessment_scope
        )
```

#### å‡†ç¡®æ€§è¯„ä¼°å™¨
```python
class AccuracyEvaluator:
    """å‡†ç¡®æ€§è¯„ä¼°å™¨"""
    
    async def evaluate(self, 
                      model_data: ModelData,
                      prediction_data: PredictionData,
                      training_data: TrainingData) -> DimensionResult:
        """è¯„ä¼°å‡†ç¡®æ€§"""
        
        records = prediction_data.valid_records
        
        if not records:
            return DimensionResult(score=0.0, status='insufficient_data')
        
        # è®¡ç®—åŸºç¡€å‡†ç¡®æ€§æŒ‡æ ‡
        accuracy_metrics = PerformanceMetricsCalculator.calculate_accuracy_metrics(records)
        
        # è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å®šçš„å‡†ç¡®æ€§æŒ‡æ ‡
        temporal_metrics = await self._calculate_temporal_metrics(records)
        
        # è®¡ç®—ä¸šåŠ¡ç›¸å…³çš„å‡†ç¡®æ€§æŒ‡æ ‡
        business_metrics = await self._calculate_business_metrics(records)
        
        # ç»¼åˆè¯„åˆ†
        score = self._calculate_accuracy_score(
            accuracy_metrics, temporal_metrics, business_metrics
        )
        
        return DimensionResult(
            score=score,
            status='completed',
            detailed_metrics={
                'accuracy_metrics': accuracy_metrics.__dict__,
                'temporal_metrics': temporal_metrics,
                'business_metrics': business_metrics
            },
            confidence_interval=self._calculate_confidence_interval(records)
        )
    
    async def _calculate_temporal_metrics(self, records: List[PredictionRecord]) -> Dict:
        """è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å®šæŒ‡æ ‡"""
        
        if len(records) < 2:
            return {}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_records = sorted(records, key=lambda x: x.timestamp)
        
        # è®¡ç®—è¶‹åŠ¿é¢„æµ‹å‡†ç¡®æ€§
        trend_accuracy = self._calculate_trend_accuracy(sorted_records)
        
        # è®¡ç®—å¤šæ­¥é¢„æµ‹å‡†ç¡®æ€§
        multi_step_accuracy = await self._calculate_multi_step_accuracy(sorted_records)
        
        # è®¡ç®—å­£èŠ‚æ€§é¢„æµ‹å‡†ç¡®æ€§
        seasonal_accuracy = await self._calculate_seasonal_accuracy(sorted_records)
        
        return {
            'trend_accuracy': trend_accuracy,
            'multi_step_accuracy': multi_step_accuracy,
            'seasonal_accuracy': seasonal_accuracy
        }
    
    def _calculate_trend_accuracy(self, records: List[PredictionRecord]) -> float:
        """è®¡ç®—è¶‹åŠ¿é¢„æµ‹å‡†ç¡®æ€§"""
        
        correct_trends = 0
        total_trends = 0
        
        for i in range(1, len(records)):
            curr = records[i]
            prev = records[i-1]
            
            if (curr.actual_value is not None and prev.actual_value is not None and
                curr.predicted_value is not None and prev.predicted_value is not None):
                
                # å®é™…è¶‹åŠ¿
                actual_trend = curr.actual_value - prev.actual_value
                # é¢„æµ‹è¶‹åŠ¿
                predicted_trend = curr.predicted_value - prev.predicted_value
                
                if actual_trend != 0:  # å¿½ç•¥æ— å˜åŒ–çš„æƒ…å†µ
                    if np.sign(actual_trend) == np.sign(predicted_trend):
                        correct_trends += 1
                    total_trends += 1
        
        return correct_trends / total_trends if total_trends > 0 else 0.0
```

### 2. æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°

#### ç¨³å®šæ€§è¯„ä¼°å™¨
```python
class StabilityEvaluator:
    """ç¨³å®šæ€§è¯„ä¼°å™¨"""
    
    async def evaluate(self,
                      model_data: ModelData,
                      prediction_data: PredictionData,
                      training_data: TrainingData) -> DimensionResult:
        """è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§"""
        
        # è®¡ç®—é¢„æµ‹æ–¹å·®ç¨³å®šæ€§
        variance_stability = await self._calculate_variance_stability(prediction_data)
        
        # è®¡ç®—æ€§èƒ½ç¨³å®šæ€§
        performance_stability = await self._calculate_performance_stability(prediction_data)
        
        # è®¡ç®—æ•°æ®æ¼‚ç§»ç¨³å®šæ€§
        drift_stability = await self._calculate_drift_stability(prediction_data)
        
        # è®¡ç®—è·¨æ—¶é—´æ®µç¨³å®šæ€§
        temporal_stability = await self._calculate_temporal_stability(prediction_data)
        
        # ç»¼åˆç¨³å®šæ€§åˆ†æ•°
        overall_stability = np.mean([
            variance_stability,
            performance_stability,
            drift_stability,
            temporal_stability
        ])
        
        return DimensionResult(
            score=overall_stability,
            status='completed',
            detailed_metrics={
                'variance_stability': variance_stability,
                'performance_stability': performance_stability,
                'drift_stability': drift_stability,
                'temporal_stability': temporal_stability
            }
        )
    
    async def _calculate_variance_stability(self, prediction_data: PredictionData) -> float:
        """è®¡ç®—é¢„æµ‹æ–¹å·®ç¨³å®šæ€§"""
        
        records = prediction_data.valid_records
        
        if len(records) < 10:
            return 0.0
        
        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„è®¡ç®—æ–¹å·®
        window_size = max(1, len(records) // 10)  # 10ä¸ªçª—å£
        variances = []
        
        for i in range(0, len(records), window_size):
            window_records = records[i:i + window_size]
            if len(window_records) > 1:
                predictions = [r.predicted_value for r in window_records if r.predicted_value is not None]
                if len(predictions) > 1:
                    variances.append(np.var(predictions))
        
        if not variances:
            return 0.0
        
        # è®¡ç®—æ–¹å·®çš„å˜å¼‚ç³»æ•°
        mean_variance = np.mean(variances)
        std_variance = np.std(variances)
        
        if mean_variance == 0:
            return 1.0 if std_variance == 0 else 0.0
        
        coefficient_of_variation = std_variance / mean_variance
        
        # è½¬æ¢ä¸ºç¨³å®šæ€§åˆ†æ•° (å˜å¼‚ç³»æ•°è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜)
        stability_score = max(0.0, 1.0 - coefficient_of_variation)
        
        return stability_score
```

### 3. é²æ£’æ€§è¯„ä¼°

#### é²æ£’æ€§è¯„ä¼°å™¨
```python
class RobustnessEvaluator:
    """é²æ£’æ€§è¯„ä¼°å™¨"""
    
    async def evaluate(self,
                      model_data: ModelData,
                      prediction_data: PredictionData,
                      training_data: TrainingData) -> DimensionResult:
        """è¯„ä¼°æ¨¡å‹é²æ£’æ€§"""
        
        # è®¡ç®—å™ªå£°é²æ£’æ€§
        noise_robustness = await self._calculate_noise_robustness(model_data, training_data)
        
        # è®¡ç®—å¼‚å¸¸å€¼é²æ£’æ€§
        outlier_robustness = await self._calculate_outlier_robustness(prediction_data)
        
        # è®¡ç®—åˆ†å¸ƒåç§»é²æ£’æ€§
        distribution_robustness = await self._calculate_distribution_robustness(prediction_data)
        
        # è®¡ç®—å¯¹æŠ—é²æ£’æ€§
        adversarial_robustness = await self._calculate_adversarial_robustness(model_data, training_data)
        
        # ç»¼åˆé²æ£’æ€§åˆ†æ•°
        overall_robustness = np.mean([
            noise_robustness,
            outlier_robustness,
            distribution_robustness,
            adversarial_robustness
        ])
        
        return DimensionResult(
            score=overall_robustness,
            status='completed',
            detailed_metrics={
                'noise_robustness': noise_robustness,
                'outlier_robustness': outlier_robustness,
                'distribution_robustness': distribution_robustness,
                'adversarial_robustness': adversarial_robustness
            }
        )
    
    async def _calculate_noise_robustness(self, model_data: ModelData, training_data: TrainingData) -> float:
        """è®¡ç®—å™ªå£°é²æ£’æ€§"""
        
        # åœ¨æµ‹è¯•æ•°æ®ä¸Šæ·»åŠ ä¸åŒç¨‹åº¦çš„å™ªå£°
        noise_levels = [0.01, 0.05, 0.1, 0.2]
        robustness_scores = []
        
        baseline_performance = await self._get_baseline_performance(model_data.model_version)
        
        for noise_level in noise_levels:
            # ç”Ÿæˆå¸¦å™ªå£°çš„æ•°æ®
            noisy_data = self._add_gaussian_noise(training_data.test_data, noise_level)
            
            # åœ¨å™ªå£°æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹
            noisy_performance = await self._evaluate_on_noisy_data(model_data.model, noisy_data)
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™ç¨‹åº¦
            performance_drop = (baseline_performance - noisy_performance) / baseline_performance
            
            # è®¡ç®—é²æ£’æ€§åˆ†æ•°
            robustness_score = max(0.0, 1.0 - performance_drop)
            robustness_scores.append(robustness_score)
        
        # å–å¹³å‡é²æ£’æ€§åˆ†æ•°
        return np.mean(robustness_scores)
```

---

## ğŸ”„ ç³»ç»Ÿé›†æˆ

### 1. ä¸ç°æœ‰æœåŠ¡é›†æˆ

#### æœåŠ¡å®¹å™¨é›†æˆ
```python
class AIServiceBootstrap:
    """AIæœåŠ¡å¯åŠ¨å™¨"""
    
    def __init__(self, service_container: UnifiedServiceContainer):
        self.container = service_container
        self.ai_services = {}
    
    async def initialize_ai_services(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        
        try:
            # æ³¨å†Œæ ¸å¿ƒAIæœåŠ¡
            await self._register_core_services()
            
            # åˆå§‹åŒ–æœåŠ¡ä¾èµ–
            await self._initialize_service_dependencies()
            
            # å¯åŠ¨äº‹ä»¶ç›‘å¬
            await self._setup_event_listeners()
            
            # å¯åŠ¨ç›‘æ§æœåŠ¡
            await self._start_monitoring_services()
            
            logger.info("AIæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def _register_core_services(self):
        """æ³¨å†Œæ ¸å¿ƒAIæœåŠ¡"""
        
        # æ¨¡å‹è®­ç»ƒæœåŠ¡
        self.container.register('model_training_service', ModelTrainingService)
        
        # é¢„æµ‹è·Ÿè¸ªæœåŠ¡
        self.container.register('prediction_tracking_service', PredictionTrackingService)
        
        # è´¨é‡è¯„ä¼°æœåŠ¡
        self.container.register('quality_assessment_service', QualityAssessmentService)
        
        # æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æœåŠ¡
        self.container.register('model_version_service', ModelVersionService)
        
        # æ€§èƒ½ç›‘æ§æœåŠ¡
        self.container.register('ai_performance_monitor', AIPerformanceMonitor)
        
        logger.info("æ ¸å¿ƒAIæœåŠ¡æ³¨å†Œå®Œæˆ")
    
    async def _initialize_service_dependencies(self):
        """åˆå§‹åŒ–æœåŠ¡ä¾èµ–"""
        
        # è·å–ä¾èµ–æœåŠ¡
        event_bus = self.container.get('event_bus')
        database_service = self.container.get('database_service')
        cache_service = self.container.get('cache_service')
        
        # ä¸ºAIæœåŠ¡è®¾ç½®ä¾èµ–
        model_training_service = self.container.get('model_training_service')
        model_training_service.set_dependencies(event_bus, database_service, cache_service)
        
        prediction_tracking_service = self.container.get('prediction_tracking_service')
        prediction_tracking_service.set_dependencies(event_bus, database_service, cache_service)
        
        logger.info("AIæœåŠ¡ä¾èµ–åˆå§‹åŒ–å®Œæˆ")
    
    async def _setup_event_listeners(self):
        """è®¾ç½®äº‹ä»¶ç›‘å¬å™¨"""
        
        event_bus = self.container.get('event_bus')
        
        # ç›‘å¬æ¨¡å‹ç›¸å…³äº‹ä»¶
        event_bus.subscribe('model_training_completed', self._on_model_training_completed)
        event_bus.subscribe('prediction_made', self._on_prediction_made)
        event_bus.subscribe('model_deployed', self._on_model_deployed)
        
        logger.info("AIäº‹ä»¶ç›‘å¬å™¨è®¾ç½®å®Œæˆ")
```

### 2. æ•°æ®åº“é›†æˆ

#### æ•°æ®åº“è¡¨ç»“æ„
```sql
-- æ¨¡å‹ç‰ˆæœ¬è¡¨
CREATE TABLE model_versions (
    version_id INTEGER PRIMARY KEY AUTOINCREMENT,
    version TEXT NOT NULL UNIQUE,
    task_id TEXT NOT NULL,
    model_type TEXT NOT NULL,
    file_path TEXT NOT NULL,
    training_metrics TEXT,  -- JSON
    validation_metrics TEXT, -- JSON
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status TEXT DEFAULT 'active',
    description TEXT
);

-- è®­ç»ƒä»»åŠ¡è¡¨
CREATE TABLE training_tasks (
    task_id TEXT PRIMARY KEY,
    task_name TEXT NOT NULL,
    description TEXT,
    model_type TEXT NOT NULL,
    model_params TEXT,  -- JSON
    training_params TEXT,  -- JSON
    status TEXT DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT
);

-- é¢„æµ‹è·Ÿè¸ªè¡¨
CREATE TABLE prediction_tracking (
    tracking_id TEXT PRIMARY KEY,
    model_version TEXT NOT NULL,
    symbol TEXT,
    input_data TEXT,  -- JSON
    predicted_value REAL,
    actual_value REAL,
    prediction_error REAL,
    confidence_score REAL,
    execution_time REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    actual_updated_at TIMESTAMP
);

-- è´¨é‡è¯„ä¼°è¡¨
CREATE TABLE quality_assessments (
    assessment_id INTEGER PRIMARY KEY AUTOINCREMENT,
    model_version TEXT NOT NULL,
    overall_score REAL,
    accuracy_score REAL,
    stability_score REAL,
    robustness_score REAL,
    efficiency_score REAL,
    detailed_metrics TEXT,  -- JSON
    recommendations TEXT,  -- JSON
    assessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ€§èƒ½ç›‘æ§è¡¨
CREATE TABLE performance_monitoring (
    monitor_id INTEGER PRIMARY KEY AUTOINCREMENT,
    model_version TEXT NOT NULL,
    metric_name TEXT NOT NULL,
    metric_value REAL,
    metric_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    alert_level TEXT
);
```

### 3. APIæ¥å£è®¾è®¡

#### RESTful API
```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

app = FastAPI(title="AI Model Training API", version="1.0.0")

class TrainingTaskRequest(BaseModel):
    task_name: str
    description: str
    model_type: str
    model_params: Dict[str, Any]
    symbols: List[str]
    start_date: str
    end_date: str
    training_params: Dict[str, Any]

class PredictionRequest(BaseModel):
    model_version: str
    input_data: Dict[str, Any]
    symbol: Optional[str] = None

class QualityAssessmentRequest(BaseModel):
    model_version: str
    assessment_scope: Optional[Dict[str, Any]] = None

@app.post("/training-tasks")
async def create_training_task(request: TrainingTaskRequest, background_tasks: BackgroundTasks):
    """åˆ›å»ºè®­ç»ƒä»»åŠ¡"""
    
    try:
        model_training_service = app.state.container.get('model_training_service')
        
        # åˆ›å»ºè®­ç»ƒä»»åŠ¡é…ç½®
        config = TrainingTaskConfig(
            task_id=str(uuid.uuid4()),
            task_name=request.task_name,
            description=request.description,
            model_type=request.model_type,
            model_params=request.model_params,
            symbols=request.symbols,
            start_date=request.start_date,
            end_date=request.end_date,
            training_params=request.training_params
        )
        
        # åˆ›å»ºä»»åŠ¡
        task_id = await model_training_service.create_training_task(config)
        
        # å¼‚æ­¥æ‰§è¡Œè®­ç»ƒ
        background_tasks.add_task(
            model_training_service.execute_training_task, task_id
        )
        
        return {
            "task_id": task_id,
            "status": "created",
            "message": "è®­ç»ƒä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predictions")
async def make_prediction(request: PredictionRequest):
    """æ‰§è¡Œé¢„æµ‹"""
    
    try:
        prediction_service = app.state.container.get('prediction_tracking_service')
        
        # æ‰§è¡Œé¢„æµ‹
        prediction_result = await prediction_service.execute_prediction(
            model_version=request.model_version,
            input_data=request.input_data
        )
        
        # è·Ÿè¸ªé¢„æµ‹ç»“æœ
        tracking_id = await prediction_service.track_prediction(
            model_version=request.model_version,
            prediction_input=request.input_data,
            prediction_output=prediction_result.output,
            symbol=request.symbol
        )
        
        return {
            "tracking_id": tracking_id,
            "prediction": prediction_result.output,
            "confidence": prediction_result.confidence,
            "execution_time": prediction_result.execution_time
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/{model_version}/quality-assessment")
async def get_quality_assessment(model_version: str):
    """è·å–æ¨¡å‹è´¨é‡è¯„ä¼°"""
    
    try:
        quality_service = app.state.container.get('quality_assessment_service')
        
        assessment_result = await quality_service.assess_model_quality(
            model_version=model_version,
            assessment_config=QualityAssessmentConfig()
        )
        
        return assessment_result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/{model_version}/performance-report")
async def get_performance_report(model_version: str, days: int = 30):
    """è·å–æ€§èƒ½æŠ¥å‘Š"""
    
    try:
        tracking_service = app.state.container.get('prediction_tracking_service')
        
        time_range = TimeRange(
            start_date=datetime.now() - timedelta(days=days),
            end_date=datetime.now()
        )
        
        report = await tracking_service.generate_performance_report(
            model_version=model_version,
            time_range=time_range
        )
        
        return report
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ“ˆ éƒ¨ç½²å’Œè¿ç»´

### 1. éƒ¨ç½²æ¶æ„

#### å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
# Dockerfile for AI Training Service
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Composeé…ç½®
```yaml
version: '3.8'

services:
  ai-training-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/ai_training
      - REDIS_URL=redis://redis:6379/0
      - EVENT_BUS_URL=rabbitmq://rabbitmq:5672
    depends_on:
      - postgres
      - redis
      - rabbitmq
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
    restart: unless-stopped

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=ai_training
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped

  redis:
    image: redis:6-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3-management
    environment:
      - RABBITMQ_DEFAULT_USER=user
      - RABBITMQ_DEFAULT_PASS=password
    ports:
      - "15672:15672"
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### 2. ç›‘æ§å’Œå‘Šè­¦

#### ç›‘æ§æŒ‡æ ‡
```python
class AIMonitoringMetrics:
    """AIç³»ç»Ÿç›‘æ§æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics = {
            # æ¨¡å‹è®­ç»ƒæŒ‡æ ‡
            'training_tasks_total': Counter('ai_training_tasks_total'),
            'training_tasks_failed': Counter('ai_training_tasks_failed'),
            'training_duration': Histogram('ai_training_duration_seconds'),
            'model_accuracy': Gauge('ai_model_accuracy'),
            
            # é¢„æµ‹æŒ‡æ ‡
            'predictions_total': Counter('ai_predictions_total'),
            'prediction_latency': Histogram('ai_prediction_latency_seconds'),
            'prediction_errors': Counter('ai_prediction_errors_total'),
            
            # è´¨é‡è¯„ä¼°æŒ‡æ ‡
            'quality_assessments_total': Counter('ai_quality_assessments_total'),
            'model_quality_score': Gauge('ai_model_quality_score'),
            'drift_detections': Counter('ai_drift_detections_total'),
            
            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            'gpu_utilization': Gauge('ai_gpu_utilization_percent'),
            'memory_usage': Gauge('ai_memory_usage_bytes'),
            'cpu_usage': Gauge('ai_cpu_usage_percent')
        }
    
    def record_training_start(self, model_type: str):
        """è®°å½•è®­ç»ƒå¼€å§‹"""
        self.metrics['training_tasks_total'].labels(model_type=model_type).inc()
    
    def record_training_completed(self, model_type: str, duration: float, success: bool):
        """è®°å½•è®­ç»ƒå®Œæˆ"""
        if success:
            self.metrics['training_duration'].labels(model_type=model_type).observe(duration)
        else:
            self.metrics['training_tasks_failed'].labels(model_type=model_type).inc()
    
    def record_prediction(self, model_version: str, latency: float, error: bool = False):
        """è®°å½•é¢„æµ‹"""
        self.metrics['predictions_total'].labels(model_version=model_version).inc()
        self.metrics['prediction_latency'].labels(model_version=model_version).observe(latency)
        
        if error:
            self.metrics['prediction_errors'].labels(model_version=model_version).inc()
```

#### å‘Šè­¦è§„åˆ™
```yaml
# prometheus-alerts.yml
groups:
  - name: ai_training_alerts
    rules:
      - alert: AITrainingTaskFailureRate
        expr: rate(ai_training_tasks_failed[5m]) / rate(ai_training_tasks_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "AIè®­ç»ƒä»»åŠ¡å¤±è´¥ç‡è¿‡é«˜"
          description: "è¿‡å»5åˆ†é’Ÿå†…AIè®­ç»ƒä»»åŠ¡å¤±è´¥ç‡è¶…è¿‡10%"

      - alert: AIPredictionLatencyHigh
        expr: histogram_quantile(0.95, rate(ai_prediction_latency_seconds_bucket[5m])) > 5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "AIé¢„æµ‹å»¶è¿Ÿè¿‡é«˜"
          description: "95%çš„é¢„æµ‹è¯·æ±‚å»¶è¿Ÿè¶…è¿‡5ç§’"

      - alert: AIModelQualityDegraded
        expr: ai_model_quality_score < 0.7
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "AIæ¨¡å‹è´¨é‡ä¸‹é™"
          description: "æ¨¡å‹è´¨é‡è¯„åˆ†ä½äº0.7"

      - alert: AIDataDriftDetected
        expr: increase(ai_drift_detections_total[1h]) > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "æ£€æµ‹åˆ°æ•°æ®æ¼‚ç§»"
          description: "è¿‡å»1å°æ—¶å†…æ£€æµ‹åˆ°è¶…è¿‡5æ¬¡æ•°æ®æ¼‚ç§»"
```

### 3. è¿ç»´è„šæœ¬

#### è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
```python
#!/usr/bin/env python3
"""
AIæ¨¡å‹è®­ç»ƒç³»ç»Ÿè¿ç»´è„šæœ¬
"""

import asyncio
import argparse
from datetime import datetime, timedelta
from typing import List, Dict

class AIOperationsManager:
    """AIè¿ç»´ç®¡ç†å™¨"""
    
    def __init__(self):
        self.container = None
    
    async def initialize(self):
        """åˆå§‹åŒ–è¿ç»´ç®¡ç†å™¨"""
        from main import app
        self.container = app.state.container
    
    async def cleanup_old_models(self, days: int = 30):
        """æ¸…ç†æ—§æ¨¡å‹æ–‡ä»¶"""
        
        model_version_service = self.container.get('model_version_service')
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        # è·å–éœ€è¦æ¸…ç†çš„æ¨¡å‹ç‰ˆæœ¬
        old_versions = await model_version_service.get_old_versions(cutoff_date)
        
        cleaned_count = 0
        for version in old_versions:
            try:
                await model_version_service.archive_version(version)
                cleaned_count += 1
                print(f"å·²å½’æ¡£æ¨¡å‹ç‰ˆæœ¬: {version}")
            except Exception as e:
                print(f"å½’æ¡£æ¨¡å‹ç‰ˆæœ¬å¤±è´¥ {version}: {str(e)}")
        
        print(f"æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_count} ä¸ªæ¨¡å‹ç‰ˆæœ¬")
    
    async def health_check(self) -> Dict:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        
        health_status = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'healthy',
            'services': {},
            'metrics': {}
        }
        
        try:
            # æ£€æŸ¥æ ¸å¿ƒæœåŠ¡
            model_training_service = self.container.get('model_training_service')
            prediction_service = self.container.get('prediction_tracking_service')
            
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            health_status['services']['model_training'] = await self._check_service_health(
                model_training_service
            )
            health_status['services']['prediction_tracking'] = await self._check_service_health(
                prediction_service
            )
            
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡
            health_status['metrics'] = await self._collect_key_metrics()
            
            # æ€»ä½“çŠ¶æ€
            if any(service['status'] != 'healthy' for service in health_status['services'].values()):
                health_status['overall_status'] = 'degraded'
            
        except Exception as e:
            health_status['overall_status'] = 'unhealthy'
            health_status['error'] = str(e)
        
        return health_status
    
    async def backup_system(self, backup_path: str):
        """ç³»ç»Ÿå¤‡ä»½"""
        
        import shutil
        import os
        
        backup_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        full_backup_path = f"{backup_path}/ai_system_backup_{backup_timestamp}"
        
        os.makedirs(full_backup_path, exist_ok=True)
        
        try:
            # å¤‡ä»½æ¨¡å‹æ–‡ä»¶
            model_path = "./models"
            if os.path.exists(model_path):
                shutil.copytree(model_path, f"{full_backup_path}/models")
            
            # å¤‡ä»½é…ç½®æ–‡ä»¶
            config_files = ["./config/", "./logs/"]
            for config_file in config_files:
                if os.path.exists(config_file):
                    if os.path.isfile(config_file):
                        shutil.copy2(config_file, full_backup_path)
                    else:
                        shutil.copytree(config_file, f"{full_backup_path}/{os.path.basename(config_file)}")
            
            # å¯¼å‡ºæ•°æ®åº“
            database_service = self.container.get('database_service')
            await database_service.export_backup(f"{full_backup_path}/database.sql")
            
            print(f"ç³»ç»Ÿå¤‡ä»½å®Œæˆ: {full_backup_path}")
            
        except Exception as e:
            print(f"ç³»ç»Ÿå¤‡ä»½å¤±è´¥: {str(e)}")
            raise

async def main():
    parser = argparse.ArgumentParser(description="AIè®­ç»ƒç³»ç»Ÿè¿ç»´å·¥å…·")
    parser.add_argument('action', choices=['cleanup', 'health', 'backup'],
                       help="æ‰§è¡Œçš„æ“ä½œ")
    parser.add_argument('--days', type=int, default=30,
                       help="æ¸…ç†æ“ä½œçš„æ—¥æœŸé˜ˆå€¼")
    parser.add_argument('--path', type=str, default='./backups',
                       help="å¤‡ä»½è·¯å¾„")
    
    args = parser.parse_args()
    
    ops_manager = AIOperationsManager()
    await ops_manager.initialize()
    
    if args.action == 'cleanup':
        await ops_manager.cleanup_old_models(args.days)
    elif args.action == 'health':
        health = await ops_manager.health_check()
        print(f"ç³»ç»Ÿå¥åº·çŠ¶æ€: {health}")
    elif args.action == 'backup':
        await ops_manager.backup_system(args.path)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ¯ æ€»ç»“

### ç³»ç»Ÿä¼˜åŠ¿

1. **å®Œæ•´æ€§**ï¼šæ¶µç›–æ¨¡å‹è®­ç»ƒã€é¢„æµ‹è·Ÿè¸ªã€è´¨é‡è¯„ä¼°çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
2. **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒæ–°æ¨¡å‹ç±»å‹å’Œè¯„ä¼°ç»´åº¦
3. **å¯é æ€§**ï¼šåŸºäºç°æœ‰æœåŠ¡å®¹å™¨æ¶æ„ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
4. **æ€§èƒ½**ï¼šå¼‚æ­¥å¤„ç†å’Œå¹¶å‘ä¼˜åŒ–ï¼Œç¡®ä¿é«˜æ€§èƒ½
5. **ç›‘æ§**ï¼šå…¨æ–¹ä½çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

### æ ¸å¿ƒä»·å€¼

1. **æå‡æ¨¡å‹è´¨é‡**ï¼šé€šè¿‡å…¨é¢çš„è´¨é‡è¯„ä¼°å’ŒæŒç»­ç›‘æ§
2. **é™ä½ç»´æŠ¤æˆæœ¬**ï¼šè‡ªåŠ¨åŒ–è®­ç»ƒå’Œéƒ¨ç½²æµç¨‹
3. **å¢å¼ºç”¨æˆ·ä½“éªŒ**ï¼šç¨³å®šå¯é çš„é¢„æµ‹æœåŠ¡
4. **æ”¯æŒä¸šåŠ¡å†³ç­–**ï¼šåŸºäºæ•°æ®çš„æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–

### å®æ–½å»ºè®®

1. **åˆ†é˜¶æ®µéƒ¨ç½²**ï¼šä»æ ¸å¿ƒåŠŸèƒ½å¼€å§‹ï¼Œé€æ­¥æ‰©å±•
2. **æ¸è¿›å¼è¿ç§»**ï¼šä¸ç°æœ‰ç³»ç»Ÿå¹¶è¡Œè¿è¡Œï¼Œé€æ­¥åˆ‡æ¢
3. **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºç›‘æ§æ•°æ®ä¸æ–­ä¼˜åŒ–ç³»ç»Ÿ
4. **å›¢é˜ŸåŸ¹è®­**ï¼šç¡®ä¿å›¢é˜ŸæŒæ¡æ–°ç³»ç»Ÿçš„ä½¿ç”¨æ–¹æ³•

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**ç”Ÿæˆæ—¶é—´**ï¼š2024-12-19  
**ç»´æŠ¤è€…**ï¼šAIè®­ç»ƒç³»ç»Ÿå¼€å‘å›¢é˜Ÿ

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [AIæ¨¡å‹è®­ç»ƒå¼€å‘è®¡åˆ’](./ai_model_training_development_plan.md) - è¯¦ç»†å¼€å‘è®¡åˆ’
- [å½¢æ€åˆ†æä¸­AIé¢„æµ‹åŠŸèƒ½æ·±åº¦åˆ†ææŠ¥å‘Š](./å½¢æ€åˆ†æä¸­AIé¢„æµ‹åŠŸèƒ½æ·±åº¦åˆ†ææŠ¥å‘Š.md) - åº”ç”¨åœºæ™¯åˆ†æ
- [æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°ç³»ç»Ÿä½¿ç”¨æŒ‡å—](./æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°ç³»ç»Ÿä½¿ç”¨æŒ‡å—.md) - è´¨é‡è¯„ä¼°æŒ‡å—