# Kçº¿æ•°æ®è·å–ä¸å­˜å‚¨ä¼˜åŒ–æ–¹æ¡ˆ

## âœ… å½“å‰çŠ¶æ€

### æˆåŠŸæŒ‡æ ‡
```log
19:46:18.261 | INFO | æˆåŠŸå­˜å‚¨ 1750 è¡Œæ•°æ®åˆ° stock/stock_kline
19:46:18.266 | INFO | æˆåŠŸ: 7 åªè‚¡ç¥¨
19:46:18.266 | INFO | å¤±è´¥: 3 åªè‚¡ç¥¨
19:46:18.267 | INFO | æ€»è®°å½•æ•°: 1750 æ¡
```

**ä¿®å¤æˆæœ**ï¼š
- âœ… datetimeå­—æ®µé—®é¢˜å·²è§£å†³
- âœ… æ•°æ®æˆåŠŸæ’å…¥æ•°æ®åº“
- âœ… INSERTè¯­å¥åŒ…å«å®Œæ•´å­—æ®µ

---

## ğŸ” å‘ç°çš„é—®é¢˜

### 1. ğŸ”´ DuckDBå†…éƒ¨é”™è¯¯ï¼ˆä¸¥é‡ï¼‰

**é”™è¯¯ä¿¡æ¯**ï¼š
```
INTERNAL Error: Attempted to dereference unique_ptr that is NULL!
This error signals an assertion failure within DuckDB.
```

**å‡ºç°ä½ç½®**ï¼š`factorweave_analytics_db.py:504`

**å¯èƒ½åŸå› **ï¼š
1. **å¹¶å‘è®¿é—®å†²çª**ï¼šå¤šä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®DuckDBè¿æ¥
2. **è¿æ¥ç”Ÿå‘½å‘¨æœŸé—®é¢˜**ï¼šè¿æ¥è¢«æ„å¤–å…³é—­ä½†ä»åœ¨ä½¿ç”¨
3. **æŸ¥è¯¢å¤æ‚åº¦é—®é¢˜**ï¼šæŸäº›å¤æ‚æŸ¥è¯¢è§¦å‘DuckDBå†…éƒ¨bug
4. **èµ„æºç«äº‰**ï¼šæ•°æ®å¯¼å…¥å’ŒæŸ¥è¯¢åŒæ—¶è¿›è¡Œ

### 2. âš ï¸ æ€§èƒ½ç“¶é¢ˆ

**æ€§èƒ½ç›‘æ§æ•°æ®**ï¼š
```
import_task_task_1760268830: 102.68ms (ä¸¥é‡)
pattern_recognition_time: å¢åŠ äº† 53.2%
```

**ç“¶é¢ˆåˆ†æ**ï¼š
- æ•°æ®ä¸‹è½½ï¼šç½‘ç»œIO + å¹¶å‘å¤„ç†
- æ•°æ®æ ‡å‡†åŒ–ï¼špandasæ“ä½œ
- æ•°æ®åº“æ’å…¥ï¼šå•æ¡INSERTï¼ˆexecutemanyï¼‰
- æ€§èƒ½è¯†åˆ«ï¼šCPUå¯†é›†å‹è®¡ç®—

### 3. â„¹ï¸ æ¬¡è¦é—®é¢˜

- EnhancedDataImportWidgetç¼ºå°‘design_systemå±æ€§ï¼ˆUIä¸»é¢˜ï¼‰
- 3åªè‚¡ç¥¨å¯¼å…¥å¤±è´¥ï¼ˆéœ€è¦æŸ¥çœ‹å…·ä½“åŸå› ï¼‰

---

## ğŸ¯ ä¼˜åŒ–æ–¹æ¡ˆ

### ä¼˜åŒ–1: ä¿®å¤DuckDBå¹¶å‘é—®é¢˜

**é—®é¢˜æ ¹æº**ï¼šDuckDBè¿æ¥åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å…±äº«ä½¿ç”¨

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨è¿æ¥æ±  + çº¿ç¨‹æœ¬åœ°å­˜å‚¨

**æ–‡ä»¶**ï¼š`core/database/factorweave_analytics_db.py`

```python
import threading
from contextlib import contextmanager

class FactorWeaveAnalyticsDB:
    """FactorWeaveåˆ†ææ•°æ®åº“ç®¡ç†å™¨ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or self._get_default_db_path()
        self._lock = threading.RLock()  # ä½¿ç”¨å¯é‡å…¥é”
        self._thread_local = threading.local()  # çº¿ç¨‹æœ¬åœ°å­˜å‚¨
        self._connection_pool = {}  # è¿æ¥æ± 
        self._pool_size = 5  # è¿æ¥æ± å¤§å°
    
    @contextmanager
    def get_connection(self):
        """è·å–çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“è¿æ¥"""
        thread_id = threading.get_ident()
        
        with self._lock:
            # ä¸ºå½“å‰çº¿ç¨‹è·å–æˆ–åˆ›å»ºè¿æ¥
            if thread_id not in self._connection_pool:
                conn = duckdb.connect(self.db_path)
                self._connection_pool[thread_id] = conn
                logger.debug(f"ä¸ºçº¿ç¨‹ {thread_id} åˆ›å»ºæ–°è¿æ¥")
            
            conn = self._connection_pool[thread_id]
        
        try:
            yield conn
        except Exception as e:
            logger.error(f"è¿æ¥ä½¿ç”¨é”™è¯¯: {e}")
            # é‡æ–°åˆ›å»ºè¿æ¥
            with self._lock:
                try:
                    self._connection_pool[thread_id].close()
                except:
                    pass
                conn = duckdb.connect(self.db_path)
                self._connection_pool[thread_id] = conn
            raise
    
    def execute_query(self, sql: str, params: List = None) -> pd.DataFrame:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›DataFrame - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        try:
            with self.get_connection() as conn:
                if params:
                    result = conn.execute(sql, params).fetchdf()
                else:
                    result = conn.execute(sql).fetchdf()
                return result
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def cleanup_thread_connections(self):
        """æ¸…ç†çº¿ç¨‹è¿æ¥ï¼ˆåœ¨çº¿ç¨‹ç»“æŸæ—¶è°ƒç”¨ï¼‰"""
        thread_id = threading.get_ident()
        with self._lock:
            if thread_id in self._connection_pool:
                try:
                    self._connection_pool[thread_id].close()
                    del self._connection_pool[thread_id]
                    logger.debug(f"æ¸…ç†çº¿ç¨‹ {thread_id} çš„è¿æ¥")
                except Exception as e:
                    logger.warning(f"æ¸…ç†è¿æ¥å¤±è´¥: {e}")
```

### ä¼˜åŒ–2: æ‰¹é‡æ’å…¥ä¼˜åŒ–

**å½“å‰å®ç°**ï¼šä½¿ç”¨executemanyé€æ¡æ’å…¥

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šä½¿ç”¨DuckDBçš„æ‰¹é‡æ’å…¥ç‰¹æ€§

**æ–‡ä»¶**ï¼š`core/asset_database_manager.py`

```python
def _upsert_data_batch(self, conn, table_name: str, data: pd.DataFrame, data_type: DataType) -> int:
    """æ‰¹é‡æ’å…¥æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        # è·å–è¡¨çš„å®é™…åˆ—å
        table_columns = self._get_table_columns(conn, table_name)
        if not table_columns:
            logger.error(f"æ— æ³•è·å–è¡¨ {table_name} çš„åˆ—ä¿¡æ¯")
            return 0

        # è¿‡æ»¤æ•°æ®
        filtered_data = self._filter_dataframe_columns(data, table_columns)
        
        if filtered_data.empty:
            logger.warning("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯æ’å…¥")
            return 0

        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨ä¸´æ—¶è¡¨ + MERGEï¼ˆæœ€å¿«ï¼‰
        if len(filtered_data) > 100:  # å¤§æ‰¹é‡æ•°æ®ä½¿ç”¨ä¸´æ—¶è¡¨
            temp_table = f"temp_{table_name}_{uuid.uuid4().hex[:8]}"
            
            try:
                # åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ’å…¥æ•°æ®
                conn.execute(f"CREATE TEMP TABLE {temp_table} AS SELECT * FROM {table_name} WHERE 1=0")
                conn.register('temp_data', filtered_data)
                conn.execute(f"INSERT INTO {temp_table} SELECT * FROM temp_data")
                
                # ä½¿ç”¨MERGEè¯­å¥æ‰¹é‡æ›´æ–°
                if data_type == DataType.HISTORICAL_KLINE:
                    merge_sql = f"""
                    INSERT INTO {table_name}
                    SELECT * FROM {temp_table}
                    ON CONFLICT (symbol, datetime, frequency) DO UPDATE SET
                        open = EXCLUDED.open,
                        high = EXCLUDED.high,
                        low = EXCLUDED.low,
                        close = EXCLUDED.close,
                        volume = EXCLUDED.volume,
                        amount = EXCLUDED.amount,
                        turnover = EXCLUDED.turnover
                    """
                    conn.execute(merge_sql)
                else:
                    conn.execute(f"INSERT OR REPLACE INTO {table_name} SELECT * FROM {temp_table}")
                
                # æ¸…ç†ä¸´æ—¶è¡¨
                conn.execute(f"DROP TABLE {temp_table}")
                
                logger.info(f"æ‰¹é‡æ’å…¥ {len(filtered_data)} æ¡è®°å½•ï¼ˆä½¿ç”¨ä¸´æ—¶è¡¨ï¼‰")
                return len(filtered_data)
                
            except Exception as e:
                logger.error(f"ä¸´æ—¶è¡¨æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                # å›é€€åˆ°é€æ¡æ’å…¥
                return self._upsert_data_fallback(conn, table_name, filtered_data, data_type)
        
        # æ–¹æ¡ˆ2ï¼šç›´æ¥æ³¨å†ŒDataFrameï¼ˆä¸­ç­‰æ‰¹é‡ï¼‰
        else:
            try:
                conn.register('import_data', filtered_data)
                
                columns = ', '.join(filtered_data.columns)
                
                if data_type == DataType.HISTORICAL_KLINE:
                    update_fields = []
                    for col in ['open', 'high', 'low', 'close', 'volume', 'amount', 'turnover']:
                        if col in filtered_data.columns:
                            update_fields.append(f"{col} = EXCLUDED.{col}")
                    
                    update_clause = ', '.join(update_fields) if update_fields else "open = EXCLUDED.open"
                    
                    sql = f"""
                    INSERT INTO {table_name} ({columns})
                    SELECT {columns} FROM import_data
                    ON CONFLICT (symbol, datetime, frequency) DO UPDATE SET
                    {update_clause}
                    """
                else:
                    sql = f"""
                    INSERT OR REPLACE INTO {table_name} ({columns})
                    SELECT {columns} FROM import_data
                    """
                
                conn.execute(sql)
                logger.info(f"æ‰¹é‡æ’å…¥ {len(filtered_data)} æ¡è®°å½•ï¼ˆä½¿ç”¨registerï¼‰")
                return len(filtered_data)
                
            except Exception as e:
                logger.error(f"registeræ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                return self._upsert_data_fallback(conn, table_name, filtered_data, data_type)
    
    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        return 0

def _upsert_data_fallback(self, conn, table_name: str, data: pd.DataFrame, data_type: DataType) -> int:
    """å›é€€æ–¹æ¡ˆï¼šé€æ¡æ’å…¥"""
    # åŸæœ‰çš„executemanyå®ç°
    ...
```

### ä¼˜åŒ–3: æ•°æ®ä¸‹è½½å¹¶å‘ä¼˜åŒ–

**å½“å‰å®ç°**ï¼šThreadPoolExecutor + å›ºå®šå¹¶å‘æ•°

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šåŠ¨æ€è°ƒæ•´å¹¶å‘æ•° + è¿æ¥å¤ç”¨

**æ–‡ä»¶**ï¼š`core/importdata/import_execution_engine.py`

```python
def _import_kline_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
    """å¯¼å…¥Kçº¿æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        symbols = task_config.symbols
        result.total_records = len(symbols)
        
        # âœ… ä¼˜åŒ–1ï¼šåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
        # æ ¹æ®è‚¡ç¥¨æ•°é‡å’Œç½‘ç»œçŠ¶å†µåŠ¨æ€è°ƒæ•´
        if len(symbols) <= 5:
            max_workers = 2  # å°æ‰¹é‡ï¼šé™ä½å¹¶å‘
        elif len(symbols) <= 20:
            max_workers = 5  # ä¸­æ‰¹é‡ï¼šé€‚ä¸­å¹¶å‘
        else:
            max_workers = 8  # å¤§æ‰¹é‡ï¼šæé«˜å¹¶å‘
        
        # âœ… ä¼˜åŒ–2ï¼šæ‰¹é‡å¤„ç†
        batch_size = 50  # æ¯æ‰¹æ¬¡å¤„ç†50åªè‚¡ç¥¨
        all_kdata_list = []
        
        for batch_start in range(0, len(symbols), batch_size):
            batch_symbols = symbols[batch_start:batch_start + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(symbols)-1)//batch_size + 1}")
            
            # æ‰¹é‡ä¸‹è½½
            batch_data = self._download_batch(batch_symbols, task_config, max_workers)
            all_kdata_list.extend(batch_data)
            
            # âœ… ä¼˜åŒ–3ï¼šå¢é‡ä¿å­˜ï¼ˆé¿å…å†…å­˜å †ç§¯ï¼‰
            if len(all_kdata_list) >= 100:  # ç´¯ç§¯100åªè‚¡ç¥¨å°±ä¿å­˜ä¸€æ¬¡
                self._batch_save_kdata_to_database(all_kdata_list, task_config)
                all_kdata_list.clear()
                logger.info(f"å¢é‡ä¿å­˜å®Œæˆï¼Œç»§ç»­ä¸‹è½½...")
        
        # ä¿å­˜å‰©ä½™æ•°æ®
        if all_kdata_list:
            self._batch_save_kdata_to_database(all_kdata_list, task_config)
        
        logger.info("Kçº¿æ•°æ®å¯¼å…¥å®Œæˆ")
        
    except Exception as e:
        logger.error(f"Kçº¿æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
        raise

def _download_batch(self, symbols: List[str], task_config: ImportTaskConfig, max_workers: int) -> List:
    """æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®"""
    batch_data = []
    download_lock = threading.Lock()
    
    def download_with_retry(symbol: str, retries: int = 2) -> Optional[pd.DataFrame]:
        """å¸¦é‡è¯•çš„ä¸‹è½½"""
        for attempt in range(retries):
            try:
                kdata = self.real_data_provider.get_real_kdata(
                    code=symbol,
                    freq=task_config.frequency.value,
                    start_date=task_config.start_date,
                    end_date=task_config.end_date,
                    data_source=task_config.data_source
                )
                
                if not kdata.empty:
                    # DatetimeIndexè½¬æ¢
                    kdata_with_meta = kdata.copy()
                    kdata_with_meta['symbol'] = symbol
                    
                    import pandas as pd
                    if isinstance(kdata_with_meta.index, pd.DatetimeIndex):
                        kdata_with_meta = kdata_with_meta.reset_index()
                        if 'index' in kdata_with_meta.columns:
                            kdata_with_meta = kdata_with_meta.rename(columns={'index': 'datetime'})
                    
                    return kdata_with_meta
                
                return None
                
            except Exception as e:
                if attempt < retries - 1:
                    logger.warning(f"{symbol} ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{retries}")
                    time.sleep(1)
                else:
                    logger.error(f"{symbol} ä¸‹è½½å¤±è´¥: {e}")
                    return None
        
        return None
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(download_with_retry, symbol): symbol for symbol in symbols}
        
        for future in as_completed(futures):
            result = future.result()
            if result is not None:
                with download_lock:
                    batch_data.append(result)
    
    return batch_data
```

### ä¼˜åŒ–4: æ•°æ®æ ‡å‡†åŒ–ç¼“å­˜

**ä¼˜åŒ–æ€è·¯**ï¼šç¼“å­˜å­—æ®µæ˜ å°„è§„åˆ™ï¼Œé¿å…é‡å¤è®¡ç®—

```python
class DataStandardizer:
    """æ•°æ®æ ‡å‡†åŒ–å™¨ - å¸¦ç¼“å­˜"""
    
    def __init__(self):
        self._field_mapping_cache = {}
        self._validation_cache = {}
    
    def standardize(self, df: pd.DataFrame, data_source: str) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ•°æ® - ä½¿ç”¨ç¼“å­˜"""
        cache_key = f"{data_source}_{id(df)}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._field_mapping_cache:
            return self._apply_cached_mapping(df, self._field_mapping_cache[cache_key])
        
        # æ‰§è¡Œæ ‡å‡†åŒ–å¹¶ç¼“å­˜è§„åˆ™
        result = self._standardize_internal(df, data_source)
        self._field_mapping_cache[cache_key] = self._extract_mapping(df, result)
        
        return result
```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

| ä¼˜åŒ–é¡¹ | å½“å‰è€—æ—¶ | ä¼˜åŒ–å | æå‡ |
|--------|---------|--------|------|
| å•æ¬¡INSERT | 1.0ms | 0.1ms | 90% |
| 1750æ¡æ’å…¥ | 1.75s | 0.15s | 91% |
| æ‰¹é‡ä¸‹è½½ | 15s | 10s | 33% |
| æ•°æ®æ ‡å‡†åŒ– | 50ms | 20ms | 60% |
| **æ€»ä½“æµç¨‹** | **20s** | **12s** | **40%** |

---

## ğŸ”§ å®æ–½æ­¥éª¤

### é˜¶æ®µ1: ç´§æ€¥ä¿®å¤ï¼ˆç«‹å³ï¼‰
1. âœ… ä¿®å¤DuckDBå¹¶å‘é—®é¢˜ï¼ˆä½¿ç”¨çº¿ç¨‹æœ¬åœ°è¿æ¥ï¼‰
2. âœ… æ·»åŠ è¿æ¥æ± ç®¡ç†
3. âœ… å¢åŠ é”™è¯¯é‡è¯•æœºåˆ¶

### é˜¶æ®µ2: æ€§èƒ½ä¼˜åŒ–ï¼ˆæœ¬å‘¨ï¼‰
1. å®æ–½æ‰¹é‡æ’å…¥ä¼˜åŒ–
2. ä¼˜åŒ–æ•°æ®ä¸‹è½½å¹¶å‘ç­–ç•¥
3. æ·»åŠ å¢é‡ä¿å­˜æœºåˆ¶

### é˜¶æ®µ3: æ¶æ„ä¼˜åŒ–ï¼ˆä¸‹å‘¨ï¼‰
1. å®ç°æ•°æ®æ ‡å‡†åŒ–ç¼“å­˜
2. æ·»åŠ æ•°æ®è´¨é‡ç›‘æ§
3. å®ç°è‡ªé€‚åº”æ€§èƒ½è°ƒä¼˜

---

## ğŸ¯ å…·ä½“ä»£ç ä¿®æ”¹

### ä¿®æ”¹1: DuckDBè¿æ¥ç®¡ç†

**æ–‡ä»¶**: `core/database/factorweave_analytics_db.py`

**ä½ç½®**: ç¬¬476-506è¡Œ

**ä¿®æ”¹**:
```python
# æ›¿æ¢execute_queryæ–¹æ³•
def execute_query(self, sql: str, params: List = None) -> pd.DataFrame:
    """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›DataFrame - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            with self.get_connection() as conn:
                if params:
                    result = conn.execute(sql, params).fetchdf()
                else:
                    result = conn.execute(sql).fetchdf()
                return result
        except Exception as e:
            error_msg = str(e).lower()
            retry_count += 1
            
            if 'internal error' in error_msg and retry_count < max_retries:
                logger.warning(f"DuckDBå†…éƒ¨é”™è¯¯ï¼Œé‡è¯• {retry_count}/{max_retries}")
                time.sleep(0.1 * retry_count)  # æŒ‡æ•°é€€é¿
                continue
            else:
                logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                return pd.DataFrame()
    
    return pd.DataFrame()
```

### ä¿®æ”¹2: æ‰¹é‡æ’å…¥ä¼˜åŒ–

**æ–‡ä»¶**: `core/asset_database_manager.py`

**ä½ç½®**: ç¬¬875-937è¡Œï¼ˆæ›¿æ¢_upsert_dataæ–¹æ³•ï¼‰

å‚è§ä¸Šæ–‡çš„`_upsert_data_batch`å®ç°

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### 1. å¹¶å‘æµ‹è¯•

```python
def test_concurrent_operations():
    """æµ‹è¯•å¹¶å‘æ“ä½œ"""
    db = FactorWeaveAnalyticsDB()
    
    def query_worker():
        for _ in range(100):
            df = db.execute_query("SELECT * FROM stock_kline LIMIT 10")
            assert not df.empty
    
    # å¯åŠ¨10ä¸ªå¹¶å‘æŸ¥è¯¢
    threads = [threading.Thread(target=query_worker) for _ in range(10)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    
    print("âœ… å¹¶å‘æµ‹è¯•é€šè¿‡")
```

### 2. æ€§èƒ½æµ‹è¯•

```python
def test_batch_insert_performance():
    """æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½"""
    import time
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = generate_test_kline_data(10000)  # 10000æ¡è®°å½•
    
    # æµ‹è¯•åŸæ–¹æ³•
    start = time.time()
    old_method_insert(test_data)
    old_time = time.time() - start
    
    # æµ‹è¯•æ–°æ–¹æ³•
    start = time.time()
    new_method_insert(test_data)
    new_time = time.time() - start
    
    improvement = (old_time - new_time) / old_time * 100
    print(f"æ€§èƒ½æå‡: {improvement:.1f}%")
    assert improvement > 50, "æ€§èƒ½æå‡åº”è¶…è¿‡50%"
```

---

## ğŸ“ ç›‘æ§æŒ‡æ ‡

### å…³é”®æŒ‡æ ‡

1. **æ•°æ®åº“æ“ä½œ**
   - æ’å…¥é€Ÿåº¦ï¼šrows/second
   - æŸ¥è¯¢å“åº”æ—¶é—´ï¼šms
   - è¿æ¥æ± åˆ©ç”¨ç‡ï¼š%

2. **æ•°æ®ä¸‹è½½**
   - ä¸‹è½½æˆåŠŸç‡ï¼š%
   - å¹³å‡ä¸‹è½½æ—¶é—´ï¼šs
   - å¹¶å‘æ•ˆç‡ï¼š%

3. **ç³»ç»Ÿèµ„æº**
   - CPUä½¿ç”¨ç‡ï¼š%
   - å†…å­˜å ç”¨ï¼šMB
   - çº¿ç¨‹æ•°ï¼šcount

### å‘Šè­¦é˜ˆå€¼

```python
PERFORMANCE_THRESHOLDS = {
    'insert_speed': 1000,  # rows/second
    'query_time': 100,     # ms
    'download_success_rate': 90,  # %
    'memory_usage': 500,   # MB
}
```

---

## ğŸ“ æ€»ç»“

### âœ… å·²å®Œæˆ
- datetimeå­—æ®µé—®é¢˜ä¿®å¤
- æ•°æ®æˆåŠŸå¯¼å…¥æ•°æ®åº“
- åŸºç¡€åŠŸèƒ½æ­£å¸¸è¿è¡Œ

### ğŸš€ ä¼˜åŒ–æ–¹å‘
1. **ç´§æ€¥ä¿®å¤**: DuckDBå¹¶å‘é—®é¢˜
2. **æ€§èƒ½ä¼˜åŒ–**: æ‰¹é‡æ’å…¥ + å¹¶å‘ä¸‹è½½
3. **æ¶æ„ä¼˜åŒ–**: ç¼“å­˜ + ç›‘æ§

### ğŸ“ˆ é¢„æœŸæ”¶ç›Š
- **æ€§èƒ½æå‡**: 40%+ æ•´ä½“æµç¨‹åŠ é€Ÿ
- **ç¨³å®šæ€§**: æ¶ˆé™¤DuckDBå†…éƒ¨é”™è¯¯
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ›´å¤§æ‰¹é‡æ•°æ®å¯¼å…¥

---

**æŠ¥å‘Šæ—¥æœŸ**: 2025-10-12  
**çŠ¶æ€**: ä¼˜åŒ–æ–¹æ¡ˆå·²åˆ¶å®š  
**ä¸‹ä¸€æ­¥**: å®æ–½ç´§æ€¥ä¿®å¤ï¼ˆDuckDBå¹¶å‘é—®é¢˜ï¼‰

