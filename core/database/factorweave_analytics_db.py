#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FactorWeave-Quant åˆ†ææ•°æ®åº“ç®¡ç†å™¨

ä½¿ç”¨DuckDBå­˜å‚¨å’Œç®¡ç†æ‰€æœ‰åˆ†æç›¸å…³çš„æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
- ç­–ç•¥æ‰§è¡Œç»“æœ
- æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç»“æœ
- å›æµ‹ç›‘æ§æ•°æ®
- æ€§èƒ½æŒ‡æ ‡
- ä¼˜åŒ–æ—¥å¿—

é›†æˆæ€§èƒ½ä¼˜åŒ–å™¨ï¼ŒåŸºäº2024å¹´æœ€æ–°DuckDBæœ€ä½³å®è·µè‡ªåŠ¨ä¼˜åŒ–é…ç½®
"""

import pandas as pd
import logging

# å®‰å…¨å¯¼å…¥DuckDB
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError as e:
    logging.warning(f"DuckDBæ¨¡å—ä¸å¯ç”¨: {e}")
    duckdb = None
    DUCKDB_AVAILABLE = False
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import threading

# å¯¼å…¥æ€§èƒ½ä¼˜åŒ–å™¨
try:
    from .duckdb_performance_optimizer import (
        DuckDBPerformanceOptimizer,
        WorkloadType,
        get_optimized_duckdb_connection
    )
    OPTIMIZER_AVAILABLE = True
except ImportError:
    OPTIMIZER_AVAILABLE = False
    logging.warning("DuckDBæ€§èƒ½ä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

    # æä¾›å¤‡ç”¨å®šä¹‰ï¼Œé¿å…NameError
    from enum import Enum

    class WorkloadType(Enum):
        """å¤‡ç”¨WorkloadTypeå®šä¹‰"""
        OLAP = "olap"
        OLTP = "oltp"
        MIXED = "mixed"

    class DuckDBPerformanceOptimizer:
        """å¤‡ç”¨DuckDBPerformanceOptimizerå®šä¹‰"""

        def __init__(self, db_path: str):
            self.db_path = db_path
            self.current_config = None

        def optimize_for_workload(self, workload_type):
            pass

        def get_connection(self):
            return None

        def get_performance_recommendations(self):
            return []

        def close(self):
            pass

    def get_optimized_duckdb_connection(db_path: str):
        """å¤‡ç”¨å‡½æ•°å®šä¹‰"""
        return None


logger = logging.getLogger(__name__)


class FactorWeaveAnalyticsDB:
    """FactorWeaveåˆ†ææ•°æ®åº“ç®¡ç†å™¨ - åŸºäºDuckDB"""

    _instances = {}  # æ”¹ä¸ºå­—å…¸ï¼Œæ”¯æŒå¤šä¸ªæ•°æ®åº“å®ä¾‹
    _lock = threading.Lock()

    def __new__(cls, db_path: str = 'db/factorweave_analytics.duckdb'):
        """å•ä¾‹æ¨¡å¼å®ç° - æŒ‰æ•°æ®åº“è·¯å¾„åŒºåˆ†å®ä¾‹"""
        db_path = str(Path(db_path).resolve())  # æ ‡å‡†åŒ–è·¯å¾„

        with cls._lock:
            if db_path not in cls._instances:
                cls._instances[db_path] = super().__new__(cls)
                cls._instances[db_path]._initialized = False

        return cls._instances[db_path]

    def __init__(self, db_path: str = 'db/factorweave_analytics.duckdb'):
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return

        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.conn = None
        self.optimizer = None

        # åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        if OPTIMIZER_AVAILABLE and DUCKDB_AVAILABLE:
            self.optimizer = DuckDBPerformanceOptimizer(str(self.db_path))
            logger.info("âœ… DuckDBæ€§èƒ½ä¼˜åŒ–å™¨å·²å¯ç”¨")
        else:
            self.optimizer = None

        self._connect()
        self._init_tables()

        self._initialized = True
        logger.info(f"âœ… FactorWeaveåˆ†ææ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def _check_connection(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥æ˜¯å¦å¯ç”¨"""
        if not DUCKDB_AVAILABLE:
            logger.warning("DuckDBä¸å¯ç”¨ï¼Œæ“ä½œè¢«è·³è¿‡")
            return False
        if self.conn is None:
            logger.warning("æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œæ“ä½œè¢«è·³è¿‡")
            return False
        return True

    def _connect(self):
        """è¿æ¥åˆ°DuckDBæ•°æ®åº“"""
        if not DUCKDB_AVAILABLE:
            logger.warning("âš ï¸ DuckDBä¸å¯ç”¨ï¼Œåˆ†ææ•°æ®åº“åŠŸèƒ½å°†è¢«ç¦ç”¨")
            self.conn = None
            return

        try:
            if OPTIMIZER_AVAILABLE and DUCKDB_AVAILABLE and self.optimizer:
                # ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–çš„è¿æ¥
                self.optimizer.optimize_for_workload(WorkloadType.OLAP)
                optimized_conn = self.optimizer.get_connection()

                if optimized_conn is not None:
                    self.conn = optimized_conn
                    logger.info(f"ğŸ“Š DuckDBè¿æ¥æˆåŠŸ (æ€§èƒ½ä¼˜åŒ–): {self.db_path}")

                    # æ˜¾ç¤ºä¼˜åŒ–é…ç½®
                    if hasattr(self.optimizer, 'current_config') and self.optimizer.current_config:
                        config = self.optimizer.current_config
                        logger.info(f"ğŸ”§ ä¼˜åŒ–é…ç½®: å†…å­˜={config.memory_limit}, çº¿ç¨‹={config.threads}")

                    # æ˜¾ç¤ºæ€§èƒ½å»ºè®®
                    recommendations = self.optimizer.get_performance_recommendations()
                    if recommendations:
                        logger.info("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
                        for rec in recommendations[:3]:  # åªæ˜¾ç¤ºå‰3æ¡
                            logger.info(f"  {rec}")
                else:
                    # ä¼˜åŒ–å™¨è¿”å›Noneï¼Œå›é€€åˆ°é»˜è®¤è¿æ¥
                    self.conn = duckdb.connect(str(self.db_path))
                    logger.info(f"ğŸ“Š DuckDBè¿æ¥æˆåŠŸ (é»˜è®¤é…ç½®-ä¼˜åŒ–å™¨å›é€€): {self.db_path}")
                    self._apply_basic_optimization()
            else:
                # ä½¿ç”¨é»˜è®¤è¿æ¥
                self.conn = duckdb.connect(str(self.db_path))
                logger.info(f"ğŸ“Š DuckDBè¿æ¥æˆåŠŸ (é»˜è®¤é…ç½®): {self.db_path}")

                # åº”ç”¨åŸºæœ¬ä¼˜åŒ–é…ç½®
                self._apply_basic_optimization()

        except Exception as e:
            logger.error(f"âŒ DuckDBè¿æ¥å¤±è´¥: {e}")
            # å³ä½¿ä¼˜åŒ–å¤±è´¥ï¼Œä¹Ÿè¦å°è¯•åŸºæœ¬è¿æ¥
            try:
                self.conn = duckdb.connect(str(self.db_path))
                logger.info(f"ğŸ“Š DuckDBåŸºæœ¬è¿æ¥æˆåŠŸ: {self.db_path}")
                self._apply_basic_optimization()
            except Exception as e2:
                logger.error(f"âŒ DuckDBåŸºæœ¬è¿æ¥ä¹Ÿå¤±è´¥: {e2}")
                raise e2

    def _apply_basic_optimization(self):
        """åº”ç”¨åŸºæœ¬ä¼˜åŒ–é…ç½®ï¼ˆå½“ä¼˜åŒ–å™¨ä¸å¯ç”¨æ—¶ï¼‰"""
        try:
            # åŸºæœ¬å†…å­˜å’Œçº¿ç¨‹é…ç½®
            import psutil

            # è·å–ç³»ç»Ÿå†…å­˜å’ŒCPUä¿¡æ¯
            memory_gb = psutil.virtual_memory().total / (1024**3)
            cpu_cores = psutil.cpu_count(logical=True)

            # ä¿å®ˆçš„å†…å­˜é…ç½®ï¼ˆä½¿ç”¨50%ç³»ç»Ÿå†…å­˜ï¼‰
            memory_limit = max(2.0, memory_gb * 0.5)

            # çº¿ç¨‹é…ç½®ï¼ˆä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°ï¼‰
            threads = min(cpu_cores, 8)  # æœ€å¤š8çº¿ç¨‹

            # åº”ç”¨é…ç½®
            self.conn.execute(f"SET memory_limit = '{memory_limit:.1f}GB'")
            self.conn.execute(f"SET threads = {threads}")
            self.conn.execute("SET enable_object_cache = true")
            self.conn.execute("SET enable_progress_bar = true")

            logger.info(f"ğŸ”§ åŸºæœ¬ä¼˜åŒ–é…ç½®: å†…å­˜={memory_limit:.1f}GB, çº¿ç¨‹={threads}")

        except Exception as e:
            logger.warning(f"åº”ç”¨åŸºæœ¬ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")

    def _init_tables(self):
        """åˆå§‹åŒ–åˆ†ææ•°æ®åº“è¡¨ç»“æ„"""
        try:
            # åˆ›å»ºåºåˆ—ç”¨äºè‡ªå¢ID
            sequences = [
                'strategy_execution_results_seq',
                'indicator_calculation_results_seq',
                'pattern_recognition_results_seq',
                'backtest_metrics_history_seq',
                'backtest_alerts_history_seq',
                'performance_metrics_seq',
                'optimization_logs_seq',
                'analysis_cache_seq'
            ]

            for seq_name in sequences:
                self.conn.execute(f"CREATE SEQUENCE IF NOT EXISTS {seq_name}")

            # 1. ç­–ç•¥æ‰§è¡Œç»“æœè¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS strategy_execution_results (
                    id INTEGER PRIMARY KEY DEFAULT nextval('strategy_execution_results_seq'),
                    strategy_name VARCHAR NOT NULL,
                    symbol VARCHAR NOT NULL,
                    execution_time TIMESTAMP NOT NULL,
                    signal_type VARCHAR NOT NULL,  -- 'buy', 'sell', 'hold'
                    price DOUBLE,
                    quantity INTEGER,
                    confidence DOUBLE,
                    reason TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 2. æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç»“æœè¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS indicator_calculation_results (
                    id INTEGER PRIMARY KEY DEFAULT nextval('indicator_calculation_results_seq'),
                    symbol VARCHAR NOT NULL,
                    indicator_name VARCHAR NOT NULL,
                    calculation_time TIMESTAMP NOT NULL,
                    timeframe VARCHAR NOT NULL,  -- '1m', '5m', '1h', '1d', etc.
                    value DOUBLE,
                    parameters TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 3. å½¢æ€è¯†åˆ«ç»“æœè¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS pattern_recognition_results (
                    id INTEGER PRIMARY KEY DEFAULT nextval('pattern_recognition_results_seq'),
                    symbol VARCHAR NOT NULL,
                    pattern_name VARCHAR NOT NULL,
                    detection_time TIMESTAMP NOT NULL,
                    confidence DOUBLE,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    pattern_data TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 4. å›æµ‹æŒ‡æ ‡å†å²è¡¨ (ä»backtest_monitor.dbè¿ç§»)
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS backtest_metrics_history (
                    id INTEGER PRIMARY KEY DEFAULT nextval('backtest_metrics_history_seq'),
                    timestamp TIMESTAMP NOT NULL,
                    current_return DOUBLE,
                    cumulative_return DOUBLE,
                    current_drawdown DOUBLE,
                    max_drawdown DOUBLE,
                    sharpe_ratio DOUBLE,
                    volatility DOUBLE,
                    var_95 DOUBLE,
                    position_count INTEGER,
                    trade_count INTEGER,
                    win_rate DOUBLE,
                    profit_factor DOUBLE,
                    execution_time DOUBLE,
                    memory_usage DOUBLE,
                    cpu_usage DOUBLE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 5. å›æµ‹é¢„è­¦å†å²è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS backtest_alerts_history (
                    id INTEGER PRIMARY KEY DEFAULT nextval('backtest_alerts_history_seq'),
                    timestamp TIMESTAMP NOT NULL,
                    level VARCHAR NOT NULL,
                    category VARCHAR NOT NULL,
                    message TEXT NOT NULL,
                    metric_name VARCHAR NOT NULL,
                    current_value DOUBLE,
                    threshold_value DOUBLE,
                    recommendation TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 6. æ€§èƒ½æŒ‡æ ‡è¡¨ (ä»optimizationç›¸å…³è¿ç§»)
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY DEFAULT nextval('performance_metrics_seq'),
                    version_id INTEGER NOT NULL,
                    pattern_name VARCHAR NOT NULL,
                    test_dataset_id VARCHAR,
                    test_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    true_positives INTEGER DEFAULT 0,
                    false_positives INTEGER DEFAULT 0,
                    true_negatives INTEGER DEFAULT 0,
                    false_negatives INTEGER DEFAULT 0,
                    precision DOUBLE,
                    recall DOUBLE,
                    f1_score DOUBLE,
                    accuracy DOUBLE,
                    execution_time DOUBLE,
                    memory_usage DOUBLE,
                    cpu_usage DOUBLE,
                    signal_quality DOUBLE,
                    confidence_avg DOUBLE,
                    confidence_std DOUBLE,
                    patterns_found INTEGER DEFAULT 0,
                    robustness_score DOUBLE,
                    parameter_sensitivity DOUBLE,
                    overall_score DOUBLE,
                    test_conditions TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 7. ä¼˜åŒ–æ—¥å¿—è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS optimization_logs (
                    id INTEGER PRIMARY KEY DEFAULT nextval('optimization_logs_seq'),
                    pattern_name VARCHAR NOT NULL,
                    optimization_session_id VARCHAR UNIQUE NOT NULL,
                    optimization_method VARCHAR NOT NULL,
                    start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    end_time TIMESTAMP,
                    status VARCHAR DEFAULT 'running',
                    initial_version_id INTEGER,
                    final_version_id INTEGER,
                    iterations INTEGER DEFAULT 0,
                    best_score DOUBLE,
                    improvement_percentage DOUBLE,
                    optimization_config TEXT,
                    optimization_log TEXT,
                    error_message TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 8. åˆ†æç¼“å­˜è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS analysis_cache (
                    id INTEGER PRIMARY KEY DEFAULT nextval('analysis_cache_seq'),
                    cache_key VARCHAR UNIQUE NOT NULL,
                    cache_type VARCHAR NOT NULL,
                    data TEXT NOT NULL,
                    expires_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæ€§èƒ½ä¼˜åŒ–çš„ç´¢å¼•
            self._create_optimized_indexes()

            logger.info("âœ… FactorWeaveåˆ†ææ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ åˆ†ææ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _create_optimized_indexes(self):
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–çš„ç´¢å¼•"""

        # åŸºäº2024å¹´DuckDBæœ€ä½³å®è·µçš„ç´¢å¼•ç­–ç•¥
        indexes = [
            # ç­–ç•¥æ‰§è¡Œç»“æœç´¢å¼• - ä¼˜åŒ–æ—¶é—´èŒƒå›´æŸ¥è¯¢
            "CREATE INDEX IF NOT EXISTS idx_strategy_symbol_time ON strategy_execution_results(symbol, execution_time)",
            "CREATE INDEX IF NOT EXISTS idx_strategy_name_time ON strategy_execution_results(strategy_name, execution_time)",
            "CREATE INDEX IF NOT EXISTS idx_strategy_signal_type ON strategy_execution_results(signal_type, execution_time)",

            # æŠ€æœ¯æŒ‡æ ‡ç»“æœç´¢å¼• - ä¼˜åŒ–æŒ‡æ ‡æŸ¥è¯¢
            "CREATE INDEX IF NOT EXISTS idx_indicator_symbol_time ON indicator_calculation_results(symbol, calculation_time)",
            "CREATE INDEX IF NOT EXISTS idx_indicator_name_time ON indicator_calculation_results(indicator_name, calculation_time)",
            "CREATE INDEX IF NOT EXISTS idx_indicator_timeframe ON indicator_calculation_results(timeframe, calculation_time)",

            # å½¢æ€è¯†åˆ«ç»“æœç´¢å¼• - ä¼˜åŒ–æ¨¡å¼æŸ¥è¯¢
            "CREATE INDEX IF NOT EXISTS idx_pattern_symbol_time ON pattern_recognition_results(symbol, recognition_time)",
            "CREATE INDEX IF NOT EXISTS idx_pattern_type_time ON pattern_recognition_results(pattern_type, recognition_time)",
            "CREATE INDEX IF NOT EXISTS idx_pattern_confidence ON pattern_recognition_results(confidence DESC, recognition_time)",

            # å›æµ‹æŒ‡æ ‡ç´¢å¼• - ä¼˜åŒ–æ—¶åºåˆ†æ
            "CREATE INDEX IF NOT EXISTS idx_backtest_metrics_time ON backtest_metrics_history(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_backtest_metrics_return ON backtest_metrics_history(total_return, created_at)",

            # å›æµ‹é¢„è­¦ç´¢å¼• - ä¼˜åŒ–å‘Šè­¦æŸ¥è¯¢
            "CREATE INDEX IF NOT EXISTS idx_backtest_alerts_time ON backtest_alerts_history(alert_time)",
            "CREATE INDEX IF NOT EXISTS idx_backtest_alerts_level ON backtest_alerts_history(severity, alert_time)",
            "CREATE INDEX IF NOT EXISTS idx_backtest_alerts_type ON backtest_alerts_history(alert_type, alert_time)",

            # æ€§èƒ½æŒ‡æ ‡ç´¢å¼• - ä¼˜åŒ–æ€§èƒ½åˆ†æ
            "CREATE INDEX IF NOT EXISTS idx_performance_pattern_time ON performance_metrics(pattern_name, test_time)",
            "CREATE INDEX IF NOT EXISTS idx_performance_score ON performance_metrics(overall_score DESC, test_time)",

            # ä¼˜åŒ–æ—¥å¿—ç´¢å¼• - ä¼˜åŒ–ä¼˜åŒ–å†å²æŸ¥è¯¢
            "CREATE INDEX IF NOT EXISTS idx_optimization_pattern_time ON optimization_logs(pattern_name, start_time)",
            "CREATE INDEX IF NOT EXISTS idx_optimization_status ON optimization_logs(status, start_time)",
            "CREATE INDEX IF NOT EXISTS idx_optimization_session ON optimization_logs(optimization_session_id)",

            # ç¼“å­˜ç´¢å¼• - ä¼˜åŒ–ç¼“å­˜è®¿é—®
            "CREATE INDEX IF NOT EXISTS idx_cache_key ON analysis_cache(cache_key)",
            "CREATE INDEX IF NOT EXISTS idx_cache_type_expires ON analysis_cache(cache_type, expires_at)",
            "CREATE INDEX IF NOT EXISTS idx_cache_expires ON analysis_cache(expires_at)"
        ]

        for index_sql in indexes:
            try:
                self.conn.execute(index_sql)
                logger.debug(f"åˆ›å»ºç´¢å¼•: {index_sql.split('idx_')[1].split(' ')[0]}")
            except Exception as e:
                logger.warning(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")

        logger.info("âœ… æ€§èƒ½ä¼˜åŒ–ç´¢å¼•åˆ›å»ºå®Œæˆ")

    def execute_query(self, sql: str, params: List = None) -> pd.DataFrame:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›DataFrame"""
        if not self._check_connection():
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame

        try:
            if params:
                result = self.conn.execute(sql, params).fetchdf()
            else:
                result = self.conn.execute(sql).fetchdf()
            return result
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def execute_command(self, sql: str, params: List = None) -> bool:
        """æ‰§è¡ŒINSERT/UPDATE/DELETEç­‰å‘½ä»¤ï¼Œè¿”å›æˆåŠŸçŠ¶æ€"""
        if not self._check_connection():
            return False  # è¿æ¥ä¸å¯ç”¨æ—¶è¿”å›False

        try:
            if params:
                self.conn.execute(sql, params)
            else:
                self.conn.execute(sql)
            return True
        except Exception as e:
            logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def insert_dataframe(self, table_name: str, df: pd.DataFrame) -> bool:
        """æ’å…¥DataFrameæ•°æ®"""
        if not self._check_connection():
            return False  # è¿æ¥ä¸å¯ç”¨æ—¶è¿”å›False

        try:
            self.conn.register('temp_df', df)
            self.conn.execute(f"INSERT INTO {table_name} SELECT * FROM temp_df")
            self.conn.unregister('temp_df')
            logger.debug(f"æˆåŠŸæ’å…¥ {len(df)} æ¡è®°å½•åˆ°è¡¨ {table_name}")
            return True
        except Exception as e:
            logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
            return False

    def get_strategy_results(self, strategy_name: str = None, symbol: str = None,
                             start_time: datetime = None, end_time: datetime = None) -> pd.DataFrame:
        """è·å–ç­–ç•¥æ‰§è¡Œç»“æœ"""
        sql = "SELECT * FROM strategy_execution_results WHERE 1=1"
        params = []

        if strategy_name:
            sql += " AND strategy_name = ?"
            params.append(strategy_name)
        if symbol:
            sql += " AND symbol = ?"
            params.append(symbol)
        if start_time:
            sql += " AND execution_time >= ?"
            params.append(start_time)
        if end_time:
            sql += " AND execution_time <= ?"
            params.append(end_time)

        sql += " ORDER BY execution_time DESC"
        return self.execute_query(sql, params)

    def get_indicator_results(self, symbol: str = None, indicator_name: str = None,
                              timeframe: str = None, limit: int = 1000) -> pd.DataFrame:
        """è·å–æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç»“æœ"""
        sql = "SELECT * FROM indicator_calculation_results WHERE 1=1"
        params = []

        if symbol:
            sql += " AND symbol = ?"
            params.append(symbol)
        if indicator_name:
            sql += " AND indicator_name = ?"
            params.append(indicator_name)
        if timeframe:
            sql += " AND timeframe = ?"
            params.append(timeframe)

        sql += " ORDER BY calculation_time DESC LIMIT ?"
        params.append(limit)

        return self.execute_query(sql, params)

    def get_pattern_results(self, symbol: str = None, pattern_name: str = None,
                            min_confidence: float = None, limit: int = 1000) -> pd.DataFrame:
        """è·å–å½¢æ€è¯†åˆ«ç»“æœ"""
        sql = "SELECT * FROM pattern_recognition_results WHERE 1=1"
        params = []

        if symbol:
            sql += " AND symbol = ?"
            params.append(symbol)
        if pattern_name:
            sql += " AND pattern_name = ?"
            params.append(pattern_name)
        if min_confidence:
            sql += " AND confidence >= ?"
            params.append(min_confidence)

        sql += " ORDER BY detection_time DESC LIMIT ?"
        params.append(limit)

        return self.execute_query(sql, params)

    def get_backtest_metrics(self, start_time: datetime = None,
                             end_time: datetime = None) -> pd.DataFrame:
        """è·å–å›æµ‹æŒ‡æ ‡å†å²"""
        sql = "SELECT * FROM backtest_metrics_history WHERE 1=1"
        params = []

        if start_time:
            sql += " AND timestamp >= ?"
            params.append(start_time)
        if end_time:
            sql += " AND timestamp <= ?"
            params.append(end_time)

        sql += " ORDER BY timestamp DESC"
        return self.execute_query(sql, params)

    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            current_time = datetime.now()
            result = self.conn.execute(
                "DELETE FROM analysis_cache WHERE expires_at < ?",
                [current_time]
            )
            deleted_count = result.fetchone()[0] if result else 0
            logger.info(f"æ¸…ç†äº† {deleted_count} æ¡è¿‡æœŸç¼“å­˜è®°å½•")
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

    def get_database_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {}

            # è·å–å„è¡¨çš„è®°å½•æ•°
            tables = [
                'strategy_execution_results',
                'indicator_calculation_results',
                'pattern_recognition_results',
                'backtest_metrics_history',
                'backtest_alerts_history',
                'performance_metrics',
                'optimization_logs',
                'analysis_cache'
            ]

            for table in tables:
                try:
                    result = self.execute_query(f"SELECT COUNT(*) as count FROM {table}")
                    stats[table] = result.iloc[0]['count']
                except:
                    stats[table] = 0

            # è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°
            if self.db_path.exists():
                file_size_bytes = self.db_path.stat().st_size
                stats['file_size_mb'] = file_size_bytes / (1024 * 1024)

            # è·å–æ€§èƒ½ç»Ÿè®¡
            if OPTIMIZER_AVAILABLE and self.optimizer and self.optimizer.current_config:
                config = self.optimizer.current_config
                stats['memory_limit'] = config.memory_limit
                stats['threads'] = config.threads
                stats['optimization_enabled'] = True
            else:
                stats['optimization_enabled'] = False

            return stats

        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def optimize_performance(self, workload_type: WorkloadType = WorkloadType.OLAP) -> bool:
        """é‡æ–°ä¼˜åŒ–æ€§èƒ½é…ç½®"""
        if not OPTIMIZER_AVAILABLE or not self.optimizer:
            logger.warning("æ€§èƒ½ä¼˜åŒ–å™¨ä¸å¯ç”¨")
            return False

        try:
            # é‡æ–°ä¼˜åŒ–é…ç½®
            success = self.optimizer.optimize_for_workload(workload_type)

            if success:
                logger.info(f"âœ… æ€§èƒ½é…ç½®å·²ä¼˜åŒ–ä¸º{workload_type.value}å·¥ä½œè´Ÿè½½")

                # æ˜¾ç¤ºæ–°çš„é…ç½®
                if self.optimizer.current_config:
                    config = self.optimizer.current_config
                    logger.info(f"ğŸ”§ æ–°é…ç½®: å†…å­˜={config.memory_limit}, çº¿ç¨‹={config.threads}")

            return success

        except Exception as e:
            logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return False

    def benchmark_performance(self) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if not OPTIMIZER_AVAILABLE or not self.optimizer:
            logger.warning("æ€§èƒ½ä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•")
            return {'error': 'æ€§èƒ½ä¼˜åŒ–å™¨ä¸å¯ç”¨'}

        # ä½¿ç”¨å®é™…çš„åˆ†ææŸ¥è¯¢è¿›è¡ŒåŸºå‡†æµ‹è¯•
        test_queries = [
            "SELECT COUNT(*) FROM strategy_execution_results",
            "SELECT strategy_name, COUNT(*), AVG(confidence) FROM strategy_execution_results GROUP BY strategy_name",
            "SELECT symbol, indicator_name, AVG(value) FROM indicator_calculation_results GROUP BY symbol, indicator_name LIMIT 100",
            "SELECT pattern_name, AVG(confidence), COUNT(*) FROM pattern_recognition_results GROUP BY pattern_name"
        ]

        return self.optimizer.benchmark_configuration(test_queries)

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            self.conn = None

        if self.optimizer:
            self.optimizer.close()
            self.optimizer = None


# å…¨å±€å®ä¾‹å’Œå·¥å‚å‡½æ•°
_analytics_db = None
_db_lock = threading.Lock()


def get_analytics_db(db_path: str = 'db/factorweave_analytics.duckdb') -> FactorWeaveAnalyticsDB:
    """è·å–åˆ†ææ•°æ®åº“å®ä¾‹"""
    global _analytics_db

    with _db_lock:
        if _analytics_db is None:
            _analytics_db = FactorWeaveAnalyticsDB(db_path)

    return _analytics_db


def create_optimized_analytics_connection(workload_type: WorkloadType = WorkloadType.OLAP) -> FactorWeaveAnalyticsDB:
    """åˆ›å»ºä¼˜åŒ–çš„åˆ†ææ•°æ®åº“è¿æ¥"""
    db = get_analytics_db()

    # åº”ç”¨å·¥ä½œè´Ÿè½½ä¼˜åŒ–
    if OPTIMIZER_AVAILABLE:
        db.optimize_performance(workload_type)

    return db
