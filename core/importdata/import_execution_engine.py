from loguru import logger
#!/usr/bin/env python3
"""
æ•°æ®å¯¼å…¥ä»»åŠ¡æ‰§è¡Œå¼•æ“

è´Ÿè´£æ‰§è¡Œæ•°æ®å¯¼å…¥ä»»åŠ¡ï¼Œæä¾›è¿›åº¦ç›‘æ§ã€çŠ¶æ€æ›´æ–°å’Œé”™è¯¯å¤„ç†
"""

import asyncio
import threading
import time
import hashlib
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Callable, Tuple
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, Future
from dataclasses import dataclass, field
from enum import Enum
from queue import Queue, Empty
from PyQt5.QtCore import QObject, pyqtSignal, QTimer

from .import_config_manager import ImportConfigManager, ImportTaskConfig, ImportProgress, ImportStatus
from .intelligent_config_manager import (
    IntelligentConfigManager,
    ConfigOptimizationLevel,
    ConfigRecommendationType
)
from core.database.table_manager import TableType
from ..services.unified_data_manager import UnifiedDataManager, get_unified_data_manager
from ..real_data_provider import RealDataProvider
from ..services.ai_prediction_service import AIPredictionService, PredictionType
from ..services.deep_analysis_service import DeepAnalysisService, PerformanceMetric, AnomalyInfo
from ..performance.factorweave_performance_integration import FactorWeavePerformanceIntegrator
from ..performance.unified_monitor import get_performance_monitor
from ..services.enhanced_performance_bridge import EnhancedPerformanceBridge, get_enhanced_performance_bridge
from ..risk_monitoring.enhanced_risk_monitor import EnhancedRiskMonitor, get_enhanced_risk_monitor
from ..services.distributed_service import DistributedService, NodeDiscovery, NodeInfo
from optimization.auto_tuner import AutoTuner, TuningTask, OptimizationConfig
from optimization.algorithm_optimizer import PerformanceEvaluator
from ..services.enhanced_data_manager import DataQualityMonitor
from ..data.enhanced_models import DataQualityMetrics, DataQuality
from ..data_validator import ValidationLevel, ValidationResult
from ..events.enhanced_event_bus import get_enhanced_event_bus, EventPriority, EnhancedEventBus
from ..async_management.enhanced_async_manager import get_enhanced_async_manager, TaskPriority, ResourceRequirement
from ..performance.cache_manager import MultiLevelCacheManager, CacheLevel

logger = logger


@dataclass
class WriteTask:
    """æ•°æ®åº“å†™å…¥ä»»åŠ¡"""
    buffer_key: str  # ç¼“å†²åŒºé”®ï¼ˆasset_type_task_idï¼‰
    data: pd.DataFrame  # å¾…å†™å…¥æ•°æ®
    asset_type: Any  # èµ„äº§ç±»å‹
    data_type: Any  # æ•°æ®ç±»å‹
    priority: int = 0  # ä¼˜å…ˆçº§ï¼ˆæš‚æœªä½¿ç”¨ï¼‰


class DatabaseWriterThread(threading.Thread):
    """
    æ•°æ®åº“å†™å…¥çº¿ç¨‹ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰

    è§£å†³DuckDBå¹¶å‘å†™å…¥æ­»é”é—®é¢˜ï¼š
    - æ‰€æœ‰å·¥ä½œçº¿ç¨‹å°†æ•°æ®æ”¾å…¥æ— é”é˜Ÿåˆ—
    - æœ¬çº¿ç¨‹å•ç‹¬æ¶ˆè´¹é˜Ÿåˆ—ï¼Œä¸²è¡Œå†™å…¥æ•°æ®åº“
    - å®Œå…¨é¿å…å†™é”ç«äº‰
    """

    def __init__(self):
        super().__init__(name="DatabaseWriter", daemon=True)

        # æ— é”é˜Ÿåˆ—
        self.write_queue = Queue(maxsize=5000)  # é™åˆ¶é˜Ÿåˆ—å¤§å°é˜²æ­¢å†…å­˜æº¢å‡º

        # æ‰¹é‡åˆå¹¶ç¼“å†²åŒºï¼ˆç›¸åŒbuffer_keyçš„æ•°æ®åˆå¹¶åä¸€æ¬¡å†™å…¥ï¼‰
        self._merge_buffer: Dict[str, List[pd.DataFrame]] = {}
        self._merge_lock = threading.RLock()

        # æ§åˆ¶æ ‡å¿—
        self._stop_event = threading.Event()
        self._stopped = False

        # ç»Ÿè®¡ä¿¡æ¯
        self._total_writes = 0
        self._failed_writes = 0
        self._queue_peak = 0
        self._stats_lock = threading.RLock()

        # âœ… ä¼˜åŒ–ï¼šæ‰¹é‡åˆå¹¶é…ç½®ï¼ˆåŠ¨æ€è°ƒæ•´ä»¥åŠ å¿«å†™å…¥é€Ÿåº¦ï¼‰
        self._batch_threshold_normal = 5  # æ­£å¸¸æ‰¹é‡é˜ˆå€¼ï¼š5ä¸ªDataFrameï¼ˆæé«˜æ‰¹é‡å†™å…¥æ•ˆç‡ï¼‰
        self._batch_threshold_medium = 3  # ä¸­ç­‰æ‰¹é‡é˜ˆå€¼ï¼š3ä¸ªDataFrame
        self._batch_threshold_urgent = 1  # ç´§æ€¥æ‰¹é‡é˜ˆå€¼ï¼šé˜Ÿåˆ—ç§¯å‹æ—¶ç«‹å³å†™å…¥
        self._queue_size_threshold_urgent = 100  # ç´§æ€¥é˜ˆå€¼è§¦å‘ç‚¹ï¼šè¶…è¿‡æ­¤å€¼ä½¿ç”¨ç´§æ€¥é˜ˆå€¼
        self._queue_size_threshold_medium = 50  # ä¸­ç­‰é˜ˆå€¼è§¦å‘ç‚¹ï¼šè¶…è¿‡æ­¤å€¼ä½¿ç”¨ä¸­ç­‰é˜ˆå€¼
        self._flush_timeout_normal = 2.0  # æ­£å¸¸è¶…æ—¶åˆ·æ–°æ—¶é—´ï¼ˆç§’ï¼‰
        self._flush_timeout_medium = 1.0  # ä¸­ç­‰è¶…æ—¶åˆ·æ–°æ—¶é—´ï¼ˆç§’ï¼‰
        self._flush_timeout_urgent = 0.5  # ç´§æ€¥è¶…æ—¶åˆ·æ–°æ—¶é—´ï¼ˆç§’ï¼‰ï¼šé˜Ÿåˆ—ç§¯å‹æ—¶å¿«é€Ÿåˆ·æ–°
        self._buffer_timestamps: Dict[str, float] = {}  # ç¼“å†²åŒºæ—¶é—´æˆ³ï¼Œç”¨äºè¶…æ—¶åˆ·æ–°

        # âœ… ä¼˜åŒ–ï¼šå¤ç”¨AssetSeparatedDatabaseManagerå®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        from ..asset_database_manager import AssetSeparatedDatabaseManager
        self._asset_manager = AssetSeparatedDatabaseManager()

        logger.info("DatabaseWriterThread åˆå§‹åŒ–å®Œæˆ")

    def put_write_task(self, task: WriteTask, timeout: float = 5.0) -> bool:
        """
        æ”¾å…¥å†™å…¥ä»»åŠ¡åˆ°é˜Ÿåˆ—

        Args:
            task: å†™å…¥ä»»åŠ¡
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸæ”¾å…¥é˜Ÿåˆ—
        """
        try:
            # âœ… ä¼˜åŒ–ï¼šè®°å½•é˜Ÿåˆ—çŠ¶æ€ï¼Œä¾¿äºæ€§èƒ½åˆ†æ
            queue_size_before = self.write_queue.qsize()
            put_start_time = time.time()

            # âœ… ä¼˜åŒ–ï¼šå¦‚æœé˜Ÿåˆ—æ¥è¿‘æ»¡è½½ï¼Œè®°å½•è­¦å‘Š
            if queue_size_before > self.write_queue.maxsize * 0.8:  # é˜Ÿåˆ—å®¹é‡5000ï¼Œè¶…è¿‡4000è­¦å‘Š
                logger.warning(f"âš ï¸  [é˜Ÿåˆ—æ¥è¿‘æ»¡è½½] å½“å‰é˜Ÿåˆ—å¤§å°: {queue_size_before}/{self.write_queue.maxsize}ï¼Œå¯èƒ½å½±å“å†™å…¥æ€§èƒ½")

            self.write_queue.put(task, timeout=timeout)

            put_duration = time.time() - put_start_time
            queue_size_after = self.write_queue.qsize()

            # âœ… ä¼˜åŒ–ï¼šå¦‚æœå…¥é˜Ÿè€—æ—¶è¾ƒé•¿ï¼Œè®°å½•è­¦å‘Šï¼ˆè¯´æ˜é˜Ÿåˆ—ç§¯å‹ä¸¥é‡ï¼‰
            if put_duration > 0.5:
                logger.warning(f"âš ï¸  [é˜Ÿåˆ—é˜»å¡] å…¥é˜Ÿè€—æ—¶:{put_duration:.2f}ç§’ | é˜Ÿåˆ—å¤§å°:{queue_size_before}â†’{queue_size_after} | buffer_key:{task.buffer_key}")

            # æ›´æ–°ç»Ÿè®¡
            with self._stats_lock:
                current_size = self.write_queue.qsize()
                if current_size > self._queue_peak:
                    self._queue_peak = current_size

            return True
        except Exception as e:
            logger.error(f"æ”¾å…¥å†™å…¥ä»»åŠ¡å¤±è´¥: {e} | é˜Ÿåˆ—å¤§å°:{self.write_queue.qsize()}")
            return False

    def run(self):
        """çº¿ç¨‹ä¸»å¾ªç¯"""
        logger.info("DatabaseWriterThread å¯åŠ¨")

        # âœ… ä¼˜åŒ–ï¼šè®°å½•æœ€åæ£€æŸ¥è¶…æ—¶ç¼“å†²åŒºçš„æ—¶é—´
        last_timeout_check = time.time()

        while not self._stop_event.is_set() or not self.write_queue.empty():
            try:
                # âœ… ä¼˜åŒ–ï¼šæ ¹æ®é˜Ÿåˆ—å¤§å°åŠ¨æ€è°ƒæ•´æ£€æŸ¥é¢‘ç‡ï¼ˆé˜Ÿåˆ—ç§¯å‹æ—¶æ›´é¢‘ç¹æ£€æŸ¥ï¼‰
                current_time = time.time()
                queue_size = self.write_queue.qsize()
                # é˜Ÿåˆ—ç§¯å‹æ—¶æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œæ­£å¸¸æ—¶æ¯1ç§’æ£€æŸ¥ä¸€æ¬¡
                check_interval = 0.5 if queue_size > self._queue_size_threshold_urgent else 1.0
                if current_time - last_timeout_check >= check_interval:
                    self._check_and_flush_timeout_buffers()
                    last_timeout_check = current_time

                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼Œé¿å…é˜»å¡å…³é—­ï¼‰
                try:
                    # âœ… ä¼˜åŒ–ï¼šå‡å°‘è¶…æ—¶æ—¶é—´ï¼ŒåŠ å¿«å“åº”é€Ÿåº¦
                    task = self.write_queue.get(timeout=1.0)
                except Empty:
                    # âœ… ä¼˜åŒ–ï¼šé˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶…æ—¶ç¼“å†²åŒºéœ€è¦åˆ·æ–°
                    self._check_and_flush_timeout_buffers()
                    last_timeout_check = time.time()
                    continue

                # æ‰§è¡Œå†™å…¥
                success = self._write_task_to_database(task)

                # æ›´æ–°ç»Ÿè®¡
                with self._stats_lock:
                    if success:
                        self._total_writes += 1
                    else:
                        self._failed_writes += 1

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.write_queue.task_done()

            except Exception as e:
                logger.error(f"DatabaseWriterThread æ‰§è¡Œé”™è¯¯: {e}")
                import traceback
                logger.error(traceback.format_exc())

        # çº¿ç¨‹é€€å‡ºå‰å¤„ç†å‰©ä½™åˆå¹¶ç¼“å†²åŒº
        self._flush_merge_buffer()

        logger.info(f"DatabaseWriterThread åœæ­¢ (æ€»å†™å…¥:{self._total_writes}, å¤±è´¥:{self._failed_writes})")
        self._stopped = True

    def _check_and_flush_timeout_buffers(self):
        """æ£€æŸ¥å¹¶åˆ·æ–°è¶…æ—¶çš„ç¼“å†²åŒº"""
        try:
            current_time = time.time()
            # âœ… ä¼˜åŒ–ï¼šæ ¹æ®é˜Ÿåˆ—å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶åˆ·æ–°æ—¶é—´
            queue_size = self.write_queue.qsize()
            if queue_size > self._queue_size_threshold_urgent:
                flush_timeout = self._flush_timeout_urgent  # ç´§æ€¥ï¼š0.5ç§’
            elif queue_size > self._queue_size_threshold_medium:
                flush_timeout = self._flush_timeout_medium  # ä¸­ç­‰ï¼š1ç§’
            else:
                flush_timeout = self._flush_timeout_normal  # æ­£å¸¸ï¼š2ç§’

            with self._merge_lock:
                buffers_to_flush = []
                for buffer_key, timestamp in list(self._buffer_timestamps.items()):
                    if current_time - timestamp >= flush_timeout:
                        if buffer_key in self._merge_buffer and self._merge_buffer[buffer_key]:
                            buffers_to_flush.append(buffer_key)

                # åˆ·æ–°è¶…æ—¶çš„ç¼“å†²åŒº
                for buffer_key in buffers_to_flush:
                    try:
                        # ä»buffer_keyè§£æasset_typeå’Œdata_type
                        parts = buffer_key.split('_', 1)
                        if len(parts) >= 1:
                            from ..plugin_types import AssetType, DataType
                            asset_type_str = parts[0]
                            asset_type = AssetType(asset_type_str)
                            data_type = DataType.HISTORICAL_KLINE  # é»˜è®¤Kçº¿æ•°æ®

                            self._flush_buffer_key(buffer_key, asset_type, data_type)
                            if buffer_key in self._buffer_timestamps:
                                del self._buffer_timestamps[buffer_key]
                    except Exception as e:
                        logger.debug(f"åˆ·æ–°è¶…æ—¶ç¼“å†²åŒºå¤±è´¥: {buffer_key}, {e}")
        except Exception as e:
            logger.debug(f"æ£€æŸ¥è¶…æ—¶ç¼“å†²åŒºå¤±è´¥: {e}")

    def _write_task_to_database(self, task: WriteTask) -> bool:
        """
        å†™å…¥å•ä¸ªä»»åŠ¡åˆ°æ•°æ®åº“

        é‡‡ç”¨æ‰¹é‡åˆå¹¶ç­–ç•¥ï¼š
        - ç›¸åŒbuffer_keyçš„æ•°æ®å…ˆæ”¾å…¥åˆå¹¶ç¼“å†²åŒº
        - è¾¾åˆ°é˜ˆå€¼æˆ–è¶…æ—¶æ—¶æ‰¹é‡å†™å…¥
        - é˜Ÿåˆ—ç§¯å‹æ—¶ä½¿ç”¨ç´§æ€¥é˜ˆå€¼ï¼Œç«‹å³å†™å…¥
        """
        try:
            # âœ… ä¼˜åŒ–ï¼šæ ¹æ®é˜Ÿåˆ—å¤§å°åŠ¨æ€è°ƒæ•´æ‰¹é‡é˜ˆå€¼ï¼ˆä¸‰çº§é˜ˆå€¼ï¼‰
            queue_size = self.write_queue.qsize()
            if queue_size > self._queue_size_threshold_urgent:
                current_batch_threshold = self._batch_threshold_urgent  # ç´§æ€¥ï¼šç«‹å³å†™å…¥
            elif queue_size > self._queue_size_threshold_medium:
                current_batch_threshold = self._batch_threshold_medium  # ä¸­ç­‰ï¼š3ä¸ªDataFrame
            else:
                current_batch_threshold = self._batch_threshold_normal  # æ­£å¸¸ï¼š5ä¸ªDataFrameï¼ˆæé«˜æ‰¹é‡å†™å…¥æ•ˆç‡ï¼‰

            with self._merge_lock:
                # æ”¾å…¥åˆå¹¶ç¼“å†²åŒº
                if task.buffer_key not in self._merge_buffer:
                    self._merge_buffer[task.buffer_key] = []
                    self._buffer_timestamps[task.buffer_key] = time.time()

                self._merge_buffer[task.buffer_key].append(task.data)

                # âœ… ä¼˜åŒ–ï¼šæ›´æ–°ç¼“å†²åŒºæ—¶é—´æˆ³ï¼ˆæ¯æ¬¡æ·»åŠ æ•°æ®æ—¶é‡ç½®ï¼‰
                self._buffer_timestamps[task.buffer_key] = time.time()

                # âœ… ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆè¾¾åˆ°æ‰¹é‡é˜ˆå€¼ï¼Œé˜Ÿåˆ—ç§¯å‹æ—¶ä½¿ç”¨ç´§æ€¥é˜ˆå€¼ï¼‰
                if len(self._merge_buffer[task.buffer_key]) >= current_batch_threshold:
                    result = self._flush_buffer_key(task.buffer_key, task.asset_type, task.data_type)
                    # æ¸…é™¤æ—¶é—´æˆ³
                    if task.buffer_key in self._buffer_timestamps:
                        del self._buffer_timestamps[task.buffer_key]
                    return result

            return True

        except Exception as e:
            logger.error(f"å†™å…¥ä»»åŠ¡å¤±è´¥: {task.buffer_key}, {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _flush_buffer_key(self, buffer_key: str, asset_type: Any, data_type: Any) -> bool:
        """åˆ·æ–°æŒ‡å®šbuffer_keyçš„æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if buffer_key not in self._merge_buffer or not self._merge_buffer[buffer_key]:
                return True

            # åˆå¹¶æ‰€æœ‰DataFrame
            data_list = self._merge_buffer[buffer_key]

            # âœ… ä¼˜åŒ–ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªDataFrameï¼Œç›´æ¥ä½¿ç”¨ï¼Œé¿å…concatå¼€é”€
            if len(data_list) == 1:
                combined_data = data_list[0]
            else:
                # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨sort=Falseæé«˜åˆå¹¶æ€§èƒ½ï¼Œå› ä¸ºæ•°æ®å·²ç»æŒ‰æ—¶é—´æ’åº
                combined_data = pd.concat(data_list, ignore_index=True, sort=False)

            record_count = len(combined_data)
            logger.info(f"ğŸ“Š [å†™å…¥çº¿ç¨‹] å†™å…¥: {buffer_key}, {record_count}æ¡è®°å½• (åˆå¹¶{len(data_list)}ä¸ªDataFrame)")

            # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨å¤ç”¨çš„AssetSeparatedDatabaseManagerå®ä¾‹
            write_start_time = time.time()
            success = self._asset_manager.store_standardized_data(
                data=combined_data,
                asset_type=asset_type,
                data_type=data_type
            )
            write_duration = time.time() - write_start_time

            if success:
                # âœ… ä¼˜åŒ–ï¼šè®°å½•å†™å…¥æ€§èƒ½
                write_speed = record_count / write_duration if write_duration > 0 else 0
                logger.info(f"âœ… [å†™å…¥çº¿ç¨‹] å†™å…¥æˆåŠŸ: {buffer_key}, {record_count}æ¡è®°å½•, è€—æ—¶: {write_duration:.2f}ç§’, é€Ÿåº¦: {write_speed:.1f}æ¡/ç§’")
                # æ¸…ç©ºå·²å†™å…¥çš„ç¼“å†²åŒº
                del self._merge_buffer[buffer_key]
            else:
                logger.error(f"âŒ [å†™å…¥çº¿ç¨‹] å†™å…¥å¤±è´¥: {buffer_key}")

            return success

        except Exception as e:
            logger.error(f"åˆ·æ–°ç¼“å†²åŒºå¤±è´¥: {buffer_key}, {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _flush_merge_buffer(self):
        """åˆ·æ–°æ‰€æœ‰åˆå¹¶ç¼“å†²åŒºï¼ˆçº¿ç¨‹ç»“æŸæ—¶è°ƒç”¨ï¼‰"""
        logger.info("åˆ·æ–°æ‰€æœ‰åˆå¹¶ç¼“å†²åŒº...")

        with self._merge_lock:
            for buffer_key in list(self._merge_buffer.keys()):
                if self._merge_buffer[buffer_key]:
                    # éœ€è¦asset_typeå’Œdata_typeï¼Œä»buffer_keyè§£æ
                    try:
                        parts = buffer_key.split('_', 1)
                        if len(parts) >= 1:
                            from ..plugin_types import AssetType, DataType
                            asset_type_str = parts[0]
                            asset_type = AssetType(asset_type_str)
                            data_type = DataType.HISTORICAL_KLINE  # é»˜è®¤Kçº¿æ•°æ®

                            self._flush_buffer_key(buffer_key, asset_type, data_type)
                    except Exception as e:
                        logger.error(f"åˆ·æ–°ç¼“å†²åŒºå¤±è´¥: {buffer_key}, {e}")

    def stop(self, wait: bool = True, timeout: float = 30.0):
        """
        åœæ­¢å†™å…¥çº¿ç¨‹

        Args:
            wait: æ˜¯å¦ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
            timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        """
        logger.info(f"åœæ­¢DatabaseWriterThread (wait={wait}, queue_size={self.write_queue.qsize()})")

        self._stop_event.set()

        if wait:
            # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
            try:
                start_time = time.time()
                while not self.write_queue.empty() and (time.time() - start_time) < timeout:
                    logger.debug(f"ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º... ({self.write_queue.qsize()}ä¸ªä»»åŠ¡)")
                    time.sleep(0.5)

                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                self.join(timeout=5.0)
            except Exception as e:
                logger.error(f"åœæ­¢å†™å…¥çº¿ç¨‹å¤±è´¥: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._stats_lock:
            # âœ… ä¿®å¤ï¼šmerge_buffer_sizeåº”è¯¥æ˜¯æ‰€æœ‰ç¼“å†²åŒºä¸­DataFrameçš„æ€»æ•°ï¼Œè€Œä¸æ˜¯ç¼“å†²åŒºæ•°é‡
            merge_buffer_size = sum(len(buffer_list) for buffer_list in self._merge_buffer.values())

            return {
                'queue_size': self.write_queue.qsize(),
                'queue_peak': self._queue_peak,
                'total_writes': self._total_writes,
                'failed_writes': self._failed_writes,
                'merge_buffer_size': merge_buffer_size,  # æ‰€æœ‰ç¼“å†²åŒºä¸­DataFrameçš„æ€»æ•°
                'is_stopped': self._stopped
            }


class TaskExecutionStatus(Enum):
    """ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class TaskExecutionResult:
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_id: str
    status: TaskExecutionStatus
    total_records: int = 0
    processed_records: int = 0
    failed_records: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    execution_time: float = 0.0
    processed_symbols_list: List[str] = field(default_factory=list)  # âœ… ä¿®å¤ï¼šå·²å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆç”¨äºæ¢å¤ï¼‰

    @property
    def progress(self) -> float:
        """è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰- UIå…¼å®¹æ€§"""
        if self.total_records == 0:
            return 0.0
        return (self.processed_records / self.total_records) * 100

    @property
    def progress_percentage(self) -> float:
        """è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self.progress


class DataImportExecutionEngine(QObject):
    """
    æ•°æ®å¯¼å…¥ä»»åŠ¡æ‰§è¡Œå¼•æ“

    åŠŸèƒ½ï¼š
    1. æ‰§è¡Œæ•°æ®å¯¼å…¥ä»»åŠ¡
    2. ç›‘æ§ä»»åŠ¡è¿›åº¦
    3. æä¾›çŠ¶æ€æ›´æ–°
    4. é”™è¯¯å¤„ç†å’Œé‡è¯•
    5. ä»»åŠ¡è°ƒåº¦å’Œç®¡ç†
    """

    # Qtä¿¡å·
    task_started = pyqtSignal(str)  # ä»»åŠ¡å¼€å§‹
    task_progress = pyqtSignal(str, float, str)  # ä»»åŠ¡è¿›åº¦ (task_id, progress, message)
    task_completed = pyqtSignal(str, object)  # ä»»åŠ¡å®Œæˆ (task_id, result)
    task_failed = pyqtSignal(str, str)  # ä»»åŠ¡å¤±è´¥ (task_id, error_message)
    task_cancelled = pyqtSignal(str)  # âœ… ä¿®å¤ï¼šæ·»åŠ ä»»åŠ¡å–æ¶ˆä¿¡å· (task_id)

    def __init__(self, config_manager: ImportConfigManager = None,
                 data_manager: UnifiedDataManager = None,
                 max_workers: int = 4,
                 enable_ai_optimization: bool = True,
                 enable_intelligent_config: bool = True,
                 enable_enhanced_performance_bridge: bool = True,
                 enable_enhanced_risk_monitoring: bool = True):
        super().__init__()

        # é…ç½®ç®¡ç†å™¨ - æ”¯æŒæ™ºèƒ½é…ç½®
        if enable_intelligent_config:
            self.config_manager = config_manager or None
            self.enable_intelligent_config = True
        else:
            self.config_manager = config_manager or ImportConfigManager()
            self.enable_intelligent_config = False

        # æ•°æ®ç®¡ç†å™¨ - å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…é˜»å¡
        self.data_manager = data_manager
        self._data_manager_initialized = data_manager is not None

        # çœŸå®æ•°æ®æä¾›å™¨ - å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…é˜»å¡
        self.real_data_provider = None
        self._real_data_provider_initialized = False

        # AIé¢„æµ‹æœåŠ¡ - æ™ºèƒ½ä¼˜åŒ–å¯¼å…¥è¿‡ç¨‹
        self.enable_ai_optimization = enable_ai_optimization
        self.ai_prediction_service = None
        self._ai_service_initialized = False
        if enable_ai_optimization:
            self._init_ai_service()

        # æ·±åº¦åˆ†ææœåŠ¡ - æ€§èƒ½ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹
        self.deep_analysis_service = DeepAnalysisService()
        self.performance_integrator = FactorWeavePerformanceIntegrator()

        # å¢å¼ºç‰ˆæ€§èƒ½æ•°æ®æ¡¥æ¥ç³»ç»Ÿ
        self.enable_enhanced_performance_bridge = enable_enhanced_performance_bridge
        self.enhanced_performance_bridge = None
        if enable_enhanced_performance_bridge:
            self._init_enhanced_performance_bridge()

        # å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»Ÿ
        self.enable_enhanced_risk_monitoring = enable_enhanced_risk_monitoring
        self.enhanced_risk_monitor = None
        if enable_enhanced_risk_monitoring:
            self._init_enhanced_risk_monitor()

        # å¤šçº§ç¼“å­˜ç³»ç»Ÿ
        self.cache_manager = self._init_cache_manager()

        # åˆ†å¸ƒå¼æœåŠ¡ç³»ç»Ÿ
        self.distributed_service = self._init_distributed_service()
        self.node_discovery = self._init_node_discovery()

        # ç›‘æ§é…ç½®
        self.enable_performance_monitoring = True
        self.enable_anomaly_detection = True
        self.enable_intelligent_caching = True
        self.enable_distributed_execution = True
        self.enable_auto_tuning = True
        self.enable_data_quality_monitoring = True
        self.enable_enhanced_event_processing = True
        self.enable_enhanced_async_management = True

        # çº¿ç¨‹æ± ï¼ˆéœ€è¦åœ¨å…¶ä»–ç»„ä»¶ä¹‹å‰åˆå§‹åŒ–ï¼‰
        self.executor = ThreadPoolExecutor(max_workers=max_workers,
                                           thread_name_prefix="ImportEngine")

        # è‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿï¼ˆéœ€è¦åœ¨çº¿ç¨‹æ± åˆå§‹åŒ–ä¹‹åï¼‰
        self.auto_tuner = self._init_auto_tuner()

        # æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ
        self.data_quality_monitor = self._init_data_quality_monitor()

        # âœ… å®æ—¶å†™å…¥æœåŠ¡ç³»ç»Ÿ
        self.realtime_write_service = None
        self.enable_realtime_write = True
        self._batch_write_buffer = {}  # {symbol: DataFrame} æ‰¹é‡å†™å…¥ç¼“å†²åŒº
        self._batch_write_lock = threading.Lock()
        self._init_realtime_write_service()

        # âœ… æ•°æ®åº“å†™å…¥çº¿ç¨‹ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼Œè§£å†³DuckDBå¹¶å‘å†™å…¥æ­»é”ï¼‰
        self.db_writer_thread = DatabaseWriterThread()
        self.db_writer_thread.start()
        logger.info("DatabaseWriterThread å·²å¯åŠ¨")

        # âœ… ä¼˜åŒ–2&3ï¼šè´¨é‡è¯„åˆ†ç¼“å­˜ï¼ˆæ•°æ®æº+æ—¥æœŸâ†’è¯„åˆ†ï¼‰
        self._quality_score_cache = {}  # key: f"{data_source}_{date}", value: score
        self._quality_cache_ttl = 3600  # ç¼“å­˜1å°æ—¶

        # å¢å¼ºç‰ˆäº‹ä»¶æ€»çº¿ç³»ç»Ÿ
        self.enhanced_event_bus = self._init_enhanced_event_bus()

        # å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨
        self.enhanced_async_manager = self._init_enhanced_async_manager()

        # ä»»åŠ¡ç®¡ç†
        self._running_tasks: Dict[str, Future] = {}
        self._task_results: Dict[str, TaskExecutionResult] = {}
        self._task_lock = threading.RLock()

        # AIä¼˜åŒ–ç»Ÿè®¡
        self._ai_optimization_stats = {
            'predictions_made': 0,
            'execution_time_saved': 0.0,
            'accuracy_improved': 0.0,
            'errors_prevented': 0
        }

        # è¿›åº¦ç›‘æ§å®šæ—¶å™¨
        self.progress_timer = QTimer()
        self.progress_timer.timeout.connect(self._update_progress)
        self.progress_timer.start(1000)  # æ¯ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦

        logger.info(f"æ•°æ®å¯¼å…¥æ‰§è¡Œå¼•æ“åˆå§‹åŒ–å®Œæˆ (AIä¼˜åŒ–: {'å¯ç”¨' if enable_ai_optimization else 'ç¦ç”¨'})")

    def _init_ai_service(self):
        """åˆå§‹åŒ–AIé¢„æµ‹æœåŠ¡"""
        try:
            self.ai_prediction_service = AIPredictionService()
            self._ai_service_initialized = True
            logger.info("AIé¢„æµ‹æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"AIé¢„æµ‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_ai_optimization = False
            self._ai_service_initialized = False

    def _predict_execution_time(self, task_config: ImportTaskConfig) -> Optional[float]:
        """ä½¿ç”¨AIé¢„æµ‹ä»»åŠ¡æ‰§è¡Œæ—¶é—´"""
        if not self.enable_ai_optimization or not self._ai_service_initialized:
            return None

        try:
            # æ„å»ºé¢„æµ‹è¾“å…¥æ•°æ®
            prediction_data = {
                'symbols_count': len(task_config.symbols),
                'data_source': task_config.data_source,
                'frequency': task_config.frequency.value,
                'mode': task_config.mode.value,
                'batch_size': task_config.batch_size,
                'max_workers': task_config.max_workers
            }

            # è°ƒç”¨AIé¢„æµ‹æœåŠ¡
            prediction_result = self.ai_prediction_service.predict(
                PredictionType.EXECUTION_TIME,
                prediction_data
            )

            if prediction_result and prediction_result.get('success'):
                predicted_time = prediction_result.get('predicted_time', 0)
                self._ai_optimization_stats['predictions_made'] += 1
                logger.info(f"AIé¢„æµ‹ä»»åŠ¡æ‰§è¡Œæ—¶é—´: {predicted_time:.2f}ç§’")
                return predicted_time

        except Exception as e:
            logger.warning(f"AIæ‰§è¡Œæ—¶é—´é¢„æµ‹å¤±è´¥: {e}")

        return None

    def _optimize_task_parameters(self, task_config: ImportTaskConfig) -> ImportTaskConfig:
        """ä½¿ç”¨AIä¼˜åŒ–ä»»åŠ¡å‚æ•°"""
        if not self.enable_ai_optimization or not self._ai_service_initialized:
            return task_config

        try:
            # è·å–å†å²æ‰§è¡Œæ•°æ®ç”¨äºä¼˜åŒ–
            historical_data = self._get_historical_execution_data(task_config)

            if historical_data:
                # ä½¿ç”¨AIé¢„æµ‹æœ€ä¼˜å‚æ•°
                optimization_result = self.ai_prediction_service.predict(
                    PredictionType.PARAMETER_OPTIMIZATION,
                    {
                        'current_config': task_config.to_dict(),
                        'historical_data': historical_data
                    }
                )

                if optimization_result and optimization_result.get('success'):
                    optimized_params = optimization_result.get('optimized_parameters', {})

                    # åº”ç”¨ä¼˜åŒ–å»ºè®®
                    if 'batch_size' in optimized_params:
                        task_config.batch_size = optimized_params['batch_size']
                    if 'max_workers' in optimized_params:
                        task_config.max_workers = optimized_params['max_workers']

                    logger.info(f"AIä¼˜åŒ–ä»»åŠ¡å‚æ•°: batch_size={task_config.batch_size}, max_workers={task_config.max_workers}")

        except Exception as e:
            logger.warning(f"AIå‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")

        return task_config

    def _get_historical_execution_data(self, task_config: ImportTaskConfig) -> List[Dict]:
        """è·å–å†å²æ‰§è¡Œæ•°æ®"""
        try:
            # ä»é…ç½®ç®¡ç†å™¨è·å–å†å²æ•°æ®
            history = self.config_manager.get_history(limit=50)

            # è¿‡æ»¤ç›¸ä¼¼ä»»åŠ¡çš„å†å²æ•°æ®
            similar_tasks = []
            for record in history:
                if (record.get('data_source') == task_config.data_source and
                        record.get('frequency') == task_config.frequency.value):
                    similar_tasks.append(record)

            return similar_tasks

        except Exception as e:
            logger.warning(f"è·å–å†å²æ‰§è¡Œæ•°æ®å¤±è´¥: {e}")
            return []

    def get_ai_optimization_stats(self) -> Dict[str, Any]:
        """è·å–AIä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return self._ai_optimization_stats.copy()

    def _init_cache_manager(self) -> Optional[MultiLevelCacheManager]:
        """åˆå§‹åŒ–å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""
        try:
            # MultiLevelCacheManagerå®é™…åªæ”¯æŒç®€å•çš„å†…å­˜ç¼“å­˜
            # å‚æ•°ï¼šmax_size (ç¼“å­˜æ¡ç›®æ•°), ttl (ç”Ÿå­˜æ—¶é—´ç§’æ•°)
            cache_manager = MultiLevelCacheManager(
                max_size=1000,
                ttl=3600  # 60åˆ†é’Ÿ = 3600ç§’
            )

            logger.info("å¤šçº§ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            return cache_manager

        except Exception as e:
            logger.error(f"ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def _init_realtime_write_service(self):
        """åˆå§‹åŒ–å®æ—¶å†™å…¥æœåŠ¡"""
        try:
            from ..services.realtime_write_service import RealtimeWriteService
            from ..services.realtime_write_config import RealtimeWriteConfig, WriteStrategy

            # åˆ›å»ºé»˜è®¤é…ç½®
            config = RealtimeWriteConfig(
                enabled=True,
                write_strategy=WriteStrategy.BATCH,  # é»˜è®¤æ‰¹é‡æ¨¡å¼
                batch_size=100,
                concurrency=4,
                max_retries=3,
                enable_performance_monitoring=True
            )

            self.realtime_write_service = RealtimeWriteService(config)
            logger.info(f"å®æ—¶å†™å…¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œç­–ç•¥: {config.write_strategy.value}")

        except Exception as e:
            logger.warning(f"å®æ—¶å†™å…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç›´æ¥å†™å…¥æ¨¡å¼")
            self.realtime_write_service = None
            self.enable_realtime_write = False

    def _cache_task_data(self, task_id: str, data_type: str, data: Any) -> bool:
        """ç¼“å­˜ä»»åŠ¡æ•°æ®"""
        if not self.enable_intelligent_caching:
            return False

        try:
            cache_key = f"task_{task_id}_{data_type}"

            # ä½¿ç”¨å¤šçº§ç¼“å­˜å­˜å‚¨
            if self.cache_manager:
                success = self.cache_manager.set(cache_key, data)
                if success:
                    logger.debug(f"æ•°æ®å·²ç¼“å­˜: {cache_key}")
                    return True

        except Exception as e:
            logger.warning(f"ç¼“å­˜æ•°æ®å¤±è´¥: {e}")

        return False

    def _get_cached_task_data(self, task_id: str, data_type: str) -> Optional[Any]:
        """è·å–ç¼“å­˜çš„ä»»åŠ¡æ•°æ®"""
        if not self.enable_intelligent_caching:
            return None

        try:
            cache_key = f"task_{task_id}_{data_type}"

            # ä¼˜å…ˆä»å¤šçº§ç¼“å­˜è·å–
            if self.cache_manager:
                data = self.cache_manager.get(cache_key)
                if data is not None:
                    logger.debug(f"ä»å¤šçº§ç¼“å­˜å‘½ä¸­: {cache_key}")
                    return data

        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")

        return None

    def _cache_configuration_data(self, config: ImportTaskConfig) -> bool:
        """ç¼“å­˜é…ç½®æ•°æ®"""
        if not self.enable_intelligent_caching:
            return False

        try:
            # ç”Ÿæˆé…ç½®ç¼“å­˜é”®
            config_hash = hashlib.md5(
                f"{config.data_source}_{config.asset_type}_{config.frequency.value}".encode()
            ).hexdigest()[:8]

            cache_key = f"config_{config_hash}"

            # ç¼“å­˜é…ç½®ç›¸å…³çš„ä¼˜åŒ–æ•°æ®
            cache_data = {
                'optimal_batch_size': config.batch_size,
                'optimal_workers': config.max_workers,
                'data_source': config.data_source,
                'frequency': config.frequency.value,
                'cached_at': datetime.now().isoformat()
            }

            if self.cache_manager:
                return self.cache_manager.set(cache_key, cache_data)

        except Exception as e:
            logger.warning(f"ç¼“å­˜é…ç½®æ•°æ®å¤±è´¥: {e}")

        return False

    def _get_cached_configuration(self, config: ImportTaskConfig) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„é…ç½®æ•°æ®"""
        if not self.enable_intelligent_caching:
            return None

        try:
            config_hash = hashlib.md5(
                f"{config.data_source}_{config.asset_type}_{config.frequency.value}".encode()
            ).hexdigest()[:8]

            cache_key = f"config_{config_hash}"

            if self.cache_manager:
                return self.cache_manager.get(cache_key)

        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜é…ç½®å¤±è´¥: {e}")

        return None

    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'intelligent_caching_enabled': self.enable_intelligent_caching,
            'cache_manager_available': self.cache_manager is not None
        }

        try:
            if self.cache_manager:
                # è·å–å¤šçº§ç¼“å­˜ç»Ÿè®¡
                cache_stats = self.cache_manager.get_statistics()
                stats['multi_level_cache'] = cache_stats

        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            stats['error'] = str(e)

        return stats

    def submit_distributed_import_task(self,
                                       task_config: 'ImportTaskConfig',
                                       priority: str = "normal") -> Optional[str]:
        """æäº¤åˆ†å¸ƒå¼å¯¼å…¥ä»»åŠ¡"""
        try:
            if not self.distributed_service:
                logger.warning("åˆ†å¸ƒå¼æœåŠ¡æœªåˆå§‹åŒ–ï¼Œæ— æ³•æäº¤åˆ†å¸ƒå¼ä»»åŠ¡")
                return None

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡
            if hasattr(self.distributed_service, 'submit_enhanced_task'):
                from ..async_management.enhanced_async_manager import TaskPriority

                # è½¬æ¢ä¼˜å…ˆçº§
                priority_map = {
                    "critical": TaskPriority.CRITICAL,
                    "high": TaskPriority.HIGH,
                    "normal": TaskPriority.NORMAL,
                    "low": TaskPriority.LOW,
                    "background": TaskPriority.BACKGROUND
                }

                task_priority = priority_map.get(priority, TaskPriority.NORMAL)

                # ä¼°ç®—èµ„æºéœ€æ±‚
                cpu_requirement = min(4.0, task_config.max_workers)
                memory_requirement = max(512, task_config.batch_size * 2)  # MB

                # æäº¤å¢å¼ºç‰ˆä»»åŠ¡
                task_id = self.distributed_service.submit_enhanced_task(
                    task_type="data_import",
                    task_data={
                        "config": task_config.to_dict(),
                        "symbols": task_config.symbols,
                        "data_source": task_config.data_source
                    },
                    priority=task_priority,
                    cpu_requirement=cpu_requirement,
                    memory_requirement=memory_requirement,
                    timeout=3600,  # 1å°æ—¶è¶…æ—¶
                    affinity_rules={
                        "data_source": task_config.data_source
                    }
                )

                logger.info(f"æäº¤å¢å¼ºç‰ˆåˆ†å¸ƒå¼å¯¼å…¥ä»»åŠ¡: {task_id}")
                return task_id
            else:
                # ä½¿ç”¨åŸå§‹åˆ†å¸ƒå¼æœåŠ¡
                task_id = self.distributed_service.submit_analysis_task(
                    stock_code=",".join(task_config.symbols[:5]),  # é™åˆ¶é•¿åº¦
                    analysis_type="import"
                )

                logger.info(f"æäº¤åŸå§‹åˆ†å¸ƒå¼å¯¼å…¥ä»»åŠ¡: {task_id}")
                return task_id

        except Exception as e:
            logger.error(f"æäº¤åˆ†å¸ƒå¼å¯¼å…¥ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def get_distributed_service_status(self) -> Dict[str, Any]:
        """è·å–åˆ†å¸ƒå¼æœåŠ¡çŠ¶æ€"""
        try:
            if not self.distributed_service:
                return {"error": "åˆ†å¸ƒå¼æœåŠ¡æœªåˆå§‹åŒ–"}

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡
            if hasattr(self.distributed_service, 'get_service_status'):
                return self.distributed_service.get_service_status()
            else:
                # åŸå§‹åˆ†å¸ƒå¼æœåŠ¡çš„åŸºæœ¬çŠ¶æ€
                return {
                    "service_running": self.distributed_service.running,
                    "service_type": "original"
                }

        except Exception as e:
            logger.error(f"è·å–åˆ†å¸ƒå¼æœåŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return {"error": str(e)}

    def _init_distributed_service(self) -> Optional[DistributedService]:
        """åˆå§‹åŒ–åˆ†å¸ƒå¼æœåŠ¡"""
        try:
            # âœ… ä½¿ç”¨ServiceContainerä¸­çš„DistributedService
            from ..containers import get_service_container

            container = get_service_container()

            if container.is_registered(DistributedService):
                distributed_service = container.resolve(DistributedService)
                logger.info("âœ… ä½¿ç”¨ServiceContainerä¸­çš„DistributedService")
                return distributed_service

            # Fallbackï¼šåˆ›å»ºæ–°å®ä¾‹
            logger.info("ServiceContainerä¸­æ— DistributedServiceï¼Œåˆ›å»ºæ–°å®ä¾‹")
            distributed_service = DistributedService()
            distributed_service.start_service()

            logger.info("åˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return distributed_service

        except ImportError:
            # å›é€€åˆ°åŸå§‹åˆ†å¸ƒå¼æœåŠ¡
            logger.warning("å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬")
            try:
                distributed_service = DistributedService(discovery_port=8888)
                distributed_service.start_service()
                logger.info("åŸå§‹åˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                return distributed_service
            except Exception as e:
                logger.error(f"åŸå§‹åˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                return None
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    def _init_node_discovery(self) -> Optional[NodeDiscovery]:
        """åˆå§‹åŒ–èŠ‚ç‚¹å‘ç°æœåŠ¡"""
        try:
            node_discovery = NodeDiscovery(discovery_port=8888)

            # æ·»åŠ èŠ‚ç‚¹å‘ç°å›è°ƒ
            node_discovery.add_discovery_callback(self._on_node_discovered)

            # å¯åŠ¨èŠ‚ç‚¹å‘ç°
            node_discovery.start_discovery()

            logger.info("èŠ‚ç‚¹å‘ç°æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return node_discovery

        except Exception as e:
            logger.error(f"èŠ‚ç‚¹å‘ç°æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    def _on_node_discovered(self, node_info: NodeInfo):
        """èŠ‚ç‚¹å‘ç°å›è°ƒ"""
        try:
            logger.info(f"å‘ç°æ–°èŠ‚ç‚¹: {node_info.node_id} ({node_info.address}:{node_info.port})")

            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ”¯æŒæ•°æ®å¯¼å…¥æœåŠ¡
            if 'import_execution' in node_info.services:
                logger.info(f"èŠ‚ç‚¹ {node_info.node_id} æ”¯æŒåˆ†å¸ƒå¼æ•°æ®å¯¼å…¥")

                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è´Ÿè½½å‡è¡¡é€»è¾‘
                self._register_distributed_node(node_info)

        except Exception as e:
            logger.error(f"å¤„ç†èŠ‚ç‚¹å‘ç°å¤±è´¥: {e}")

    def _register_distributed_node(self, node_info: NodeInfo):
        """æ³¨å†Œåˆ†å¸ƒå¼èŠ‚ç‚¹"""
        try:
            if not hasattr(self, '_distributed_nodes'):
                self._distributed_nodes = {}

            self._distributed_nodes[node_info.node_id] = {
                'node_info': node_info,
                'last_seen': datetime.now(),
                'task_count': 0,
                'available': True
            }

            logger.info(f"å·²æ³¨å†Œåˆ†å¸ƒå¼èŠ‚ç‚¹: {node_info.node_id}")

        except Exception as e:
            logger.error(f"æ³¨å†Œåˆ†å¸ƒå¼èŠ‚ç‚¹å¤±è´¥: {e}")

    def _can_distribute_task(self, task_config: ImportTaskConfig) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯ä»¥åˆ†å¸ƒå¼æ‰§è¡Œ"""
        if not self.enable_distributed_execution:
            return False

        try:
            # âœ… ä½¿ç”¨çœŸå®çš„DistributedServiceæ£€æŸ¥èŠ‚ç‚¹
            if not self.distributed_service:
                logger.debug("åˆ†å¸ƒå¼æœåŠ¡æœªåˆå§‹åŒ–")
                return False

            # è·å–å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨
            nodes_status = self.distributed_service.get_all_nodes_status()

            if not nodes_status:
                logger.debug("æ— å¯ç”¨åˆ†å¸ƒå¼èŠ‚ç‚¹")
                return False

            available_nodes = [
                node for node in nodes_status
                if node.get('status') in ['active', 'idle'] and node.get('current_tasks', 0) < 3
            ]

            # åªæœ‰å½“ä»»åŠ¡è¶³å¤Ÿå¤§ä¸”æœ‰å¯ç”¨èŠ‚ç‚¹æ—¶æ‰åˆ†å¸ƒå¼æ‰§è¡Œ
            symbol_count = len(task_config.symbols)
            can_distribute = symbol_count >= 100 and len(available_nodes) > 0

            if can_distribute:
                logger.info(f"âœ… ä»»åŠ¡å¯åˆ†å¸ƒå¼æ‰§è¡Œ: {symbol_count}ä¸ªè‚¡ç¥¨ï¼Œ{len(available_nodes)}ä¸ªå¯ç”¨èŠ‚ç‚¹")

            return can_distribute

        except Exception as e:
            logger.error(f"æ£€æŸ¥åˆ†å¸ƒå¼æ‰§è¡Œæ¡ä»¶å¤±è´¥: {e}")
            return False

    def _distribute_task(self, task_config: ImportTaskConfig) -> bool:
        """åˆ†å¸ƒå¼æ‰§è¡Œä»»åŠ¡"""
        if not self._can_distribute_task(task_config):
            return False

        try:
            # âœ… ä½¿ç”¨çœŸå®çš„DistributedServiceæäº¤ä»»åŠ¡
            logger.info(f"å¼€å§‹åˆ†å¸ƒå¼æ‰§è¡Œä»»åŠ¡: {task_config.task_id}")

            # æ„é€ å¯¼å…¥é…ç½®
            import_config = {
                "symbols": task_config.symbols,
                "data_source": task_config.data_source,
                "start_date": task_config.start_date,
                "end_date": task_config.end_date,
                "frequency": task_config.frequency,
                "asset_type": task_config.asset_type.value if hasattr(task_config.asset_type, 'value') else str(task_config.asset_type),
                "batch_size": task_config.batch_size,
                "parallel_workers": task_config.max_workers
            }

            # æäº¤æ•°æ®å¯¼å…¥ä»»åŠ¡åˆ°åˆ†å¸ƒå¼æœåŠ¡
            task_id = self.distributed_service.submit_data_import_task(import_config)

            if task_id:
                logger.info(f"âœ… æˆåŠŸæäº¤åˆ†å¸ƒå¼ä»»åŠ¡: {task_id}")

                # è®°å½•ä»»åŠ¡IDç”¨äºåç»­è·Ÿè¸ª
                if not hasattr(self, '_distributed_task_ids'):
                    self._distributed_task_ids = {}
                self._distributed_task_ids[task_config.task_id] = task_id

                return True
            else:
                logger.warning("åˆ†å¸ƒå¼ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ— ä»»åŠ¡IDè¿”å›")
                return False

        except Exception as e:
            logger.error(f"åˆ†å¸ƒå¼æ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
            return False

    def _select_best_node(self) -> Optional[Dict[str, Any]]:
        """é€‰æ‹©æœ€ä½³åˆ†å¸ƒå¼èŠ‚ç‚¹"""
        try:
            if not hasattr(self, '_distributed_nodes'):
                return None

            available_nodes = [
                node for node in self._distributed_nodes.values()
                if node['available'] and node['task_count'] < 3
            ]

            if not available_nodes:
                return None

            # é€‰æ‹©ä»»åŠ¡æ•°æœ€å°‘çš„èŠ‚ç‚¹
            best_node = min(available_nodes, key=lambda x: x['task_count'])
            return best_node

        except Exception as e:
            logger.error(f"é€‰æ‹©æœ€ä½³èŠ‚ç‚¹å¤±è´¥: {e}")
            return None

    def _split_task(self, task_config: ImportTaskConfig) -> List[ImportTaskConfig]:
        """åˆ†å‰²ä»»åŠ¡ä¸ºå­ä»»åŠ¡"""
        try:
            subtasks = []
            symbols = task_config.symbols
            chunk_size = max(50, len(symbols) // 4)  # æ¯ä¸ªå­ä»»åŠ¡è‡³å°‘50ä¸ªè‚¡ç¥¨

            for i in range(0, len(symbols), chunk_size):
                chunk_symbols = symbols[i:i + chunk_size]

                # åˆ›å»ºå­ä»»åŠ¡é…ç½®
                subtask_config = ImportTaskConfig(
                    task_id=f"{task_config.task_id}_subtask_{i//chunk_size}",
                    name=f"{task_config.name}_å­ä»»åŠ¡_{i//chunk_size}",
                    symbols=chunk_symbols,
                    data_source=task_config.data_source,
                    asset_type=task_config.asset_type,
                    frequency=task_config.frequency,
                    mode=task_config.mode,
                    batch_size=task_config.batch_size,
                    max_workers=min(task_config.max_workers, 2)  # å­ä»»åŠ¡ä½¿ç”¨è¾ƒå°‘çº¿ç¨‹
                )

                subtasks.append(subtask_config)

            logger.info(f"ä»»åŠ¡å·²åˆ†å‰²ä¸º {len(subtasks)} ä¸ªå­ä»»åŠ¡")
            return subtasks

        except Exception as e:
            logger.error(f"åˆ†å‰²ä»»åŠ¡å¤±è´¥: {e}")
            return []

    def _send_subtask_to_node(self, subtask: ImportTaskConfig, node: Dict[str, Any]) -> bool:
        """å‘é€å­ä»»åŠ¡åˆ°åˆ†å¸ƒå¼èŠ‚ç‚¹"""
        try:
            node_info = node['node_info']

            # è¿™é‡Œåº”è¯¥é€šè¿‡ç½‘ç»œå‘é€ä»»åŠ¡åˆ°è¿œç¨‹èŠ‚ç‚¹
            # ç”±äºè¿™æ˜¯é›†æˆç°æœ‰åŠŸèƒ½ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿå‘é€è¿‡ç¨‹
            logger.info(f"å‘é€å­ä»»åŠ¡ {subtask.task_id} åˆ°èŠ‚ç‚¹ {node_info.node_id}")

            # æ›´æ–°èŠ‚ç‚¹ä»»åŠ¡è®¡æ•°
            node['task_count'] += 1

            return True

        except Exception as e:
            logger.error(f"å‘é€å­ä»»åŠ¡åˆ°èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def get_distributed_status(self) -> Dict[str, Any]:
        """è·å–åˆ†å¸ƒå¼æœåŠ¡çŠ¶æ€"""
        status = {
            'distributed_execution_enabled': self.enable_distributed_execution,
            'distributed_service_available': self.distributed_service is not None,
            'node_discovery_available': self.node_discovery is not None,
            'discovered_nodes': 0,
            'available_nodes': 0,
            'service_type': 'unknown'
        }

        try:
            # è·å–å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡çŠ¶æ€
            enhanced_status = self.get_distributed_service_status()
            if 'error' not in enhanced_status:
                status.update(enhanced_status)
                status['service_type'] = 'enhanced' if 'load_balancing_strategy' in enhanced_status else 'original'
            if hasattr(self, '_distributed_nodes'):
                status['discovered_nodes'] = len(self._distributed_nodes)
                status['available_nodes'] = len([
                    node for node in self._distributed_nodes.values()
                    if node['available']
                ])

                status['nodes_detail'] = [
                    {
                        'node_id': node_id,
                        'address': node['node_info'].address,
                        'port': node['node_info'].port,
                        'task_count': node['task_count'],
                        'available': node['available'],
                        'last_seen': node['last_seen'].isoformat()
                    }
                    for node_id, node in self._distributed_nodes.items()
                ]

        except Exception as e:
            logger.error(f"è·å–åˆ†å¸ƒå¼çŠ¶æ€å¤±è´¥: {e}")
            status['error'] = str(e)

        return status

    def _register_import_event_handlers(self, event_bus: EnhancedEventBus):
        """æ³¨å†Œæ•°æ®å¯¼å…¥ç›¸å…³çš„äº‹ä»¶å¤„ç†å™¨"""
        try:
            # ä»»åŠ¡å¼€å§‹äº‹ä»¶å¤„ç†å™¨
            event_bus.subscribe_enhanced(
                "import_task_started",
                self._handle_import_task_started_event,
                priority=3
            )

            # ä»»åŠ¡è¿›åº¦æ›´æ–°äº‹ä»¶å¤„ç†å™¨
            event_bus.subscribe_enhanced(
                "import_task_progress",
                self._handle_import_task_progress_event,
                priority=4
            )

            # ä»»åŠ¡å®Œæˆäº‹ä»¶å¤„ç†å™¨
            event_bus.subscribe_enhanced(
                "import_task_completed",
                self._handle_import_task_completed_event,
                priority=2
            )

            # ä»»åŠ¡å¤±è´¥äº‹ä»¶å¤„ç†å™¨
            event_bus.subscribe_enhanced(
                "import_task_failed",
                self._handle_import_task_failed_event,
                priority=1
            )

            logger.info("æ•°æ®å¯¼å…¥äº‹ä»¶å¤„ç†å™¨æ³¨å†Œå®Œæˆ")

        except Exception as e:
            logger.error(f"æ³¨å†Œäº‹ä»¶å¤„ç†å™¨å¤±è´¥: {e}")

    def _handle_import_task_started_event(self, event):
        """å¤„ç†å¯¼å…¥ä»»åŠ¡å¼€å§‹äº‹ä»¶"""
        try:
            task_id = event.data.get('task_id')
            task_name = event.data.get('task_name', 'Unknown')

            logger.info(f"äº‹ä»¶å¤„ç† - å¯¼å…¥ä»»åŠ¡å¼€å§‹: {task_name} ({task_id})")

            # å‘é€Qtä¿¡å·
            self.task_started.emit(task_id, task_name)

        except Exception as e:
            logger.error(f"å¤„ç†å¯¼å…¥ä»»åŠ¡å¼€å§‹äº‹ä»¶å¤±è´¥: {e}")

    def _handle_import_task_progress_event(self, event):
        """å¤„ç†å¯¼å…¥ä»»åŠ¡è¿›åº¦äº‹ä»¶"""
        try:
            task_id = event.data.get('task_id')
            progress = event.data.get('progress', 0)
            status = event.data.get('status', 'unknown')

            # å‘é€Qtä¿¡å·
            self.progress_updated.emit(task_id, progress, status)

        except Exception as e:
            logger.error(f"å¤„ç†å¯¼å…¥ä»»åŠ¡è¿›åº¦äº‹ä»¶å¤±è´¥: {e}")

    def _handle_import_task_completed_event(self, event):
        """å¤„ç†å¯¼å…¥ä»»åŠ¡å®Œæˆäº‹ä»¶"""
        try:
            task_id = event.data.get('task_id')
            task_name = event.data.get('task_name', 'Unknown')
            execution_time = event.data.get('execution_time', 0)
            result = event.data.get('result')

            logger.info(f"äº‹ä»¶å¤„ç† - å¯¼å…¥ä»»åŠ¡å®Œæˆ: {task_name} ({execution_time:.2f}s)")

            # å‘é€Qtä¿¡å·
            self.task_completed.emit(task_id, result)

        except Exception as e:
            logger.error(f"å¤„ç†å¯¼å…¥ä»»åŠ¡å®Œæˆäº‹ä»¶å¤±è´¥: {e}")

    def _handle_import_task_failed_event(self, event):
        """å¤„ç†å¯¼å…¥ä»»åŠ¡å¤±è´¥äº‹ä»¶"""
        try:
            task_id = event.data.get('task_id')
            task_name = event.data.get('task_name', 'Unknown')
            error = event.data.get('error', 'Unknown error')

            logger.error(f"äº‹ä»¶å¤„ç† - å¯¼å…¥ä»»åŠ¡å¤±è´¥: {task_name} - {error}")

            # å‘é€Qtä¿¡å·
            self.task_failed.emit(task_id, error)

        except Exception as e:
            logger.error(f"å¤„ç†å¯¼å…¥ä»»åŠ¡å¤±è´¥äº‹ä»¶å¤±è´¥: {e}")

    def submit_enhanced_async_task(self,
                                   func: Callable,
                                   *args,
                                   task_name: str = None,
                                   priority: TaskPriority = TaskPriority.NORMAL,
                                   timeout: float = None,
                                   resource_requirements: ResourceRequirement = None,
                                   **kwargs) -> Optional[str]:
        """æäº¤å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡"""
        if not self.enable_enhanced_async_management or not self.enhanced_async_manager:
            logger.warning("å¢å¼ºç‰ˆå¼‚æ­¥ç®¡ç†å™¨æœªå¯ç”¨æˆ–æœªåˆå§‹åŒ–")
            return None

        try:
            task_id = self.enhanced_async_manager.submit_task(
                func=func,
                *args,
                name=task_name or getattr(func, '__name__', 'unnamed_task'),
                priority=priority,
                timeout=timeout,
                resource_requirements=resource_requirements or ResourceRequirement(),
                **kwargs
            )

            return task_id

        except Exception as e:
            logger.error(f"æäº¤å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def publish_import_event(self,
                             event_name: str,
                             event_data: Dict[str, Any],
                             priority: EventPriority = EventPriority.NORMAL,
                             correlation_id: str = None):
        """å‘å¸ƒå¯¼å…¥ç›¸å…³äº‹ä»¶"""
        if not self.enable_enhanced_event_processing or not self.enhanced_event_bus:
            return

        try:
            self.enhanced_event_bus.publish_enhanced(
                event_name=event_name,
                event_data=event_data,
                priority=priority,
                source="import_engine",
                correlation_id=correlation_id,
                tags={"module": "data_import"}
            )
        except Exception as e:
            logger.error(f"å‘å¸ƒå¯¼å…¥äº‹ä»¶å¤±è´¥: {e}")

    def get_enhanced_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç‰ˆå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        # äº‹ä»¶æ€»çº¿ç»Ÿè®¡
        if self.enhanced_event_bus:
            stats['event_bus'] = self.enhanced_event_bus.get_enhanced_stats()

        # å¼‚æ­¥ç®¡ç†å™¨ç»Ÿè®¡
        if self.enhanced_async_manager:
            stats['async_manager'] = self.enhanced_async_manager.get_stats()

        return stats

    def get_database_writer_stats(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åº“å†™å…¥çº¿ç¨‹ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡å­—å…¸ï¼ŒåŒ…å«ï¼š
            - queue_size: é˜Ÿåˆ—å½“å‰å¤§å°
            - queue_peak: é˜Ÿåˆ—å³°å€¼å¤§å°
            - total_writes: æ€»å†™å…¥æ¬¡æ•°
            - failed_writes: å¤±è´¥å†™å…¥æ¬¡æ•°
            - merge_buffer_size: åˆå¹¶ç¼“å†²åŒºå¤§å°
            - is_stopped: æ˜¯å¦å·²åœæ­¢
        """
        if hasattr(self, 'db_writer_thread'):
            return self.db_writer_thread.get_stats()
        else:
            return {
                'queue_size': 0,
                'queue_peak': 0,
                'total_writes': 0,
                'failed_writes': 0,
                'merge_buffer_size': 0,
                'is_stopped': True
            }

    def get_tongdaxin_ip_stats(self) -> Dict[str, Any]:
        """
        è·å–é€šè¾¾ä¿¡IPä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰

        Returns:
            IPç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
            - total_connections: æ€»è¿æ¥æ•°
            - active_servers: æ´»è·ƒæœåŠ¡å™¨æ•°
            - healthy_ips: å¥åº·IPæ•°
            - limited_ips: é™æµIPæ•°
            - failed_ips: æ•…éšœIPæ•°
            - ip_stats: IPè¯¦ç»†ç»Ÿè®¡åˆ—è¡¨
        """
        try:
            # ä»UnifiedDataManagerè·å–é€šè¾¾ä¿¡æ’ä»¶
            from core.services.unified_data_manager import get_unified_data_manager
            unified_manager = get_unified_data_manager()

            if not unified_manager:
                logger.debug("IPç›‘æ§: UnifiedDataManagerä¸å¯ç”¨")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': 'UnifiedDataManagerä¸å¯ç”¨'
                }

            # è·å–æ’ä»¶ä¸­å¿ƒ - é€šè¿‡_uni_plugin_managerè®¿é—®
            # UnifiedDataManageræ²¡æœ‰ç›´æ¥çš„plugin_centerå±æ€§ï¼Œéœ€è¦é€šè¿‡_uni_plugin_managerè®¿é—®
            uni_plugin_manager = getattr(unified_manager, '_uni_plugin_manager', None)
            if not uni_plugin_manager:
                # å°è¯•é€šè¿‡get_uni_plugin_manager()æ–¹æ³•è·å–
                if hasattr(unified_manager, 'get_uni_plugin_manager'):
                    uni_plugin_manager = unified_manager.get_uni_plugin_manager()

            if not uni_plugin_manager:
                logger.debug("IPç›‘æ§: UniPluginDataManagerä¸å¯ç”¨")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': 'UniPluginDataManagerä¸å¯ç”¨'
                }

            # ä»UniPluginDataManagerè·å–plugin_center
            plugin_center = getattr(uni_plugin_manager, 'plugin_center', None)
            if not plugin_center:
                logger.debug("IPç›‘æ§: æ’ä»¶ä¸­å¿ƒä¸å¯ç”¨")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': 'æ’ä»¶ä¸­å¿ƒä¸å¯ç”¨'
                }

            # æŸ¥æ‰¾é€šè¾¾ä¿¡æ’ä»¶
            tongdaxin_plugin_id = 'data_sources.stock.tongdaxin_plugin'
            plugin = plugin_center.get_plugin(tongdaxin_plugin_id)

            if not plugin:
                logger.debug(f"IPç›‘æ§: é€šè¾¾ä¿¡æ’ä»¶æœªæ‰¾åˆ° (ID: {tongdaxin_plugin_id})")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': f'é€šè¾¾ä¿¡æ’ä»¶æœªæ‰¾åˆ° (ID: {tongdaxin_plugin_id})'
                }

            # è·å–è¿æ¥æ± ä¿¡æ¯
            connection_pool = getattr(plugin, 'connection_pool', None)
            use_connection_pool = getattr(plugin, 'use_connection_pool', False)
            server_list = getattr(plugin, 'server_list', None)
            plugin_state = getattr(plugin, 'plugin_state', None)

            # âœ… ä¿®å¤ï¼šå¦‚æœè¿æ¥æ± æœªåˆå§‹åŒ–ï¼Œå°è¯•è§¦å‘å¼‚æ­¥è¿æ¥ï¼ˆå¦‚æœæ’ä»¶çŠ¶æ€å…è®¸ï¼‰
            if use_connection_pool and not connection_pool:
                # æ£€æŸ¥æ’ä»¶çŠ¶æ€
                from plugins.plugin_interface import PluginState
                if plugin_state == PluginState.INITIALIZED:
                    # æ’ä»¶å·²åˆå§‹åŒ–ä½†æœªè¿æ¥ï¼Œå°è¯•è§¦å‘å¼‚æ­¥è¿æ¥
                    logger.debug("IPç›‘æ§: è¿æ¥æ± æœªåˆå§‹åŒ–ï¼Œå°è¯•è§¦å‘å¼‚æ­¥è¿æ¥...")
                    try:
                        if hasattr(plugin, 'connect_async'):
                            connection_future = plugin.connect_async()
                            if connection_future:
                                logger.debug("IPç›‘æ§: å·²è§¦å‘å¼‚æ­¥è¿æ¥ï¼Œè¿æ¥æ± å°†åœ¨åå°åˆå§‹åŒ–")
                                return {
                                    'total_connections': 0,
                                    'active_servers': 0,
                                    'healthy_ips': 0,
                                    'limited_ips': 0,
                                    'failed_ips': 0,
                                    'ip_stats': [],
                                    'error_message': 'è¿æ¥æ± æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...'
                                }
                    except Exception as e:
                        logger.debug(f"IPç›‘æ§: è§¦å‘å¼‚æ­¥è¿æ¥å¤±è´¥: {e}")

                # å¦‚æœæ­£åœ¨è¿æ¥ä¸­ï¼Œæ˜¾ç¤ºè¿æ¥ä¸­çŠ¶æ€
                if plugin_state == PluginState.CONNECTING:
                    logger.debug("IPç›‘æ§: è¿æ¥æ± æ­£åœ¨åˆå§‹åŒ–ä¸­...")
                    return {
                        'total_connections': 0,
                        'active_servers': 0,
                        'healthy_ips': 0,
                        'limited_ips': 0,
                        'failed_ips': 0,
                        'ip_stats': [],
                        'error_message': 'è¿æ¥æ± æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...'
                    }

                # å…¶ä»–æƒ…å†µï¼Œæ˜¾ç¤ºæœªåˆå§‹åŒ–æç¤º
                logger.debug("IPç›‘æ§: è¿æ¥æ± æ¨¡å¼å·²å¯ç”¨ä½†è¿æ¥æ± æœªåˆå§‹åŒ–")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': 'è¿æ¥æ± æœªåˆå§‹åŒ–ï¼ˆè¯·å…ˆè¿æ¥æ•°æ®æºä»¥åˆå§‹åŒ–è¿æ¥æ± ï¼‰'
                }

            if not connection_pool:
                logger.debug(f"IPç›‘æ§: è¿æ¥æ± ä¸å¯ç”¨ (use_connection_pool={use_connection_pool}, has_server_list={bool(server_list)})")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': f'è¿æ¥æ± ä¸å¯ç”¨ (use_connection_pool={use_connection_pool}, è¿æ¥æ± æœªåˆå§‹åŒ–æˆ–åˆå§‹åŒ–å¤±è´¥)'
                }

            # è·å–è¿æ¥æ± ä¿¡æ¯
            pool_info = connection_pool.get_connection_pool_info()

            if not pool_info:
                logger.debug("IPç›‘æ§: è¿æ¥æ± ä¿¡æ¯ä¸ºç©º")
                return {
                    'total_connections': 0,
                    'active_servers': 0,
                    'healthy_ips': 0,
                    'limited_ips': 0,
                    'failed_ips': 0,
                    'ip_stats': [],
                    'error_message': 'è¿æ¥æ± ä¿¡æ¯ä¸ºç©º'
                }

            # è½¬æ¢IPç»Ÿè®¡ä¸ºåˆ—è¡¨æ ¼å¼
            ip_stats_dict = pool_info.get('ip_stats', {})
            ip_stats_list = []

            if isinstance(ip_stats_dict, dict):
                for server_key, stats in ip_stats_dict.items():
                    if not isinstance(stats, dict):
                        logger.debug(f"IPç›‘æ§: è·³è¿‡æ— æ•ˆçš„statsæ•°æ® (server_key={server_key}, type={type(stats)})")
                        continue

                    # âœ… ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰æœ‰æ•ˆå€¼ï¼Œé¿å…æ˜¾ç¤ºç©ºç™½
                    ip = stats.get('ip', '')
                    port = stats.get('port', '')
                    use_count = stats.get('use_count', 0) or 0
                    success_count = stats.get('success_count', 0) or 0
                    failure_count = stats.get('failure_count', 0) or 0
                    avg_response_time = stats.get('avg_response_time', 0.0) or 0.0
                    status = stats.get('status', 'healthy') or 'healthy'
                    success_rate = stats.get('success_rate', 0.0) or 0.0

                    # âœ… ä¿®å¤ï¼šå¦‚æœIPæˆ–ç«¯å£ä¸ºç©ºï¼Œå°è¯•ä»server_keyè§£æ
                    if not ip or not port:
                        try:
                            if ':' in server_key:
                                parsed_ip, parsed_port = server_key.split(':', 1)
                                ip = ip or parsed_ip.strip()
                                port = port or parsed_port.strip()
                        except Exception as e:
                            logger.debug(f"IPç›‘æ§: ä»server_keyè§£æIP/ç«¯å£å¤±è´¥: {server_key}, {e}")

                    # âœ… ä¿®å¤ï¼šå¦‚æœæ•°æ®ä»ç„¶ä¸å®Œæ•´ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡
                    if not ip:
                        logger.debug(f"IPç›‘æ§: IPåœ°å€ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ¡è®°å½• (server_key={server_key})")
                        continue

                    ip_stats_list.append({
                        'ip': ip,
                        'port': port,
                        'use_count': use_count,
                        'success_count': success_count,
                        'failure_count': failure_count,
                        'avg_response_time': avg_response_time,
                        'status': status,
                        'success_rate': success_rate,
                        'last_used': stats.get('last_used')
                    })

            result = {
                'total_connections': pool_info.get('total_connections', 0),
                'active_servers': pool_info.get('active_servers', 0),
                'healthy_ips': pool_info.get('healthy_ips', 0),
                'limited_ips': pool_info.get('limited_ips', 0),
                'failed_ips': pool_info.get('failed_ips', 0),
                'ip_stats': ip_stats_list
            }

            logger.debug(f"IPç›‘æ§: è·å–åˆ° {len(ip_stats_list)} ä¸ªIPç»Ÿè®¡ï¼Œå¥åº·IP: {result['healthy_ips']}, æ€»è¿æ¥: {result['total_connections']}")
            return result

        except Exception as e:
            logger.error(f"è·å–é€šè¾¾ä¿¡IPç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'total_connections': 0,
                'active_servers': 0,
                'healthy_ips': 0,
                'limited_ips': 0,
                'failed_ips': 0,
                'ip_stats': []
            }

    def _init_auto_tuner(self) -> Optional[AutoTuner]:
        """åˆå§‹åŒ–è‡ªåŠ¨è°ƒä¼˜å™¨"""
        try:
            # ç¡®ä¿PerformanceEvaluatorå¯ç”¨
            try:
                from optimization.algorithm_optimizer import PerformanceEvaluator
                evaluator = PerformanceEvaluator(debug_mode=False)
                logger.debug("PerformanceEvaluatoråˆå§‹åŒ–æˆåŠŸ")
            except Exception as eval_error:
                logger.warning(f"PerformanceEvaluatoråˆå§‹åŒ–å¤±è´¥: {eval_error}")
                # ç»§ç»­åˆå§‹åŒ–AutoTunerï¼Œå®ƒå¯èƒ½æœ‰å†…ç½®çš„è¯„ä¼°å™¨

            # é…ç½®è‡ªåŠ¨è°ƒä¼˜å™¨
            max_workers = min(4, self.executor._max_workers)  # ä½¿ç”¨è¾ƒå°‘çš„å·¥ä½œçº¿ç¨‹
            auto_tuner = AutoTuner(max_workers=max_workers, debug_mode=False)

            logger.info("è‡ªåŠ¨è°ƒä¼˜å™¨åˆå§‹åŒ–æˆåŠŸ")
            return auto_tuner

        except Exception as e:
            logger.error(f"è‡ªåŠ¨è°ƒä¼˜å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def _auto_tune_task_parameters(self, task_config: ImportTaskConfig) -> ImportTaskConfig:
        """ä½¿ç”¨AutoTunerè‡ªåŠ¨è°ƒä¼˜ä»»åŠ¡å‚æ•°"""
        if not self.enable_auto_tuning or not self.auto_tuner:
            return task_config

        try:
            logger.info("å¼€å§‹AutoTunerè‡ªåŠ¨è°ƒä¼˜...")

            # åˆ›å»ºè°ƒä¼˜é…ç½®
            tuning_config = OptimizationConfig(
                target_metric='execution_time',
                method='bayesian',  # å‚æ•°åæ˜¯'method'ä¸æ˜¯'optimization_method'
                max_iterations=10
                # OptimizationConfigä¸æ”¯æŒearly_stoppingå‚æ•°
            )

            # åˆ›å»ºè°ƒä¼˜ä»»åŠ¡
            tuning_task = TuningTask(
                pattern_name=f"import_task_{task_config.data_source}",
                priority=1,
                config=tuning_config
            )

            # å®šä¹‰å‚æ•°ç©ºé—´
            parameter_space = {
                'batch_size': {
                    'type': 'integer',
                    'min': 500,
                    'max': 5000,
                    'current': task_config.batch_size
                },
                'max_workers': {
                    'type': 'integer',
                    'min': 2,
                    'max': min(8, self.executor._max_workers),
                    'current': task_config.max_workers
                }
            }

            # æ‰§è¡Œè‡ªåŠ¨è°ƒä¼˜
            tuning_result = self._execute_auto_tuning(tuning_task, parameter_space, task_config)

            if tuning_result and tuning_result.get('success'):
                optimized_params = tuning_result.get('optimized_parameters', {})

                # åº”ç”¨ä¼˜åŒ–å‚æ•°
                if 'batch_size' in optimized_params:
                    task_config.batch_size = optimized_params['batch_size']
                if 'max_workers' in optimized_params:
                    task_config.max_workers = optimized_params['max_workers']

                logger.info(f" AutoTunerä¼˜åŒ–å®Œæˆ: batch_size={task_config.batch_size}, max_workers={task_config.max_workers}")
                logger.info(f" é¢„æœŸæ€§èƒ½æå‡: {tuning_result.get('improvement_percentage', 0):.1f}%")
            else:
                logger.warning("AutoTunerè°ƒä¼˜æœªæ‰¾åˆ°æ›´ä¼˜å‚æ•°ï¼Œä¿æŒåŸé…ç½®")

        except Exception as e:
            logger.error(f"AutoTunerè°ƒä¼˜å¤±è´¥: {e}")

        return task_config

    def _execute_auto_tuning(self, tuning_task: TuningTask, parameter_space: Dict[str, Any],
                             base_config: ImportTaskConfig) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œè‡ªåŠ¨è°ƒä¼˜"""
        try:
            # å®šä¹‰ç›®æ ‡å‡½æ•°
            def objective_function(params: Dict[str, Any]) -> float:
                """è°ƒä¼˜ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–æ‰§è¡Œæ—¶é—´"""
                try:
                    # åˆ›å»ºæµ‹è¯•é…ç½®
                    test_config = ImportTaskConfig(
                        task_id=f"tuning_test_{int(time.time())}",
                        name="è°ƒä¼˜æµ‹è¯•ä»»åŠ¡",
                        symbols=base_config.symbols[:min(10, len(base_config.symbols))],  # ä½¿ç”¨å°‘é‡è‚¡ç¥¨æµ‹è¯•
                        data_source=base_config.data_source,
                        asset_type=base_config.asset_type,
                        data_type=base_config.data_type,  # æ·»åŠ å¿…éœ€çš„data_typeå‚æ•°
                        frequency=base_config.frequency,
                        mode=base_config.mode,
                        batch_size=params.get('batch_size', base_config.batch_size),
                        max_workers=params.get('max_workers', base_config.max_workers)
                    )

                    # æ¨¡æ‹Ÿæ‰§è¡Œå¹¶æµ‹é‡æ€§èƒ½
                    start_time = time.time()

                    # è¿™é‡Œåº”è¯¥æ‰§è¡Œå®é™…çš„æ•°æ®å¯¼å…¥æµ‹è¯•
                    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æ€§èƒ½ä¼°ç®—
                    estimated_time = self._estimate_execution_time(test_config)

                    execution_time = time.time() - start_time + estimated_time

                    logger.debug(f"è°ƒä¼˜æµ‹è¯• - batch_size: {params['batch_size']}, "
                                 f"max_workers: {params['max_workers']}, "
                                 f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")

                    return execution_time

                except Exception as e:
                    logger.error(f"è°ƒä¼˜ç›®æ ‡å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
                    return float('inf')

            # ä½¿ç”¨AutoTuneræ‰§è¡Œä¼˜åŒ–
            best_params = None
            best_score = float('inf')

            # ç½‘æ ¼æœç´¢ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
            batch_sizes = [500, 1000, 2000, 3000, 5000]
            worker_counts = [2, 3, 4, 6, 8]

            for batch_size in batch_sizes:
                if batch_size < parameter_space['batch_size']['min'] or batch_size > parameter_space['batch_size']['max']:
                    continue

                for workers in worker_counts:
                    if workers < parameter_space['max_workers']['min'] or workers > parameter_space['max_workers']['max']:
                        continue

                    params = {'batch_size': batch_size, 'max_workers': workers}
                    score = objective_function(params)

                    if score < best_score:
                        best_score = score
                        best_params = params

            if best_params:
                # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
                current_params = {
                    'batch_size': parameter_space['batch_size']['current'],
                    'max_workers': parameter_space['max_workers']['current']
                }
                current_score = objective_function(current_params)

                improvement = max(0, (current_score - best_score) / current_score * 100)

                return {
                    'success': True,
                    'optimized_parameters': best_params,
                    'improvement_percentage': improvement,
                    'best_score': best_score,
                    'current_score': current_score
                }
            else:
                return {'success': False, 'reason': 'æœªæ‰¾åˆ°æ›´ä¼˜å‚æ•°'}

        except Exception as e:
            logger.error(f"æ‰§è¡Œè‡ªåŠ¨è°ƒä¼˜å¤±è´¥: {e}")
            return None

    def _estimate_execution_time(self, config: ImportTaskConfig) -> float:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´ï¼ˆç”¨äºè°ƒä¼˜ï¼‰"""
        try:
            # åŸºäºé…ç½®å‚æ•°çš„ç®€å•æ—¶é—´ä¼°ç®—æ¨¡å‹
            symbol_count = len(config.symbols)
            batch_size = config.batch_size
            max_workers = config.max_workers

            # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
            base_time = symbol_count * 0.1  # æ¯ä¸ªè‚¡ç¥¨0.1ç§’åŸºç¡€æ—¶é—´

            # æ‰¹æ¬¡å¤§å°å½±å“
            batch_factor = 1.0 + (1000 - batch_size) / 1000 * 0.3  # æ‰¹æ¬¡è¶Šå°ï¼Œå¼€é”€è¶Šå¤§

            # å¹¶å‘å½±å“
            worker_factor = 1.0 / min(max_workers, symbol_count)  # å·¥ä½œçº¿ç¨‹æ•°å½±å“

            estimated_time = base_time * batch_factor * worker_factor

            return max(0.1, estimated_time)  # æœ€å°0.1ç§’

        except Exception as e:
            logger.error(f"ä¼°ç®—æ‰§è¡Œæ—¶é—´å¤±è´¥: {e}")
            return 1.0  # é»˜è®¤1ç§’

    def get_auto_tuning_status(self) -> Dict[str, Any]:
        """è·å–è‡ªåŠ¨è°ƒä¼˜çŠ¶æ€"""
        status = {
            'auto_tuning_enabled': self.enable_auto_tuning,
            'auto_tuner_available': self.auto_tuner is not None
        }

        try:
            if self.auto_tuner:
                # è·å–è°ƒä¼˜å™¨çŠ¶æ€
                tuner_status = self.auto_tuner.get_status()
                status.update({
                    'active_tasks': tuner_status.get('active_tasks', 0),
                    'completed_tasks': tuner_status.get('completed_tasks', 0),
                    'failed_tasks': tuner_status.get('failed_tasks', 0),
                    'total_improvement': tuner_status.get('total_improvement', 0)
                })

        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨è°ƒä¼˜çŠ¶æ€å¤±è´¥: {e}")
            status['error'] = str(e)

        return status

    def _init_data_quality_monitor(self) -> Optional[DataQualityMonitor]:
        """åˆå§‹åŒ–æ•°æ®è´¨é‡ç›‘æ§å™¨"""
        try:
            data_quality_monitor = DataQualityMonitor()
            logger.info("æ•°æ®è´¨é‡ç›‘æ§å™¨åˆå§‹åŒ–æˆåŠŸ")
            return data_quality_monitor

        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡ç›‘æ§å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    def _validate_imported_data(self, task_id: str, data: pd.DataFrame,
                                data_source: str, data_type: str = 'kdata') -> ValidationResult:
        """éªŒè¯å¯¼å…¥çš„æ•°æ®è´¨é‡"""
        logger.info(f"[æ•°æ®è´¨é‡éªŒè¯] å¼€å§‹éªŒè¯ - ä»»åŠ¡: {task_id}, æ•°æ®æº: {data_source}, ç±»å‹: {data_type}, è®°å½•æ•°: {len(data) if not data.empty else 0}")

        if not self.enable_data_quality_monitoring or not self.data_quality_monitor:
            logger.debug(f"[æ•°æ®è´¨é‡éªŒè¯] è´¨é‡ç›‘æ§æœªå¯ç”¨ï¼Œè·³è¿‡éªŒè¯")
            return ValidationResult(
                is_valid=True,
                quality_score=0.8,
                quality_level=DataQuality.GOOD,
                errors=[],
                warnings=[],
                suggestions=[],
                metrics={},
                validation_time=datetime.now()
            )

        try:
            logger.info(f" å¼€å§‹æ•°æ®è´¨é‡éªŒè¯: {task_id}")

            # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿datetimeæ˜¯åˆ—è€Œä¸æ˜¯ç´¢å¼•
            # è§£å†³"'datetime' is both an index level and a column label"é”™è¯¯
            if data.index.name == 'datetime' or isinstance(data.index, pd.DatetimeIndex):
                logger.debug("[æ•°æ®è´¨é‡éªŒè¯] æ£€æµ‹åˆ°datetimeè¢«è®¾ç½®ä¸ºç´¢å¼•ï¼Œå°†å…¶è½¬æ¢å›åˆ—")
                data = data.reset_index(drop=False)
                if 'index' in data.columns:
                    data = data.drop('index', axis=1)
                if data.index.name is not None:
                    data = data.reset_index(drop=True)

            # ç¡®ä¿datetimeåˆ—å­˜åœ¨ä¸”æ˜¯datetimeç±»å‹
            if 'datetime' not in data.columns:
                logger.warning("[æ•°æ®è´¨é‡éªŒè¯] æ•°æ®ä¸­æ²¡æœ‰datetimeåˆ—ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¢å¤")
                if 'date' in data.columns:
                    data['datetime'] = pd.to_datetime(data['date'])
                else:
                    logger.error("[æ•°æ®è´¨é‡éªŒè¯] æ— æ³•æ‰¾åˆ°datetimeæˆ–dateåˆ—")
                    return ValidationResult(
                        is_valid=False,
                        quality_score=0.0,
                        quality_level=DataQuality.POOR,
                        errors=["ç¼ºå°‘datetimeå­—æ®µ"],
                        warnings=[],
                        suggestions=["æ£€æŸ¥æ•°æ®æºæ˜¯å¦æä¾›äº†æ—¶é—´å­—æ®µ"],
                        metrics={},
                        validation_time=datetime.now()
                    )
            else:
                data['datetime'] = pd.to_datetime(data['datetime'])

            # ğŸ¯ æ™ºèƒ½è¯†åˆ«æ•°æ®ç”¨é€”ï¼ˆä¸€æ¬¡æ€§è°ƒç”¨ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
            # è¿™ä¸ªå€¼å°†è¢«ç”¨äºè´¨é‡è¯„åˆ†è®¡ç®—å’Œåç»­çš„è®°å½•è´¨é‡æŒ‡æ ‡
            data_usage = self._infer_data_usage(data, task_id)

            # âœ… ä¼˜åŒ–2&3ï¼šæ£€æŸ¥ç¼“å­˜ï¼ˆç›¸åŒæ•°æ®æº+æ—¥æœŸï¼‰
            from datetime import datetime
            cache_key = f"{data_source}_{datetime.now().date().isoformat()}"

            if cache_key in self._quality_score_cache:
                cached_data = self._quality_score_cache[cache_key]
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if (datetime.now() - cached_data['timestamp']).seconds < self._quality_cache_ttl:
                    quality_score = cached_data['score']
                    logger.info(f"[è´¨é‡è¯„åˆ†ç¼“å­˜] ä½¿ç”¨ç¼“å­˜è¯„åˆ†: {quality_score:.3f} (æ•°æ®æº: {data_source})")
                    # å³ä½¿ä½¿ç”¨ç¼“å­˜ï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
                    if 'symbol' in data.columns:
                        new_symbols = set(data['symbol'].unique())
                        cached_symbols = cached_data.get('symbols', set())
                        if new_symbols - cached_symbols:  # æœ‰æ–°symbol
                            logger.info(f"[å¢é‡è¯„åˆ†] å‘ç°æ–°symbol: {len(new_symbols - cached_symbols)}ä¸ªï¼Œé‡æ–°è®¡ç®—")
                            # âœ… ä½¿ç”¨å·²è¯†åˆ«çš„data_usageï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
                            quality_score = self.data_quality_monitor.calculate_quality_score(
                                data, data_type, data_usage=data_usage, data_source=data_source
                            )
                            # æ›´æ–°ç¼“å­˜
                            self._quality_score_cache[cache_key] = {
                                'score': quality_score,
                                'timestamp': datetime.now(),
                                'symbols': new_symbols
                            }
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è®¡ç®—
                    quality_score = self.data_quality_monitor.calculate_quality_score(
                        data, data_type, data_usage=data_usage, data_source=data_source
                    )
                    logger.info(f"[è´¨é‡è¯„åˆ†è®¡ç®—] ç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è®¡ç®—: {quality_score:.3f}")
                    self._quality_score_cache[cache_key] = {
                        'score': quality_score,
                        'timestamp': datetime.now(),
                        'symbols': set(data['symbol'].unique()) if 'symbol' in data.columns else set()
                    }
            else:
                # æ— ç¼“å­˜ï¼Œé¦–æ¬¡è®¡ç®—
                quality_score = self.data_quality_monitor.calculate_quality_score(
                    data, data_type, data_usage=data_usage, data_source=data_source
                )
                logger.info(f"[è´¨é‡è¯„åˆ†è®¡ç®—] é¦–æ¬¡è®¡ç®—: {quality_score:.3f}")
                self._quality_score_cache[cache_key] = {
                    'score': quality_score,
                    'timestamp': datetime.now(),
                    'symbols': set(data['symbol'].unique()) if 'symbol' in data.columns else set()
                }

            # è®°å½•è´¨é‡æŒ‡æ ‡ï¼ˆå†™å…¥SQLiteï¼‰- æ”¯æŒæ™ºèƒ½æƒé‡
            table_name = f"{data_source}_{data_type}"
            logger.debug(f"[æ•°æ®è´¨é‡éªŒè¯] è®°å½•è´¨é‡æŒ‡æ ‡åˆ°SQLite - æ’ä»¶: {data_source}, è¡¨: {table_name}, ç”¨é€”: {data_usage}")
            self.data_quality_monitor.record_quality_metrics(
                plugin_name=data_source,
                table_name=table_name,
                data=data,
                data_type=data_type,
                data_usage=data_usage,  # ğŸ†• ä¼ é€’ç”¨é€”å‚æ•°
                data_source=data_source  # ğŸ†• ä¼ é€’æ•°æ®æºå‚æ•°
            )

            # âœ… å…³é”®ï¼šå°†è´¨é‡è¯„åˆ†å†™å…¥DuckDBçš„data_quality_monitorè¡¨
            #    è¿™æ ·unified_best_quality_klineè§†å›¾æ‰èƒ½ä½¿ç”¨å®é™…è¯„åˆ†
            try:
                from ..asset_database_manager import get_asset_separated_database_manager
                from ..plugin_types import AssetType
                from datetime import date

                # âœ… ä¼˜åŒ–1ï¼šæ‰¹é‡å†™å…¥è´¨é‡è¯„åˆ†ï¼ˆæå‡æ€§èƒ½ï¼‰
                if 'symbol' in data.columns:
                    asset_manager = get_asset_separated_database_manager()
                    symbols = data['symbol'].unique()
                    logger.info(f"[è´¨é‡è¯„åˆ†å†™å…¥] å¼€å§‹æ‰¹é‡å†™å…¥è´¨é‡è¯„åˆ†åˆ°DuckDB - æ€»symbolæ•°: {len(symbols)}")

                    # æŒ‰èµ„äº§ç±»å‹åˆ†ç»„æ‰¹é‡å†™å…¥
                    from collections import defaultdict
                    quality_records_by_asset = defaultdict(list)

                    # é¢„å…ˆè®¡ç®—æ‰€æœ‰symbolçš„è´¨é‡æŒ‡æ ‡
                    for symbol in symbols:
                        try:
                            # ç¡®å®šèµ„äº§ç±»å‹
                            asset_type = AssetType.STOCK_A if str(symbol).endswith(('.SZ', '.SH')) else AssetType.STOCK_A

                            symbol_data = data[data['symbol'] == symbol]
                            monitor_id = f"{symbol}_{data_source}_{date.today().isoformat()}"
                            missing_count = int(symbol_data.isnull().sum().sum())
                            total_cells = symbol_data.size
                            completeness_score = 1.0 - (missing_count / total_cells) if total_cells > 0 else 1.0

                            quality_records_by_asset[asset_type].append([
                                monitor_id,
                                symbol,
                                data_source,
                                date.today(),
                                quality_score,
                                0,  # anomaly_count
                                missing_count,
                                completeness_score,
                                f"Records: {len(symbol_data)}, Quality: {quality_score:.3f}"
                            ])
                        except Exception as e:
                            logger.warning(f"[è´¨é‡è¯„åˆ†å‡†å¤‡] å‡†å¤‡{symbol}è´¨é‡è®°å½•å¤±è´¥: {e}")

                    # æ‰¹é‡å†™å…¥ï¼ˆæŒ‰èµ„äº§ç±»å‹ï¼‰
                    total_written = 0
                    for asset_type, records in quality_records_by_asset.items():
                        try:
                            with asset_manager.get_connection(asset_type) as conn:
                                # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥
                                conn.executemany("""
                                    INSERT OR REPLACE INTO data_quality_monitor 
                                    (monitor_id, symbol, data_source, check_date, quality_score, 
                                     anomaly_count, missing_count, completeness_score, details)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """, records)
                                total_written += len(records)
                                logger.debug(f"[è´¨é‡è¯„åˆ†å†™å…¥] æ‰¹é‡å†™å…¥{asset_type.value}: {len(records)}æ¡è®°å½•")
                        except Exception as e:
                            logger.warning(f"[è´¨é‡è¯„åˆ†å†™å…¥] æ‰¹é‡å†™å…¥{asset_type.value}å¤±è´¥: {e}")

                    logger.info(f"[è´¨é‡è¯„åˆ†å†™å…¥] æ‰¹é‡å†™å…¥å®Œæˆ - æˆåŠŸ: {total_written}/{len(symbols)}æ¡")
            except Exception as e:
                logger.warning(f"[è´¨é‡è¯„åˆ†å†™å…¥] å†™å…¥è´¨é‡è¯„åˆ†åˆ°DuckDBå¤±è´¥: {e}")
                logger.debug(f"[è´¨é‡è¯„åˆ†å†™å…¥] å¼‚å¸¸å †æ ˆ: ", exc_info=True)

            # åˆ›å»ºè¯¦ç»†çš„éªŒè¯ç»“æœ
            validation_result = self._create_detailed_validation_result(
                data, quality_score, data_source, data_type
            )

            # è®°å½•è´¨é‡è¯„ä¼°ç»“æœ
            quality_level = validation_result.quality_level
            logger.info(f" æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆ: {quality_level.value}, è¯„åˆ†: {quality_score:.3f}")

            if quality_score < 0.7:
                logger.warning(f" æ•°æ®è´¨é‡è¾ƒå·® (è¯„åˆ†: {quality_score:.3f})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æº")

            return validation_result

        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
            error_msg = f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"
            logger.error(f"[æ•°æ®è´¨é‡éªŒè¯] å¼‚å¸¸è¯¦æƒ…: {error_msg}")
            return ValidationResult(
                is_valid=False,
                quality_score=0.0,
                quality_level=DataQuality.POOR,
                errors=[error_msg],
                warnings=[],
                suggestions=["æ£€æŸ¥æ•°æ®æºè¿æ¥", "éªŒè¯æ•°æ®æ ¼å¼"],
                metrics={},
                validation_time=datetime.now()
            )

    def _infer_data_usage(self, data: pd.DataFrame, task_id: str = None) -> str:
        """
        æ™ºèƒ½è¯†åˆ«æ•°æ®ç”¨é€”

        è¯†åˆ«é€»è¾‘ï¼š
        1. æ£€æŸ¥ä»»åŠ¡IDä¸­çš„å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        2. æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆdatetimeåˆ—ï¼‰
        3. æ£€æŸ¥æ•°æ®é‡å’Œæ—¶é—´è·¨åº¦

        Returns:
            'historical', 'realtime', 'backtest', 'live_trading', 'general'
        """
        try:
            logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] å¼€å§‹è¯†åˆ« - ä»»åŠ¡ID: {task_id}, æ•°æ®é‡: {len(data)}")

            # æ–¹æ³•1: æ£€æŸ¥ä»»åŠ¡IDä¸­çš„å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            if task_id:
                task_id_lower = task_id.lower()
                if 'backtest' in task_id_lower or 'å›æµ‹' in task_id:
                    logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•1-ä»»åŠ¡IDå…³é”®è¯è¯†åˆ« â†’ backtest (å…³é”®è¯: {task_id})")
                    return 'backtest'
                elif 'realtime' in task_id_lower or 'å®æ—¶' in task_id:
                    logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•1-ä»»åŠ¡IDå…³é”®è¯è¯†åˆ« â†’ realtime (å…³é”®è¯: {task_id})")
                    return 'realtime'
                elif 'live' in task_id_lower or 'trading' in task_id_lower or 'äº¤æ˜“' in task_id:
                    logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•1-ä»»åŠ¡IDå…³é”®è¯è¯†åˆ« â†’ live_trading (å…³é”®è¯: {task_id})")
                    return 'live_trading'
                elif 'historical' in task_id_lower or 'å†å²' in task_id:
                    logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•1-ä»»åŠ¡IDå…³é”®è¯è¯†åˆ« â†’ historical (å…³é”®è¯: {task_id})")
                    return 'historical'
                else:
                    logger.debug(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•1-ä»»åŠ¡IDæœªåŒ¹é…å…³é”®è¯ï¼Œç»§ç»­æ£€æŸ¥æ•°æ®ç‰¹å¾")
            else:
                logger.debug(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•1-ä»»åŠ¡IDä¸ºç©ºï¼Œè·³è¿‡å…³é”®è¯æ£€æŸ¥")

            # æ–¹æ³•2: æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆåŠæ—¶æ€§ï¼‰
            if 'datetime' in data.columns and not data.empty:
                try:
                    latest_time = pd.to_datetime(data['datetime']).max()
                    earliest_time = pd.to_datetime(data['datetime']).min()
                    current_time = pd.Timestamp.now()
                    delay_minutes = (current_time - latest_time).total_seconds() / 60
                    time_span_days = (latest_time - earliest_time).days

                    logger.debug(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•2-æ—¶é—´åˆ†æ â†’ æœ€æ–°æ—¶é—´: {latest_time}, "
                                 f"å»¶è¿Ÿ: {delay_minutes:.1f}åˆ†é’Ÿ, æ—¶é—´è·¨åº¦: {time_span_days}å¤©")

                    # 5åˆ†é’Ÿå†…çš„æ•°æ® â†’ å®ç›˜äº¤æ˜“ç”¨é€”
                    if delay_minutes <= 5:
                        logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•2-æ•°æ®æ–°é²œåº¦è¯†åˆ« â†’ live_trading "
                                    f"(å»¶è¿Ÿ: {delay_minutes:.1f}åˆ†é’Ÿ â‰¤ 5åˆ†é’Ÿ)")
                        return 'live_trading'
                    # 1å°æ—¶å†…çš„æ•°æ® â†’ å®æ—¶è¡Œæƒ…ç”¨é€”
                    elif delay_minutes <= 60:
                        logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•2-æ•°æ®æ–°é²œåº¦è¯†åˆ« â†’ realtime "
                                    f"(å»¶è¿Ÿ: {delay_minutes:.1f}åˆ†é’Ÿ â‰¤ 60åˆ†é’Ÿ)")
                        return 'realtime'
                    # 1å¤©ä»¥ä¸Šçš„æ•°æ® â†’ å†å²æ•°æ®æˆ–å›æµ‹ç”¨é€”
                    elif delay_minutes > 1440:  # 1å¤©
                        # è¿›ä¸€æ­¥åˆ¤æ–­æ˜¯å¦ç”¨äºå›æµ‹ï¼ˆæ—¶é—´è·¨åº¦è¶…è¿‡3ä¸ªæœˆï¼‰
                        if time_span_days > 90:  # è¶…è¿‡3ä¸ªæœˆæ•°æ®ï¼Œå¯èƒ½ç”¨äºå›æµ‹
                            logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•2-æ•°æ®æ–°é²œåº¦è¯†åˆ« â†’ backtest "
                                        f"(å»¶è¿Ÿ: {delay_minutes/1440:.1f}å¤©, æ—¶é—´è·¨åº¦: {time_span_days}å¤© > 90å¤©)")
                            return 'backtest'
                        else:
                            logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•2-æ•°æ®æ–°é²œåº¦è¯†åˆ« â†’ historical "
                                        f"(å»¶è¿Ÿ: {delay_minutes/1440:.1f}å¤©, æ—¶é—´è·¨åº¦: {time_span_days}å¤© â‰¤ 90å¤©)")
                            return 'historical'
                    else:
                        logger.debug(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•2-æ—¶é—´ç‰¹å¾æœªæ˜ç¡®åŒ¹é…ï¼Œç»§ç»­æ£€æŸ¥æ•°æ®é‡")

                except Exception as e:
                    logger.warning(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•2-æ—¶é—´æ£€æŸ¥å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨æ–¹æ³•3")
            else:
                logger.debug(f"[æ•°æ®ç”¨é€”æ¨æ–­] æ–¹æ³•2-æ•°æ®ä¸­æ— datetimeåˆ—æˆ–æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ—¶é—´åˆ†æ")

            # æ–¹æ³•3: æ£€æŸ¥æ•°æ®é‡å’Œæ—¶é—´è·¨åº¦
            data_count = len(data)
            if data_count > 500:  # å¤§é‡å†å²æ•°æ®
                logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•3-æ•°æ®é‡è¯†åˆ« â†’ backtest (æ•°æ®é‡: {data_count} > 500)")
                return 'backtest'
            elif data_count < 50:  # å°‘é‡æ•°æ®
                logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… æ–¹æ³•3-æ•°æ®é‡è¯†åˆ« â†’ realtime (æ•°æ®é‡: {data_count} < 50)")
                return 'realtime'

            # é»˜è®¤ï¼šé€šç”¨åœºæ™¯
            logger.info(f"[æ•°æ®ç”¨é€”æ¨æ–­] âœ… é»˜è®¤åœºæ™¯ â†’ general (æ•°æ®é‡: {data_count}, æ— æ˜ç¡®ç‰¹å¾)")
            return 'general'

        except Exception as e:
            logger.error(f"[æ•°æ®ç”¨é€”æ¨æ–­] âŒ æ¨æ–­å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼ general", exc_info=True)
            return 'general'

    def _create_detailed_validation_result(self, data: pd.DataFrame, quality_score: float,
                                           data_source: str, data_type: str) -> ValidationResult:
        """åˆ›å»ºè¯¦ç»†çš„éªŒè¯ç»“æœ"""
        try:
            issues = []

            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if data.empty:
                issues.append("æ•°æ®ä¸ºç©º")
                logger.warning(f"[æ•°æ®éªŒè¯] æ•°æ®ä¸ºç©ºï¼Œæ•°æ®æº: {data_source}, ç±»å‹: {data_type}")
                return ValidationResult(
                    is_valid=False,
                    quality_score=0.0,
                    quality_level=DataQuality.POOR,
                    errors=issues,
                    warnings=[],
                    suggestions=["æ£€æŸ¥æ•°æ®æºæ˜¯å¦æ­£å¸¸", "éªŒè¯æŸ¥è¯¢æ¡ä»¶"],
                    metrics={"total_records": 0},
                    validation_time=datetime.now()
                )

            # æ£€æŸ¥ç©ºå€¼
            null_percentage = data.isnull().sum().sum() / data.size
            if null_percentage > 0.1:
                issues.append(f"ç©ºå€¼æ¯”ä¾‹è¿‡é«˜: {null_percentage:.1%}")

            # æ£€æŸ¥é‡å¤æ•°æ®
            duplicate_percentage = data.duplicated().sum() / len(data)
            if duplicate_percentage > 0.05:
                issues.append(f"é‡å¤æ•°æ®æ¯”ä¾‹è¿‡é«˜: {duplicate_percentage:.1%}")

            # æ£€æŸ¥æ•°æ®èŒƒå›´ï¼ˆé’ˆå¯¹Kçº¿æ•°æ®ï¼‰
            if data_type == 'kdata':
                numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                available_columns = [col for col in numeric_columns if col in data.columns]

                for col in available_columns:
                    if col in ['open', 'high', 'low', 'close']:
                        # ä»·æ ¼æ•°æ®åº”è¯¥å¤§äº0
                        if (data[col] <= 0).any():
                            issues.append(f"{col}åˆ—å­˜åœ¨éæ­£æ•°ä»·æ ¼")
                    elif col == 'volume':
                        # æˆäº¤é‡åº”è¯¥å¤§äºç­‰äº0
                        if (data[col] < 0).any():
                            issues.append(f"{col}åˆ—å­˜åœ¨è´Ÿæ•°æˆäº¤é‡")

                # æ£€æŸ¥ä»·æ ¼é€»è¾‘å…³ç³»
                if all(col in data.columns for col in ['high', 'low', 'open', 'close']):
                    # æœ€é«˜ä»·åº”è¯¥ >= æœ€ä½ä»·
                    if (data['high'] < data['low']).any():
                        issues.append("å­˜åœ¨æœ€é«˜ä»·å°äºæœ€ä½ä»·çš„å¼‚å¸¸æ•°æ®")

                    # å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·åº”è¯¥åœ¨æœ€é«˜ä»·å’Œæœ€ä½ä»·ä¹‹é—´
                    if ((data['open'] > data['high']) | (data['open'] < data['low'])).any():
                        issues.append("å­˜åœ¨å¼€ç›˜ä»·è¶…å‡ºæœ€é«˜æœ€ä½ä»·èŒƒå›´çš„å¼‚å¸¸æ•°æ®")

                    if ((data['close'] > data['high']) | (data['close'] < data['low'])).any():
                        issues.append("å­˜åœ¨æ”¶ç›˜ä»·è¶…å‡ºæœ€é«˜æœ€ä½ä»·èŒƒå›´çš„å¼‚å¸¸æ•°æ®")

            # ç¡®å®šè´¨é‡ç­‰çº§
            if quality_score >= 0.95:
                quality_level = DataQuality.EXCELLENT
            elif quality_score >= 0.85:
                quality_level = DataQuality.GOOD
            elif quality_score >= 0.70:
                quality_level = DataQuality.FAIR
            else:
                quality_level = DataQuality.POOR

            is_valid = quality_score >= 0.70 and len(issues) == 0

            # ç”Ÿæˆå»ºè®®
            suggestions = []
            if quality_score < 0.7:
                suggestions.append("æ•°æ®è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æº")
            if null_percentage > 0.1:
                suggestions.append("ç©ºå€¼æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®æ•°æ®æ¸…æ´—")
            if duplicate_percentage > 0.05:
                suggestions.append("å­˜åœ¨è¾ƒå¤šé‡å¤æ•°æ®ï¼Œå»ºè®®å»é‡")

            # è®°å½•éªŒè¯è¯¦æƒ…
            logger.info(f"[æ•°æ®éªŒè¯] æ•°æ®æº: {data_source}, ç±»å‹: {data_type}, è´¨é‡è¯„åˆ†: {quality_score:.3f}, "
                        f"è´¨é‡ç­‰çº§: {quality_level.value}, è®°å½•æ•°: {len(data)}, "
                        f"ç©ºå€¼ç‡: {null_percentage:.2%}, é‡å¤ç‡: {duplicate_percentage:.2%}")

            return ValidationResult(
                is_valid=is_valid,
                quality_score=quality_score,
                quality_level=quality_level,
                errors=issues,
                warnings=[f"ç©ºå€¼æ¯”ä¾‹: {null_percentage:.1%}", f"é‡å¤æ•°æ®æ¯”ä¾‹: {duplicate_percentage:.1%}"] if (null_percentage > 0 or duplicate_percentage > 0) else [],
                suggestions=suggestions,
                metrics={
                    "total_records": len(data),
                    "null_records": int(data.isnull().sum().sum()),
                    "duplicate_records": int(data.duplicated().sum()),
                    "completeness_score": 1.0 - null_percentage,
                    "accuracy_score": quality_score,
                    "data_source": data_source,
                    "data_type": data_type
                },
                validation_time=datetime.now()
            )

        except Exception as e:
            error_msg = f"éªŒè¯ç»“æœåˆ›å»ºå¤±è´¥: {str(e)}"
            logger.error(f"[æ•°æ®éªŒè¯] {error_msg}, æ•°æ®æº: {data_source}, ç±»å‹: {data_type}")
            return ValidationResult(
                is_valid=False,
                quality_score=0.0,
                quality_level=DataQuality.POOR,
                errors=[error_msg],
                warnings=[],
                suggestions=["æ£€æŸ¥æ•°æ®æ ¼å¼", "éªŒè¯æ•°æ®å®Œæ•´æ€§"],
                metrics={},
                validation_time=datetime.now()
            )

    def _handle_quality_issues(self, validation_result: ValidationResult, task_id: str):
        """å¤„ç†æ•°æ®è´¨é‡é—®é¢˜"""
        if not validation_result.is_valid or validation_result.quality_level == DataQuality.POOR:
            logger.warning(f"[è´¨é‡é—®é¢˜å¤„ç†] ä»»åŠ¡ {task_id} æ•°æ®è´¨é‡é—®é¢˜:")
            for error in validation_result.errors:
                logger.warning(f"  - é”™è¯¯: {error}")
            for warning in validation_result.warnings:
                logger.warning(f"  - è­¦å‘Š: {warning}")

            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªåŠ¨ä¿®å¤é€»è¾‘
            metrics = validation_result.metrics
            if metrics.get('duplicate_records', 0) > 0:
                logger.info(f"  å»ºè®®: æ¸…ç† {metrics['duplicate_records']} æ¡é‡å¤æ•°æ®")

            if metrics.get('null_records', 0) > 0:
                logger.info(f"  å»ºè®®: å¤„ç† {metrics['null_records']} ä¸ªç©ºå€¼")

            # è¾“å‡ºå»ºè®®
            for suggestion in validation_result.suggestions:
                logger.info(f"  å»ºè®®: {suggestion}")

    def get_data_quality_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®è´¨é‡ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'data_quality_monitoring_enabled': self.enable_data_quality_monitoring,
            'data_quality_monitor_available': self.data_quality_monitor is not None
        }

        try:
            if self.data_quality_monitor:
                # è¿™é‡Œå¯ä»¥æ·»åŠ ä»æ•°æ®åº“è·å–å†å²è´¨é‡ç»Ÿè®¡çš„é€»è¾‘
                stats.update({
                    'monitoring_active': True,
                    'quality_checks_performed': 0,  # å¯ä»¥ä»æ•°æ®åº“ç»Ÿè®¡
                    'average_quality_score': 0.0,   # å¯ä»¥ä»æ•°æ®åº“è®¡ç®—
                    'last_check_time': datetime.now().isoformat()
                })

        except Exception as e:
            logger.error(f"è·å–æ•°æ®è´¨é‡ç»Ÿè®¡å¤±è´¥: {e}")
            stats['error'] = str(e)

        return stats

    def _start_performance_monitoring(self, task_id: str):
        """å¯åŠ¨ä»»åŠ¡æ€§èƒ½ç›‘æ§"""
        if not self.enable_performance_monitoring:
            return

        try:
            # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶çš„ç³»ç»ŸçŠ¶æ€
            self.deep_analysis_service.record_metric(
                f"task_start_{task_id}",
                time.time(),
                "import_task"
            )

            # å¯åŠ¨æ€§èƒ½é›†æˆå™¨ç›‘æ§
            self.performance_integrator.start_monitoring()

            logger.info(f"ä»»åŠ¡ {task_id} æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")

        except Exception as e:
            logger.warning(f"å¯åŠ¨æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")

    def _stop_performance_monitoring(self, task_id: str, execution_time: float):
        """åœæ­¢ä»»åŠ¡æ€§èƒ½ç›‘æ§"""
        if not self.enable_performance_monitoring:
            return

        try:
            # è®°å½•ä»»åŠ¡æ‰§è¡Œæ—¶é—´
            self.deep_analysis_service.record_operation_timing(
                f"import_task_{task_id}",
                execution_time
            )

            # è®°å½•ä»»åŠ¡å®Œæˆæ—¶çš„ç³»ç»ŸçŠ¶æ€
            self.deep_analysis_service.record_metric(
                f"task_end_{task_id}",
                time.time(),
                "import_task"
            )

            # åˆ†ææ€§èƒ½ç“¶é¢ˆ
            bottlenecks = self.deep_analysis_service.analyze_bottlenecks()
            if bottlenecks:
                logger.info(f"ä»»åŠ¡ {task_id} æ€§èƒ½ç“¶é¢ˆåˆ†æ: {len(bottlenecks)} ä¸ªç“¶é¢ˆç‚¹")
                for bottleneck in bottlenecks[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç“¶é¢ˆ
                    logger.info(f"  - {bottleneck.component}: {bottleneck.avg_duration:.2f}ms ({bottleneck.severity})")

            logger.info(f"ä»»åŠ¡ {task_id} æ€§èƒ½ç›‘æ§å·²åœæ­¢")

        except Exception as e:
            logger.warning(f"åœæ­¢æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")

    def _detect_anomalies(self, task_id: str) -> List[AnomalyInfo]:
        """æ£€æµ‹ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸"""
        if not self.enable_anomaly_detection:
            return []

        try:
            anomalies = self.deep_analysis_service.detect_anomalies()

            if anomalies:
                logger.warning(f"ä»»åŠ¡ {task_id} æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")
                for anomaly in anomalies:
                    logger.warning(f"  - {anomaly.metric_name}: {anomaly.description} (ä¸¥é‡ç¨‹åº¦: {anomaly.severity})")

            return anomalies

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return []

    def _monitor_task_progress(self, task_id: str, progress: float, message: str):
        """ç›‘æ§ä»»åŠ¡è¿›åº¦å¹¶æ£€æµ‹å¼‚å¸¸"""
        try:
            # è®°å½•è¿›åº¦æŒ‡æ ‡
            if self.enable_performance_monitoring:
                self.deep_analysis_service.record_metric(
                    f"task_progress_{task_id}",
                    progress,
                    "import_progress"
                )

            # æ£€æµ‹è¿›åº¦å¼‚å¸¸
            if self.enable_anomaly_detection:
                # å¦‚æœè¿›åº¦é•¿æ—¶é—´æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜
                current_time = time.time()
                if hasattr(self, '_last_progress_time'):
                    time_diff = current_time - self._last_progress_time
                    if time_diff > 300 and progress == getattr(self, '_last_progress', 0):  # 5åˆ†é’Ÿæ²¡æœ‰è¿›åº¦å˜åŒ–
                        logger.warning(f"ä»»åŠ¡ {task_id} å¯èƒ½å­˜åœ¨è¿›åº¦åœæ»é—®é¢˜")

                self._last_progress_time = current_time
                self._last_progress = progress

            # å‘é€è¿›åº¦ä¿¡å·
            self.task_progress.emit(task_id, progress, message)

        except Exception as e:
            logger.error(f"ç›‘æ§ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

    def get_performance_report(self, task_id: str = None) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'monitoring_enabled': self.enable_performance_monitoring,
                'anomaly_detection_enabled': self.enable_anomaly_detection
            }

            if self.enable_performance_monitoring:
                # è·å–æ€§èƒ½ç»Ÿè®¡
                bottlenecks = self.deep_analysis_service.analyze_bottlenecks()
                report['bottlenecks'] = [
                    {
                        'component': b.component,
                        'avg_duration': b.avg_duration,
                        'call_count': b.call_count,
                        'severity': b.severity
                    } for b in bottlenecks[:5]
                ]

                # è·å–ç³»ç»ŸæŒ‡æ ‡
                system_metrics = self.deep_analysis_service.get_system_metrics()
                report['system_metrics'] = system_metrics

            if self.enable_anomaly_detection:
                # è·å–å¼‚å¸¸ä¿¡æ¯
                anomalies = self.deep_analysis_service.detect_anomalies()
                report['anomalies'] = [
                    {
                        'metric_name': a.metric_name,
                        'value': a.value,
                        'threshold': a.threshold,
                        'severity': a.severity,
                        'description': a.description,
                        'timestamp': a.timestamp.isoformat()
                    } for a in anomalies
                ]

            return report

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e)}

    def _ensure_data_manager(self):
        """ç¡®ä¿æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–"""
        if not self._data_manager_initialized:
            try:
                logger.info("å»¶è¿Ÿåˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
                self.data_manager = get_unified_data_manager()
                self._data_manager_initialized = True
                logger.info("æ•°æ®ç®¡ç†å™¨å»¶è¿Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f" æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æ•°æ®ç®¡ç†å™¨æ›¿ä»£
                self.data_manager = None
                self._data_manager_initialized = False

    def _ensure_real_data_provider(self):
        """ç¡®ä¿çœŸå®æ•°æ®æä¾›å™¨å·²åˆå§‹åŒ–"""
        if not self._real_data_provider_initialized:
            try:
                logger.info("å»¶è¿Ÿåˆå§‹åŒ–çœŸå®æ•°æ®æä¾›å™¨...")
                self.real_data_provider = RealDataProvider()
                self._real_data_provider_initialized = True
                logger.info("çœŸå®æ•°æ®æä¾›å™¨å»¶è¿Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f" çœŸå®æ•°æ®æä¾›å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æ›¿ä»£
                self.real_data_provider = None
                self._real_data_provider_initialized = False

    def _ensure_asset_database_manager(self):
        """ç¡®ä¿èµ„äº§æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–"""
        if not hasattr(self, 'asset_manager') or self.asset_manager is None:
            try:
                logger.info("åˆå§‹åŒ–èµ„äº§æ•°æ®åº“ç®¡ç†å™¨...")
                from ..asset_database_manager import AssetSeparatedDatabaseManager
                self.asset_manager = AssetSeparatedDatabaseManager()
                logger.info("èµ„äº§æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"èµ„äº§æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.asset_manager = None

    def _get_data_source_plugin(self, plugin_id: str):
        """è·å–æŒ‡å®šçš„æ•°æ®æºæ’ä»¶å®ä¾‹"""
        try:
            # ä»æ’ä»¶ç®¡ç†å™¨è·å–æ’ä»¶å®ä¾‹
            from ..plugin_manager import get_plugin_manager
            plugin_manager = get_plugin_manager()

            if plugin_manager:
                # è·å–æ•°æ®æºæ’ä»¶
                plugin_instance = plugin_manager.get_data_source_plugin(plugin_id)
                if plugin_instance:
                    logger.info(f"è·å–æ•°æ®æºæ’ä»¶æˆåŠŸ: {plugin_id}")
                    return plugin_instance
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æºæ’ä»¶: {plugin_id}")

            # å¦‚æœæ’ä»¶ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
            if plugin_id.startswith('examples.'):
                module_name = plugin_id.replace('examples.', 'plugins.examples.')
                try:
                    import importlib
                    module = importlib.import_module(module_name)

                    # æŸ¥æ‰¾æ’ä»¶ç±»
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if (hasattr(attr, '__bases__') and
                                any('IDataSourcePlugin' in str(base) for base in attr.__bases__)):
                            plugin_instance = attr()
                            logger.info(f"ç›´æ¥å¯¼å…¥æ•°æ®æºæ’ä»¶æˆåŠŸ: {plugin_id}")
                            return plugin_instance

                except ImportError as e:
                    logger.error(f"ç›´æ¥å¯¼å…¥æ•°æ®æºæ’ä»¶å¤±è´¥ {plugin_id}: {e}")

            return None

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºæ’ä»¶å¤±è´¥ {plugin_id}: {e}")
            return None

    def start_task(self, task_id: str) -> bool:
        """
        å¯åŠ¨ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        try:
            logger.info(f" å¼€å§‹å¯åŠ¨ä»»åŠ¡: {task_id}")

            # è·å–ä»»åŠ¡é…ç½®
            task_config = self.config_manager.get_import_task(task_id)
            if not task_config:
                logger.error(f" ä»»åŠ¡é…ç½®ä¸å­˜åœ¨: {task_id}")
                return False

            logger.info(f" æ‰¾åˆ°ä»»åŠ¡é…ç½®: {task_config.name}, è‚¡ç¥¨æ•°é‡: {len(task_config.symbols)}")

            # æ™ºèƒ½é…ç½®ä¼˜åŒ–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if self.enable_intelligent_config:
                logger.info("å¼€å§‹æ™ºèƒ½é…ç½®ä¼˜åŒ–...")
                intelligent_config = self._apply_intelligent_optimization(task_config, ConfigOptimizationLevel.BALANCED)
                if intelligent_config:
                    task_config = intelligent_config
                    logger.info(f" æ™ºèƒ½é…ç½®ä¼˜åŒ–å®Œæˆ: batch_size={task_config.batch_size}, max_workers={task_config.max_workers}")

            # æ£€æŸ¥ç¼“å­˜çš„é…ç½®ä¼˜åŒ–
            cached_config = self._get_cached_configuration(task_config)
            if cached_config and self.enable_intelligent_caching:
                logger.info("ä½¿ç”¨ç¼“å­˜çš„é…ç½®ä¼˜åŒ–")
                task_config.batch_size = cached_config.get('optimal_batch_size', task_config.batch_size)
                task_config.max_workers = cached_config.get('optimal_workers', task_config.max_workers)

            # AutoTunerè‡ªåŠ¨è°ƒä¼˜
            if self.enable_auto_tuning:
                task_config = self._auto_tune_task_parameters(task_config)

            # AIä¼˜åŒ–ä»»åŠ¡å‚æ•°
            if self.enable_ai_optimization:
                logger.info("å¼€å§‹AIä¼˜åŒ–ä»»åŠ¡å‚æ•°...")
                task_config = self._optimize_task_parameters(task_config)

                # ç¼“å­˜ä¼˜åŒ–åçš„é…ç½®
                self._cache_configuration_data(task_config)

                # AIé¢„æµ‹æ‰§è¡Œæ—¶é—´
                predicted_time = self._predict_execution_time(task_config)
                if predicted_time:
                    logger.info(f" AIé¢„æµ‹æ‰§è¡Œæ—¶é—´: {predicted_time:.2f}ç§’")

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†å¸ƒå¼æ‰§è¡Œ
            if self.enable_distributed_execution and self._can_distribute_task(task_config):
                logger.info("ä»»åŠ¡ç¬¦åˆåˆ†å¸ƒå¼æ‰§è¡Œæ¡ä»¶ï¼Œå°è¯•åˆ†å¸ƒå¼æ‰§è¡Œ...")
                if self._distribute_task(task_config):
                    logger.info(f"ä»»åŠ¡ {task_id} å·²åˆ†å¸ƒå¼æ‰§è¡Œ")
                    return True
                else:
                    logger.info("åˆ†å¸ƒå¼æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°æ‰§è¡Œ")

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²åœ¨è¿è¡Œ
            with self._task_lock:
                if task_id in self._running_tasks:
                    logger.warning(f"ä»»åŠ¡å·²åœ¨è¿è¡Œ: {task_id}")
                    return False

            # ä»»åŠ¡å¯åŠ¨å‰é¢„æ£€ï¼šç¡®ä¿é€šè¾¾ä¿¡è¿æ¥æ± å·²å¡«å……å¯ç”¨IP
            try:
                from core.services.unified_data_manager import get_unified_data_manager
                unified_manager = get_unified_data_manager()
                plugin_center = getattr(unified_manager, 'plugin_center', None)
                if plugin_center:
                    tdx_plugin = plugin_center.get_plugin('data_sources.stock.tongdaxin_plugin')
                    if tdx_plugin and getattr(tdx_plugin, 'use_connection_pool', False):
                        pool = getattr(tdx_plugin, 'connection_pool', None)
                        needs_prewarm = True
                        if pool:
                            try:
                                info = pool.get_connection_pool_info()
                                needs_prewarm = int(info.get('total_connections', 0)) == 0
                            except Exception:
                                needs_prewarm = True
                        if needs_prewarm:
                            logger.info("é¢„æ£€ï¼šé€šè¾¾ä¿¡è¿æ¥æ± ä¸ºç©ºï¼Œå¼€å§‹æœåŠ¡å™¨å‘ç°ä¸å¥åº·æ£€æµ‹ä»¥å¡«å……è¿æ¥æ± ...")
                            ok = False
                            if hasattr(tdx_plugin, 'ensure_pool_populated'):
                                ok = bool(tdx_plugin.ensure_pool_populated())
                            if ok:
                                logger.info("é¢„æ£€ï¼šé€šè¾¾ä¿¡è¿æ¥æ± å·²å‡†å¤‡å°±ç»ª")
                            else:
                                logger.warning("é¢„æ£€ï¼šé€šè¾¾ä¿¡è¿æ¥æ± æœªèƒ½å°±ç»ªï¼Œå°†å›é€€åˆ°å•è¿æ¥æ¨¡å¼ç»§ç»­ä»»åŠ¡")
            except Exception as precheck_err:
                logger.warning(f"é¢„æ£€ï¼šé€šè¾¾ä¿¡è¿æ¥æ± å‡†å¤‡å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{precheck_err}")

            # åˆ›å»ºä»»åŠ¡æ‰§è¡Œç»“æœ
            result = TaskExecutionResult(
                task_id=task_id,
                status=TaskExecutionStatus.PENDING,
                start_time=datetime.now()
            )

            with self._task_lock:
                self._task_results[task_id] = result

            # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            future = self.executor.submit(self._execute_task, task_config, result)

            with self._task_lock:
                self._running_tasks[task_id] = future

            # å¯åŠ¨å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§
            if self.enable_enhanced_performance_bridge:
                self.start_enhanced_performance_monitoring()
                logger.info("å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å·²å¯åŠ¨")

            # å¯åŠ¨å¢å¼ºç‰ˆé£é™©ç›‘æ§
            if self.enable_enhanced_risk_monitoring:
                self.start_enhanced_risk_monitoring()
                logger.info("å¢å¼ºç‰ˆé£é™©ç›‘æ§å·²å¯åŠ¨")

            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            self._start_performance_monitoring(task_id)

            # å‘é€ä»»åŠ¡å¼€å§‹ä¿¡å·
            self.task_started.emit(task_id)

            logger.info(f"ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {task_id}")
            return True

        except Exception as e:
            logger.error(f"å¯åŠ¨ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
            self.task_failed.emit(task_id, str(e))
            return False

    def stop_task(self, task_id: str) -> bool:
        """
        åœæ­¢ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåœæ­¢
        """
        try:
            with self._task_lock:
                if task_id not in self._running_tasks:
                    logger.warning(f"ä»»åŠ¡æœªåœ¨è¿è¡Œ: {task_id}")
                    # âœ… ä¿®å¤ï¼šå³ä½¿ä»»åŠ¡ä¸åœ¨è¿è¡Œä¸­ï¼Œä¹Ÿæ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¯èƒ½ä»»åŠ¡å·²å®Œæˆæˆ–å·²å–æ¶ˆ
                    if task_id in self._task_results:
                        result = self._task_results[task_id]
                        if result.status == TaskExecutionStatus.CANCELLED:
                            logger.info(f"ä»»åŠ¡å·²å¤„äºå–æ¶ˆçŠ¶æ€: {task_id}")
                            return True
                        elif result.status == TaskExecutionStatus.COMPLETED:
                            logger.info(f"ä»»åŠ¡å·²å®Œæˆ: {task_id}")
                            return True
                    return False

                # âœ… ä¿®å¤ï¼šå…ˆæ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºCANCELLEDï¼Œè®©æ‰§è¡Œä¸­çš„ä»»åŠ¡èƒ½å¤Ÿæ£€æŸ¥å¹¶é€€å‡º
                if task_id in self._task_results:
                    self._task_results[task_id].status = TaskExecutionStatus.CANCELLED
                    logger.info(f"ä»»åŠ¡çŠ¶æ€å·²æ ‡è®°ä¸ºå–æ¶ˆ: {task_id}")

                # å°è¯•å–æ¶ˆFutureï¼ˆå¦‚æœä»»åŠ¡è¿˜æœªå¼€å§‹æ‰§è¡Œï¼Œcancel()ä¼šè¿”å›Trueï¼‰
                future = self._running_tasks[task_id]
                cancelled = future.cancel()

                if cancelled:
                    # FutureæˆåŠŸå–æ¶ˆï¼ˆä»»åŠ¡è¿˜æœªå¼€å§‹æ‰§è¡Œï¼‰
                    logger.info(f"ä»»åŠ¡Futureå·²å–æ¶ˆï¼ˆä»»åŠ¡æœªå¼€å§‹æ‰§è¡Œï¼‰: {task_id}")
                else:
                    # Futureæ— æ³•å–æ¶ˆï¼ˆä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œï¼‰ï¼Œä½†æˆ‘ä»¬å·²ç»è®¾ç½®äº†çŠ¶æ€ä¸ºCANCELLED
                    # æ‰§è¡Œä¸­çš„ä»»åŠ¡ä¼šæ£€æŸ¥result.statuså¹¶é€€å‡º
                    logger.info(f"ä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œï¼Œæ— æ³•å–æ¶ˆFutureï¼Œä½†å·²è®¾ç½®å–æ¶ˆçŠ¶æ€: {task_id}")
                    logger.info(f"æ‰§è¡Œä¸­çš„ä»»åŠ¡å°†åœ¨ä¸‹æ¬¡æ£€æŸ¥æ—¶æ£€æµ‹åˆ°å–æ¶ˆçŠ¶æ€å¹¶é€€å‡º")

                # æ›´æ–°ä»»åŠ¡ç»“æœ
                if task_id in self._task_results:
                    self._task_results[task_id].end_time = datetime.now()
                    if self._task_results[task_id].start_time:
                        self._task_results[task_id].execution_time = (
                            self._task_results[task_id].end_time - self._task_results[task_id].start_time
                        ).total_seconds()

                # ç§»é™¤è¿è¡Œä¸­çš„ä»»åŠ¡ï¼ˆæ— è®ºcancel()æ˜¯å¦æˆåŠŸï¼‰
                del self._running_tasks[task_id]

                # åœæ­¢å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§
                if self.enable_enhanced_performance_bridge:
                    self.stop_enhanced_performance_monitoring()
                    logger.info("å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å·²åœæ­¢")

                # åœæ­¢å¢å¼ºç‰ˆé£é™©ç›‘æ§
                if self.enable_enhanced_risk_monitoring:
                    self.stop_enhanced_risk_monitoring()
                    logger.info("å¢å¼ºç‰ˆé£é™©ç›‘æ§å·²åœæ­¢")

                # âœ… ä¿®å¤ï¼šå‘é€ä»»åŠ¡å–æ¶ˆä¿¡å·
                self.task_cancelled.emit(task_id)

                logger.info(f"ä»»åŠ¡åœæ­¢æˆåŠŸ: {task_id}")
                return True

        except Exception as e:
            logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥ {task_id}: {e}", exc_info=True)
            return False

    def get_task_status(self, task_id: str) -> Optional[TaskExecutionResult]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            TaskExecutionResult: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        with self._task_lock:
            return self._task_results.get(task_id)

    def get_running_tasks(self) -> List[str]:
        """è·å–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨"""
        with self._task_lock:
            return list(self._running_tasks.keys())

    def stop_all_tasks(self) -> bool:
        """
        åœæ­¢æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡

        Returns:
            bool: æ˜¯å¦æˆåŠŸåœæ­¢æ‰€æœ‰ä»»åŠ¡
        """
        try:
            with self._task_lock:
                running_task_ids = list(self._running_tasks.keys())

                if not running_task_ids:
                    logger.info("æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡éœ€è¦åœæ­¢")
                    return True

                logger.info(f"åœæ­¢ {len(running_task_ids)} ä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")

                success_count = 0
                for task_id in running_task_ids:
                    if self.stop_task(task_id):
                        success_count += 1

                logger.info(f"æˆåŠŸåœæ­¢ {success_count}/{len(running_task_ids)} ä¸ªä»»åŠ¡")
                return success_count == len(running_task_ids)

        except Exception as e:
            logger.error(f"åœæ­¢æ‰€æœ‰ä»»åŠ¡å¤±è´¥: {e}")
            return False

    def _execute_task(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """
        æ‰§è¡Œä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘

        Args:
            task_config: ä»»åŠ¡é…ç½®
            result: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f" å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_config.task_id}")
            logger.info(f" ä»»åŠ¡è¯¦æƒ…: æ•°æ®ç±»å‹={getattr(task_config, 'data_type', 'Kçº¿æ•°æ®')}, è‚¡ç¥¨æ•°é‡={len(task_config.symbols)}")

            # âœ… ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„è¿›åº¦å¹¶æ¢å¤
            saved_progress = self.config_manager.get_progress(task_config.task_id)
            if saved_progress and saved_progress.status == ImportStatus.RUNNING:
                logger.info(f"ğŸ“‹ [è¿›åº¦æ¢å¤] å‘ç°å·²ä¿å­˜çš„è¿›åº¦: task_id={task_config.task_id}")
                logger.info(f"   å·²å¤„ç†è®°å½•: {saved_progress.imported_records}/{saved_progress.total_records}")

                # æ¢å¤å·²å¤„ç†çš„è®°å½•æ•°
                result.processed_records = saved_progress.imported_records
                result.failed_records = saved_progress.error_count
                result.total_records = saved_progress.total_records or len(task_config.symbols)

                # âœ… ä¿®å¤ï¼šä½¿ç”¨processed_symbols_listè¿‡æ»¤å·²å¤„ç†çš„è‚¡ç¥¨
                if hasattr(saved_progress, 'processed_symbols_list') and saved_progress.processed_symbols_list:
                    processed_symbols = set(saved_progress.processed_symbols_list)
                    original_symbols = task_config.symbols.copy()
                    remaining_symbols = [s for s in original_symbols if s not in processed_symbols]

                    if remaining_symbols:
                        logger.info(f"ğŸ“‹ [è¿›åº¦æ¢å¤] å·²å¤„ç†{len(processed_symbols)}ä¸ªè‚¡ç¥¨ï¼Œå‰©ä½™{len(remaining_symbols)}ä¸ªè‚¡ç¥¨ç»§ç»­å¤„ç†")
                        task_config.symbols = remaining_symbols
                        # æ›´æ–°total_recordsä¸ºåŸå§‹æ€»æ•°ï¼ˆä¸æ”¹å˜æ€»æ•°ï¼Œåªæ”¹å˜å¾…å¤„ç†åˆ—è¡¨ï¼‰
                        result.total_records = saved_progress.total_records or len(original_symbols)
                        # æ¢å¤å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨åˆ°resultä¸­
                        result.processed_symbols_list = saved_progress.processed_symbols_list.copy()
                    else:
                        logger.info(f"ğŸ“‹ [è¿›åº¦æ¢å¤] æ‰€æœ‰è‚¡ç¥¨å·²å¤„ç†å®Œæˆï¼Œä»»åŠ¡æ ‡è®°ä¸ºå®Œæˆ")
                        result.status = TaskExecutionStatus.COMPLETED
                        result.success = True
                        result.end_time = datetime.now()
                        # æ›´æ–°è¿›åº¦ä¸ºå®ŒæˆçŠ¶æ€
                        progress = ImportProgress(
                            task_id=task_config.task_id,
                            status=ImportStatus.COMPLETED,
                            total_symbols=saved_progress.total_symbols,
                            processed_symbols=saved_progress.processed_symbols,
                            total_records=result.total_records,
                            imported_records=result.processed_records,
                            error_count=result.failed_records,
                            start_time=saved_progress.start_time if hasattr(saved_progress, 'start_time') and saved_progress.start_time else datetime.now().isoformat(),
                            end_time=result.end_time.isoformat(),
                            error_message=None,
                            processed_symbols_list=saved_progress.processed_symbols_list
                        )
                        self.config_manager.update_progress(progress)
                        self.task_completed.emit(task_config.task_id, result)
                        return
                else:
                    # å¦‚æœæ²¡æœ‰processed_symbols_listï¼Œå°è¯•åŸºäºprocessed_symbolsæ•°é‡è·³è¿‡
                    if saved_progress.processed_symbols > 0:
                        logger.warning(f"ğŸ“‹ [è¿›åº¦æ¢å¤] ç¼ºå°‘processed_symbols_listï¼ŒåŸºäºprocessed_symbolsæ•°é‡è·³è¿‡å‰{saved_progress.processed_symbols}ä¸ªè‚¡ç¥¨")
                        if saved_progress.processed_symbols < len(task_config.symbols):
                            task_config.symbols = task_config.symbols[saved_progress.processed_symbols:]
                            logger.info(f"ğŸ“‹ [è¿›åº¦æ¢å¤] è·³è¿‡å‰{saved_progress.processed_symbols}ä¸ªè‚¡ç¥¨ï¼Œå‰©ä½™{len(task_config.symbols)}ä¸ªè‚¡ç¥¨ç»§ç»­å¤„ç†")
                        else:
                            logger.info(f"ğŸ“‹ [è¿›åº¦æ¢å¤] æ‰€æœ‰è‚¡ç¥¨å·²å¤„ç†å®Œæˆ")
                            result.status = TaskExecutionStatus.COMPLETED
                            result.success = True
                            result.end_time = datetime.now()
                            progress = ImportProgress(
                                task_id=task_config.task_id,
                                status=ImportStatus.COMPLETED,
                                total_symbols=saved_progress.total_symbols,
                                processed_symbols=saved_progress.processed_symbols,
                                total_records=result.total_records,
                                imported_records=result.processed_records,
                                error_count=result.failed_records,
                                start_time=saved_progress.start_time if hasattr(saved_progress, 'start_time') and saved_progress.start_time else datetime.now().isoformat(),
                                end_time=result.end_time.isoformat(),
                                error_message=None,
                                processed_symbols_list=[]
                            )
                            self.config_manager.update_progress(progress)
                            self.task_completed.emit(task_config.task_id, result)
                            return

                logger.info(f"âœ… [è¿›åº¦æ¢å¤] ä»»åŠ¡å°†ä»ç¬¬{result.processed_records + 1}æ¡è®°å½•ç»§ç»­æ‰§è¡Œ")

            # âœ… ä¿®å¤ï¼šåœ¨æ‰§è¡Œå‰æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å–æ¶ˆ
            if result.status == TaskExecutionStatus.CANCELLED:
                logger.info(f"âš ï¸ [ä»»åŠ¡å·²å–æ¶ˆ] {task_config.task_id} åœ¨æ‰§è¡Œå‰å·²å–æ¶ˆï¼Œè·³è¿‡æ‰§è¡Œ")
                result.end_time = datetime.now()
                if result.start_time:
                    result.execution_time = (result.end_time - result.start_time).total_seconds()
                self.task_cancelled.emit(task_config.task_id)
                return

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result.status = TaskExecutionStatus.RUNNING

            # å¦‚æœtotal_recordsæœªè®¾ç½®ï¼Œä½¿ç”¨symbolsæ•°é‡
            if result.total_records == 0:
                result.total_records = len(task_config.symbols)

            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒçš„å¯¼å…¥é€»è¾‘
            data_type = getattr(task_config, 'data_type', 'Kçº¿æ•°æ®')  # é»˜è®¤ä¸ºKçº¿æ•°æ®
            logger.info(f" æ‰§è¡Œæ•°æ®ç±»å‹: {data_type}")

            if data_type == "Kçº¿æ•°æ®":
                logger.info("å¼€å§‹å¯¼å…¥Kçº¿æ•°æ®")
                self._import_kline_data(task_config, result)
            elif data_type == "å®æ—¶è¡Œæƒ…":
                logger.info("å¼€å§‹å¯¼å…¥å®æ—¶è¡Œæƒ…")
                self._import_realtime_data(task_config, result)
            elif data_type == "åŸºæœ¬é¢æ•°æ®":
                logger.info("å¼€å§‹å¯¼å…¥åŸºæœ¬é¢æ•°æ®")
                self._import_fundamental_data(task_config, result)
            else:
                logger.warning(f" ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨Kçº¿æ•°æ®: {data_type}")
                self._import_kline_data(task_config, result)

            # âœ… ä¿®å¤ï¼šæ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨å®Œæˆå‰è¢«å–æ¶ˆ
            if result.status == TaskExecutionStatus.CANCELLED:
                logger.info(f"âš ï¸ [ä»»åŠ¡å·²å–æ¶ˆ] {task_config.task_id} åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¢«å–æ¶ˆ")
                result.end_time = datetime.now()
                if result.start_time:
                    result.execution_time = (result.end_time - result.start_time).total_seconds()
                # ä¸å‘é€task_completedä¿¡å·ï¼Œå› ä¸ºä»»åŠ¡æ˜¯è¢«å–æ¶ˆçš„
                return

            # ä»»åŠ¡å®Œæˆ
            result.status = TaskExecutionStatus.COMPLETED
            result.success = True
            result.end_time = datetime.now()
            result.execution_time = (result.end_time - result.start_time).total_seconds()

            # è®°å½•æ™ºèƒ½é…ç½®æ€§èƒ½åé¦ˆ
            if self.enable_intelligent_config:
                self.record_task_performance_feedback(task_config.task_id, result)

            # è®°å½•å¢å¼ºæ€§èƒ½æ•°æ®
            if self.enable_enhanced_performance_bridge and result.success:
                execution_time = (result.end_time - result.start_time).total_seconds()
                self.record_custom_performance_metric(
                    f"task_execution_time_{task_config.task_id}",
                    execution_time,
                    "task_performance"
                )
                self.record_custom_performance_metric(
                    f"task_success_rate_{task_config.task_id}",
                    1.0,
                    "task_quality"
                )

            # æ›´æ–°é…ç½®ç®¡ç†å™¨ä¸­çš„è¿›åº¦
            # âœ… ä¿®å¤ï¼šè·å–å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¦‚æœresultä¸­æœ‰ï¼‰
            processed_symbols_list = getattr(result, 'processed_symbols_list', [])
            if not processed_symbols_list and hasattr(result, 'processed_records') and result.processed_records > 0:
                # å¦‚æœæ²¡æœ‰processed_symbols_listï¼Œå°è¯•ä»task_configä¸­è·å–æ‰€æœ‰è‚¡ç¥¨ï¼ˆå› ä¸ºéƒ½å¤„ç†å®Œäº†ï¼‰
                processed_symbols_list = task_config.symbols.copy() if hasattr(task_config, 'symbols') else []

            progress = ImportProgress(
                task_id=task_config.task_id,
                status=ImportStatus.COMPLETED,
                total_symbols=len(task_config.symbols) if hasattr(task_config, 'symbols') else 0,
                processed_symbols=result.processed_records + result.failed_records,
                total_records=result.total_records,
                imported_records=result.processed_records,
                error_count=result.failed_records,
                start_time=result.start_time.isoformat() if result.start_time else datetime.now().isoformat(),
                end_time=result.end_time.isoformat() if result.end_time else datetime.now().isoformat(),
                error_message=result.error_message,
                processed_symbols_list=processed_symbols_list  # âœ… ä¿å­˜å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨
            )
            self.config_manager.update_progress(progress)

            # åœæ­¢æ€§èƒ½ç›‘æ§å¹¶æ£€æµ‹å¼‚å¸¸
            execution_time = (result.end_time - result.start_time).total_seconds()
            self._stop_performance_monitoring(task_config.task_id, execution_time)

            # æ£€æµ‹æ‰§è¡Œå¼‚å¸¸
            anomalies = self._detect_anomalies(task_config.task_id)
            if anomalies:
                logger.warning(f"ä»»åŠ¡ {task_config.task_id} æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")

            # å‘é€å®Œæˆä¿¡å·
            self.task_completed.emit(task_config.task_id, result)

            logger.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task_config.task_id}")

        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task_config.task_id}: {e}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result.status = TaskExecutionStatus.FAILED
            result.error_message = str(e)
            result.end_time = datetime.now()

            # åœæ­¢æ€§èƒ½ç›‘æ§
            if result.start_time and result.end_time:
                execution_time = (result.end_time - result.start_time).total_seconds()
                self._stop_performance_monitoring(task_config.task_id, execution_time)

            # å‘é€å¤±è´¥ä¿¡å·
            self.task_failed.emit(task_config.task_id, str(e))

        finally:
            # âœ… ä»»åŠ¡ç»“æŸæ—¶ç­‰å¾…å†™å…¥é˜Ÿåˆ—æ¸…ç©ºï¼ˆDatabaseWriterThreadä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            if hasattr(self, 'db_writer_thread'):
                queue_size = self.db_writer_thread.write_queue.qsize()
                if queue_size > 0:
                    logger.info(f"ä»»åŠ¡ç»“æŸï¼Œç­‰å¾…é˜Ÿåˆ—æ¸…ç©º: {task_config.task_id}, é˜Ÿåˆ—å‰©ä½™:{queue_size}ä¸ªä»»åŠ¡")
                    # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©ºï¼ˆæœ€å¤š30ç§’ï¼‰
                    import time
                    start_time = time.time()
                    while self.db_writer_thread.write_queue.qsize() > 0 and (time.time() - start_time) < 30:
                        time.sleep(0.5)
                    logger.info(f"é˜Ÿåˆ—å·²æ¸…ç©ºï¼Œè€—æ—¶:{time.time()-start_time:.2f}ç§’")

            # æ¸…ç†è¿è¡Œä¸­çš„ä»»åŠ¡
            with self._task_lock:
                if task_config.task_id in self._running_tasks:
                    del self._running_tasks[task_config.task_id]

    def _save_kdata_to_database(self, symbol: str, kdata: 'pd.DataFrame', task_config: ImportTaskConfig):
        """ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒå®æ—¶/æ‰¹é‡æ¨¡å¼ï¼‰"""
        try:
            # âœ… ä¼˜åŒ–ï¼šå¤ç”¨AssetSeparatedDatabaseManagerå®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
            from ..asset_database_manager import AssetSeparatedDatabaseManager
            from ..plugin_types import AssetType, DataType

            # å¤ç”¨å®ä¾‹ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
            if not hasattr(self, '_metadata_asset_manager'):
                self._metadata_asset_manager = AssetSeparatedDatabaseManager()
            asset_manager = self._metadata_asset_manager

            # âœ… ä¿®å¤ï¼šå…ˆæ·»åŠ symbolå­—æ®µï¼Œå†æ ‡å‡†åŒ–
            if 'symbol' not in kdata.columns:
                kdata['symbol'] = symbol
                logger.debug(f"æ·»åŠ symbolå­—æ®µ: {symbol}")

            # æ ‡å‡†åŒ–æ•°æ®å­—æ®µï¼Œç¡®ä¿ä¸è¡¨ç»“æ„åŒ¹é…
            # âœ… ä¿®å¤ï¼šä¼ é€’data_sourceå‚æ•°ï¼Œç¡®ä¿ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°æ®åŒ…å«æ­£ç¡®çš„æ•°æ®æºæ ‡è¯†
            kdata = self._standardize_kline_data_fields(kdata, data_source=task_config.data_source)

            # ä½¿ç”¨ä»»åŠ¡é…ç½®ä¸­çš„èµ„äº§ç±»å‹ï¼Œä¸å†è¿›è¡Œæ¨æ–­
            asset_type = task_config.asset_type

            # âœ… æ”¹è¿›ï¼šç»Ÿä¸€èµ„äº§ç±»å‹è½¬æ¢é€»è¾‘ï¼Œæ”¯æŒä¸‰ç§æ ¼å¼
            if isinstance(asset_type, str):
                from core.ui_asset_type_utils import UIAssetTypeUtils
                try:
                    # 1. å°è¯•ç›´æ¥ä½œä¸ºæšä¸¾å€¼å­—ç¬¦ä¸²è½¬æ¢ï¼ˆå¦‚"stock_a"ï¼‰
                    asset_type = AssetType(asset_type)
                    logger.debug(f"èµ„äº§ç±»å‹ä»æšä¸¾å€¼å­—ç¬¦ä¸²è½¬æ¢: {asset_type.value}")
                except ValueError:
                    # 2. å°è¯•ä»ä¸­æ–‡æ˜¾ç¤ºåç§°è½¬æ¢ï¼ˆå¦‚"Aè‚¡"ï¼‰
                    asset_type = UIAssetTypeUtils.REVERSE_MAPPING.get(asset_type)
                    if asset_type is None:
                        # 3. ä½¿ç”¨é»˜è®¤å€¼
                        logger.warning(f"æ— æ³•è§£æèµ„äº§ç±»å‹: {task_config.asset_type}ï¼Œä½¿ç”¨é»˜è®¤å€¼ STOCK_A")
                        asset_type = AssetType.STOCK_A
                    else:
                        logger.debug(f"èµ„äº§ç±»å‹ä»ä¸­æ–‡åç§°è½¬æ¢: {task_config.asset_type} -> {asset_type.value}")

            # âœ… ä¼˜åŒ–ï¼šä¿å­˜èµ„äº§å…ƒæ•°æ®æ”¹ä¸ºå¼‚æ­¥ï¼ˆé¿å…é˜»å¡ä¸»æµç¨‹ï¼‰
            # å…ƒæ•°æ®ä¿å­˜ç§»åˆ°åå°çº¿ç¨‹ï¼Œä¸é˜»å¡Kçº¿æ•°æ®å…¥é˜Ÿ
            self._save_asset_metadata_async(symbol, asset_type, task_config, kdata)

            # âœ… æ–°æ–¹æ¡ˆï¼šç»Ÿä¸€ä½¿ç”¨å†™å…¥é˜Ÿåˆ—ï¼ˆDatabaseWriterThreadï¼‰
            # ç”Ÿæˆbuffer_key
            buffer_key = f"{asset_type.value}_{task_config.task_id}"

            # åˆ›å»ºå†™å…¥ä»»åŠ¡
            write_task = WriteTask(
                buffer_key=buffer_key,
                data=kdata.copy(),  # å¤åˆ¶æ•°æ®é¿å…åç»­ä¿®æ”¹å½±å“
                asset_type=asset_type,
                data_type=DataType.HISTORICAL_KLINE
            )

            # âœ… ä¼˜åŒ–ï¼šæ”¾å…¥é˜Ÿåˆ—ï¼ˆè®°å½•é˜Ÿåˆ—çŠ¶æ€ï¼Œä¾¿äºæ€§èƒ½åˆ†æï¼‰
            queue_size_before = self.db_writer_thread.write_queue.qsize()
            queue_start_time = time.time()

            success = self.db_writer_thread.put_write_task(write_task, timeout=10.0)

            queue_put_duration = time.time() - queue_start_time
            queue_size_after = self.db_writer_thread.write_queue.qsize()
            mode = "é˜Ÿåˆ—å†™å…¥"

            if success:
                # âœ… ä¼˜åŒ–ï¼šè®°å½•è¯¦ç»†çš„é˜Ÿåˆ—æ“ä½œä¿¡æ¯
                if queue_put_duration > 0.1:  # å¦‚æœå…¥é˜Ÿè€—æ—¶è¶…è¿‡0.1ç§’ï¼Œè®°å½•è­¦å‘Š
                    logger.warning(f"âš ï¸  [é˜Ÿåˆ—ç§¯å‹] {symbol} | å…¥é˜Ÿè€—æ—¶:{queue_put_duration:.2f}ç§’ | é˜Ÿåˆ—å¤§å°:{queue_size_before}â†’{queue_size_after} | å¯èƒ½é˜Ÿåˆ—ç§¯å‹ä¸¥é‡")
                logger.debug(f"Kçº¿æ•°æ®ä¿å­˜æˆåŠŸ({mode}æ¨¡å¼): {symbol}, {len(kdata)}æ¡è®°å½• | é˜Ÿåˆ—:{queue_size_before}â†’{queue_size_after}")
            else:
                logger.error(f"Kçº¿æ•°æ®ä¿å­˜å¤±è´¥({mode}æ¨¡å¼): {symbol}")

        except Exception as e:
            logger.error(f"ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {symbol}, {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    def _save_asset_metadata(self, symbol: str, asset_type, task_config: ImportTaskConfig, kdata: 'pd.DataFrame' = None):
        """
        ä¿å­˜èµ„äº§å…ƒæ•°æ®åˆ°æ•°æ®åº“

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            asset_type: èµ„äº§ç±»å‹
            task_config: ä»»åŠ¡é…ç½®
            kdata: Kçº¿æ•°æ®DataFrameï¼ˆç”¨äºæå–è‚¡ç¥¨åç§°ç­‰ä¿¡æ¯ï¼‰
        """
        try:
            from ..asset_database_manager import AssetSeparatedDatabaseManager
            from ..plugin_types import AssetType

            # âœ… ä¼˜åŒ–ï¼šå¤ç”¨AssetSeparatedDatabaseManagerå®ä¾‹
            if not hasattr(self, '_metadata_asset_manager'):
                self._metadata_asset_manager = AssetSeparatedDatabaseManager()
            asset_manager = self._metadata_asset_manager

            # âœ… ä»Kçº¿æ•°æ®ä¸­æå–å…ƒæ•°æ®ä¿¡æ¯
            stock_name = symbol  # é»˜è®¤ä½¿ç”¨symbol
            stock_market = self._infer_market_from_symbol(symbol)
            stock_exchange = self._infer_exchange_from_market(stock_market)

            # åˆå§‹åŒ–å¯é€‰å­—æ®µ
            sector = None
            industry = None
            industry_code = None
            listing_date = None
            total_shares = None
            circulating_shares = None

            if kdata is not None and not kdata.empty:
                # è·å–ç¬¬ä¸€è¡Œæ•°æ®ï¼ˆå…ƒæ•°æ®ä¿¡æ¯é€šå¸¸åœ¨æ¯è¡Œéƒ½ç›¸åŒï¼‰
                first_row = kdata.iloc[0]

                # æå–è‚¡ç¥¨åç§°
                if 'name' in kdata.columns:
                    name_value = first_row.get('name')
                    if name_value and str(name_value).strip() and str(name_value) != 'None':
                        stock_name = str(name_value).strip()
                        logger.debug(f"ä»Kçº¿æ•°æ®è·å–è‚¡ç¥¨åç§°: {symbol} -> {stock_name}")

                # âœ… ä¼˜åŒ–ï¼šå¦‚æœKçº¿æ•°æ®ä¸­æ²¡æœ‰nameï¼Œå…ˆæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰å…ƒæ•°æ®ï¼Œå†å°è¯•ä»å¤–éƒ¨APIè·å–
                if stock_name == symbol:
                    logger.debug(f"Kçº¿æ•°æ®ä¸­æœªåŒ…å«è‚¡ç¥¨åç§°ï¼Œå°è¯•è·å–: {symbol}")

                    # âœ… ä¿®å¤ï¼šå…ˆæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰å…ƒæ•°æ®ï¼ˆé¿å…é‡å¤APIè°ƒç”¨ï¼‰
                    try:
                        from ..asset_database_manager import AssetSeparatedDatabaseManager
                        asset_manager = AssetSeparatedDatabaseManager.get_instance()
                        existing_metadata = asset_manager.get_asset_metadata(symbol, asset_type)
                        if existing_metadata and existing_metadata.get('name'):
                            stock_name = existing_metadata['name']
                            logger.debug(f"âœ… ä»æ•°æ®åº“è·å–è‚¡ç¥¨åç§°: {symbol} -> {stock_name}")
                            # åŒæ—¶è·å–è¡Œä¸šæ¿å—ä¿¡æ¯
                            if existing_metadata.get('industry'):
                                industry = existing_metadata['industry']
                            if existing_metadata.get('sector'):
                                sector = existing_metadata['sector']
                            if existing_metadata.get('listing_date'):
                                listing_date = self._normalize_date_format(existing_metadata['listing_date'])
                    except Exception as e:
                        logger.debug(f"ä»æ•°æ®åº“è·å–å…ƒæ•°æ®å¤±è´¥ {symbol}: {e}")

                    # âœ… ä¿®å¤ï¼šå¦‚æœæ•°æ®åº“ä¸­ä¹Ÿæ²¡æœ‰ï¼Œæ‰å°è¯•ä»å¤–éƒ¨APIè·å–ï¼ˆæ·»åŠ è¶…æ—¶ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡ï¼‰
                    if stock_name == symbol:
                        logger.debug(f"æ•°æ®åº“ä¸­ä¹Ÿæ²¡æœ‰è‚¡ç¥¨åç§°ï¼Œå°è¯•ä»å…ƒæ•°æ®å¢å¼ºå™¨è·å–: {symbol}")
                        try:
                            from ..utils.stock_metadata_enhancer import get_metadata_enhancer
                            enhancer = get_metadata_enhancer()
                            # âœ… ä¼˜åŒ–ï¼šæ·»åŠ è¶…æ—¶æœºåˆ¶ï¼Œé¿å…å¤–éƒ¨APIè°ƒç”¨é˜»å¡å¤ªä¹…
                            import threading

                            enhanced_data = None
                            api_error = None

                            def fetch_metadata():
                                nonlocal enhanced_data, api_error
                                try:
                                    # âœ… ä¼˜åŒ–ï¼šæ‰¹é‡è·å–å…ƒæ•°æ®ï¼ˆè™½ç„¶åªæœ‰ä¸€ä¸ªsymbolï¼Œä½†åˆ©ç”¨ç¼“å­˜æœºåˆ¶ï¼‰
                                    enhanced_data = enhancer.enhance_stock_metadata_batch([symbol], source='akshare')
                                except Exception as e:
                                    api_error = e

                            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œå¸¦è¶…æ—¶
                            fetch_thread = threading.Thread(target=fetch_metadata, daemon=True)
                            fetch_thread.start()
                            fetch_thread.join(timeout=5.0)  # âœ… ä¼˜åŒ–ï¼šå¢åŠ è¶…æ—¶æ—¶é—´åˆ°5ç§’ï¼Œé¿å…é¢‘ç¹è¶…æ—¶

                            if fetch_thread.is_alive():
                                # âœ… ä¼˜åŒ–ï¼šè¶…æ—¶æ—¶ä¸è®°å½•è­¦å‘Šï¼Œåªè®°å½•debugæ—¥å¿—ï¼ˆå› ä¸ºå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰
                                logger.debug(f"ä»å¤–éƒ¨APIè·å–å…ƒæ•°æ®è¶…æ—¶: {symbol}ï¼Œè·³è¿‡ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰")
                            elif enhanced_data and symbol in enhanced_data:
                                metadata = enhanced_data[symbol]
                                if 'name' in metadata and metadata['name']:
                                    stock_name = metadata['name']
                                    logger.info(f"âœ… ä»å¤–éƒ¨APIè·å–è‚¡ç¥¨åç§°: {symbol} -> {stock_name}")
                                # åŒæ—¶è·å–è¡Œä¸šæ¿å—ä¿¡æ¯
                                if 'industry' in metadata and metadata['industry']:
                                    industry = metadata['industry']
                                    logger.debug(f"ä»å¤–éƒ¨APIè·å–è¡Œä¸š: {symbol} -> {industry}")
                                if 'sector' in metadata and metadata['sector']:
                                    sector = metadata['sector']
                                    logger.debug(f"ä»å¤–éƒ¨APIè·å–æ¿å—: {symbol} -> {sector}")
                                if 'listing_date' in metadata and metadata['listing_date']:
                                    # âœ… æ ¹æœ¬ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆæ”¯æŒINTEGERå’Œå­—ç¬¦ä¸²ï¼‰
                                    raw_date = metadata['listing_date']
                                    listing_date = self._normalize_date_format(raw_date)
                                    if listing_date:
                                        logger.debug(f"ä»å¤–éƒ¨APIè·å–ä¸Šå¸‚æ—¥æœŸ: {symbol} -> {listing_date} (åŸå€¼:{raw_date})")
                                    else:
                                        logger.warning(f"ä¸Šå¸‚æ—¥æœŸæ ¼å¼æ— æ•ˆ: {symbol}, åŸå€¼={raw_date}")
                            elif api_error:
                                logger.debug(f"ä»å¤–éƒ¨APIè·å–å…ƒæ•°æ®å¤±è´¥ {symbol}: {api_error}ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰")
                        except Exception as e:
                            logger.debug(f"ä»å¤–éƒ¨APIè·å–å…ƒæ•°æ®å¤±è´¥ {symbol}: {e}ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰")

                # æå–å¸‚åœºä¿¡æ¯
                if 'market' in kdata.columns:
                    market_value = first_row.get('market')
                    if market_value and str(market_value).strip():
                        stock_market = str(market_value).strip().lower()
                        stock_exchange = self._infer_exchange_from_market(stock_market)

                # âœ… æå–è¡Œä¸šæ¿å—ä¿¡æ¯ï¼ˆæ”¯æŒå¤šç§å­—æ®µåç§°å˜ä½“ï¼‰
                # æ¿å—å­—æ®µå˜ä½“ï¼šsector, sector_name, plate, plate_name, æ¿å—, æ‰€å±æ¿å—
                sector_fields = ['sector', 'sector_name', 'sectorname', 'plate', 'plate_name', 'æ¿å—', 'æ‰€å±æ¿å—']
                for field in sector_fields:
                    if field in kdata.columns:
                        sector_value = first_row.get(field)
                        if sector_value and str(sector_value).strip() and str(sector_value).strip() not in ['', 'None', 'nan', 'æœªçŸ¥']:
                            sector = str(sector_value).strip()
                            logger.debug(f"ä»Kçº¿æ•°æ®è·å–æ¿å—: {symbol} -> {sector} (å­—æ®µ:{field})")
                            break

                # è¡Œä¸šå­—æ®µå˜ä½“ï¼šindustry, industry_name, industryname, è¡Œä¸š, æ‰€å±è¡Œä¸š
                industry_fields = ['industry', 'industry_name', 'industryname', 'è¡Œä¸š', 'æ‰€å±è¡Œä¸š']
                for field in industry_fields:
                    if field in kdata.columns:
                        industry_value = first_row.get(field)
                        if industry_value and str(industry_value).strip() and str(industry_value).strip() not in ['', 'None', 'nan', 'æœªçŸ¥']:
                            industry = str(industry_value).strip()
                            logger.debug(f"ä»Kçº¿æ•°æ®è·å–è¡Œä¸š: {symbol} -> {industry} (å­—æ®µ:{field})")
                            break

                # è¡Œä¸šä»£ç å­—æ®µå˜ä½“ï¼šindustry_code, industrycode, industry_id, è¡Œä¸šä»£ç 
                industry_code_fields = ['industry_code', 'industrycode', 'industry_id', 'è¡Œä¸šä»£ç ']
                for field in industry_code_fields:
                    if field in kdata.columns:
                        code_value = first_row.get(field)
                        if code_value and str(code_value).strip() and str(code_value).strip() not in ['', 'None', 'nan']:
                            industry_code = str(code_value).strip()
                            logger.debug(f"ä»Kçº¿æ•°æ®è·å–è¡Œä¸šä»£ç : {symbol} -> {industry_code} (å­—æ®µ:{field})")
                            break

                # âœ… æå–ä¸Šå¸‚æ—¥æœŸï¼ˆå¦‚æœKçº¿æ•°æ®ä¸­æœ‰ï¼‰
                for date_col in ['listing_date', 'list_date', 'ipo_date']:
                    if date_col in kdata.columns:
                        date_value = first_row.get(date_col)
                        if date_value:
                            # âœ… ä½¿ç”¨ç»Ÿä¸€çš„æ—¥æœŸæ ¼å¼è½¬æ¢æ–¹æ³•
                            normalized_date = self._normalize_date_format(date_value)
                            if normalized_date:
                                listing_date = normalized_date
                                logger.debug(f"ä»Kçº¿æ•°æ®è·å–ä¸Šå¸‚æ—¥æœŸ: {symbol} -> {listing_date} (åŸå€¼:{date_value})")
                                break

                # âœ… æå–è‚¡æœ¬ä¿¡æ¯ï¼ˆå¦‚æœKçº¿æ•°æ®ä¸­æœ‰ï¼‰
                if 'total_shares' in kdata.columns:
                    shares_value = first_row.get('total_shares')
                    if shares_value and shares_value > 0:
                        total_shares = int(shares_value)
                        logger.debug(f"ä»Kçº¿æ•°æ®è·å–æ€»è‚¡æœ¬: {symbol} -> {total_shares}")

                if 'circulating_shares' in kdata.columns:
                    circ_value = first_row.get('circulating_shares')
                    if circ_value and circ_value > 0:
                        circulating_shares = int(circ_value)
                        logger.debug(f"ä»Kçº¿æ•°æ®è·å–æµé€šè‚¡æœ¬: {symbol} -> {circulating_shares}")

            # âœ… æ ¹æ®èµ„äº§ç±»å‹æ¨æ–­è´§å¸
            currency = self._infer_currency_from_asset_type(asset_type, stock_market)

            # âœ… æ„å»ºå…ƒæ•°æ®å­—å…¸ï¼ˆä»…åŒ…å«éNoneçš„å­—æ®µï¼Œé¿å…è¦†ç›–å·²æœ‰æ•°æ®ï¼‰
            metadata = {
                'symbol': symbol,
                'name': stock_name,
                'market': stock_market,
                'exchange': stock_exchange,
                'asset_type': asset_type.value,
                'listing_status': 'active',
                'currency': currency,
                'base_currency': currency,
                'quote_currency': currency,
                'primary_data_source': task_config.data_source if hasattr(task_config, 'data_source') else 'unknown',
                'data_sources': [task_config.data_source] if hasattr(task_config, 'data_source') else [],
            }

            # åªæ·»åŠ ä»Kçº¿æ•°æ®ä¸­æå–åˆ°çš„å­—æ®µ
            if sector:
                metadata['sector'] = sector
            if industry:
                metadata['industry'] = industry
            if industry_code:
                metadata['industry_code'] = industry_code
            if listing_date:
                metadata['listing_date'] = listing_date
            if total_shares:
                metadata['total_shares'] = total_shares
            if circulating_shares:
                metadata['circulating_shares'] = circulating_shares

            logger.debug(f"èµ„äº§å…ƒæ•°æ®å‡†å¤‡å®Œæˆ: {symbol} | åç§°:{stock_name} | è¡Œä¸š:{industry} | æ¿å—:{sector} | ä¸Šå¸‚æ—¥æœŸ:{listing_date}")

            # ä¿å­˜å…ƒæ•°æ®
            success = asset_manager.upsert_asset_metadata(symbol, asset_type, metadata)
            if success:
                logger.debug(f"ä¿å­˜èµ„äº§å…ƒæ•°æ®æˆåŠŸ: {symbol} ({stock_name})")
            else:
                logger.warning(f"ä¿å­˜èµ„äº§å…ƒæ•°æ®å¤±è´¥: {symbol}")

        except Exception as e:
            logger.warning(f"ä¿å­˜èµ„äº§å…ƒæ•°æ®å¼‚å¸¸: {symbol}, {e}")

    def _save_asset_metadata_async(self, symbol: str, asset_type, task_config: ImportTaskConfig, kdata: 'pd.DataFrame' = None):
        """
        å¼‚æ­¥ä¿å­˜èµ„äº§å…ƒæ•°æ®ï¼ˆé¿å…é˜»å¡ä¸»æµç¨‹ï¼‰

        å°†å…ƒæ•°æ®ä¿å­˜æ“ä½œç§»åˆ°åå°çº¿ç¨‹æ‰§è¡Œï¼Œä¸é˜»å¡Kçº¿æ•°æ®å…¥é˜Ÿ
        """
        try:
            # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œå…ƒæ•°æ®ä¿å­˜ï¼Œé¿å…é˜»å¡
            def save_metadata_task():
                try:
                    self._save_asset_metadata(symbol, asset_type, task_config, kdata)
                except Exception as e:
                    logger.debug(f"å¼‚æ­¥ä¿å­˜èµ„äº§å…ƒæ•°æ®å¤±è´¥: {symbol}, {e}")

            # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œï¼ˆå¦‚æœçº¿ç¨‹æ± å¯ç”¨ï¼‰
            if hasattr(self, 'executor') and self.executor:
                self.executor.submit(save_metadata_task)
            else:
                # å¦‚æœæ²¡æœ‰çº¿ç¨‹æ± ï¼Œä½¿ç”¨æ–°çº¿ç¨‹æ‰§è¡Œ
                import threading
                thread = threading.Thread(target=save_metadata_task, daemon=True, name=f"MetadataSaver-{symbol}")
                thread.start()
        except Exception as e:
            logger.debug(f"å¯åŠ¨å¼‚æ­¥å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {symbol}, {e}")

    def _normalize_date_format(self, date_value) -> str:
        """
        ç»Ÿä¸€æ—¥æœŸæ ¼å¼è½¬æ¢ï¼ˆæ ¹æœ¬ä¿®å¤ï¼šæ”¯æŒå¤šç§æ ¼å¼ï¼‰

        Args:
            date_value: æ—¥æœŸå€¼ï¼Œå¯èƒ½æ˜¯INTEGER (19990727), å­—ç¬¦ä¸² ('1999-07-27', '19990727'), æˆ–datetimeå¯¹è±¡

        Returns:
            str: YYYY-MM-DDæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        if date_value is None:
            return None

        try:
            import pandas as pd
            from datetime import datetime

            # å¦‚æœæ˜¯æ•´æ•°ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            if isinstance(date_value, (int, float)):
                date_str = str(int(date_value))
                if len(date_str) == 8:  # YYYYMMDD
                    year = date_str[:4]
                    month = date_str[4:6]
                    day = date_str[6:8]
                    return f"{year}-{month}-{day}"
                else:
                    logger.warning(f"æ—¥æœŸæ•´æ•°æ ¼å¼ä¸æ­£ç¡®: {date_value}")
                    return None

            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            elif isinstance(date_value, str):
                date_str = date_value.strip()
                # å¦‚æœå·²ç»æ˜¯YYYY-MM-DDæ ¼å¼
                if len(date_str) == 10 and date_str[4] == '-' and date_str[7] == '-':
                    return date_str
                # å¦‚æœæ˜¯YYYYMMDDæ ¼å¼å­—ç¬¦ä¸²
                elif len(date_str) == 8 and date_str.isdigit():
                    year = date_str[:4]
                    month = date_str[4:6]
                    day = date_str[6:8]
                    return f"{year}-{month}-{day}"
                # å°è¯•ç”¨pandasè§£æ
                else:
                    parsed_date = pd.to_datetime(date_str)
                    return parsed_date.strftime('%Y-%m-%d')

            # å¦‚æœæ˜¯datetimeå¯¹è±¡
            elif isinstance(date_value, (datetime, pd.Timestamp)):
                return pd.to_datetime(date_value).strftime('%Y-%m-%d')

            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ—¥æœŸç±»å‹: {type(date_value)}, å€¼={date_value}")
                return None

        except Exception as e:
            logger.error(f"æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥: {date_value}, é”™è¯¯={e}")
            return None

    def _infer_market_from_symbol(self, symbol: str) -> str:
        """ä»è‚¡ç¥¨ä»£ç æ¨æ–­å¸‚åœº"""
        symbol_clean = symbol.split('.')[0] if '.' in symbol else symbol

        if symbol_clean.startswith('6'):
            return 'sh'  # ä¸Šæµ·
        elif symbol_clean.startswith(('0', '3')):
            return 'sz'  # æ·±åœ³
        elif symbol_clean.startswith(('4', '8')):
            return 'bj'  # åŒ—äº¬
        else:
            return 'unknown'

    def _infer_exchange_from_market(self, market: str) -> str:
        """ä»å¸‚åœºä»£ç æ¨æ–­äº¤æ˜“æ‰€åç§°"""
        exchange_map = {
            'sh': 'SSE',      # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€ (Shanghai Stock Exchange)
            'sz': 'SZSE',     # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€ (Shenzhen Stock Exchange)
            'bj': 'BSE',      # åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€ (Beijing Stock Exchange)
            'hk': 'HKEX',     # é¦™æ¸¯äº¤æ˜“æ‰€ (Hong Kong Exchange)
            'us': 'NASDAQ',   # çº³æ–¯è¾¾å…‹ï¼ˆé»˜è®¤ï¼Œä¹Ÿå¯èƒ½æ˜¯NYSEï¼‰
        }
        return exchange_map.get(market.lower(), market.upper())

    def _infer_currency_from_asset_type(self, asset_type, market: str) -> str:
        """ä»èµ„äº§ç±»å‹å’Œå¸‚åœºæ¨æ–­è´§å¸"""
        from ..plugin_types import AssetType

        # æ ¹æ®èµ„äº§ç±»å‹æ¨æ–­
        if asset_type == AssetType.STOCK_A:
            return 'CNY'  # äººæ°‘å¸
        elif asset_type == AssetType.STOCK_HK:
            return 'HKD'  # æ¸¯å¸
        elif asset_type == AssetType.STOCK_US:
            return 'USD'  # ç¾å…ƒ
        elif asset_type == AssetType.CRYPTO:
            return 'USDT'  # åŠ å¯†è´§å¸é€šå¸¸ç”¨USDTè®¡ä»·
        elif asset_type == AssetType.FUTURES:
            # æœŸè´§æ ¹æ®å¸‚åœºåˆ¤æ–­
            if market in ['sh', 'sz', 'bj']:
                return 'CNY'
            elif market == 'hk':
                return 'HKD'
            else:
                return 'USD'
        else:
            # é»˜è®¤æ ¹æ®å¸‚åœºåˆ¤æ–­
            market_currency_map = {
                'sh': 'CNY', 'sz': 'CNY', 'bj': 'CNY',
                'hk': 'HKD',
                'us': 'USD',
            }
            return market_currency_map.get(market.lower(), 'CNY')

    def _write_data_immediately(self, kdata: 'pd.DataFrame', asset_type, data_type) -> bool:
        """ç«‹å³å†™å…¥æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            from ..asset_database_manager import AssetSeparatedDatabaseManager
            asset_manager = AssetSeparatedDatabaseManager()

            success = asset_manager.store_standardized_data(
                data=kdata,
                asset_type=asset_type,
                data_type=data_type
            )
            return success
        except Exception as e:
            logger.error(f"ç«‹å³å†™å…¥æ•°æ®å¤±è´¥: {e}")
            return False

    def _add_to_batch_buffer(self, symbol: str, kdata: 'pd.DataFrame', asset_type, task_config: ImportTaskConfig) -> bool:
        """å°†æ•°æ®åŠ å…¥æ‰¹é‡å†™å…¥ç¼“å†²åŒº"""
        try:
            with self._batch_write_lock:
                buffer_key = f"{asset_type.value}_{task_config.task_id}"

                if buffer_key not in self._batch_write_buffer:
                    self._batch_write_buffer[buffer_key] = {
                        'data': [],
                        'asset_type': asset_type,
                        'task_config': task_config,
                        'count': 0
                    }

                self._batch_write_buffer[buffer_key]['data'].append(kdata)
                self._batch_write_buffer[buffer_key]['count'] += len(kdata)

                logger.debug(f"æ•°æ®åŠ å…¥ç¼“å†²åŒº: {symbol}, å½“å‰ç¼“å†²åŒºå¤§å°: {self._batch_write_buffer[buffer_key]['count']}")

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡é˜ˆå€¼
                batch_size = self.realtime_write_service.config.batch_size if self.realtime_write_service else 100
                if self._batch_write_buffer[buffer_key]['count'] >= batch_size:
                    logger.info(f"ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼({batch_size})ï¼Œè§¦å‘æ‰¹é‡å†™å…¥")
                    return self._flush_batch_buffer(buffer_key)

                return True

        except Exception as e:
            logger.error(f"åŠ å…¥æ‰¹é‡ç¼“å†²åŒºå¤±è´¥: {symbol}, {e}")
            return False

    def _flush_batch_buffer(self, buffer_key: str = None) -> bool:
        """åˆ·æ–°æ‰¹é‡å†™å…¥ç¼“å†²åŒºåˆ°æ•°æ®åº“"""
        try:
            # âœ… ä¿®å¤æ­»é”ï¼šåˆ†ä¸¤æ­¥æ“ä½œï¼Œå…ˆå–æ•°æ®ï¼ˆæŒæœ‰é”ï¼‰ï¼Œå†å†™å…¥ï¼ˆé‡Šæ”¾é”ï¼‰
            buffers_to_write = []

            # ç¬¬ä¸€æ­¥ï¼šå¿«é€ŸæŒæœ‰é”ï¼Œå–å‡ºæ•°æ®å¹¶æ¸…ç©ºç¼“å†²åŒº
            with self._batch_write_lock:
                keys_to_flush = [buffer_key] if buffer_key else list(self._batch_write_buffer.keys())

                for key in keys_to_flush:
                    if key not in self._batch_write_buffer:
                        continue

                    buffer_data = self._batch_write_buffer[key]
                    if not buffer_data['data']:
                        continue

                    # å¤åˆ¶æ•°æ®åˆ°ä¸´æ—¶åˆ—è¡¨
                    buffers_to_write.append({
                        'key': key,
                        'data': buffer_data['data'].copy(),  # å¤åˆ¶åˆ—è¡¨
                        'asset_type': buffer_data['asset_type']
                    })

                    # ç«‹å³æ¸…ç©ºç¼“å†²åŒºï¼Œå…è®¸æ–°æ•°æ®å†™å…¥
                    del self._batch_write_buffer[key]
                    logger.debug(f"ç¼“å†²åŒºå·²æ¸…ç©ºï¼Œå‡†å¤‡å†™å…¥: {key}, {len(buffer_data['data'])}ä¸ªDataFrame")

            # ç¬¬äºŒæ­¥ï¼šé‡Šæ”¾é”åæ‰§è¡Œè€—æ—¶çš„æ•°æ®åº“IOæ“ä½œ
            import pandas as pd
            from ..asset_database_manager import AssetSeparatedDatabaseManager
            from ..plugin_types import DataType

            for buffer_info in buffers_to_write:
                key = buffer_info['key']
                data_list = buffer_info['data']
                asset_type = buffer_info['asset_type']

                # åˆå¹¶æ‰€æœ‰DataFrame
                combined_data = pd.concat(data_list, ignore_index=True)
                logger.info(f"å¼€å§‹æ‰¹é‡å†™å…¥: {key}, {len(combined_data)}æ¡è®°å½•")

                # å†™å…¥æ•°æ®åº“ï¼ˆä¸æŒæœ‰é”ï¼‰
                asset_manager = AssetSeparatedDatabaseManager()
                success = asset_manager.store_standardized_data(
                    data=combined_data,
                    asset_type=asset_type,
                    data_type=DataType.HISTORICAL_KLINE
                )

                if success:
                    logger.info(f"âœ… æ‰¹é‡åˆ·æ–°æˆåŠŸ: {key}, {len(combined_data)}æ¡è®°å½•")
                else:
                    logger.error(f"âŒ æ‰¹é‡åˆ·æ–°å¤±è´¥: {key}")
                    return False

            return True

        except Exception as e:
            logger.error(f"åˆ·æ–°æ‰¹é‡ç¼“å†²åŒºå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def flush_all_buffers(self):
        """åˆ·æ–°æ‰€æœ‰æ‰¹é‡å†™å…¥ç¼“å†²åŒºï¼ˆä»»åŠ¡ç»“æŸæ—¶è°ƒç”¨ï¼‰"""
        logger.info("å¼€å§‹åˆ·æ–°æ‰€æœ‰æ‰¹é‡å†™å…¥ç¼“å†²åŒº...")
        try:
            success = self._flush_batch_buffer()  # ä¸ä¼ å‚æ•°ï¼Œåˆ·æ–°æ‰€æœ‰
            if success:
                logger.info("æ‰€æœ‰æ‰¹é‡ç¼“å†²åŒºåˆ·æ–°å®Œæˆ")
            else:
                logger.warning("éƒ¨åˆ†æ‰¹é‡ç¼“å†²åŒºåˆ·æ–°å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºå¤±è´¥: {e}")
            return False

    def update_write_strategy(self, strategy: str):
        """
        æ›´æ–°å†™å…¥ç­–ç•¥

        Args:
            strategy: å†™å…¥ç­–ç•¥ ('realtime', 'batch', 'adaptive')
        """
        try:
            if not self.realtime_write_service:
                logger.warning("å®æ—¶å†™å…¥æœåŠ¡æœªå¯ç”¨ï¼Œæ— æ³•æ›´æ–°ç­–ç•¥")
                return False

            from ..services.realtime_write_config import WriteStrategy

            strategy_map = {
                'realtime': WriteStrategy.REALTIME,
                'batch': WriteStrategy.BATCH,
                'adaptive': WriteStrategy.ADAPTIVE
            }

            if strategy.lower() in strategy_map:
                old_strategy = self.realtime_write_service.config.write_strategy
                self.realtime_write_service.config.write_strategy = strategy_map[strategy.lower()]
                logger.info(f"å†™å…¥ç­–ç•¥å·²æ›´æ–°: {old_strategy.value} -> {strategy.lower()}")

                # å¦‚æœä»æ‰¹é‡æ¨¡å¼åˆ‡æ¢ï¼Œå…ˆåˆ·æ–°ç¼“å†²åŒº
                if old_strategy == WriteStrategy.BATCH:
                    logger.info("ä»æ‰¹é‡æ¨¡å¼åˆ‡æ¢ï¼Œåˆ·æ–°ç°æœ‰ç¼“å†²åŒº")
                    self.flush_all_buffers()

                return True
            else:
                logger.warning(f"æœªçŸ¥çš„å†™å…¥ç­–ç•¥: {strategy}")
                return False

        except Exception as e:
            logger.error(f"æ›´æ–°å†™å…¥ç­–ç•¥å¤±è´¥: {e}")
            return False

    def get_write_strategy(self) -> str:
        """è·å–å½“å‰å†™å…¥ç­–ç•¥"""
        if self.realtime_write_service:
            return self.realtime_write_service.config.write_strategy.value
        return "direct"

    def get_buffer_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å†²åŒºçŠ¶æ€ä¿¡æ¯"""
        try:
            with self._batch_write_lock:
                status = {
                    'buffer_count': len(self._batch_write_buffer),
                    'total_records': sum(buf['count'] for buf in self._batch_write_buffer.values()),
                    'buffers': []
                }

                for key, buf in self._batch_write_buffer.items():
                    status['buffers'].append({
                        'key': key,
                        'record_count': buf['count'],
                        'dataframe_count': len(buf['data'])
                    })

                return status
        except Exception as e:
            logger.error(f"è·å–ç¼“å†²åŒºçŠ¶æ€å¤±è´¥: {e}")
            return {'buffer_count': 0, 'total_records': 0, 'buffers': []}

    def _standardize_kline_data_fields(self, df, data_source: str = None) -> 'pd.DataFrame':
        """æ ‡å‡†åŒ–Kçº¿æ•°æ®å­—æ®µï¼Œç¡®ä¿ä¸è¡¨ç»“æ„åŒ¹é…"""
        import pandas as pd  # åœ¨å‡½æ•°å¼€å¤´å¯¼å…¥ï¼Œé¿å…åç»­å¼•ç”¨é”™è¯¯

        try:
            if df.empty:
                return df

            # âœ… æ­¥éª¤1: å¦‚æœdatetimeæ˜¯indexï¼Œå°†å…¶é‡ç½®ä¸ºåˆ—
            if isinstance(df.index, pd.DatetimeIndex):
                logger.debug("æ£€æµ‹åˆ°DatetimeIndexï¼Œè½¬æ¢ä¸ºdatetimeåˆ—")
                # âœ… ä¿®å¤ï¼šæ£€æŸ¥datetimeåˆ—æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤æ’å…¥
                if 'datetime' not in df.columns:
                    df = df.reset_index()
                    # å¦‚æœresetåçš„åˆ—åä¸º'index'æˆ–'date'ï¼Œé‡å‘½åä¸ºdatetime
                    if 'index' in df.columns and 'datetime' not in df.columns:
                        df = df.rename(columns={'index': 'datetime'})
                        logger.debug("å·²å°†'index'åˆ—é‡å‘½åä¸º'datetime'")
                    elif 'date' in df.columns and 'datetime' not in df.columns:
                        df = df.rename(columns={'date': 'datetime'})
                        logger.debug("å·²å°†'date'åˆ—é‡å‘½åä¸º'datetime'")
                else:
                    # datetimeåˆ—å·²å­˜åœ¨ï¼Œåªéœ€é‡ç½®ç´¢å¼•ä¸ºé»˜è®¤æ•°å­—ç´¢å¼•
                    df = df.reset_index(drop=True)
                    logger.debug("datetimeåˆ—å·²å­˜åœ¨ï¼Œé‡ç½®ä¸ºé»˜è®¤ç´¢å¼•")

            # âœ… æ­¥éª¤2: å¦‚æœæœ‰'date'åˆ—ä½†æ²¡æœ‰'datetime'åˆ—ï¼Œé‡å‘½å
            if 'date' in df.columns and 'datetime' not in df.columns:
                df = df.rename(columns={'date': 'datetime'})
                logger.debug("å·²å°†'date'åˆ—é‡å‘½åä¸º'datetime'")

            # å¤„ç†å­—æ®µåç§°æ˜ å°„ï¼ˆcode -> symbolï¼‰
            if 'code' in df.columns:
                if 'symbol' not in df.columns:
                    # å¦‚æœæ²¡æœ‰symbolåˆ—ï¼Œå°†codeé‡å‘½åä¸ºsymbol
                    df = df.rename(columns={'code': 'symbol'})
                    logger.debug("å·²å°†'code'åˆ—é‡å‘½åä¸º'symbol'")
                else:
                    # å¦‚æœå·²æœ‰symbolåˆ—ï¼Œåˆ é™¤codeåˆ—é¿å…å†²çª
                    df = df.drop('code', axis=1)
                    logger.debug("å·²åˆ é™¤'code'åˆ—(å·²å­˜åœ¨'symbol'åˆ—)")

            # åŸºç¡€å­—æ®µæ˜ å°„å’Œé»˜è®¤å€¼
            # æ ‡å‡†é‡åŒ–è¡¨å­—æ®µï¼ˆ20å­—æ®µ - æ–¹æ¡ˆBï¼‰+ è¡Œä¸šåˆ†ç±»å­—æ®µï¼ˆä»…ç”¨äºå…ƒæ•°æ®æå–ï¼‰
            # åŒ…æ‹¬åŸºç¡€OHLCVã€å¤æƒæ•°æ®ã€æ‰©å±•äº¤æ˜“æ•°æ®ã€å…ƒæ•°æ®ã€è¡Œä¸šåˆ†ç±»
            field_defaults = {
                # åŸºç¡€OHLCVå­—æ®µï¼ˆ8ä¸ªï¼‰
                'symbol': '',
                'datetime': None,
                'open': 0.0,
                'high': 0.0,
                'low': 0.0,
                'close': 0.0,
                'volume': 0,
                'amount': 0.0,
                'turnover': 0.0,

                # å¤æƒæ•°æ®ï¼ˆ2ä¸ªï¼‰- é‡åŒ–å›æµ‹å¿…éœ€
                'adj_close': None,      # å¤æƒæ”¶ç›˜ä»·
                'adj_factor': 1.0,      # å¤æƒå› å­ï¼ˆé»˜è®¤1.0=ä¸å¤æƒï¼‰

                # æ‰©å±•äº¤æ˜“æ•°æ®ï¼ˆ2ä¸ªï¼‰
                'turnover_rate': None,  # æ¢æ‰‹ç‡ï¼ˆè¡Œä¸šæ ‡å‡†ï¼‰
                'vwap': None,           # æˆäº¤é‡åŠ æƒå‡ä»·ï¼ˆæœºæ„å¸¸ç”¨ï¼‰

                # å…ƒæ•°æ®ï¼ˆ6ä¸ªï¼‰
                'name': None,
                'market': None,
                'frequency': '1d',      # ğŸ”§ ä¿®å¤ï¼šé¢‘ç‡å­—æ®µé»˜è®¤å€¼ä¸º'1d'
                'period': None,
                'data_source': data_source if data_source else 'unknown',  # âœ… ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„data_sourceå‚æ•°ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 'unknown'
                'created_at': None,
                'updated_at': None,

                # è¡Œä¸šåˆ†ç±»å­—æ®µï¼ˆ3ä¸ªï¼‰- ä»…ç”¨äºä¼ é€’ç»™_save_asset_metadataï¼Œä¸å­˜å…¥Kçº¿è¡¨
                'sector': None,         # æ¿å—
                'industry': None,       # è¡Œä¸š
                'industry_code': None,  # è¡Œä¸šä»£ç 

                # æ¶¨è·Œæ•°æ®
                'change': None,
                'change_pct': None,
            }

            # æ·»åŠ ç¼ºå¤±çš„å¿…éœ€å­—æ®µ
            for field, default_value in field_defaults.items():
                if field not in df.columns:
                    df[field] = default_value

            # âœ… ä¿®å¤ï¼šå•ç‹¬å¤„ç†data_sourceå­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ•°æ®æºæ ‡è¯†
            # å¦‚æœä¼ å…¥äº†data_sourceå‚æ•°ï¼ˆæ¥è‡ªä»»åŠ¡é…ç½®ï¼‰ï¼Œå§‹ç»ˆä½¿ç”¨å®ƒï¼ˆè¿™æ˜¯æœ€æƒå¨çš„æ•°æ®æºæ ‡è¯†ï¼‰
            if data_source:
                df['data_source'] = data_source
                logger.debug(f"âœ… è®¾ç½®data_sourceå­—æ®µ: {data_source} (æ¥è‡ªä»»åŠ¡é…ç½®)")
            elif 'data_source' not in df.columns:
                # å¦‚æœæ²¡æœ‰ä¼ å…¥data_sourceå‚æ•°ä¸”å­—æ®µä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼'unknown'
                df['data_source'] = 'unknown'
                logger.warning(f"âš ï¸ data_sourceå­—æ®µä¸å­˜åœ¨ä¸”æœªä¼ å…¥å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼'unknown'")

            # ğŸ”§ ä¿®å¤ï¼šè§„èŒƒåŒ–frequencyå­—æ®µå€¼ï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼ï¼‰
            if 'frequency' in df.columns:
                # é¢‘ç‡å€¼è§„èŒƒåŒ–æ˜ å°„ï¼ˆç»Ÿä¸€ä¸ºæ ‡å‡†æ ¼å¼ï¼‰
                frequency_normalization_map = {
                    # æ—¥çº¿
                    'D': '1d', 'd': '1d', 'day': '1d', 'daily': '1d', 'Day': '1d', 'Daily': '1d',
                    '1D': '1d', '1d': '1d',
                    # å‘¨çº¿
                    'W': '1w', 'w': '1w', 'week': '1w', 'weekly': '1w', 'Week': '1w', 'Weekly': '1w',
                    '1W': '1w', '1w': '1w',
                    # æœˆçº¿
                    'M': '1M', 'm': '1M', 'month': '1M', 'monthly': '1M', 'Month': '1M', 'Monthly': '1M',
                    '1M': '1M',
                    # åˆ†é’Ÿçº¿
                    '1': '1min', '1min': '1min', '5': '5min', '5min': '5min',
                    '15': '15min', '15min': '15min', '30': '30min', '30min': '30min',
                    '60': '60min', '60min': '60min', '1H': '60min', '1h': '60min',
                }

                # åº”ç”¨è§„èŒƒåŒ–æ˜ å°„
                def normalize_frequency(freq_value):
                    """è§„èŒƒåŒ–é¢‘ç‡å€¼"""
                    if pd.isna(freq_value) or freq_value is None or freq_value == '':
                        return '1d'  # é»˜è®¤å€¼

                    freq_str = str(freq_value).strip()
                    normalized = frequency_normalization_map.get(freq_str)
                    if normalized:
                        if normalized != freq_str:
                            logger.debug(f"ğŸ”§ é¢‘ç‡è§„èŒƒåŒ–: '{freq_str}' -> '{normalized}'")
                        return normalized
                    else:
                        logger.warning(f"âš ï¸  æœªçŸ¥çš„é¢‘ç‡å€¼: '{freq_str}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ '1d'")
                        return '1d'

                # å‘é‡åŒ–å¤„ç†é¢‘ç‡åˆ—
                original_frequencies = df['frequency'].copy()
                df['frequency'] = df['frequency'].apply(normalize_frequency)

                # ç»Ÿè®¡é¢‘ç‡å˜åŒ–
                changed_mask = original_frequencies != df['frequency']
                if changed_mask.any():
                    change_count = changed_mask.sum()
                    logger.info(f"ğŸ“Š [é¢‘ç‡è§„èŒƒåŒ–ç»Ÿè®¡] å…±{change_count}æ¡è®°å½•çš„é¢‘ç‡è¢«è§„èŒƒåŒ–")
                    logger.debug(f"   åŸå§‹é¢‘ç‡åˆ†å¸ƒ: {original_frequencies.value_counts().to_dict()}")
                    logger.debug(f"   è§„èŒƒåŒ–åé¢‘ç‡åˆ†å¸ƒ: {df['frequency'].value_counts().to_dict()}")

            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            numeric_fields = ['open', 'high', 'low', 'close', 'volume', 'amount']
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0)

            # ç¡®ä¿datetimeå­—æ®µæ ¼å¼æ­£ç¡®ä¸”ä¸ä¸ºç©º
            if 'datetime' in df.columns:
                df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
                # åˆ é™¤datetimeä¸ºç©ºçš„è¡Œï¼ˆæ•°æ®åº“NOT NULLçº¦æŸï¼‰
                null_datetime_count = df['datetime'].isna().sum()
                if null_datetime_count > 0:
                    logger.warning(f"å‘ç° {null_datetime_count} æ¡datetimeä¸ºç©ºçš„è®°å½•ï¼Œå°†è¢«è¿‡æ»¤")
                    df = df[df['datetime'].notna()]
            else:
                # å¦‚æœæ²¡æœ‰datetimeåˆ—ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–æ—¶é—´åˆ—
                time_columns = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
                if time_columns:
                    logger.warning(f"æœªæ‰¾åˆ°datetimeåˆ—ï¼Œå°è¯•ä½¿ç”¨ {time_columns[0]}")
                    df['datetime'] = pd.to_datetime(df[time_columns[0]], errors='coerce')
                    df = df[df['datetime'].notna()]
                else:
                    # æœ€åå°è¯•ï¼šæ£€æŸ¥æ˜¯å¦æœ‰DatetimeIndexä½†è¿˜æ²¡è¢«é‡ç½®
                    if isinstance(df.index, pd.DatetimeIndex):
                        logger.warning("å‘ç°DatetimeIndexä½†æœªè¢«é‡ç½®ä¸ºdatetimeåˆ—ï¼Œæ­£åœ¨ä¿®å¤")
                        df = df.reset_index()
                        if 'index' in df.columns:
                            df = df.rename(columns={'index': 'datetime'})
                    else:
                        logger.error(f"æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œæ— æ³•æ ‡å‡†åŒ–æ•°æ®ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
                        return pd.DataFrame()

            # ç¡®ä¿symbolå­—æ®µä¸ä¸ºç©º
            if 'symbol' in df.columns:
                df['symbol'] = df['symbol'].fillna('').astype(str)

            # åˆ é™¤codeåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…ä¸symbolæ··æ·†
            if 'code' in df.columns:
                logger.debug("åˆ é™¤codeåˆ—ï¼ˆå·²æœ‰symbolåˆ—ï¼‰")
                df = df.drop(columns=['code'])

            # è®¾ç½®é»˜è®¤æ—¶é—´æˆ³
            if 'created_at' in df.columns and df['created_at'].isna().all():
                df['created_at'] = pd.Timestamp.now()

            # æ™ºèƒ½è®¡ç®—å¤æƒä»·æ ¼ï¼ˆå¦‚æœæ•°æ®æºæ²¡æœ‰æä¾›ï¼‰
            if 'adj_close' in df.columns:
                # å¦‚æœadj_closeä¸ºç©ºä½†æœ‰adj_factorï¼Œåˆ™è®¡ç®—
                mask = df['adj_close'].isna() & df['adj_factor'].notna()
                if mask.any():
                    df.loc[mask, 'adj_close'] = df.loc[mask, 'close'] * df.loc[mask, 'adj_factor']

                # å¦‚æœadj_closeå’Œadj_factoréƒ½ä¸ºç©ºï¼Œè®¾ç½®adj_close=closeï¼ˆä¸å¤æƒï¼‰
                mask = df['adj_close'].isna()
                if mask.any():
                    df.loc[mask, 'adj_close'] = df.loc[mask, 'close']

            # æ™ºèƒ½è®¡ç®—VWAPï¼ˆå¦‚æœæ•°æ®æºæ²¡æœ‰æä¾›ï¼‰
            if 'vwap' in df.columns and df['vwap'].isna().all():
                # vwap = amount / volume
                df['vwap'] = df['amount'] / df['volume'].replace(0, pd.NA)

            # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿datetimeå­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
            if 'datetime' not in df.columns:
                logger.error(f"æ ‡å‡†åŒ–å®Œæˆä½†ç¼ºå°‘datetimeå­—æ®µï¼å¯ç”¨åˆ—: {df.columns.tolist()}")
                return pd.DataFrame()  # è¿”å›ç©ºDataFrameï¼Œé¿å…æ’å…¥å¤±è´¥

            if df['datetime'].isna().all():
                logger.error(f"æ ‡å‡†åŒ–å®Œæˆä½†datetimeå­—æ®µå…¨ä¸ºç©ºï¼")
                return pd.DataFrame()

            logger.info(f"âœ… æ•°æ®å­—æ®µæ ‡å‡†åŒ–å®Œæˆï¼Œå­—æ®µæ•°: {len(df.columns)}, è®°å½•æ•°: {len(df)}")
            logger.debug(f"ğŸ“‹ æ ‡å‡†åŒ–åçš„åˆ—: {df.columns.tolist()}")
            logger.debug(f"ğŸ“Š é¢‘ç‡åˆ†å¸ƒ: {df['frequency'].value_counts().to_dict() if 'frequency' in df.columns else 'æ— é¢‘ç‡åˆ—'}")
            return df

        except Exception as e:
            logger.error(f"æ ‡å‡†åŒ–Kçº¿æ•°æ®å­—æ®µå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return df

    def _enrich_kline_data_with_metadata(self, df: 'pd.DataFrame') -> 'pd.DataFrame':
        """
        è¡¥å…¨Kçº¿æ•°æ®çš„å…ƒæ•°æ®å­—æ®µï¼ˆname, marketï¼‰

        ç­–ç•¥ï¼š
        1. å°è¯•ä»èµ„äº§åˆ—è¡¨è·å–nameå’Œmarket
        2. å¦‚æœå¤±è´¥ï¼Œä»symbolæ¨æ–­market
        3. nameæ— æ³•æ¨æ–­åˆ™ä¿æŒä¸ºç©º

        Args:
            df: Kçº¿æ•°æ®DataFrame

        Returns:
            è¡¥å…¨åçš„DataFrame
        """
        import pandas as pd

        try:
            if df.empty or 'symbol' not in df.columns:
                logger.warning("æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘symbolå­—æ®µï¼Œè·³è¿‡å…ƒæ•°æ®è¡¥å…¨")
                return df

            logger.info(f"å¼€å§‹è¡¥å…¨Kçº¿æ•°æ®å…ƒæ•°æ®: {len(df)} æ¡è®°å½•")

            # ç­–ç•¥1: å°è¯•ä»èµ„äº§åˆ—è¡¨è·å–nameå’Œmarket
            try:
                from ..services.unified_data_manager import get_unified_data_manager
                unified_manager = get_unified_data_manager()

                if unified_manager:
                    # è·å–èµ„äº§åˆ—è¡¨
                    asset_list_df = unified_manager.get_asset_list()

                    if not asset_list_df.empty:
                        # å‡†å¤‡æ˜ å°„å­—å…¸
                        symbol_to_info = {}
                        for _, row in asset_list_df.iterrows():
                            symbol = row.get('symbol', row.get('code', ''))
                            symbol_to_info[symbol] = {
                                'name': row.get('name', ''),
                                'market': row.get('market', '')
                            }

                        # è¡¥å…¨nameå­—æ®µ
                        if 'name' in df.columns:
                            def enrich_name(row):
                                if pd.notna(row['name']) and row['name']:
                                    return row['name']  # å·²æœ‰nameï¼Œä¿æŒä¸å˜
                                info = symbol_to_info.get(row['symbol'], {})
                                return info.get('name', None)

                            df['name'] = df.apply(enrich_name, axis=1)
                            enriched_count = df['name'].notna().sum()
                            logger.info(f"ä»èµ„äº§åˆ—è¡¨è¡¥å…¨äº† {enriched_count} æ¡è®°å½•çš„nameå­—æ®µ")

                        # è¡¥å…¨marketå­—æ®µ
                        if 'market' in df.columns:
                            def enrich_market(row):
                                if pd.notna(row['market']) and row['market']:
                                    return row['market']  # å·²æœ‰marketï¼Œä¿æŒä¸å˜
                                info = symbol_to_info.get(row['symbol'], {})
                                return info.get('market', None)

                            df['market'] = df.apply(enrich_market, axis=1)
                            enriched_count = df['market'].notna().sum()
                            logger.info(f"ä»èµ„äº§åˆ—è¡¨è¡¥å…¨äº† {enriched_count} æ¡è®°å½•çš„marketå­—æ®µ")
                    else:
                        logger.debug("èµ„äº§åˆ—è¡¨ä¸ºç©ºï¼Œå°†ä½¿ç”¨symbolæ¨æ–­market")
                else:
                    logger.debug("UnifiedDataManagerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨symbolæ¨æ–­market")

            except Exception as e:
                logger.debug(f"ä»èµ„äº§åˆ—è¡¨è¡¥å…¨å…ƒæ•°æ®å¤±è´¥ï¼ˆéå…³é”®é”™è¯¯ï¼‰: {e}")

            # ç­–ç•¥2: ä»symbolæ¨æ–­marketï¼ˆä½œä¸ºåå¤‡æˆ–è¡¥å……ï¼‰
            if 'market' in df.columns:
                def infer_market_from_symbol(row):
                    """ä»symbolæ¨æ–­market"""
                    # å¦‚æœå·²æœ‰æœ‰æ•ˆmarketï¼Œä¿æŒä¸å˜
                    if pd.notna(row['market']) and row['market'] and row['market'] != 'unknown':
                        return row['market']

                    symbol = str(row['symbol'])

                    # æ ¹æ®åç¼€åˆ¤æ–­
                    if symbol.endswith('.SH'):
                        return 'SH'
                    elif symbol.endswith('.SZ'):
                        return 'SZ'
                    elif symbol.endswith('.BJ'):
                        return 'BJ'

                    # æ ¹æ®å‰ç¼€åˆ¤æ–­ï¼ˆå»é™¤åç¼€åï¼‰
                    code = symbol.split('.')[0]
                    if code.startswith('6'):
                        return 'SH'  # æ²ªå¸‚Aè‚¡
                    elif code.startswith(('0', '3')):
                        return 'SZ'  # æ·±å¸‚Aè‚¡/åˆ›ä¸šæ¿
                    elif code.startswith(('4', '8')):
                        return 'BJ'  # åŒ—äº¤æ‰€

                    return 'unknown'

                df['market'] = df.apply(infer_market_from_symbol, axis=1)
                inferred_count = (df['market'] != 'unknown').sum()
                logger.info(f"ä»symbolæ¨æ–­äº† {inferred_count} æ¡è®°å½•çš„marketå­—æ®µ")

            # ç­–ç•¥3: ç»Ÿè®¡è¡¥å…¨ç»“æœ
            stats = {
                'total_records': len(df),
                'name_filled': df['name'].notna().sum() if 'name' in df.columns else 0,
                'market_filled': df['market'].notna().sum() if 'market' in df.columns else 0,
            }

            logger.info(f"å…ƒæ•°æ®è¡¥å…¨å®Œæˆ: "
                        f"æ€»è®°å½•={stats['total_records']}, "
                        f"nameå¡«å……ç‡={stats['name_filled']/stats['total_records']*100:.1f}%, "
                        f"marketå¡«å……ç‡={stats['market_filled']/stats['total_records']*100:.1f}%")

            return df

        except Exception as e:
            logger.error(f"è¡¥å…¨Kçº¿æ•°æ®å…ƒæ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return df

    def _import_realtime_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥å®æ—¶è¡Œæƒ…æ•°æ®"""
        try:
            symbols = task_config.symbols
            result.total_records = len(symbols)

            for i, symbol in enumerate(symbols):
                if result.status == TaskExecutionStatus.CANCELLED:
                    break

                try:
                    # è·å–å®æ—¶è¡Œæƒ…æ•°æ®
                    quote_data = self.real_data_provider.get_real_quote(symbol)

                    if quote_data:
                        # å°†å®æ—¶æ•°æ®è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
                        if isinstance(quote_data, dict):
                            import pandas as pd
                            quote_df = pd.DataFrame([quote_data])
                            self._save_realtime_data_to_database(symbol, quote_df, task_config.asset_type)
                        logger.info(f"æˆåŠŸå¯¼å…¥å¹¶ä¿å­˜ {symbol} çš„å®æ—¶è¡Œæƒ…æ•°æ®")
                        result.processed_records += 1
                    else:
                        logger.warning(f"æœªè·å–åˆ° {symbol} çš„å®æ—¶è¡Œæƒ…æ•°æ®")
                        result.failed_records += 1

                except Exception as e:
                    logger.error(f"å¯¼å…¥ {symbol} å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
                    result.failed_records += 1

                time.sleep(0.05)  # å®æ—¶æ•°æ®å¤„ç†æ›´å¿«

        except Exception as e:
            raise Exception(f"å®æ—¶è¡Œæƒ…å¯¼å…¥å¤±è´¥: {e}")

    def _import_single_symbol_kline(self, symbol: str, index: int, total: int, task_config: ImportTaskConfig) -> dict:
        """
        å¯¼å…¥å•ä¸ªè‚¡ç¥¨çš„Kçº¿æ•°æ®ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰

        Returns:
            dict: {'symbol': str, 'success': bool, 'record_count': int, 'error': str}
        """
        import time
        import threading

        task_start_time = time.time()
        thread_id = threading.current_thread().name

        try:
            # âœ… ä¿®å¤ï¼šåœ¨æ‰§è¡Œå‰æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å–æ¶ˆ
            if hasattr(self, '_task_results') and task_config.task_id in self._task_results:
                task_result = self._task_results[task_config.task_id]
                if task_result.status == TaskExecutionStatus.CANCELLED:
                    logger.info(f"âš ï¸ [ä»»åŠ¡å·²å–æ¶ˆ] {symbol} è·³è¿‡æ‰§è¡Œ")
                    return {'symbol': symbol, 'success': False, 'record_count': 0, 'error': 'ä»»åŠ¡å·²å–æ¶ˆ'}

            logger.info(f"ğŸ”µ [å¼€å§‹] {symbol} ({index+1}/{total}) | çº¿ç¨‹:{thread_id}")

            # 1. ä»çœŸå®æ•°æ®æä¾›è€…è·å–Kçº¿æ•°æ®ï¼ˆå…³é”®ç›‘æ§ç‚¹1ï¼šç½‘ç»œè¯·æ±‚ï¼‰
            network_start = time.time()
            logger.debug(f"â±ï¸  [ç½‘ç»œè¯·æ±‚å¼€å§‹] {symbol} | çº¿ç¨‹:{thread_id}")

            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ è¯¦ç»†çš„å‚æ•°æ—¥å¿—
            logger.debug(f"ğŸ“ [è°ƒç”¨å‚æ•°] code={symbol}, freq={task_config.frequency.value if hasattr(task_config.frequency, 'value') else task_config.frequency}, "
                         f"asset_type={task_config.asset_type}, data_source={task_config.data_source}")

            kdata = self.real_data_provider.get_real_kdata(
                code=symbol,
                freq=task_config.frequency.value if hasattr(task_config.frequency, 'value') else str(task_config.frequency),
                start_date=task_config.start_date,
                end_date=task_config.end_date,
                data_source=task_config.data_source,
                asset_type=task_config.asset_type
            )

            network_elapsed = time.time() - network_start
            logger.info(f"â±ï¸  [ç½‘ç»œè¯·æ±‚å®Œæˆ] {symbol} | è€—æ—¶:{network_elapsed:.2f}ç§’ | çº¿ç¨‹:{thread_id}")

            # ğŸ”§ ä¿®å¤ï¼šå…³é”®ç›‘æ§ç‚¹1 - æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ•°æ®
            if kdata is None:
                logger.error(f"âŒ [æ•°æ®ä¸ºNone] {symbol} | è°ƒç”¨get_real_kdata()è¿”å›Noneï¼Œè¿™è¡¨æ˜æ•°æ®æºå¯èƒ½ä¸å¯ç”¨æˆ–è¿”å›äº†å¼‚å¸¸å€¼")
                return {'symbol': symbol, 'success': False, 'record_count': 0, 'error': 'æ•°æ®æä¾›è€…è¿”å›None'}

            if kdata.empty:
                logger.warning(f"âŒ [æ•°æ®ä¸ºç©º] {symbol} | ä»real_data_providerè·å–åˆ°ç©ºæ•°æ®ï¼Œå¯èƒ½åŸå› ï¼š")
                logger.warning(f"   1. æ•°æ®æº(å¦‚Tushare/AKShare)æ— æ­¤è‚¡ç¥¨æ•°æ®")
                logger.warning(f"   2. æ•°æ®æºAPIè°ƒç”¨å¤±è´¥æˆ–æ— æƒé™")
                logger.warning(f"   3. æ—¥æœŸèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®")
                logger.warning(f"   4. æ•°æ®æºè¿”å›å¼‚å¸¸")
                logger.warning(f"   è¯¦ç»†æ£€æŸ¥ï¼šæ•°æ®æº={task_config.data_source}, è‚¡ç¥¨={symbol}, æ—¥æœŸèŒƒå›´={task_config.start_date}~{task_config.end_date}")
                return {'symbol': symbol, 'success': False, 'record_count': 0, 'error': 'æœªè·å–åˆ°æ•°æ®'}

            # ğŸ”§ ä¿®å¤ï¼šéªŒè¯æ•°æ®çš„åŸºæœ¬å®Œæ•´æ€§
            if 'datetime' not in kdata.columns and 'timestamp' not in kdata.columns:
                logger.error(f"âŒ [æ•°æ®æ ¼å¼é”™è¯¯] {symbol} | æ•°æ®ç¼ºå°‘datetime/timestampåˆ—ï¼Œæ•°æ®åˆ—={kdata.columns.tolist()}")
                return {'symbol': symbol, 'success': False, 'record_count': 0, 'error': 'æ•°æ®æ ¼å¼æ— æ•ˆ'}

            logger.info(f"âœ… [æ•°æ®è·å–æˆåŠŸ] {symbol} | æ¡æ•°:{len(kdata)} | åˆ—æ•°:{len(kdata.columns)} | è€—æ—¶:{network_elapsed:.2f}ç§’")
            logger.debug(f"ğŸ“Š [æ•°æ®å­—æ®µ] {kdata.columns.tolist()}")

            # 2. âœ… æ•°æ®è´¨é‡éªŒè¯
            if self.enable_data_quality_monitoring:
                validation_start = time.time()
                validation_result = self._validate_imported_data(
                    task_id=task_config.task_id,
                    data=kdata,
                    data_source=task_config.data_source,
                    data_type='kdata'
                )
                validation_elapsed = time.time() - validation_start
                logger.debug(f"â±ï¸  [è´¨é‡éªŒè¯] {symbol} | è¯„åˆ†:{validation_result.quality_score:.3f} | è€—æ—¶:{validation_elapsed:.2f}ç§’")

            # 3. ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“ï¼ˆå…³é”®ç›‘æ§ç‚¹2ï¼šæ•°æ®åº“å†™å…¥ï¼‰
            db_start = time.time()
            logger.debug(f"â±ï¸  [æ•°æ®åº“å†™å…¥å¼€å§‹] {symbol} | çº¿ç¨‹:{thread_id}")

            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ asset_typeè°ƒè¯•æ—¥å¿—
            logger.debug(f"ğŸ“ [èµ„äº§ç±»å‹] åŸå§‹å€¼={task_config.asset_type}, ç±»å‹={type(task_config.asset_type)}")

            # âœ… ä¿®å¤ï¼šè®°å½•é˜Ÿåˆ—çŠ¶æ€ï¼Œä¾¿äºåˆ†ææ€§èƒ½é—®é¢˜
            queue_size_before = self.db_writer_thread.write_queue.qsize() if hasattr(self, 'db_writer_thread') else 0

            self._save_kdata_to_database(symbol, kdata, task_config)

            # âœ… ä¿®å¤ï¼šåªè®¡ç®—æ”¾å…¥é˜Ÿåˆ—çš„æ—¶é—´ï¼Œä¸åŒ…å«ç­‰å¾…é˜Ÿåˆ—æ¶ˆè´¹çš„æ—¶é—´
            # çœŸæ­£çš„æ•°æ®åº“å†™å…¥æ˜¯å¼‚æ­¥çš„ï¼Œåœ¨DatabaseWriterThreadä¸­å®Œæˆ
            db_elapsed = time.time() - db_start
            queue_size_after = self.db_writer_thread.write_queue.qsize() if hasattr(self, 'db_writer_thread') else 0

            logger.info(f"â±ï¸  [æ•°æ®å…¥é˜Ÿå®Œæˆ] {symbol} | å…¥é˜Ÿè€—æ—¶:{db_elapsed:.2f}ç§’ | é˜Ÿåˆ—å¤§å°:{queue_size_before}â†’{queue_size_after} | çº¿ç¨‹:{thread_id}")

            total_elapsed = time.time() - task_start_time
            logger.info(f"ğŸŸ¢ [å®Œæˆ] {symbol} | æ€»è€—æ—¶:{total_elapsed:.2f}ç§’ (ç½‘ç»œ:{network_elapsed:.2f}s, æ•°æ®åº“:{db_elapsed:.2f}s) | çº¿ç¨‹:{thread_id}")

            return {'symbol': symbol, 'success': True, 'record_count': len(kdata), 'error': None}

        except Exception as e:
            error_msg = str(e)
            total_elapsed = time.time() - task_start_time
            logger.error(f"ğŸ”´ [å¤±è´¥] {symbol} | æ€»è€—æ—¶:{total_elapsed:.2f}ç§’ | é”™è¯¯:{error_msg} | çº¿ç¨‹:{thread_id}")
            logger.error(f"ğŸ“‹ [è°ƒè¯•ä¿¡æ¯] ä»»åŠ¡é…ç½®: task_id={task_config.task_id}, asset_type={task_config.asset_type}, data_source={task_config.data_source}")
            import traceback
            logger.error(traceback.format_exc())
            return {'symbol': symbol, 'success': False, 'record_count': 0, 'error': error_msg}

    def _import_kline_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥Kçº¿æ•°æ®ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        try:
            # ç¡®ä¿çœŸå®æ•°æ®æä¾›å™¨å·²åˆå§‹åŒ–
            if not self._real_data_provider_initialized:
                self._ensure_real_data_provider()

            if self.real_data_provider is None:
                raise Exception("çœŸå®æ•°æ®æä¾›å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯¼å…¥Kçº¿æ•°æ®")

            symbols = task_config.symbols
            # âœ… ä¿®å¤ï¼šå¦‚æœtotal_recordså·²è®¾ç½®ï¼ˆä»è¿›åº¦æ¢å¤ï¼‰ï¼Œä¸è¦è¦†ç›–
            if result.total_records == 0:
                result.total_records = len(symbols)

            # âœ… ä¿®å¤ï¼šåˆå§‹åŒ–å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨ï¼ˆç”¨äºè¿›åº¦æ¢å¤ï¼‰
            processed_symbols_set = set()
            if hasattr(result, 'processed_symbols_list') and result.processed_symbols_list:
                processed_symbols_set = set(result.processed_symbols_list)
                logger.debug(f"ğŸ“‹ [è¿›åº¦æ¢å¤] ä»resultæ¢å¤å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨: {len(processed_symbols_set)}ä¸ªè‚¡ç¥¨")

            # âœ… ä½¿ç”¨max_workersè¿›è¡Œå¹¶è¡Œå¤„ç†
            max_workers = min(task_config.max_workers, len(symbols)) if hasattr(task_config, 'max_workers') else 1

            if max_workers > 1:
                logger.info(f"ğŸ“Š [å¹¶è¡Œæ¨¡å¼] å¼€å§‹å¯¼å…¥: {len(symbols)}ä¸ªè‚¡ç¥¨ï¼Œmax_workers={max_workers}")
                logger.info(f"ğŸ“Š [ä»»åŠ¡é˜Ÿåˆ—] å·²æäº¤{len(symbols)}ä¸ªä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼Œç­‰å¾…æ‰§è¡Œ...")

                from concurrent.futures import ThreadPoolExecutor, as_completed
                import threading
                import time

                # åˆ›å»ºçº¿ç¨‹é”ç”¨äºæ›´æ–°resultï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                result_lock = threading.Lock()
                batch_start_time = time.time()

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_symbol = {
                        executor.submit(self._import_single_symbol_kline, symbol, i, len(symbols), task_config): symbol
                        for i, symbol in enumerate(symbols)
                    }

                    logger.info(f"ğŸ“Š [çº¿ç¨‹æ± çŠ¶æ€] å·²æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œå¼€å§‹æ‰§è¡Œ...")

                    # æ”¶é›†ç»“æœ
                    completed_count = 0
                    for future in as_completed(future_to_symbol):
                        completed_count += 1

                        # æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                        if result.status == TaskExecutionStatus.CANCELLED:
                            logger.info("âš ï¸ [ä»»åŠ¡å–æ¶ˆ] åœæ­¢å¯¼å…¥")
                            executor.shutdown(wait=False)
                            break

                        try:
                            import_result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶

                            symbol = import_result['symbol']
                            with result_lock:
                                if import_result['success']:
                                    result.processed_records += 1
                                    processed_symbols_set.add(symbol)  # âœ… è®°å½•å·²å¤„ç†çš„è‚¡ç¥¨
                                else:
                                    result.failed_records += 1
                                    # å¤±è´¥ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤å°è¯•ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
                                    processed_symbols_set.add(symbol)

                            # âœ… ä¿®å¤ï¼šå®æ—¶æ›´æ–°è¿›åº¦ï¼ˆåŒ…å«å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨ï¼‰
                            progress = ImportProgress(
                                task_id=task_config.task_id,
                                status=ImportStatus.RUNNING,
                                total_symbols=len(symbols),
                                processed_symbols=result.processed_records + result.failed_records,
                                total_records=result.total_records,
                                imported_records=result.processed_records,
                                error_count=result.failed_records,
                                start_time=result.start_time.isoformat() if result.start_time else datetime.now().isoformat(),
                                end_time=None,
                                error_message=None,
                                processed_symbols_list=list(processed_symbols_set)  # âœ… ä¿å­˜å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨
                            )
                            self.config_manager.update_progress(progress)

                            # æ›´æ–°è¿›åº¦
                            progress_ratio = (result.processed_records + result.failed_records) / result.total_records
                            elapsed = time.time() - batch_start_time
                            avg_time = elapsed / completed_count if completed_count > 0 else 0
                            eta = avg_time * (len(symbols) - completed_count) if completed_count > 0 else 0

                            logger.info(f"ğŸ“Š [è¿›åº¦] {completed_count}/{len(symbols)} | æˆåŠŸ:{result.processed_records} å¤±è´¥:{result.failed_records} | å¹³å‡è€—æ—¶:{avg_time:.2f}s | é¢„è®¡å‰©ä½™:{eta:.1f}s")

                            # âœ… ä¿®å¤ï¼šåœ¨è¿›åº¦æ¶ˆæ¯ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰ï¼Œä»¥ä¾¿UIå¯ä»¥æå–å¹¶è®°å½•åˆ°é”™è¯¯è¡¨
                            if import_result['success']:
                                progress_message = f"å¯¼å…¥è‚¡ç¥¨æ•°æ®: {symbol} ({result.processed_records + result.failed_records}/{result.total_records})"
                            else:
                                error_info = import_result.get('error', 'æœªçŸ¥é”™è¯¯')
                                progress_message = f"å¯¼å…¥è‚¡ç¥¨æ•°æ®: {symbol} ({result.processed_records + result.failed_records}/{result.total_records}) | {symbol}å¤±è´¥: {error_info}"

                            self.task_progress.emit(
                                task_config.task_id,
                                progress_ratio,
                                progress_message
                            )

                        except TimeoutError:
                            symbol = future_to_symbol[future]
                            logger.error(f"â° [è¶…æ—¶] {symbol} æ‰§è¡Œè¶…è¿‡300ç§’ï¼Œfuture.result()è¶…æ—¶")
                            with result_lock:
                                result.failed_records += 1
                                processed_symbols_set.add(symbol)  # âœ… è¶…æ—¶ä¹Ÿè®°å½•
                        except Exception as e:
                            symbol = future_to_symbol[future]
                            logger.error(f"ğŸ”´ [å¼‚å¸¸] {symbol} å¤„ç†ç»“æœå¤±è´¥: {e}")
                            with result_lock:
                                result.failed_records += 1
                                processed_symbols_set.add(symbol)  # âœ… å¼‚å¸¸ä¹Ÿè®°å½•

                total_elapsed = time.time() - batch_start_time
                logger.info(f"ğŸ“Š [å¹¶è¡Œå®Œæˆ] æ€»è€—æ—¶:{total_elapsed:.2f}ç§’ | æˆåŠŸ:{result.processed_records} å¤±è´¥:{result.failed_records}")

                # âœ… ä¿®å¤ï¼šå°†å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨è®¾ç½®åˆ°resultä¸­
                result.processed_symbols_list = list(processed_symbols_set)
            else:
                logger.info(f"å¼€å§‹ä¸²è¡Œå¯¼å…¥Kçº¿æ•°æ®: {len(symbols)}ä¸ªè‚¡ç¥¨")

                for i, symbol in enumerate(symbols):
                    if result.status == TaskExecutionStatus.CANCELLED:
                        logger.info("ä»»åŠ¡è¢«å–æ¶ˆï¼Œåœæ­¢å¯¼å…¥")
                        break

                    import_result = self._import_single_symbol_kline(symbol, i, len(symbols), task_config)

                    if import_result['success']:
                        result.processed_records += 1
                        processed_symbols_set.add(symbol)  # âœ… è®°å½•å·²å¤„ç†çš„è‚¡ç¥¨
                    else:
                        result.failed_records += 1
                        processed_symbols_set.add(symbol)  # âœ… å¤±è´¥ä¹Ÿè®°å½•

                    # âœ… ä¿®å¤ï¼šå®æ—¶æ›´æ–°è¿›åº¦ï¼ˆåŒ…å«å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨ï¼‰
                    progress = ImportProgress(
                        task_id=task_config.task_id,
                        status=ImportStatus.RUNNING,
                        total_symbols=len(symbols),
                        processed_symbols=result.processed_records + result.failed_records,
                        total_records=result.total_records,
                        imported_records=result.processed_records,
                        error_count=result.failed_records,
                        start_time=result.start_time.isoformat() if result.start_time else datetime.now().isoformat(),
                        end_time=None,
                        error_message=None,
                        processed_symbols_list=list(processed_symbols_set)  # âœ… ä¿å­˜å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨
                    )
                    self.config_manager.update_progress(progress)

                    # âœ… ä¿®å¤ï¼šåœ¨è¿›åº¦æ¶ˆæ¯ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰ï¼Œä»¥ä¾¿UIå¯ä»¥æå–å¹¶è®°å½•åˆ°é”™è¯¯è¡¨
                    if import_result['success']:
                        progress_message = f"å¯¼å…¥è‚¡ç¥¨æ•°æ®: {symbol} ({i+1}/{len(symbols)})"
                    else:
                        error_info = import_result.get('error', 'æœªçŸ¥é”™è¯¯')
                        progress_message = f"å¯¼å…¥è‚¡ç¥¨æ•°æ®: {symbol} ({i+1}/{len(symbols)}) | {symbol}å¤±è´¥: {error_info}"

                    # æ›´æ–°è¿›åº¦
                    progress_ratio = (i + 1) / len(symbols)
                    self.task_progress.emit(
                        task_config.task_id,
                        progress_ratio,
                        progress_message
                    )

                # âœ… ä¿®å¤ï¼šå°†å·²å¤„ç†è‚¡ç¥¨åˆ—è¡¨è®¾ç½®åˆ°resultä¸­
                result.processed_symbols_list = list(processed_symbols_set)

                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(0.1)

            logger.info(f"Kçº¿æ•°æ®å¯¼å…¥å®Œæˆ: æˆåŠŸ {result.processed_records}/{result.total_records}, å¤±è´¥ {result.failed_records}")

        except Exception as e:
            logger.error(f"Kçº¿æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise Exception(f"Kçº¿æ•°æ®å¯¼å…¥å¤±è´¥: {e}")

    def _import_fundamental_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥åŸºæœ¬é¢æ•°æ®"""
        try:
            symbols = task_config.symbols
            result.total_records = len(symbols)

            for i, symbol in enumerate(symbols):
                if result.status == TaskExecutionStatus.CANCELLED:
                    break

                try:
                    # è·å–åŸºæœ¬é¢æ•°æ®
                    fundamental_data = self.real_data_provider.get_real_fundamental_data(symbol)

                    if fundamental_data:
                        # å°†åŸºæœ¬é¢æ•°æ®è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
                        if isinstance(fundamental_data, (dict, list)):
                            import pandas as pd
                            if isinstance(fundamental_data, dict):
                                fund_df = pd.DataFrame([fundamental_data])
                            else:
                                fund_df = pd.DataFrame(fundamental_data)
                            self._save_fundamental_data_to_database(symbol, fund_df, "åŸºæœ¬é¢æ•°æ®", task_config.asset_type)
                        logger.info(f"æˆåŠŸå¯¼å…¥å¹¶ä¿å­˜ {symbol} çš„åŸºæœ¬é¢æ•°æ®")
                        result.processed_records += 1
                    else:
                        logger.warning(f"æœªè·å–åˆ° {symbol} çš„åŸºæœ¬é¢æ•°æ®")
                        result.failed_records += 1

                except Exception as e:
                    logger.error(f"å¯¼å…¥ {symbol} åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                    result.failed_records += 1

                time.sleep(0.2)  # åŸºæœ¬é¢æ•°æ®å¤„ç†è¾ƒæ…¢

        except Exception as e:
            raise Exception(f"åŸºæœ¬é¢æ•°æ®å¯¼å…¥å¤±è´¥: {e}")

    def _update_progress(self):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        with self._task_lock:
            for task_id, result in self._task_results.items():
                if result.status == TaskExecutionStatus.RUNNING:
                    progress = result.progress_percentage
                    message = f"å·²å¤„ç† {result.processed_records}/{result.total_records} æ¡è®°å½•"

                    # ä½¿ç”¨ç›‘æ§åŠŸèƒ½å‘é€è¿›åº¦ä¿¡å·
                    self._monitor_task_progress(task_id, progress, message)

    # ==================== æ™ºèƒ½é…ç½®ç®¡ç†åŠŸèƒ½ ====================

    def _apply_intelligent_optimization(self, config: ImportTaskConfig,
                                        optimization_level: ConfigOptimizationLevel = ConfigOptimizationLevel.BALANCED) -> Optional[ImportTaskConfig]:
        """åº”ç”¨æ™ºèƒ½é…ç½®ä¼˜åŒ–"""
        if not self.enable_intelligent_config or not isinstance(self.config_manager, IntelligentConfigManager):
            return None

        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½é…ç½®ä¼˜åŒ–: {config.task_id}")

            # ä½¿ç”¨æ™ºèƒ½é…ç½®ç®¡ç†å™¨ç”Ÿæˆä¼˜åŒ–é…ç½®
            optimized_config = self.config_manager.generate_intelligent_config(
                config,
                optimization_level
            )

            logger.info(f"æ™ºèƒ½é…ç½®ä¼˜åŒ–å®Œæˆ: {config.task_id}")
            return optimized_config

        except Exception as e:
            logger.error(f"æ™ºèƒ½é…ç½®ä¼˜åŒ–å¤±è´¥: {e}")
            return None

    def generate_config_recommendations(self, task_id: str,
                                        recommendation_type: ConfigRecommendationType = ConfigRecommendationType.BALANCED) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé…ç½®æ¨è"""
        if not self.enable_intelligent_config or not isinstance(self.config_manager, IntelligentConfigManager):
            return []

        try:
            recommendations = self.config_manager.generate_config_recommendations(
                task_id, recommendation_type
            )

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾¿äºUIæ˜¾ç¤º
            return [
                {
                    'recommendation_id': rec.recommendation_id,
                    'recommendation_type': rec.recommendation_type.value,
                    'recommended_changes': rec.recommended_changes,
                    'expected_improvement': rec.expected_improvement,
                    'confidence_score': rec.confidence_score,
                    'reasoning': rec.reasoning,
                    'created_at': rec.created_at
                }
                for rec in recommendations
            ]

        except Exception as e:
            logger.error(f"ç”Ÿæˆé…ç½®æ¨èå¤±è´¥: {e}")
            return []

    def detect_and_resolve_config_conflicts(self, auto_resolve: bool = True) -> Dict[str, Any]:
        """æ£€æµ‹å¹¶è§£å†³é…ç½®å†²çª"""
        if not self.enable_intelligent_config or not isinstance(self.config_manager, IntelligentConfigManager):
            return {'conflicts_detected': 0, 'conflicts_resolved': 0, 'message': 'æ™ºèƒ½é…ç½®æœªå¯ç”¨'}

        try:
            # æ£€æµ‹é…ç½®å†²çª
            conflicts = self.config_manager.detect_config_conflicts()

            result = {
                'conflicts_detected': len(conflicts),
                'conflicts_resolved': 0,
                'conflicts': []
            }

            # è½¬æ¢å†²çªä¿¡æ¯ä¸ºå­—å…¸æ ¼å¼
            for conflict in conflicts:
                conflict_info = {
                    'conflict_id': conflict.conflict_id,
                    'config_ids': conflict.config_ids,
                    'conflict_type': conflict.conflict_type,
                    'description': conflict.description,
                    'severity': conflict.severity,
                    'auto_resolvable': conflict.auto_resolvable,
                    'suggested_resolution': conflict.suggested_resolution
                }
                result['conflicts'].append(conflict_info)

            # è‡ªåŠ¨è§£å†³å†²çª
            if auto_resolve and conflicts:
                resolution_results = self.config_manager.auto_resolve_conflicts(conflicts)
                result['conflicts_resolved'] = resolution_results['resolved']
                result['resolution_details'] = resolution_results['details']

            logger.info(f"é…ç½®å†²çªæ£€æµ‹å®Œæˆ: å‘ç°{len(conflicts)}ä¸ªå†²çª")
            return result

        except Exception as e:
            logger.error(f"é…ç½®å†²çªæ£€æµ‹å¤±è´¥: {e}")
            return {'error': str(e), 'conflicts_detected': 0, 'conflicts_resolved': 0}

    def record_task_performance_feedback(self, task_id: str, execution_result: TaskExecutionResult):
        """è®°å½•ä»»åŠ¡æ€§èƒ½åé¦ˆç”¨äºæ™ºèƒ½å­¦ä¹ """
        if not self.enable_intelligent_config or not isinstance(self.config_manager, IntelligentConfigManager):
            return

        try:
            # è·å–ä»»åŠ¡é…ç½®
            config = self.config_manager.get_import_task(task_id)
            if not config:
                return

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            execution_time = execution_result.duration or 0
            success_rate = 1.0 if execution_result.success else 0.0
            error_rate = 1.0 - success_rate
            throughput = (execution_result.processed_records / execution_time) if execution_time > 0 else 0

            # è®°å½•æ€§èƒ½åé¦ˆ
            self.config_manager.record_performance_feedback(
                config, execution_time, success_rate, error_rate, throughput
            )

            logger.info(f"è®°å½•ä»»åŠ¡æ€§èƒ½åé¦ˆ: {task_id}")

        except Exception as e:
            logger.error(f"è®°å½•æ€§èƒ½åé¦ˆå¤±è´¥: {e}")

    def get_intelligent_config_statistics(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½é…ç½®ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_intelligent_config or not isinstance(self.config_manager, IntelligentConfigManager):
            return {
                'intelligent_config_enabled': False,
                'message': 'æ™ºèƒ½é…ç½®æœªå¯ç”¨'
            }

        try:
            stats = self.config_manager.get_intelligent_statistics()
            stats['intelligent_config_enabled'] = True
            return stats

        except Exception as e:
            logger.error(f"è·å–æ™ºèƒ½é…ç½®ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'intelligent_config_enabled': True,
                'error': str(e),
                'message': 'è·å–æ™ºèƒ½é…ç½®ç»Ÿè®¡å¤±è´¥'
            }

    # ==================== å¢å¼ºç‰ˆæ€§èƒ½æ¡¥æ¥ç³»ç»ŸåŠŸèƒ½ ====================

    def _init_enhanced_performance_bridge(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆæ€§èƒ½æ•°æ®æ¡¥æ¥ç³»ç»Ÿ"""
        try:
            self.enhanced_performance_bridge = get_enhanced_performance_bridge()
            logger.info("å¢å¼ºç‰ˆæ€§èƒ½æ•°æ®æ¡¥æ¥ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¢å¼ºç‰ˆæ€§èƒ½æ¡¥æ¥ç³»ç»Ÿå¤±è´¥: {e}")
            self.enhanced_performance_bridge = None

    def _init_enhanced_risk_monitor(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»Ÿ"""
        try:
            self.enhanced_risk_monitor = get_enhanced_risk_monitor()
            logger.info("å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»Ÿå¤±è´¥: {e}")
            self.enhanced_risk_monitor = None

    def _init_enhanced_event_bus(self) -> Optional[EnhancedEventBus]:
        """åˆå§‹åŒ–å¢å¼ºç‰ˆäº‹ä»¶æ€»çº¿"""
        try:
            enhanced_event_bus = get_enhanced_event_bus()

            # æ³¨å†Œæ•°æ®å¯¼å…¥ç›¸å…³çš„äº‹ä»¶å¤„ç†å™¨
            self._register_import_event_handlers(enhanced_event_bus)

            logger.info("å¢å¼ºç‰ˆäº‹ä»¶æ€»çº¿åˆå§‹åŒ–å®Œæˆ")
            return enhanced_event_bus
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆäº‹ä»¶æ€»çº¿åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    def _init_enhanced_async_manager(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨"""
        try:
            enhanced_async_manager = get_enhanced_async_manager()

            # é…ç½®ä»»åŠ¡ç®¡ç†å™¨
            enhanced_async_manager.max_workers = self.executor._max_workers

            logger.info("å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            return enhanced_async_manager
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    def start_enhanced_performance_monitoring(self):
        """å¯åŠ¨å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return False

        try:
            self.enhanced_performance_bridge.start_enhanced_monitoring()
            logger.info("å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
            return True
        except Exception as e:
            logger.error(f"å¯åŠ¨å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            return False

    def stop_enhanced_performance_monitoring(self):
        """åœæ­¢å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return False

        try:
            self.enhanced_performance_bridge.stop_enhanced_monitoring()
            logger.info("å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å·²åœæ­¢")
            return True
        except Exception as e:
            logger.error(f"åœæ­¢å¢å¼ºç‰ˆæ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            return False

    def get_enhanced_performance_summary(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç‰ˆæ€§èƒ½æ‘˜è¦"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return {
                'enhanced_performance_bridge_enabled': False,
                'message': 'å¢å¼ºç‰ˆæ€§èƒ½æ¡¥æ¥ç³»ç»Ÿæœªå¯ç”¨'
            }

        try:
            summary = self.enhanced_performance_bridge.get_performance_summary()
            summary['enhanced_performance_bridge_enabled'] = True
            return summary
        except Exception as e:
            logger.error(f"è·å–å¢å¼ºç‰ˆæ€§èƒ½æ‘˜è¦å¤±è´¥: {e}")
            return {
                'enhanced_performance_bridge_enabled': True,
                'error': str(e),
                'message': 'è·å–å¢å¼ºç‰ˆæ€§èƒ½æ‘˜è¦å¤±è´¥'
            }

    def get_performance_anomalies(self, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–æ€§èƒ½å¼‚å¸¸"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return []

        try:
            return self.enhanced_performance_bridge.get_recent_anomalies(hours)
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½å¼‚å¸¸å¤±è´¥: {e}")
            return []

    def get_performance_trends(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ€§èƒ½è¶‹åŠ¿"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return {}

        try:
            return self.enhanced_performance_bridge.get_performance_trends()
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½è¶‹åŠ¿å¤±è´¥: {e}")
            return {}

    def get_performance_optimization_suggestions(self, priority_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return []

        try:
            return self.enhanced_performance_bridge.get_optimization_suggestions(priority_filter)
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®å¤±è´¥: {e}")
            return []

    def get_metric_performance_history(self, metric_name: str, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–æŒ‡æ ‡æ€§èƒ½å†å²"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return []

        try:
            return self.enhanced_performance_bridge.get_metric_history(metric_name, hours)
        except Exception as e:
            logger.error(f"è·å–æŒ‡æ ‡æ€§èƒ½å†å²å¤±è´¥: {e}")
            return []

    def resolve_performance_anomaly(self, anomaly_id: str) -> bool:
        """è§£å†³æ€§èƒ½å¼‚å¸¸"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return False

        try:
            return self.enhanced_performance_bridge.resolve_anomaly(anomaly_id)
        except Exception as e:
            logger.error(f"è§£å†³æ€§èƒ½å¼‚å¸¸å¤±è´¥: {e}")
            return False

    def apply_performance_optimization(self, suggestion_id: str) -> bool:
        """åº”ç”¨æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return False

        try:
            return self.enhanced_performance_bridge.apply_optimization_suggestion(suggestion_id)
        except Exception as e:
            logger.error(f"åº”ç”¨æ€§èƒ½ä¼˜åŒ–å»ºè®®å¤±è´¥: {e}")
            return False

    def record_custom_performance_metric(self, metric_name: str, value: float, category: str = "custom"):
        """è®°å½•è‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡"""
        if not self.enable_enhanced_performance_bridge or not self.enhanced_performance_bridge:
            return

        try:
            # é€šè¿‡æ·±åº¦åˆ†ææœåŠ¡è®°å½•æŒ‡æ ‡ï¼Œå¢å¼ºæ¡¥æ¥ç³»ç»Ÿä¼šè‡ªåŠ¨æ”¶é›†
            self.deep_analysis_service.record_metric(metric_name, value, category)
            logger.debug(f"è®°å½•è‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡: {metric_name} = {value}")
        except Exception as e:
            logger.error(f"è®°å½•è‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")

    def get_comprehensive_performance_report(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'engine_status': {
                    'ai_optimization_enabled': self.enable_ai_optimization,
                    'intelligent_config_enabled': self.enable_intelligent_config,
                    'enhanced_performance_bridge_enabled': self.enable_enhanced_performance_bridge,
                    'enhanced_risk_monitoring_enabled': self.enable_enhanced_risk_monitoring,
                    'performance_monitoring_enabled': self.enable_performance_monitoring,
                    'anomaly_detection_enabled': self.enable_anomaly_detection,
                    'intelligent_caching_enabled': self.enable_intelligent_caching,
                    'distributed_execution_enabled': self.enable_distributed_execution,
                    'auto_tuning_enabled': self.enable_auto_tuning,
                    'data_quality_monitoring_enabled': self.enable_data_quality_monitoring
                }
            }

            # æ·»åŠ å„ä¸ªç³»ç»Ÿçš„ç»Ÿè®¡ä¿¡æ¯
            if self.enable_ai_optimization:
                report['ai_optimization_stats'] = self.get_ai_optimization_stats()

            if self.enable_performance_monitoring:
                report['performance_report'] = self.get_performance_report()

            if self.enable_intelligent_caching:
                report['cache_statistics'] = self.get_cache_statistics()

            if self.enable_distributed_execution:
                report['distributed_status'] = self.get_distributed_status()

            if self.enable_auto_tuning:
                report['auto_tuning_status'] = self.get_auto_tuning_status()

            if self.enable_data_quality_monitoring:
                report['data_quality_statistics'] = self.get_data_quality_statistics()

            if self.enable_intelligent_config:
                report['intelligent_config_statistics'] = self.get_intelligent_config_statistics()

            if self.enable_enhanced_performance_bridge:
                report['enhanced_performance_summary'] = self.get_enhanced_performance_summary()
                report['performance_anomalies'] = self.get_performance_anomalies(1)  # æœ€è¿‘1å°æ—¶
                report['performance_trends'] = self.get_performance_trends()
                report['optimization_suggestions'] = self.get_performance_optimization_suggestions('high')  # é«˜ä¼˜å…ˆçº§å»ºè®®

            if self.enable_enhanced_risk_monitoring:
                report['risk_status'] = self.get_current_risk_status()
                report['risk_alerts'] = self.get_risk_alerts(1, False)  # æœ€è¿‘1å°æ—¶æœªè§£å†³çš„é¢„è­¦
                report['risk_scenarios'] = self.get_risk_scenarios(3)  # å‰3ä¸ªé£é™©æƒ…æ™¯
                report['risk_dashboard'] = self.get_risk_dashboard_data()

            return report

        except Exception as e:
            logger.error(f"è·å–ç»¼åˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}

    # ==================== å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»ŸåŠŸèƒ½ ====================

    def start_enhanced_risk_monitoring(self):
        """å¯åŠ¨å¢å¼ºç‰ˆé£é™©ç›‘æ§"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return False

        try:
            self.enhanced_risk_monitor.start_monitoring()
            logger.info("å¢å¼ºç‰ˆé£é™©ç›‘æ§å·²å¯åŠ¨")
            return True
        except Exception as e:
            logger.error(f"å¯åŠ¨å¢å¼ºç‰ˆé£é™©ç›‘æ§å¤±è´¥: {e}")
            return False

    def stop_enhanced_risk_monitoring(self):
        """åœæ­¢å¢å¼ºç‰ˆé£é™©ç›‘æ§"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return False

        try:
            self.enhanced_risk_monitor.stop_monitoring()
            logger.info("å¢å¼ºç‰ˆé£é™©ç›‘æ§å·²åœæ­¢")
            return True
        except Exception as e:
            logger.error(f"åœæ­¢å¢å¼ºç‰ˆé£é™©ç›‘æ§å¤±è´¥: {e}")
            return False

    def get_current_risk_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰é£é™©çŠ¶æ€"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return {
                'enhanced_risk_monitoring_enabled': False,
                'message': 'å¢å¼ºç‰ˆé£é™©ç›‘æ§ç³»ç»Ÿæœªå¯ç”¨'
            }

        try:
            status = self.enhanced_risk_monitor.get_current_risk_status()
            status['enhanced_risk_monitoring_enabled'] = True
            return status
        except Exception as e:
            logger.error(f"è·å–é£é™©çŠ¶æ€å¤±è´¥: {e}")
            return {
                'enhanced_risk_monitoring_enabled': True,
                'error': str(e),
                'message': 'è·å–é£é™©çŠ¶æ€å¤±è´¥'
            }

    def get_risk_alerts(self, hours: int = 24, resolved: bool = False) -> List[Dict[str, Any]]:
        """è·å–é£é™©é¢„è­¦"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return []

        try:
            return self.enhanced_risk_monitor.get_risk_alerts(hours, resolved)
        except Exception as e:
            logger.error(f"è·å–é£é™©é¢„è­¦å¤±è´¥: {e}")
            return []

    def resolve_risk_alert(self, alert_id: str, resolution_action: str = "") -> bool:
        """è§£å†³é£é™©é¢„è­¦"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return False

        try:
            return self.enhanced_risk_monitor.resolve_alert(alert_id, resolution_action)
        except Exception as e:
            logger.error(f"è§£å†³é£é™©é¢„è­¦å¤±è´¥: {e}")
            return False

    def get_risk_scenarios(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–é£é™©æƒ…æ™¯"""
        if not self.enable_enhanced_risk_monitoring or not self.enhanced_risk_monitor:
            return []

        try:
            return self.enhanced_risk_monitor.get_risk_scenarios(limit)
        except Exception as e:
            logger.error(f"è·å–é£é™©æƒ…æ™¯å¤±è´¥: {e}")
            return []

    def get_risk_dashboard_data(self) -> Dict[str, Any]:
        """è·å–é£é™©ä»ªè¡¨æ¿æ•°æ®"""
        try:
            dashboard_data = {
                'timestamp': datetime.now().isoformat(),
                'risk_monitoring_enabled': self.enable_enhanced_risk_monitoring
            }

            if self.enable_enhanced_risk_monitoring and self.enhanced_risk_monitor:
                # å½“å‰é£é™©çŠ¶æ€
                dashboard_data['current_status'] = self.get_current_risk_status()

                # æœ€è¿‘é¢„è­¦
                dashboard_data['recent_alerts'] = self.get_risk_alerts(24, False)

                # é£é™©æƒ…æ™¯
                dashboard_data['risk_scenarios'] = self.get_risk_scenarios(5)

                # é£é™©è¶‹åŠ¿ï¼ˆæœ€è¿‘7å¤©ï¼‰
                dashboard_data['risk_trends'] = self._get_risk_trends(7)

                # é£é™©åˆ†å¸ƒ
                dashboard_data['risk_distribution'] = self._get_risk_distribution()

            return dashboard_data

        except Exception as e:
            logger.error(f"è·å–é£é™©ä»ªè¡¨æ¿æ•°æ®å¤±è´¥: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}

    def _get_risk_trends(self, days: int) -> Dict[str, List[Dict[str, Any]]]:
        """è·å–é£é™©è¶‹åŠ¿æ•°æ®"""
        try:
            if not self.enhanced_risk_monitor:
                return {}

            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„é£é™©è¶‹åŠ¿åˆ†æé€»è¾‘
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            trends = {
                'market_risk': [
                    {'date': (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d'),
                     'value': np.random.uniform(0.2, 0.8)}
                    for i in range(days, 0, -1)
                ],
                'liquidity_risk': [
                    {'date': (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d'),
                     'value': np.random.uniform(0.1, 0.6)}
                    for i in range(days, 0, -1)
                ],
                'concentration_risk': [
                    {'date': (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d'),
                     'value': np.random.uniform(0.3, 0.7)}
                    for i in range(days, 0, -1)
                ]
            }

            return trends

        except Exception as e:
            logger.error(f"è·å–é£é™©è¶‹åŠ¿å¤±è´¥: {e}")
            return {}

    def _get_risk_distribution(self) -> Dict[str, int]:
        """è·å–é£é™©åˆ†å¸ƒ"""
        try:
            if not self.enhanced_risk_monitor:
                return {}

            # è·å–å½“å‰é£é™©çŠ¶æ€ä¸­çš„åˆ†å¸ƒä¿¡æ¯
            status = self.get_current_risk_status()
            return status.get('risk_distribution', {})

        except Exception as e:
            logger.error(f"è·å–é£é™©åˆ†å¸ƒå¤±è´¥: {e}")
            return {}

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢è¿›åº¦å®šæ—¶å™¨
            if self.progress_timer.isActive():
                self.progress_timer.stop()

            # å–æ¶ˆæ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
            with self._task_lock:
                for task_id in list(self._running_tasks.keys()):
                    self.stop_task(task_id)

            # âœ… åœæ­¢æ•°æ®åº“å†™å…¥çº¿ç¨‹ï¼ˆç­‰å¾…é˜Ÿåˆ—æ¸…ç©ºï¼‰
            if hasattr(self, 'db_writer_thread'):
                logger.info("åœæ­¢DatabaseWriterThread...")
                self.db_writer_thread.stop(wait=True, timeout=30.0)
                stats = self.db_writer_thread.get_stats()
                logger.info(f"DatabaseWriterThreadç»Ÿè®¡: {stats}")

            # å…³é—­çº¿ç¨‹æ± 
            self.executor.shutdown(wait=True)

            logger.info("æ•°æ®å¯¼å…¥æ‰§è¡Œå¼•æ“æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†æ‰§è¡Œå¼•æ“å¤±è´¥: {e}")


def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    from PyQt5.QtWidgets import QApplication

    app = QApplication(sys.argv)

    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = DataImportExecutionEngine()

    # æµ‹è¯•ä»»åŠ¡é…ç½®
    from .import_config_manager import ImportTaskConfig, ImportMode, DataFrequency

    task_config = ImportTaskConfig(
        task_id="test_task_001",
        name="æµ‹è¯•Kçº¿æ•°æ®å¯¼å…¥",
        data_source="HIkyuu",
        asset_type="è‚¡ç¥¨",
        data_type="Kçº¿æ•°æ®",
        symbols=["000001", "000002"],
        frequency=DataFrequency.DAILY,
        mode=ImportMode.MANUAL
    )

    # æ·»åŠ ä»»åŠ¡é…ç½®
    engine.config_manager.add_import_task(task_config)

    # å¯åŠ¨ä»»åŠ¡
    success = engine.start_task("test_task_001")
    logger.info(f"ä»»åŠ¡å¯åŠ¨: {'æˆåŠŸ' if success else 'å¤±è´¥'}")

    # è¿è¡Œåº”ç”¨
    try:
        app.exec_()
    finally:
        engine.cleanup()


if __name__ == "__main__":
    main()
