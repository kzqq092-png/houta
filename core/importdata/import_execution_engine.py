#!/usr/bin/env python3
"""
æ•°æ®å¯¼å…¥ä»»åŠ¡æ‰§è¡Œå¼•æ“

è´Ÿè´£æ‰§è¡Œæ•°æ®å¯¼å…¥ä»»åŠ¡ï¼Œæä¾›è¿›åº¦ç›‘æ§ã€çŠ¶æ€æ›´æ–°å’Œé”™è¯¯å¤„ç†
"""

import asyncio
import logging
import threading
import time
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, Future
from dataclasses import dataclass
from enum import Enum
from PyQt5.QtCore import QObject, pyqtSignal, QTimer

from .import_config_manager import ImportConfigManager, ImportTaskConfig, ImportProgress, ImportStatus
from ..services.unified_data_manager import UnifiedDataManager
from ..real_data_provider import RealDataProvider

logger = logging.getLogger(__name__)


class TaskExecutionStatus(Enum):
    """ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class TaskExecutionResult:
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_id: str
    status: TaskExecutionStatus
    total_records: int = 0
    processed_records: int = 0
    failed_records: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    execution_time: float = 0.0

    @property
    def progress_percentage(self) -> float:
        """è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_records == 0:
            return 0.0
        return (self.processed_records / self.total_records) * 100


class DataImportExecutionEngine(QObject):
    """
    æ•°æ®å¯¼å…¥ä»»åŠ¡æ‰§è¡Œå¼•æ“

    åŠŸèƒ½ï¼š
    1. æ‰§è¡Œæ•°æ®å¯¼å…¥ä»»åŠ¡
    2. ç›‘æ§ä»»åŠ¡è¿›åº¦
    3. æä¾›çŠ¶æ€æ›´æ–°
    4. é”™è¯¯å¤„ç†å’Œé‡è¯•
    5. ä»»åŠ¡è°ƒåº¦å’Œç®¡ç†
    """

    # Qtä¿¡å·
    task_started = pyqtSignal(str)  # ä»»åŠ¡å¼€å§‹
    task_progress = pyqtSignal(str, float, str)  # ä»»åŠ¡è¿›åº¦ (task_id, progress, message)
    task_completed = pyqtSignal(str, object)  # ä»»åŠ¡å®Œæˆ (task_id, result)
    task_failed = pyqtSignal(str, str)  # ä»»åŠ¡å¤±è´¥ (task_id, error_message)

    def __init__(self, config_manager: ImportConfigManager = None,
                 data_manager: UnifiedDataManager = None,
                 max_workers: int = 4):
        super().__init__()

        # é…ç½®ç®¡ç†å™¨
        self.config_manager = config_manager or ImportConfigManager()

        # æ•°æ®ç®¡ç†å™¨ - å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…é˜»å¡
        self.data_manager = data_manager
        self._data_manager_initialized = data_manager is not None

        # çœŸå®æ•°æ®æä¾›å™¨ - å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…é˜»å¡
        self.real_data_provider = None
        self._real_data_provider_initialized = False

        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=max_workers,
                                           thread_name_prefix="ImportEngine")

        # ä»»åŠ¡ç®¡ç†
        self._running_tasks: Dict[str, Future] = {}
        self._task_results: Dict[str, TaskExecutionResult] = {}
        self._task_lock = threading.RLock()

        # è¿›åº¦ç›‘æ§å®šæ—¶å™¨
        self.progress_timer = QTimer()
        self.progress_timer.timeout.connect(self._update_progress)
        self.progress_timer.start(1000)  # æ¯ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦

        logger.info("æ•°æ®å¯¼å…¥æ‰§è¡Œå¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def _ensure_data_manager(self):
        """ç¡®ä¿æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–"""
        if not self._data_manager_initialized:
            try:
                logger.info("ğŸ”„ å»¶è¿Ÿåˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
                self.data_manager = UnifiedDataManager()
                self._data_manager_initialized = True
                logger.info("âœ… æ•°æ®ç®¡ç†å™¨å»¶è¿Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æ•°æ®ç®¡ç†å™¨æ›¿ä»£
                self.data_manager = None
                self._data_manager_initialized = False

    def _ensure_real_data_provider(self):
        """ç¡®ä¿çœŸå®æ•°æ®æä¾›å™¨å·²åˆå§‹åŒ–"""
        if not self._real_data_provider_initialized:
            try:
                logger.info("ğŸ”„ å»¶è¿Ÿåˆå§‹åŒ–çœŸå®æ•°æ®æä¾›å™¨...")
                self.real_data_provider = RealDataProvider()
                self._real_data_provider_initialized = True
                logger.info("âœ… çœŸå®æ•°æ®æä¾›å™¨å»¶è¿Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ çœŸå®æ•°æ®æä¾›å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æ›¿ä»£
                self.real_data_provider = None
                self._real_data_provider_initialized = False

    def start_task(self, task_id: str) -> bool:
        """
        å¯åŠ¨ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹å¯åŠ¨ä»»åŠ¡: {task_id}")

            # è·å–ä»»åŠ¡é…ç½®
            task_config = self.config_manager.get_import_task(task_id)
            if not task_config:
                logger.error(f"âŒ ä»»åŠ¡é…ç½®ä¸å­˜åœ¨: {task_id}")
                return False

            logger.info(f"âœ… æ‰¾åˆ°ä»»åŠ¡é…ç½®: {task_config.name}, è‚¡ç¥¨ä»£ç : {task_config.symbols}")

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²åœ¨è¿è¡Œ
            with self._task_lock:
                if task_id in self._running_tasks:
                    logger.warning(f"ä»»åŠ¡å·²åœ¨è¿è¡Œ: {task_id}")
                    return False

            # åˆ›å»ºä»»åŠ¡æ‰§è¡Œç»“æœ
            result = TaskExecutionResult(
                task_id=task_id,
                status=TaskExecutionStatus.PENDING,
                start_time=datetime.now()
            )

            with self._task_lock:
                self._task_results[task_id] = result

            # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            future = self.executor.submit(self._execute_task, task_config, result)

            with self._task_lock:
                self._running_tasks[task_id] = future

            # å‘é€ä»»åŠ¡å¼€å§‹ä¿¡å·
            self.task_started.emit(task_id)

            logger.info(f"ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {task_id}")
            return True

        except Exception as e:
            logger.error(f"å¯åŠ¨ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
            self.task_failed.emit(task_id, str(e))
            return False

    def stop_task(self, task_id: str) -> bool:
        """
        åœæ­¢ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåœæ­¢
        """
        try:
            with self._task_lock:
                if task_id not in self._running_tasks:
                    logger.warning(f"ä»»åŠ¡æœªåœ¨è¿è¡Œ: {task_id}")
                    return False

                # å–æ¶ˆä»»åŠ¡
                future = self._running_tasks[task_id]
                cancelled = future.cancel()

                if cancelled:
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    if task_id in self._task_results:
                        self._task_results[task_id].status = TaskExecutionStatus.CANCELLED
                        self._task_results[task_id].end_time = datetime.now()

                    # ç§»é™¤è¿è¡Œä¸­çš„ä»»åŠ¡
                    del self._running_tasks[task_id]

                    logger.info(f"ä»»åŠ¡åœæ­¢æˆåŠŸ: {task_id}")
                    return True
                else:
                    logger.warning(f"ä»»åŠ¡æ— æ³•å–æ¶ˆ: {task_id}")
                    return False

        except Exception as e:
            logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
            return False

    def get_task_status(self, task_id: str) -> Optional[TaskExecutionResult]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            TaskExecutionResult: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        with self._task_lock:
            return self._task_results.get(task_id)

    def get_running_tasks(self) -> List[str]:
        """è·å–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨"""
        with self._task_lock:
            return list(self._running_tasks.keys())

    def _execute_task(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """
        æ‰§è¡Œä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘

        Args:
            task_config: ä»»åŠ¡é…ç½®
            result: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_config.task_id}")
            logger.info(f"ğŸ“Š ä»»åŠ¡è¯¦æƒ…: æ•°æ®ç±»å‹={getattr(task_config, 'data_type', 'Kçº¿æ•°æ®')}, è‚¡ç¥¨={task_config.symbols}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result.status = TaskExecutionStatus.RUNNING

            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒçš„å¯¼å…¥é€»è¾‘
            data_type = getattr(task_config, 'data_type', 'Kçº¿æ•°æ®')  # é»˜è®¤ä¸ºKçº¿æ•°æ®
            logger.info(f"ğŸ”„ æ‰§è¡Œæ•°æ®ç±»å‹: {data_type}")

            if data_type == "Kçº¿æ•°æ®":
                logger.info("ğŸ“ˆ å¼€å§‹å¯¼å…¥Kçº¿æ•°æ®")
                self._import_kline_data(task_config, result)
            elif data_type == "å®æ—¶è¡Œæƒ…":
                logger.info("âš¡ å¼€å§‹å¯¼å…¥å®æ—¶è¡Œæƒ…")
                self._import_realtime_data(task_config, result)
            elif data_type == "åŸºæœ¬é¢æ•°æ®":
                logger.info("ğŸ“‹ å¼€å§‹å¯¼å…¥åŸºæœ¬é¢æ•°æ®")
                self._import_fundamental_data(task_config, result)
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨Kçº¿æ•°æ®: {data_type}")
                self._import_kline_data(task_config, result)

            # ä»»åŠ¡å®Œæˆ
            result.status = TaskExecutionStatus.COMPLETED
            result.end_time = datetime.now()
            result.execution_time = (result.end_time - result.start_time).total_seconds()

            # æ›´æ–°é…ç½®ç®¡ç†å™¨ä¸­çš„è¿›åº¦
            progress = ImportProgress(
                task_id=task_config.task_id,
                status=ImportStatus.COMPLETED,
                total_records=result.total_records,
                processed_records=result.processed_records,
                failed_records=result.failed_records,
                start_time=result.start_time.isoformat(),
                end_time=result.end_time.isoformat(),
                error_message=result.error_message
            )
            self.config_manager.update_progress(progress)

            # å‘é€å®Œæˆä¿¡å·
            self.task_completed.emit(task_config.task_id, result)

            logger.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task_config.task_id}")

        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task_config.task_id}: {e}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result.status = TaskExecutionStatus.FAILED
            result.error_message = str(e)
            result.end_time = datetime.now()

            # å‘é€å¤±è´¥ä¿¡å·
            self.task_failed.emit(task_config.task_id, str(e))

        finally:
            # æ¸…ç†è¿è¡Œä¸­çš„ä»»åŠ¡
            with self._task_lock:
                if task_config.task_id in self._running_tasks:
                    del self._running_tasks[task_config.task_id]

    def _save_kdata_to_database(self, symbol: str, kdata: 'pd.DataFrame', task_config: ImportTaskConfig):
        """ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            # è·å–DuckDBæ“ä½œå®ä¾‹
            from ..database.duckdb_operations import get_duckdb_operations
            from ..database.table_manager import get_table_manager

            duckdb_ops = get_duckdb_operations()
            table_manager = get_table_manager()

            if not duckdb_ops or not table_manager:
                logger.warning("DuckDBæ“ä½œæˆ–è¡¨ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
                return

            # ç¡®å®šè¡¨å
            frequency = task_config.frequency.value if hasattr(task_config, 'frequency') else 'D'
            table_name = f"kline_data_{frequency.lower()}"

            # ç¡®ä¿è¡¨å­˜åœ¨ - ä½¿ç”¨ç»Ÿä¸€çš„DuckDBæ•°æ®åº“
            db_path = "db/kline_stock.duckdb"
            table_manager.ensure_table_exists(db_path, table_name)

            # æ·»åŠ symbolåˆ—
            kdata_with_symbol = kdata.copy()
            kdata_with_symbol['symbol'] = symbol
            kdata_with_symbol['import_time'] = pd.Timestamp.now()

            # æ’å…¥æ•°æ®ï¼ˆä½¿ç”¨upserté¿å…é‡å¤ï¼‰
            result = duckdb_ops.insert_dataframe(
                database_path=db_path,
                table_name=table_name,
                data=kdata_with_symbol,
                upsert=True,
                conflict_columns=['symbol', 'datetime'] if 'datetime' in kdata_with_symbol.columns else ['symbol']
            )

            if result.success:
                logger.info(f"âœ… Kçº¿æ•°æ®ä¿å­˜åˆ°DuckDBæˆåŠŸ: {symbol}, {len(kdata)}æ¡è®°å½•")
            else:
                logger.error(f"âŒ Kçº¿æ•°æ®ä¿å­˜å¤±è´¥: {symbol}")

        except Exception as e:
            logger.error(f"ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥ {symbol}: {e}")

    def _save_fundamental_data_to_database(self, symbol: str, data: 'pd.DataFrame', data_type: str):
        """ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            from ..database.duckdb_operations import get_duckdb_operations
            from ..database.table_manager import get_table_manager

            duckdb_ops = get_duckdb_operations()
            table_manager = get_table_manager()

            if not duckdb_ops or not table_manager:
                logger.warning("DuckDBæ“ä½œæˆ–è¡¨ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
                return

            # ç¡®å®šè¡¨å
            table_name = f"fundamental_{data_type.lower().replace(' ', '_')}"

            # ç¡®ä¿è¡¨å­˜åœ¨ - ä½¿ç”¨ç»Ÿä¸€çš„DuckDBæ•°æ®åº“
            db_path = "db/kline_stock.duckdb"
            table_manager.ensure_table_exists(db_path, table_name)

            # æ·»åŠ symbolåˆ—
            data_with_symbol = data.copy()
            data_with_symbol['symbol'] = symbol
            data_with_symbol['import_time'] = pd.Timestamp.now()

            # æ’å…¥æ•°æ®
            result = duckdb_ops.insert_dataframe(
                database_path=db_path,
                table_name=table_name,
                data=data_with_symbol,
                upsert=True,
                conflict_columns=['symbol', 'date'] if 'date' in data_with_symbol.columns else ['symbol']
            )

            if result.success:
                logger.info(f"âœ… åŸºæœ¬é¢æ•°æ®ä¿å­˜åˆ°DuckDBæˆåŠŸ: {symbol}, {len(data)}æ¡è®°å½•")
            else:
                logger.error(f"âŒ åŸºæœ¬é¢æ•°æ®ä¿å­˜å¤±è´¥: {symbol}")

        except Exception as e:
            logger.error(f"ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥ {symbol}: {e}")

    def _save_realtime_data_to_database(self, symbol: str, data: 'pd.DataFrame'):
        """ä¿å­˜å®æ—¶æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            from ..database.duckdb_operations import get_duckdb_operations
            from ..database.table_manager import get_table_manager

            duckdb_ops = get_duckdb_operations()
            table_manager = get_table_manager()

            if not duckdb_ops or not table_manager:
                logger.warning("DuckDBæ“ä½œæˆ–è¡¨ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
                return

            # ç¡®å®šè¡¨å
            table_name = "realtime_data"

            # ç¡®ä¿è¡¨å­˜åœ¨ - ä½¿ç”¨ç»Ÿä¸€çš„DuckDBæ•°æ®åº“
            db_path = "db/kline_stock.duckdb"
            table_manager.ensure_table_exists(db_path, table_name)

            # æ·»åŠ symbolåˆ—
            data_with_symbol = data.copy()
            data_with_symbol['symbol'] = symbol
            data_with_symbol['import_time'] = pd.Timestamp.now()

            # æ’å…¥æ•°æ®ï¼ˆå®æ—¶æ•°æ®é€šå¸¸ä¸éœ€è¦upsertï¼Œç›´æ¥æ’å…¥ï¼‰
            result = duckdb_ops.insert_dataframe(
                database_path=db_path,
                table_name=table_name,
                data=data_with_symbol,
                upsert=False
            )

            if result.success:
                logger.info(f"âœ… å®æ—¶æ•°æ®ä¿å­˜åˆ°DuckDBæˆåŠŸ: {symbol}, {len(data)}æ¡è®°å½•")
            else:
                logger.error(f"âŒ å®æ—¶æ•°æ®ä¿å­˜å¤±è´¥: {symbol}")

        except Exception as e:
            logger.error(f"ä¿å­˜å®æ—¶æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥ {symbol}: {e}")

    def _import_kline_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥Kçº¿æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘ä¸‹è½½+æ‰¹é‡ä¿å­˜ï¼‰"""
        try:
            # ç¡®ä¿æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–
            self._ensure_data_manager()

            # ç¡®ä¿çœŸå®æ•°æ®æä¾›å™¨å·²åˆå§‹åŒ–
            self._ensure_real_data_provider()

            symbols = task_config.symbols
            result.total_records = len(symbols)

            logger.info(f"ğŸ“ˆ å¼€å§‹å¯¼å…¥Kçº¿æ•°æ®ï¼Œè‚¡ç¥¨åˆ—è¡¨: {symbols}")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {task_config.start_date} åˆ° {task_config.end_date}")
            logger.info(f"ğŸ“Š é¢‘ç‡: {task_config.frequency}")
            logger.info(f"ğŸš€ ä½¿ç”¨å¹¶å‘ä¸‹è½½æ¨¡å¼ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {task_config.max_workers}")

            # ä½¿ç”¨å¹¶å‘ä¸‹è½½ä¼˜åŒ–æ€§èƒ½
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading

            # ç”¨äºæ”¶é›†æ‰€æœ‰ä¸‹è½½çš„æ•°æ®
            all_kdata_list = []
            download_lock = threading.Lock()

            # è¿›åº¦è·Ÿè¸ª
            completed_count = 0
            progress_lock = threading.Lock()

            def download_single_stock(symbol: str) -> dict:
                """ä¸‹è½½å•åªè‚¡ç¥¨çš„æ•°æ®"""
                nonlocal completed_count  # å£°æ˜å¿…é¡»åœ¨å‡½æ•°å¼€å¤´
                try:
                    # å‘é€è¿›åº¦æ›´æ–°ä¿¡å·
                    self.task_progress.emit(
                        task_config.task_id,
                        (completed_count / len(symbols)) * 100,
                        f"æ­£åœ¨ä¸‹è½½ {symbol} çš„Kçº¿æ•°æ®..."
                    )

                    logger.info(f"ğŸ”„ [{completed_count + 1}/{len(symbols)}] æ­£åœ¨è·å– {symbol} çš„Kçº¿æ•°æ®...")

                    # ä½¿ç”¨çœŸå®æ•°æ®æä¾›å™¨è·å–Kçº¿æ•°æ®
                    kdata = self.real_data_provider.get_real_kdata(
                        code=symbol,
                        freq=task_config.frequency.value,
                        start_date=task_config.start_date,
                        end_date=task_config.end_date
                    )

                    # æ›´æ–°è¿›åº¦
                    with progress_lock:
                        completed_count += 1

                    if not kdata.empty:
                        # æ·»åŠ symbolåˆ—å’Œæ—¶é—´æˆ³
                        kdata_with_meta = kdata.copy()
                        kdata_with_meta['symbol'] = symbol
                        kdata_with_meta['import_time'] = pd.Timestamp.now()

                        # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°åˆ—è¡¨
                        with download_lock:
                            all_kdata_list.append(kdata_with_meta)

                        logger.info(f"âœ… [{completed_count}/{len(symbols)}] {symbol} æ•°æ®è·å–æˆåŠŸ: {len(kdata)} æ¡è®°å½•")
                        return {'symbol': symbol, 'status': 'success', 'records': len(kdata)}
                    else:
                        logger.warning(f"âš ï¸ [{completed_count}/{len(symbols)}] æœªè·å–åˆ° {symbol} çš„Kçº¿æ•°æ®")
                        return {'symbol': symbol, 'status': 'no_data', 'records': 0}

                except Exception as e:
                    with progress_lock:
                        completed_count += 1
                    logger.error(f"âŒ [{completed_count}/{len(symbols)}] å¯¼å…¥ {symbol} Kçº¿æ•°æ®å¤±è´¥: {e}")
                    return {'symbol': symbol, 'status': 'error', 'error': str(e), 'records': 0}

            # å¹¶å‘ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            max_workers = min(task_config.max_workers, len(symbols), 8)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
            logger.info(f"ğŸ”„ å¯åŠ¨ {max_workers} ä¸ªå¹¶å‘ä¸‹è½½çº¿ç¨‹...")

            download_results = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
                future_to_symbol = {executor.submit(download_single_stock, symbol): symbol
                                    for symbol in symbols}

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_symbol):
                    if result.status == TaskExecutionStatus.CANCELLED:
                        break

                    try:
                        download_result = future.result()
                        download_results.append(download_result)

                        # æ›´æ–°ä»»åŠ¡ç»“æœç»Ÿè®¡
                        if download_result['status'] == 'success':
                            result.processed_records += 1
                        else:
                            result.failed_records += 1

                    except Exception as e:
                        symbol = future_to_symbol[future]
                        logger.error(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸ {symbol}: {e}")
                        result.failed_records += 1

            # æ‰¹é‡ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ•°æ®åº“
            if all_kdata_list and result.status != TaskExecutionStatus.CANCELLED:
                logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡ä¿å­˜æ•°æ®åˆ°DuckDBï¼Œå…± {len(all_kdata_list)} åªè‚¡ç¥¨çš„æ•°æ®...")
                self._batch_save_kdata_to_database(all_kdata_list, task_config)
                logger.info(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆ")

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            success_count = sum(1 for r in download_results if r['status'] == 'success')
            failed_count = sum(1 for r in download_results if r['status'] in ['error', 'no_data'])
            total_records = sum(r.get('records', 0) for r in download_results)

            logger.info(f"ğŸ“ˆ Kçº¿æ•°æ®å¯¼å…¥å®Œæˆç»Ÿè®¡:")
            logger.info(f"  âœ… æˆåŠŸ: {success_count} åªè‚¡ç¥¨")
            logger.info(f"  âŒ å¤±è´¥: {failed_count} åªè‚¡ç¥¨")
            logger.info(f"  ğŸ“Š æ€»è®°å½•æ•°: {total_records} æ¡")

        except Exception as e:
            raise Exception(f"Kçº¿æ•°æ®å¯¼å…¥å¤±è´¥: {e}")

    def _batch_save_kdata_to_database(self, all_kdata_list: list, task_config: ImportTaskConfig):
        """æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if not all_kdata_list:
                logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
                return

            # è·å–DuckDBæ“ä½œå®ä¾‹
            from ..database.duckdb_operations import get_duckdb_operations
            from ..database.table_manager import get_table_manager

            duckdb_ops = get_duckdb_operations()
            table_manager = get_table_manager()

            if not duckdb_ops or not table_manager:
                logger.warning("DuckDBæ“ä½œæˆ–è¡¨ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
                return

            # ç¡®å®šè¡¨å
            frequency = task_config.frequency.value if hasattr(task_config, 'frequency') else 'D'
            table_name = f"kline_data_{frequency.lower()}"

            # ç¡®ä¿è¡¨å­˜åœ¨
            db_path = "db/kline_stock.duckdb"
            table_manager.ensure_table_exists(db_path, table_name)

            # åˆå¹¶æ‰€æœ‰æ•°æ®
            import pandas as pd
            combined_data = pd.concat(all_kdata_list, ignore_index=True)

            logger.info(f"ğŸ“Š å‡†å¤‡æ‰¹é‡æ’å…¥ {len(combined_data)} æ¡Kçº¿æ•°æ®è®°å½•")

            # æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆä½¿ç”¨upserté¿å…é‡å¤ï¼‰
            result = duckdb_ops.insert_dataframe(
                database_path=db_path,
                table_name=table_name,
                data=combined_data,
                batch_size=5000,  # å¢å¤§æ‰¹å¤„ç†å¤§å°ä»¥æé«˜æ€§èƒ½
                upsert=True,
                conflict_columns=['symbol', 'datetime'] if 'datetime' in combined_data.columns else ['symbol']
            )

            if result.success:
                logger.info(f"âœ… æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®æˆåŠŸ: {result.rows_inserted} æ¡è®°å½•ï¼Œè€—æ—¶: {result.execution_time:.2f}ç§’")
                if result.failed_batches:
                    logger.warning(f"âš ï¸ æœ‰ {len(result.failed_batches)} ä¸ªæ‰¹æ¬¡ä¿å­˜å¤±è´¥")
            else:
                logger.error(f"âŒ æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®å¤±è´¥: {result.error_message}")

        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _import_realtime_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥å®æ—¶è¡Œæƒ…æ•°æ®"""
        try:
            symbols = task_config.symbols
            result.total_records = len(symbols)

            for i, symbol in enumerate(symbols):
                if result.status == TaskExecutionStatus.CANCELLED:
                    break

                try:
                    # è·å–å®æ—¶è¡Œæƒ…æ•°æ®
                    quote_data = self.real_data_provider.get_real_quote(symbol)

                    if quote_data:
                        # å°†å®æ—¶æ•°æ®è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
                        if isinstance(quote_data, dict):
                            import pandas as pd
                            quote_df = pd.DataFrame([quote_data])
                            self._save_realtime_data_to_database(symbol, quote_df)
                        logger.info(f"æˆåŠŸå¯¼å…¥å¹¶ä¿å­˜ {symbol} çš„å®æ—¶è¡Œæƒ…æ•°æ®")
                        result.processed_records += 1
                    else:
                        logger.warning(f"æœªè·å–åˆ° {symbol} çš„å®æ—¶è¡Œæƒ…æ•°æ®")
                        result.failed_records += 1

                except Exception as e:
                    logger.error(f"å¯¼å…¥ {symbol} å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
                    result.failed_records += 1

                time.sleep(0.05)  # å®æ—¶æ•°æ®å¤„ç†æ›´å¿«

        except Exception as e:
            raise Exception(f"å®æ—¶è¡Œæƒ…å¯¼å…¥å¤±è´¥: {e}")

    def _import_fundamental_data(self, task_config: ImportTaskConfig, result: TaskExecutionResult):
        """å¯¼å…¥åŸºæœ¬é¢æ•°æ®"""
        try:
            symbols = task_config.symbols
            result.total_records = len(symbols)

            for i, symbol in enumerate(symbols):
                if result.status == TaskExecutionStatus.CANCELLED:
                    break

                try:
                    # è·å–åŸºæœ¬é¢æ•°æ®
                    fundamental_data = self.real_data_provider.get_real_fundamental_data(symbol)

                    if fundamental_data:
                        # å°†åŸºæœ¬é¢æ•°æ®è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
                        if isinstance(fundamental_data, (dict, list)):
                            import pandas as pd
                            if isinstance(fundamental_data, dict):
                                fund_df = pd.DataFrame([fundamental_data])
                            else:
                                fund_df = pd.DataFrame(fundamental_data)
                            self._save_fundamental_data_to_database(symbol, fund_df, "åŸºæœ¬é¢æ•°æ®")
                        logger.info(f"æˆåŠŸå¯¼å…¥å¹¶ä¿å­˜ {symbol} çš„åŸºæœ¬é¢æ•°æ®")
                        result.processed_records += 1
                    else:
                        logger.warning(f"æœªè·å–åˆ° {symbol} çš„åŸºæœ¬é¢æ•°æ®")
                        result.failed_records += 1

                except Exception as e:
                    logger.error(f"å¯¼å…¥ {symbol} åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                    result.failed_records += 1

                time.sleep(0.2)  # åŸºæœ¬é¢æ•°æ®å¤„ç†è¾ƒæ…¢

        except Exception as e:
            raise Exception(f"åŸºæœ¬é¢æ•°æ®å¯¼å…¥å¤±è´¥: {e}")

    def _update_progress(self):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        with self._task_lock:
            for task_id, result in self._task_results.items():
                if result.status == TaskExecutionStatus.RUNNING:
                    progress = result.progress_percentage
                    message = f"å·²å¤„ç† {result.processed_records}/{result.total_records} æ¡è®°å½•"

                    # å‘é€è¿›åº¦ä¿¡å·
                    self.task_progress.emit(task_id, progress, message)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢è¿›åº¦å®šæ—¶å™¨
            if self.progress_timer.isActive():
                self.progress_timer.stop()

            # å–æ¶ˆæ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
            with self._task_lock:
                for task_id in list(self._running_tasks.keys()):
                    self.stop_task(task_id)

            # å…³é—­çº¿ç¨‹æ± 
            self.executor.shutdown(wait=True)

            logger.info("æ•°æ®å¯¼å…¥æ‰§è¡Œå¼•æ“æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†æ‰§è¡Œå¼•æ“å¤±è´¥: {e}")


def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    from PyQt5.QtWidgets import QApplication

    app = QApplication(sys.argv)

    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = DataImportExecutionEngine()

    # æµ‹è¯•ä»»åŠ¡é…ç½®
    from .import_config_manager import ImportTaskConfig, ImportMode, DataFrequency

    task_config = ImportTaskConfig(
        task_id="test_task_001",
        name="æµ‹è¯•Kçº¿æ•°æ®å¯¼å…¥",
        data_source="HIkyuu",
        asset_type="è‚¡ç¥¨",
        data_type="Kçº¿æ•°æ®",
        symbols=["000001", "000002"],
        frequency=DataFrequency.DAILY,
        mode=ImportMode.MANUAL
    )

    # æ·»åŠ ä»»åŠ¡é…ç½®
    engine.config_manager.add_import_task(task_config)

    # å¯åŠ¨ä»»åŠ¡
    success = engine.start_task("test_task_001")
    print(f"ä»»åŠ¡å¯åŠ¨: {'æˆåŠŸ' if success else 'å¤±è´¥'}")

    # è¿è¡Œåº”ç”¨
    try:
        app.exec_()
    finally:
        engine.cleanup()


if __name__ == "__main__":
    main()
