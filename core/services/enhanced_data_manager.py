from loguru import logger
"""
å¢å¼ºæ•°æ®ç®¡ç†å™¨å®ç°

åŸºäºç°æœ‰UnifiedDataManageræ‰©å±•ï¼Œé›†æˆå¤šæ•°æ®æºæ’ä»¶æ•°æ®å­˜å‚¨æ¶æ„ï¼Œ
ç¡®ä¿ä¸ç°æœ‰ç³»ç»Ÿæ— ç¼é›†æˆï¼Œé¿å…é‡å¤è®¾è®¡ã€‚

ç‰ˆæœ¬: 2.0
ä½œè€…: FactorWeave-Quant Team
æ—¥æœŸ: 2025-01-27
"""

import sqlite3
import pandas as pd
import duckdb
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, date
from pathlib import Path
import json
import threading
from dataclasses import dataclass
from decimal import Decimal

# å¯¼å…¥ç°æœ‰ç³»ç»Ÿç»„ä»¶
from .unified_data_manager import UnifiedDataManager
from ..tet_data_pipeline import TETDataPipeline, StandardQuery, StandardData
from ..plugin_types import AssetType, DataType
from ..data_source_extensions import IDataSourcePlugin
from ..data.enhanced_models import (
    EnhancedStockInfo, EnhancedKlineData, FinancialStatement,
    MacroEconomicData, DataQualityMetrics, generate_table_name
)

logger = logger


@dataclass
class EnhancedStandardQuery(StandardQuery):
    """å¢å¼ºæ ‡å‡†æŸ¥è¯¢ï¼ˆæ‰©å±•ç°æœ‰StandardQueryï¼‰"""

    # æ–°å¢å­—æ®µ
    data_quality_threshold: float = 0.8      # æ•°æ®è´¨é‡é˜ˆå€¼
    enable_cache: bool = True                # æ˜¯å¦å¯ç”¨ç¼“å­˜
    cache_ttl_minutes: int = 5               # ç¼“å­˜TTL
    enable_validation: bool = True           # æ˜¯å¦å¯ç”¨æ•°æ®éªŒè¯
    output_format: str = "pandas"            # è¾“å‡ºæ ¼å¼
    include_metadata: bool = True            # æ˜¯å¦åŒ…å«å…ƒæ•°æ®

    # é«˜çº§æŸ¥è¯¢å‚æ•°
    aggregation_level: Optional[str] = None  # èšåˆçº§åˆ«
    filters: Dict[str, Any] = None           # è¿‡æ»¤æ¡ä»¶
    sort_by: Optional[str] = None            # æ’åºå­—æ®µ
    limit: Optional[int] = None              # è®°å½•é™åˆ¶

    def __post_init__(self):
        super().__post_init__()
        if self.filters is None:
            self.filters = {}


class PluginTableManager:
    """æ’ä»¶è¡¨ç®¡ç†å™¨"""

    def __init__(self, duckdb_path: str = "data/factorweave_analytics.duckdb"):
        self.duckdb_path = Path(duckdb_path)
        self.duckdb_path.parent.mkdir(parents=True, exist_ok=True)
        self._lock = threading.Lock()
        self._conn = None

    def get_connection(self):
        """è·å–DuckDBè¿æ¥"""
        if self._conn is None:
            self._conn = duckdb.connect(str(self.duckdb_path))
        return self._conn

    def create_plugin_tables(self, plugin: IDataSourcePlugin) -> bool:
        """ä¸ºæ’ä»¶åˆ›å»ºä¸“ç”¨æ•°æ®è¡¨"""
        try:
            with self._lock:
                plugin_name = plugin.plugin_info.id
                supported_data_types = plugin.plugin_info.supported_data_types

                conn = self.get_connection()

                for data_type in supported_data_types:
                    if data_type == DataType.HISTORICAL_KLINE:
                        self._create_kline_tables(conn, plugin_name)
                    elif data_type == DataType.FUNDAMENTAL:
                        self._create_fundamental_table(conn, plugin_name)
                    elif data_type == DataType.FINANCIAL_STATEMENT:
                        self._create_financial_table(conn, plugin_name)
                    elif data_type == DataType.MACRO_ECONOMIC:
                        self._create_macro_table(conn, plugin_name)

                logger.info(f" æ’ä»¶è¡¨åˆ›å»ºå®Œæˆ: {plugin_name}")
                return True

        except Exception as e:
            logger.error(f" åˆ›å»ºæ’ä»¶è¡¨å¤±è´¥: {e}")
            return False

    def _create_kline_tables(self, conn, plugin_name: str):
        """åˆ›å»ºKçº¿æ•°æ®è¡¨"""
        periods = ['1m', '5m', '15m', '30m', '1h', '1d', '1w', '1M']

        for period in periods:
            table_name = generate_table_name(plugin_name, "kline_data", period)

            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            if self._table_exists(conn, table_name):
                continue

            sql = f"""
            CREATE TABLE {table_name} (
                symbol VARCHAR NOT NULL,
                datetime TIMESTAMP NOT NULL,
                
                -- åŸºç¡€OHLCV
                open DECIMAL(12,4) NOT NULL,
                high DECIMAL(12,4) NOT NULL,
                low DECIMAL(12,4) NOT NULL,
                close DECIMAL(12,4) NOT NULL,
                volume BIGINT NOT NULL,
                amount DECIMAL(20,2),
                
                -- å¤æƒæ•°æ®ï¼ˆWindæ ‡å‡†ï¼‰
                adj_close DECIMAL(12,4),
                adj_open DECIMAL(12,4),
                adj_high DECIMAL(12,4),
                adj_low DECIMAL(12,4),
                adj_factor DECIMAL(10,6),
                
                -- æŠ€æœ¯æŒ‡æ ‡é¢„è®¡ç®—
                ma5 DECIMAL(12,4),
                ma10 DECIMAL(12,4),
                ma20 DECIMAL(12,4),
                ma60 DECIMAL(12,4),
                rsi_14 DECIMAL(8,4),
                macd_dif DECIMAL(8,4),
                macd_dea DECIMAL(8,4),
                macd_histogram DECIMAL(8,4),
                kdj_k DECIMAL(8,4),
                kdj_d DECIMAL(8,4),
                kdj_j DECIMAL(8,4),
                
                -- å¸‚åœºå¾®è§‚ç»“æ„ï¼ˆBloombergæ ‡å‡†ï¼‰
                vwap DECIMAL(12,4),
                twap DECIMAL(12,4),
                bid_price DECIMAL(12,4),
                ask_price DECIMAL(12,4),
                bid_volume BIGINT,
                ask_volume BIGINT,
                spread DECIMAL(8,4),
                
                -- èµ„é‡‘æµå‘ï¼ˆä¸œæ–¹è´¢å¯Œæ ‡å‡†ï¼‰
                net_inflow_large DECIMAL(20,2),
                net_inflow_medium DECIMAL(20,2),
                net_inflow_small DECIMAL(20,2),
                net_inflow_main DECIMAL(20,2),
                
                -- å¸‚åœºæƒ…ç»ª
                turnover_rate DECIMAL(8,4),
                amplitude DECIMAL(8,4),
                change_pct DECIMAL(8,4),
                change_amount DECIMAL(8,4),
                
                -- æ‰©å±•å­—æ®µ
                plugin_specific_data JSON,
                
                -- å…ƒæ•°æ®
                data_source VARCHAR NOT NULL,
                data_quality_score DECIMAL(4,3),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                
                PRIMARY KEY (symbol, datetime)
            );
            """

            conn.execute(sql)

            # åˆ›å»ºç´¢å¼•
            conn.execute(f"CREATE INDEX idx_{table_name}_symbol_datetime ON {table_name}(symbol, datetime);")
            conn.execute(f"CREATE INDEX idx_{table_name}_datetime ON {table_name}(datetime);")
            conn.execute(f"CREATE INDEX idx_{table_name}_data_source ON {table_name}(data_source);")

            logger.info(f" åˆ›å»ºKçº¿è¡¨: {table_name}")

    def _create_fundamental_table(self, conn, plugin_name: str):
        """åˆ›å»ºåŸºæœ¬é¢æ•°æ®è¡¨"""
        table_name = generate_table_name(plugin_name, "stock_fundamental")

        if self._table_exists(conn, table_name):
            return

        sql = f"""
        CREATE TABLE {table_name} (
            symbol VARCHAR NOT NULL,
            trade_date DATE NOT NULL,
            
            -- åŸºæœ¬ä¿¡æ¯
            name VARCHAR,
            market VARCHAR,
            exchange VARCHAR,
            industry_l1 VARCHAR,
            industry_l2 VARCHAR,
            industry_l3 VARCHAR,
            sector VARCHAR,
            concept_plates VARCHAR[],
            
            -- ä¸Šå¸‚ä¿¡æ¯
            list_date DATE,
            delist_date DATE,
            list_status VARCHAR,
            
            -- å¸‚å€¼ä¿¡æ¯
            total_shares BIGINT,
            float_shares BIGINT,
            market_cap DECIMAL(20,2),
            float_market_cap DECIMAL(20,2),
            
            -- ä¼°å€¼æŒ‡æ ‡
            pe_ratio DECIMAL(10,4),
            pb_ratio DECIMAL(10,4),
            ps_ratio DECIMAL(10,4),
            pcf_ratio DECIMAL(10,4),
            ev_ebitda DECIMAL(10,4),
            
            -- ç›ˆåˆ©èƒ½åŠ›
            roe DECIMAL(8,4),
            roa DECIMAL(8,4),
            gross_margin DECIMAL(8,4),
            net_margin DECIMAL(8,4),
            
            -- é£é™©æŒ‡æ ‡
            beta DECIMAL(8,6),
            volatility_30d DECIMAL(8,6),
            volatility_252d DECIMAL(8,6),
            
            -- æŠ€æœ¯æŒ‡æ ‡
            ma5 DECIMAL(10,4),
            ma10 DECIMAL(10,4),
            ma20 DECIMAL(10,4),
            ma60 DECIMAL(10,4),
            
            -- æ‰©å±•å­—æ®µ
            plugin_specific_data JSON,
            
            -- å…ƒæ•°æ®
            data_source VARCHAR NOT NULL,
            data_quality_score DECIMAL(4,3),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            PRIMARY KEY (symbol, trade_date)
        );
        """

        conn.execute(sql)

        # åˆ›å»ºç´¢å¼•
        conn.execute(f"CREATE INDEX idx_{table_name}_symbol_date ON {table_name}(symbol, trade_date);")
        conn.execute(f"CREATE INDEX idx_{table_name}_industry ON {table_name}(industry_l1, industry_l2);")
        conn.execute(f"CREATE INDEX idx_{table_name}_market ON {table_name}(market);")

        logger.info(f" åˆ›å»ºåŸºæœ¬é¢è¡¨: {table_name}")

    def _create_financial_table(self, conn, plugin_name: str):
        """åˆ›å»ºè´¢åŠ¡æŠ¥è¡¨æ•°æ®è¡¨"""
        table_name = generate_table_name(plugin_name, "financial_statements")

        if self._table_exists(conn, table_name):
            return

        sql = f"""
        CREATE TABLE {table_name} (
            symbol VARCHAR NOT NULL,
            report_date DATE NOT NULL,
            report_type VARCHAR NOT NULL, -- annual/semi_annual/quarterly/monthly
            
            -- èµ„äº§è´Ÿå€ºè¡¨
            total_assets DECIMAL(20,2),
            total_liabilities DECIMAL(20,2),
            shareholders_equity DECIMAL(20,2),
            current_assets DECIMAL(20,2),
            current_liabilities DECIMAL(20,2),
            cash_and_equivalents DECIMAL(20,2),
            
            -- åˆ©æ¶¦è¡¨
            total_revenue DECIMAL(20,2),
            operating_revenue DECIMAL(20,2),
            operating_cost DECIMAL(20,2),
            gross_profit DECIMAL(20,2),
            operating_profit DECIMAL(20,2),
            net_profit DECIMAL(20,2),
            net_profit_parent DECIMAL(20,2),
            
            -- ç°é‡‘æµé‡è¡¨
            operating_cash_flow DECIMAL(20,2),
            investing_cash_flow DECIMAL(20,2),
            financing_cash_flow DECIMAL(20,2),
            free_cash_flow DECIMAL(20,2),
            
            -- è´¢åŠ¡æ¯”ç‡
            current_ratio DECIMAL(8,4),
            quick_ratio DECIMAL(8,4),
            debt_to_equity DECIMAL(8,4),
            interest_coverage DECIMAL(8,4),
            
            -- æ‰©å±•å­—æ®µ
            plugin_specific_data JSON,
            
            -- å…ƒæ•°æ®
            data_source VARCHAR NOT NULL,
            data_quality_score DECIMAL(4,3),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            PRIMARY KEY (symbol, report_date, report_type)
        );
        """

        conn.execute(sql)

        # åˆ›å»ºç´¢å¼•
        conn.execute(f"CREATE INDEX idx_{table_name}_symbol_date ON {table_name}(symbol, report_date);")
        conn.execute(f"CREATE INDEX idx_{table_name}_report_date ON {table_name}(report_date);")
        conn.execute(f"CREATE INDEX idx_{table_name}_report_type ON {table_name}(report_type);")

        logger.info(f" åˆ›å»ºè´¢åŠ¡æŠ¥è¡¨è¡¨: {table_name}")

    def _create_macro_table(self, conn, plugin_name: str):
        """åˆ›å»ºå®è§‚ç»æµæ•°æ®è¡¨"""
        table_name = generate_table_name(plugin_name, "macro_economic")

        if self._table_exists(conn, table_name):
            return

        sql = f"""
        CREATE TABLE {table_name} (
            indicator_code VARCHAR NOT NULL,
            date DATE NOT NULL,
            
            -- åŸºæœ¬ä¿¡æ¯
            indicator_name VARCHAR NOT NULL,
            value DECIMAL(20,6),
            unit VARCHAR,
            frequency VARCHAR, -- daily/weekly/monthly/quarterly/yearly
            
            -- åˆ†ç±»ä¿¡æ¯
            category_l1 VARCHAR,
            category_l2 VARCHAR,
            category_l3 VARCHAR,
            
            -- åœ°åŒºä¿¡æ¯
            country VARCHAR,
            region VARCHAR,
            
            -- æ•°æ®å±æ€§
            is_seasonally_adjusted BOOLEAN DEFAULT FALSE,
            is_preliminary BOOLEAN DEFAULT FALSE,
            revision_count INTEGER DEFAULT 0,
            
            -- æ‰©å±•å­—æ®µ
            plugin_specific_data JSON,
            
            -- å…ƒæ•°æ®
            data_source VARCHAR NOT NULL,
            data_quality_score DECIMAL(4,3),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            PRIMARY KEY (indicator_code, date)
        );
        """

        conn.execute(sql)

        # åˆ›å»ºç´¢å¼•
        conn.execute(f"CREATE INDEX idx_{table_name}_indicator_date ON {table_name}(indicator_code, date);")
        conn.execute(f"CREATE INDEX idx_{table_name}_category ON {table_name}(category_l1, category_l2);")
        conn.execute(f"CREATE INDEX idx_{table_name}_country_region ON {table_name}(country, region);")

        logger.info(f" åˆ›å»ºå®è§‚æ•°æ®è¡¨: {table_name}")

    def _table_exists(self, conn, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            result = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                [table_name]
            ).fetchone()
            return result is not None
        except:
            return False


class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, sqlite_path: str = "data/factorweave_system.sqlite"):
        self.sqlite_path = Path(sqlite_path)
        self.sqlite_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_tables()

    def _init_tables(self):
        """åˆå§‹åŒ–æ•°æ®è´¨é‡ç›‘æ§è¡¨"""
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS data_quality_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        plugin_name TEXT NOT NULL,
                        table_name TEXT NOT NULL,
                        metric_date DATE NOT NULL,
                        
                        -- å®Œæ•´æ€§æŒ‡æ ‡
                        total_records INTEGER DEFAULT 0,
                        null_records INTEGER DEFAULT 0,
                        duplicate_records INTEGER DEFAULT 0,
                        completeness_score DECIMAL(5,4) DEFAULT 0,
                        
                        -- å‡†ç¡®æ€§æŒ‡æ ‡
                        validation_errors INTEGER DEFAULT 0,
                        format_errors INTEGER DEFAULT 0,
                        range_errors INTEGER DEFAULT 0,
                        accuracy_score DECIMAL(5,4) DEFAULT 0,
                        
                        -- åŠæ—¶æ€§æŒ‡æ ‡
                        data_delay_minutes INTEGER DEFAULT 0,
                        timeliness_score DECIMAL(5,4) DEFAULT 0,
                        
                        -- ä¸€è‡´æ€§æŒ‡æ ‡
                        consistency_errors INTEGER DEFAULT 0,
                        consistency_score DECIMAL(5,4) DEFAULT 0,
                        
                        -- ç»¼åˆè¯„åˆ†
                        overall_score DECIMAL(5,4) DEFAULT 0,
                        
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(plugin_name, table_name, metric_date)
                    )
                """)

                logger.info("æ•°æ®è´¨é‡ç›‘æ§è¡¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f" æ•°æ®è´¨é‡ç›‘æ§è¡¨åˆå§‹åŒ–å¤±è´¥: {e}")

    def start_monitoring(self) -> bool:
        """å¯åŠ¨æ•°æ®è´¨é‡ç›‘æ§"""
        try:
            logger.info("å¯åŠ¨æ•°æ®è´¨é‡ç›‘æ§...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®šæ—¶ç›‘æ§é€»è¾‘
            # ç›®å‰åªæ˜¯æ ‡è®°ç›‘æ§å·²å¯åŠ¨
            self._monitoring_active = True
            logger.info("æ•°æ®è´¨é‡ç›‘æ§å·²å¯åŠ¨")
            return True
        except Exception as e:
            logger.error(f"å¯åŠ¨æ•°æ®è´¨é‡ç›‘æ§å¤±è´¥: {e}")
            return False

    def validate_data_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®è´¨é‡

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®

        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        try:
            result = {
                'overall_quality': 'good',
                'completeness': 0.95,
                'accuracy': 0.98,
                'consistency': 0.92,
                'timeliness': 0.88,
                'issues': [],
                'recommendations': []
            }

            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if not data or len(data) == 0:
                result['completeness'] = 0.0
                result['overall_quality'] = 'poor'
                result['issues'].append('æ•°æ®ä¸ºç©º')
                result['recommendations'].append('æ£€æŸ¥æ•°æ®æºè¿æ¥')

            # æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§
            if 'records' in data:
                records = data['records']
                if isinstance(records, list) and len(records) > 0:
                    # æ¨¡æ‹Ÿæ•°æ®è´¨é‡æ£€æŸ¥
                    null_count = sum(1 for record in records if not record)
                    if null_count > len(records) * 0.1:  # è¶…è¿‡10%çš„ç©ºè®°å½•
                        result['accuracy'] = max(0.0, 1.0 - (null_count / len(records)))
                        result['issues'].append(f'å‘ç°{null_count}æ¡ç©ºè®°å½•')
                        result['recommendations'].append('æ¸…ç†ç©ºè®°å½•')

            # æ›´æ–°æ€»ä½“è´¨é‡è¯„çº§
            avg_score = (result['completeness'] + result['accuracy'] +
                         result['consistency'] + result['timeliness']) / 4

            if avg_score >= 0.9:
                result['overall_quality'] = 'excellent'
            elif avg_score >= 0.8:
                result['overall_quality'] = 'good'
            elif avg_score >= 0.6:
                result['overall_quality'] = 'fair'
            else:
                result['overall_quality'] = 'poor'

            return result

        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {
                'overall_quality': 'error',
                'completeness': 0.0,
                'accuracy': 0.0,
                'consistency': 0.0,
                'timeliness': 0.0,
                'issues': [f'éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}'],
                'recommendations': ['æ£€æŸ¥æ•°æ®æ ¼å¼å’ŒéªŒè¯é€»è¾‘']
            }

    def calculate_quality_score(self, data: pd.DataFrame, data_type: str, 
                                data_usage: str = 'general', data_source: str = None) -> float:
        """
        è®¡ç®—æ•°æ®è´¨é‡ç»¼åˆè¯„åˆ†ï¼ˆæ™ºèƒ½æƒé‡ç³»ç»Ÿï¼‰
        
        Args:
            data: å¾…è¯„ä¼°çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹
            data_usage: æ•°æ®ç”¨é€” ('historical', 'realtime', 'backtest', 'live_trading', 'general')
            data_source: æ•°æ®æºåç§°ï¼ˆç”¨äºå¯é æ€§è°ƒæ•´ï¼‰
            
        Returns:
            ç»¼åˆè´¨é‡è¯„åˆ† (0.0-1.0)
        """
        if data is None or data.empty:
            return 0.0

        scores = {}

        # å®Œæ•´æ€§æ£€æŸ¥
        scores['completeness'] = self._check_completeness(data)

        # å‡†ç¡®æ€§æ£€æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«å¼‚å¸¸å€¼æ£€æµ‹ï¼‰
        scores['accuracy'] = self._check_accuracy_enhanced(data, data_type)

        # ä¸€è‡´æ€§æ£€æŸ¥
        scores['consistency'] = self._check_consistency(data, data_type)

        # åŠæ—¶æ€§æ£€æŸ¥
        scores['timeliness'] = self._check_timeliness(data)

        # ğŸ¯ æ™ºèƒ½æƒé‡é…ç½®ï¼šæ ¹æ®æ•°æ®ç”¨é€”åŠ¨æ€è°ƒæ•´
        weights = self._get_dynamic_weights(data_usage)
        
        # è®¡ç®—åŸºç¡€è¯„åˆ†
        base_score = sum(scores[key] * weights[key] for key in scores)
        
        # ğŸŒŸ æ•°æ®æºå¯é æ€§è°ƒæ•´ï¼ˆÂ±5%ï¼‰
        source_adjustment = self._get_source_reliability_adjustment(data_source)
        
        # æœ€ç»ˆè¯„åˆ† = åŸºç¡€è¯„åˆ† Ã— æ•°æ®æºç³»æ•°
        final_score = min(1.0, base_score * source_adjustment)

        logger.debug(f"[è´¨é‡è¯„åˆ†] ç”¨é€”:{data_usage}, åŸºç¡€:{base_score:.3f}, "
                    f"æ•°æ®æº:{data_source}, è°ƒæ•´ç³»æ•°:{source_adjustment:.2f}, "
                    f"æœ€ç»ˆ:{final_score:.3f}")

        return round(final_score, 4)
    
    def _get_dynamic_weights(self, data_usage: str) -> dict:
        """
        æ ¹æ®æ•°æ®ç”¨é€”è¿”å›æ™ºèƒ½æƒé‡é…ç½®
        
        æƒé‡ç­–ç•¥ï¼š
        - å†å²åˆ†æï¼šå‡†ç¡®æ€§>å®Œæ•´æ€§>ä¸€è‡´æ€§>åŠæ—¶æ€§
        - å®ç›˜äº¤æ˜“ï¼šåŠæ—¶æ€§>å‡†ç¡®æ€§>å®Œæ•´æ€§>ä¸€è‡´æ€§
        - å›æµ‹éªŒè¯ï¼šå‡†ç¡®æ€§>ä¸€è‡´æ€§>å®Œæ•´æ€§>åŠæ—¶æ€§
        - é€šç”¨åœºæ™¯ï¼šå¹³è¡¡é…ç½®
        """
        weight_profiles = {
            # å†å²æ•°æ®åˆ†æï¼ˆå›æµ‹ã€ç ”ç©¶ã€å­¦ä¹ ï¼‰
            'historical': {
                'completeness': 0.30,
                'accuracy': 0.40,     # æé«˜å‡†ç¡®æ€§æƒé‡
                'consistency': 0.25,   # æé«˜ä¸€è‡´æ€§æƒé‡
                'timeliness': 0.05     # ğŸ”½ å¤§å¹…é™ä½åŠæ—¶æ€§æƒé‡
            },
            
            # å›æµ‹éªŒè¯
            'backtest': {
                'completeness': 0.25,
                'accuracy': 0.35,
                'consistency': 0.30,   # å›æµ‹éœ€è¦é«˜ä¸€è‡´æ€§
                'timeliness': 0.10
            },
            
            # å®æ—¶è¡Œæƒ…
            'realtime': {
                'completeness': 0.25,
                'accuracy': 0.30,
                'consistency': 0.15,
                'timeliness': 0.30     # ğŸ”¼ æé«˜åŠæ—¶æ€§æƒé‡
            },
            
            # å®ç›˜äº¤æ˜“
            'live_trading': {
                'completeness': 0.20,
                'accuracy': 0.35,
                'consistency': 0.10,
                'timeliness': 0.35     # ğŸ”¼ æœ€é«˜åŠæ—¶æ€§æƒé‡
            },
            
            # é€šç”¨åœºæ™¯ï¼ˆé»˜è®¤ï¼‰
            'general': {
                'completeness': 0.30,
                'accuracy': 0.30,
                'consistency': 0.20,
                'timeliness': 0.20
            }
        }
        
        weights = weight_profiles.get(data_usage, weight_profiles['general'])
        logger.debug(f"[æƒé‡é…ç½®] ç”¨é€”:{data_usage}, æƒé‡:{weights}")
        return weights
    
    def _get_source_reliability_adjustment(self, data_source: str) -> float:
        """
        æ•°æ®æºå¯é æ€§ç³»æ•°
        
        åŸºäºæ•°æ®æºçš„å†å²è¡¨ç°å’Œä¸šç•Œå£ç¢‘è°ƒæ•´è¯„åˆ†
        ç³»æ•°èŒƒå›´ï¼š0.95-1.05 (Â±5%)
        """
        if not data_source:
            return 1.0
        
        # æ•°æ®æºå¯é æ€§è¯„çº§ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        source_reliability = {
            # é«˜å¯é æ€§æ•°æ®æº (+3~5%)
            'tushare': 1.05,           # ä¸“ä¸šé‡‘èæ•°æ®
            'wind': 1.05,              # Windä¸‡å¾—
            'tongdaxin': 1.03,         # é€šè¾¾ä¿¡
            
            # æ ‡å‡†å¯é æ€§æ•°æ®æº (0~2%)
            'akshare': 1.00,           # å¼€æºæ•°æ®
            'baostock': 1.00,
            'eastmoney': 1.02,         # ä¸œæ–¹è´¢å¯Œ
            
            # å¾…éªŒè¯æ•°æ®æº (-2~0%)
            'unknown': 0.98,
            'test': 0.95,
        }
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
        source_lower = data_source.lower()
        
        # æ¨¡ç³ŠåŒ¹é…
        for key, coefficient in source_reliability.items():
            if key in source_lower:
                logger.debug(f"[æ•°æ®æºè°ƒæ•´] {data_source} -> ç³»æ•°:{coefficient}")
                return coefficient
        
        # é»˜è®¤æ ‡å‡†ç³»æ•°
        return 1.0

    def _check_completeness(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        if data.empty:
            return 0.0

        total_cells = data.size
        null_cells = data.isnull().sum().sum()

        completeness = (total_cells - null_cells) / total_cells
        return completeness

    def _check_accuracy(self, data: pd.DataFrame, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§ï¼ˆä¿ç•™æ—§æ–¹æ³•ä»¥å…¼å®¹ï¼‰"""
        return self._check_accuracy_enhanced(data, data_type)
    
    def _check_accuracy_enhanced(self, data: pd.DataFrame, data_type: str) -> float:
        """
        æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ–°å¢æ£€æµ‹é¡¹ï¼š
        1. OHLCé€»è¾‘å…³ç³»
        2. æˆäº¤é‡åˆæ³•æ€§
        3. ğŸ†• ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨æ£€æµ‹
        4. ğŸ†• æˆäº¤é‡å¼‚å¸¸æ£€æµ‹
        5. ğŸ†• é›¶å€¼/é‡å¤å€¼æ£€æµ‹
        """
        accuracy_score = 1.0
        
        if data_type == "kline" or "kline" in data_type:
            # ============ åŸæœ‰æ£€æŸ¥é¡¹ ============
            
            # 1. Kçº¿æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥
            required_cols = ['open', 'high', 'low', 'close']
            if all(col in data.columns for col in required_cols):
                # æ£€æŸ¥OHLCé€»è¾‘å…³ç³»
                invalid_ohlc = (
                    (data['high'] < data['open']) |
                    (data['high'] < data['close']) |
                    (data['low'] > data['open']) |
                    (data['low'] > data['close']) |
                    (data['high'] < data['low'])  # æ–°å¢ï¼šæœ€é«˜ä»·<æœ€ä½ä»·
                )

                if invalid_ohlc.any():
                    error_rate = invalid_ohlc.sum() / len(data)
                    accuracy_score -= error_rate * 0.5
                    logger.debug(f"[å‡†ç¡®æ€§] OHLCé€»è¾‘é”™è¯¯ç‡: {error_rate:.2%}")

            # 2. æ£€æŸ¥æˆäº¤é‡æ˜¯å¦ä¸ºè´Ÿæ•°
            if 'volume' in data.columns:
                negative_volume = (data['volume'] < 0).sum()
                if negative_volume > 0:
                    error_rate = negative_volume / len(data)
                    accuracy_score -= error_rate * 0.3
                    logger.debug(f"[å‡†ç¡®æ€§] è´Ÿæˆäº¤é‡é”™è¯¯ç‡: {error_rate:.2%}")
            
            # ============ æ–°å¢æ£€æŸ¥é¡¹ ============
            
            # 3. ğŸ†• ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨æ£€æµ‹ï¼ˆå•æ—¥æ¶¨è·Œå¹…è¶…è¿‡30%è§†ä¸ºå¼‚å¸¸ï¼‰
            if 'close' in data.columns and len(data) > 1:
                try:
                    close_pct_change = data['close'].pct_change().abs()
                    extreme_changes = close_pct_change > 0.30  # 30%é˜ˆå€¼
                    if extreme_changes.any():
                        # æ’é™¤åœç‰Œå¤ç‰Œç­‰æ­£å¸¸æƒ…å†µï¼ˆè¿ç»­å¤šæ—¥å¼‚å¸¸æ‰æ‰£åˆ†ï¼‰
                        extreme_count = extreme_changes.sum()
                        if extreme_count > len(data) * 0.02:  # è¶…è¿‡2%çš„æ•°æ®å¼‚å¸¸
                            error_rate = extreme_count / len(data)
                            accuracy_score -= error_rate * 0.15
                            logger.debug(f"[å‡†ç¡®æ€§] ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨: {extreme_count}æ¡ ({error_rate:.2%})")
                except Exception as e:
                    logger.debug(f"[å‡†ç¡®æ€§] ä»·æ ¼æ³¢åŠ¨æ£€æµ‹å¤±è´¥: {e}")
            
            # 4. ğŸ†• æˆäº¤é‡å¼‚å¸¸æ£€æµ‹ï¼ˆæˆäº¤é‡çªå¢10å€ä»¥ä¸Šï¼‰
            if 'volume' in data.columns and len(data) > 5:
                try:
                    volume_mean = data['volume'].rolling(window=5).mean()
                    volume_ratio = data['volume'] / volume_mean
                    extreme_volume = volume_ratio > 10  # 10å€é˜ˆå€¼
                    if extreme_volume.any():
                        extreme_count = extreme_volume.sum()
                        if extreme_count > len(data) * 0.01:  # è¶…è¿‡1%
                            error_rate = extreme_count / len(data)
                            accuracy_score -= error_rate * 0.10
                            logger.debug(f"[å‡†ç¡®æ€§] æˆäº¤é‡å¼‚å¸¸: {extreme_count}æ¡ ({error_rate:.2%})")
                except Exception as e:
                    logger.debug(f"[å‡†ç¡®æ€§] æˆäº¤é‡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            
            # 5. ğŸ†• é›¶å€¼æ£€æµ‹ï¼ˆæ”¶ç›˜ä»·ä¸º0é€šå¸¸æ˜¯é”™è¯¯æ•°æ®ï¼‰
            if 'close' in data.columns:
                zero_prices = (data['close'] == 0).sum()
                if zero_prices > 0:
                    error_rate = zero_prices / len(data)
                    accuracy_score -= error_rate * 0.4
                    logger.debug(f"[å‡†ç¡®æ€§] é›¶ä»·æ ¼æ•°æ®: {zero_prices}æ¡ ({error_rate:.2%})")
            
            # 6. ğŸ†• ä»·æ ¼ç›¸ç­‰æ£€æµ‹ï¼ˆOHLCå…¨éƒ¨ç›¸ç­‰å¯èƒ½æ˜¯åœç‰Œæ•°æ®ï¼‰
            if all(col in data.columns for col in ['open', 'high', 'low', 'close']):
                all_equal = (
                    (data['open'] == data['high']) &
                    (data['high'] == data['low']) &
                    (data['low'] == data['close'])
                )
                equal_count = all_equal.sum()
                if equal_count > len(data) * 0.2:  # è¶…è¿‡20%è§†ä¸ºå¼‚å¸¸
                    error_rate = equal_count / len(data) - 0.2  # å…è®¸20%åœç‰Œ
                    if error_rate > 0:
                        accuracy_score -= error_rate * 0.1
                        logger.debug(f"[å‡†ç¡®æ€§] OHLCå…¨ç›¸ç­‰: {equal_count}æ¡ ({equal_count/len(data):.2%})")

        return max(0.0, accuracy_score)

    def _check_consistency(self, data: pd.DataFrame, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        consistency_score = 1.0

        # æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
        if 'datetime' in data.columns and len(data) > 1:
            data_sorted = data.sort_values('datetime')
            time_diffs = pd.to_datetime(data_sorted['datetime']).diff()

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„æ—¶é—´è·³è·ƒ
            if len(time_diffs) > 1:
                median_diff = time_diffs.median()
                if pd.notna(median_diff):
                    outliers = time_diffs > median_diff * 3

                    if outliers.any():
                        error_rate = outliers.sum() / len(time_diffs)
                        consistency_score -= error_rate * 0.2

        return consistency_score

    def _check_timeliness(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®åŠæ—¶æ€§"""
        if 'datetime' in data.columns and not data.empty:
            try:
                latest_time = pd.to_datetime(data['datetime']).max()
                current_time = pd.Timestamp.now()

                # è®¡ç®—æ•°æ®å»¶è¿Ÿï¼ˆåˆ†é’Ÿï¼‰
                delay_minutes = (current_time - latest_time).total_seconds() / 60

                # æ ¹æ®å»¶è¿Ÿæ—¶é—´è®¡ç®—åŠæ—¶æ€§è¯„åˆ†
                if delay_minutes <= 5:
                    return 1.0
                elif delay_minutes <= 30:
                    return 0.8
                elif delay_minutes <= 60:
                    return 0.6
                elif delay_minutes <= 1440:  # 1å¤©
                    return 0.4
                else:
                    return 0.2
            except:
                return 1.0

        return 1.0

    def record_quality_metrics(self, plugin_name: str, table_name: str,
                               data: pd.DataFrame, data_type: str,
                               data_usage: str = 'general', data_source: str = None):
        """
        è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡ï¼ˆæ”¯æŒæ™ºèƒ½æƒé‡ï¼‰
        
        Args:
            plugin_name: æ’ä»¶åç§°
            table_name: è¡¨å
            data: æ•°æ®
            data_type: æ•°æ®ç±»å‹
            data_usage: æ•°æ®ç”¨é€”ï¼ˆç”¨äºæ™ºèƒ½æƒé‡ï¼‰
            data_source: æ•°æ®æºï¼ˆç”¨äºå¯é æ€§è°ƒæ•´ï¼‰
        """
        try:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            completeness = self._check_completeness(data)
            accuracy = self._check_accuracy_enhanced(data, data_type)
            consistency = self._check_consistency(data, data_type)
            timeliness = self._check_timeliness(data)
            
            # ğŸ¯ ä½¿ç”¨æ™ºèƒ½æƒé‡è®¡ç®—ç»¼åˆè¯„åˆ†
            overall_score = self.calculate_quality_score(
                data, data_type, 
                data_usage=data_usage, 
                data_source=data_source
            )

            # ç»Ÿè®¡ä¿¡æ¯
            total_records = len(data) if data is not None else 0
            null_records = data.isnull().sum().sum() if data is not None else 0
            duplicate_records = data.duplicated().sum() if data is not None else 0

            # ä¿å­˜åˆ°æ•°æ®åº“
            with sqlite3.connect(self.sqlite_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO data_quality_metrics (
                        plugin_name, table_name, metric_date,
                        total_records, null_records, duplicate_records, completeness_score,
                        accuracy_score, timeliness_score, consistency_score, overall_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    plugin_name, table_name, date.today(),
                    total_records, null_records, duplicate_records, completeness,
                    accuracy, timeliness, consistency, overall_score
                ))
                
                logger.debug(f"[è´¨é‡è®°å½•] {plugin_name}.{table_name} - "
                           f"ç”¨é€”:{data_usage}, è¯„åˆ†:{overall_score:.3f}")

        except Exception as e:
            logger.error(f" è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")


class FieldMappingManager:
    """å­—æ®µæ˜ å°„ç®¡ç†å™¨"""

    def __init__(self, sqlite_path: str = "data/factorweave_system.sqlite"):
        self.sqlite_path = Path(sqlite_path)
        self.sqlite_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_tables()

    def _init_tables(self):
        """åˆå§‹åŒ–å­—æ®µæ˜ å°„è¡¨"""
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS field_mappings (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        plugin_name TEXT NOT NULL,
                        data_type TEXT NOT NULL,
                        source_field TEXT NOT NULL,
                        target_field TEXT NOT NULL,
                        field_type TEXT NOT NULL,
                        transform_rule TEXT DEFAULT '{}',
                        validation_rule TEXT DEFAULT '{}',
                        is_required BOOLEAN DEFAULT 0,
                        default_value TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(plugin_name, data_type, source_field)
                    )
                """)

                logger.info("å­—æ®µæ˜ å°„è¡¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f" å­—æ®µæ˜ å°„è¡¨åˆå§‹åŒ–å¤±è´¥: {e}")

    def register_plugin_mappings(self, plugin: IDataSourcePlugin) -> bool:
        """æ³¨å†Œæ’ä»¶å­—æ®µæ˜ å°„"""
        try:
            plugin_name = plugin.plugin_info.id

            # è·å–æ’ä»¶çš„å­—æ®µæ˜ å°„é…ç½®
            if hasattr(plugin.plugin_info, 'capabilities') and plugin.plugin_info.capabilities:
                field_mappings = plugin.plugin_info.capabilities.get('field_mappings', {})

                with sqlite3.connect(self.sqlite_path) as conn:
                    for data_type, mappings in field_mappings.items():
                        for source_field, target_field in mappings.items():
                            conn.execute("""
                                INSERT OR REPLACE INTO field_mappings (
                                    plugin_name, data_type, source_field, target_field, field_type
                                ) VALUES (?, ?, ?, ?, ?)
                            """, (plugin_name, data_type, source_field, target_field, 'string'))

                logger.info(f" æ’ä»¶å­—æ®µæ˜ å°„æ³¨å†Œå®Œæˆ: {plugin_name}")
                return True

        except Exception as e:
            logger.error(f" æ³¨å†Œæ’ä»¶å­—æ®µæ˜ å°„å¤±è´¥: {e}")
            return False

    def get_field_mapping(self, plugin_name: str, data_type: str) -> Dict[str, str]:
        """è·å–å­—æ®µæ˜ å°„"""
        try:
            with sqlite3.connect(self.sqlite_path) as conn:
                cursor = conn.execute("""
                    SELECT source_field, target_field FROM field_mappings
                    WHERE plugin_name = ? AND data_type = ?
                """, (plugin_name, data_type))

                mappings = {}
                for row in cursor.fetchall():
                    mappings[row[0]] = row[1]

                return mappings

        except Exception as e:
            logger.error(f" è·å–å­—æ®µæ˜ å°„å¤±è´¥: {e}")
            return {}
