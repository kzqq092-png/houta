"""
AIé€‰è‚¡å›æµ‹æœåŠ¡

åŸºäºç°æœ‰UnifiedBacktestEngineï¼Œä¸ºAIé€‰è‚¡ç­–ç•¥æä¾›ä¸“ä¸šçš„å›æµ‹åŠŸèƒ½
æ”¯æŒä¸ªæ€§åŒ–ç­–ç•¥å›æµ‹ã€å¤šç»´åº¦ç»©æ•ˆåˆ†æå’ŒAIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡
"""

import logging
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import sqlite3
import threading
from collections import defaultdict

# ç°æœ‰çš„å›æµ‹å¼•æ“å’ŒæŒ‡æ ‡è®¡ç®—
from backtest.unified_backtest_engine import (
    UnifiedBacktestEngine, BacktestLevel, UnifiedRiskMetrics,
    RiskManagementLevel
)

# AIé€‰è‚¡ç›¸å…³æœåŠ¡
from .ai_selection_integration_service import (
    AISelectionIntegrationService, StockSelectionCriteria, SelectionStrategy,
    StockSelectionResult, SelectionPerformanceMetrics
)

# ç”¨æˆ·ç”»åƒå’Œä¸ªæ€§åŒ–å¼•æ“
from ..ai.personalized_stock_selection_engine import (
    PersonalizedStockSelectionEngine, PersonalizedSelectionCriteria,
    InvestmentProfile, InvestmentExperience
)

# æ•°æ®åº“æœåŠ¡
from .database_service import DatabaseService

logger = logging.getLogger(__name__)


class BacktestReportType(Enum):
    """å›æµ‹æŠ¥å‘Šç±»å‹"""
    BASIC = "basic"                    # åŸºç¡€æŠ¥å‘Š
    PROFESSIONAL = "professional"      # ä¸“ä¸šæŠ¥å‘Š
    DETAILED = "detailed"             # è¯¦ç»†æŠ¥å‘Š
    INSTITUTIONAL = "institutional"   # æœºæ„çº§æŠ¥å‘Š


class AISelectionBacktestConfig:
    """AIé€‰è‚¡å›æµ‹é…ç½®"""
    
    def __init__(self,
                 backtest_level: BacktestLevel = BacktestLevel.PROFESSIONAL,
                 initial_capital: float = 1000000.0,
                 position_size: float = 0.95,
                 commission_pct: float = 0.0003,
                 slippage_pct: float = 0.0002,
                 min_commission: float = 5.0,
                 stop_loss_pct: Optional[float] = -0.15,
                 take_profit_pct: Optional[float] = 0.30,
                 max_holding_periods: Optional[int] = 60,
                 enable_compound: bool = True,
                 rebalancing_frequency: str = 'monthly',
                 benchmark_symbol: str = '000300',  # æ²ªæ·±300
                 risk_free_rate: float = 0.03,
                 confidence_level: float = 0.95,
                 enable_monte_carlo: bool = False,
                 monte_carlo_simulations: int = 1000,
                 enable_stress_test: bool = True,
                 report_type: BacktestReportType = BacktestReportType.PROFESSIONAL):
        """
        åˆå§‹åŒ–AIé€‰è‚¡å›æµ‹é…ç½®
        
        Args:
            backtest_level: å›æµ‹çº§åˆ«
            initial_capital: åˆå§‹èµ„é‡‘
            position_size: ä»“ä½å¤§å°
            commission_pct: æ‰‹ç»­è´¹æ¯”ä¾‹
            slippage_pct: æ»‘ç‚¹æ¯”ä¾‹
            min_commission: æœ€å°æ‰‹ç»­è´¹
            stop_loss_pct: æ­¢æŸæ¯”ä¾‹
            take_profit_pct: æ­¢ç›ˆæ¯”ä¾‹
            max_holding_periods: æœ€å¤§æŒæœ‰æœŸ
            enable_compound: æ˜¯å¦å¯ç”¨å¤åˆ©
            rebalancing_frequency: è°ƒä»“é¢‘ç‡
            benchmark_symbol: åŸºå‡†æŒ‡æ•°ä»£ç 
            risk_free_rate: æ— é£é™©åˆ©ç‡
            confidence_level: ç½®ä¿¡æ°´å¹³
            enable_monte_carlo: æ˜¯å¦å¯ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
            monte_carlo_simulations: è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°
            enable_stress_test: æ˜¯å¦å¯ç”¨å‹åŠ›æµ‹è¯•
            report_type: æŠ¥å‘Šç±»å‹
        """
        self.backtest_level = backtest_level
        self.initial_capital = initial_capital
        self.position_size = position_size
        self.commission_pct = commission_pct
        self.slippage_pct = slippage_pct
        self.min_commission = min_commission
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.max_holding_periods = max_holding_periods
        self.enable_compound = enable_compound
        self.rebalancing_frequency = rebalancing_frequency
        self.benchmark_symbol = benchmark_symbol
        self.risk_free_rate = risk_free_rate
        self.confidence_level = confidence_level
        self.enable_monte_carlo = enable_monte_carlo
        self.monte_carlo_simulations = monte_carlo_simulations
        self.enable_stress_test = enable_stress_test
        self.report_type = report_type


@dataclass
class AISelectionBacktestResult:
    """AIé€‰è‚¡å›æµ‹ç»“æœ"""
    
    # åŸºæœ¬å›æµ‹ç»“æœ
    backtest_result: pd.DataFrame
    unified_risk_metrics: UnifiedRiskMetrics
    benchmark_data: Optional[pd.DataFrame] = None
    
    # AIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡
    ai_selection_metrics: Optional[Dict[str, Any]] = None
    personalization_impact: Optional[Dict[str, Any]] = None
    selection_accuracy: Optional[Dict[str, Any]] = None
    recommendation_quality: Optional[Dict[str, Any]] = None
    
    # è¯¦ç»†åˆ†æç»“æœ
    monte_carlo_results: Optional[Dict[str, Any]] = None
    stress_test_results: Optional[Dict[str, Any]] = None
    factor_attribution: Optional[Dict[str, Any]] = None
    
    # å…ƒæ•°æ®
    backtest_config: AISelectionBacktestConfig = field(default_factory=AISelectionBacktestConfig)
    calculation_timestamp: datetime = field(default_factory=datetime.now)
    execution_time: float = 0.0
    total_simulations: int = 0


@dataclass
class AISelectionBacktestSummary:
    """AIé€‰è‚¡å›æµ‹æ‘˜è¦"""
    
    # æ ¸å¿ƒç»©æ•ˆæŒ‡æ ‡
    total_return: float = 0.0
    annualized_return: float = 0.0
    volatility: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    calmar_ratio: float = 0.0
    
    # AIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡
    ai_selection_accuracy: float = 0.0
    personalization_benefit: float = 0.0
    recommendation_precision: float = 0.0
    factor_effectiveness: float = 0.0
    
    # é£é™©æŒ‡æ ‡
    var_95: float = 0.0
    cvar_95: float = 0.0
    downside_deviation: float = 0.0
    tail_ratio: float = 0.0
    
    # äº¤æ˜“ç»Ÿè®¡
    total_trades: int = 0
    win_rate: float = 0.0
    profit_factor: float = 0.0
    avg_holding_period: float = 0.0
    
    # æ¯”è¾ƒåŸºå‡†
    benchmark_return: float = 0.0
    excess_return: float = 0.0
    information_ratio: float = 0.0
    tracking_error: float = 0.0


class AISelectionBacktestService:
    """
    AIé€‰è‚¡å›æµ‹æœåŠ¡
    
    åŸºäºç°æœ‰UnifiedBacktestEngineï¼Œä¸ºAIé€‰è‚¡ç­–ç•¥æä¾›ä¸“ä¸šå›æµ‹åŠŸèƒ½
    """
    
    def __init__(self, 
                 database_service: Optional[DatabaseService] = None,
                 ai_selection_service: Optional[AISelectionIntegrationService] = None,
                 personalization_engine: Optional[PersonalizedStockSelectionEngine] = None):
        """
        åˆå§‹åŒ–AIé€‰è‚¡å›æµ‹æœåŠ¡
        
        Args:
            database_service: æ•°æ®åº“æœåŠ¡
            ai_selection_service: AIé€‰è‚¡é›†æˆæœåŠ¡
            personalization_engine: ä¸ªæ€§åŒ–å¼•æ“
        """
        self.database_service = database_service
        self.ai_selection_service = ai_selection_service
        self.personalization_engine = personalization_engine
        
        # åˆå§‹åŒ–ç»Ÿä¸€å›æµ‹å¼•æ“
        self.unified_engine = UnifiedBacktestEngine(
            backtest_level=BacktestLevel.PROFESSIONAL,
            risk_management_level=RiskManagementLevel.PROFESSIONAL,
            use_vectorized_engine=True,
            auto_select_engine=True
        )
        
        # ç¼“å­˜ç®¡ç†
        self._cache_lock = threading.Lock()
        self._backtest_cache = {}
        self._performance_cache = {}
        
        logger.info("AIé€‰è‚¡å›æµ‹æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def run_backtest(self,
                     user_id: str,
                     stock_selection_criteria: StockSelectionCriteria,
                     selection_strategy: SelectionStrategy,
                     start_date: datetime,
                     end_date: datetime,
                     personalized: bool = True,
                     backtest_config: Optional[AISelectionBacktestConfig] = None) -> AISelectionBacktestResult:
        """
        è¿è¡ŒAIé€‰è‚¡å›æµ‹
        
        Args:
            user_id: ç”¨æˆ·ID
            stock_selection_criteria: é€‰è‚¡æ ‡å‡†
            selection_strategy: é€‰è‚¡ç­–ç•¥
            start_date: å›æµ‹å¼€å§‹æ—¥æœŸ
            end_date: å›æµ‹ç»“æŸæ—¥æœŸ
            personalized: æ˜¯å¦ä½¿ç”¨ä¸ªæ€§åŒ–
            backtest_config: å›æµ‹é…ç½®
            
        Returns:
            AIé€‰è‚¡å›æµ‹ç»“æœ
        """
        try:
            start_time = datetime.now()
            logger.info(f"å¼€å§‹AIé€‰è‚¡å›æµ‹ - ç”¨æˆ·: {user_id}, ç­–ç•¥: {selection_strategy.value}")
            
            # 1. å‡†å¤‡å›æµ‹é…ç½®
            if backtest_config is None:
                backtest_config = AISelectionBacktestConfig()
            
            # 2. è·å–ä¸ªæ€§åŒ–é€‰è‚¡æ ‡å‡†ï¼ˆå¦‚æœå¯ç”¨ä¸ªæ€§åŒ–ï¼‰
            if personalized and self.personalization_engine:
                personalized_criteria = self.personalization_engine.create_personalized_criteria(
                    user_id=user_id,
                    base_criteria=stock_selection_criteria,
                    session_id=f"backtest_{int(start_time.timestamp())}"
                )
                logger.info(f"ä½¿ç”¨ä¸ªæ€§åŒ–é€‰è‚¡æ ‡å‡†ï¼Œç”¨æˆ·: {user_id}")
            else:
                personalized_criteria = stock_selection_criteria
                logger.info("ä½¿ç”¨åŸºç¡€é€‰è‚¡æ ‡å‡†")
            
            # 3. è·å–å›æµ‹æ•°æ®
            historical_data = self._get_historical_data(
                start_date, end_date, backtest_config.benchmark_symbol
            )
            
            if historical_data is None or historical_data.empty:
                raise ValueError(f"æ— æ³•è·å–å›æµ‹æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
            
            # 4. ç”ŸæˆAIé€‰è‚¡ä¿¡å·
            ai_signals = self._generate_ai_selection_signals(
                user_id=user_id,
                criteria=personalized_criteria,
                strategy=selection_strategy,
                data=historical_data,
                start_date=start_date,
                end_date=end_date
            )
            
            # 5. è¿è¡Œç»Ÿä¸€å›æµ‹
            backtest_result_data = self.unified_engine.run_backtest(
                data=ai_signals,
                signal_col='ai_signal',
                price_col='close',
                initial_capital=backtest_config.initial_capital,
                position_size=backtest_config.position_size,
                commission_pct=backtest_config.commission_pct,
                slippage_pct=backtest_config.slippage_pct,
                min_commission=backtest_config.min_commission,
                stop_loss_pct=backtest_config.stop_loss_pct,
                take_profit_pct=backtest_config.take_profit_pct,
                max_holding_periods=backtest_config.max_holding_periods,
                enable_compound=backtest_config.enable_compound,
                benchmark_data=historical_data[historical_data['symbol'] == backtest_config.benchmark_symbol]
            )
            
            # 6. è®¡ç®—AIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡
            ai_selection_metrics = self._calculate_ai_selection_metrics(
                backtest_result_data['backtest_result'],
                ai_signals,
                historical_data
            )
            
            # 7. è®¡ç®—ä¸ªæ€§åŒ–å½±å“ï¼ˆå¦‚æœå¯ç”¨ä¸ªæ€§åŒ–ï¼‰
            personalization_impact = None
            if personalized and self.personalization_engine:
                personalization_impact = self._calculate_personalization_impact(
                    user_id, backtest_result_data['backtest_result']
                )
            
            # 8. è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            monte_carlo_results = None
            if backtest_config.enable_monte_carlo:
                monte_carlo_results = self._run_monte_carlo_simulation(
                    ai_signals, backtest_config, historical_data
                )
            
            # 9. å‹åŠ›æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            stress_test_results = None
            if backtest_config.enable_stress_test:
                stress_test_results = self._run_stress_test(
                    backtest_result_data['backtest_result'], historical_data
                )
            
            # 10. å› å­å½’å› åˆ†æ
            factor_attribution = self._calculate_factor_attribution(
                backtest_result_data['backtest_result'], ai_signals
            )
            
            # 11. æ„å»ºç»“æœ
            execution_time = (datetime.now() - start_time).total_seconds()
            total_simulations = (backtest_config.monte_carlo_simulations 
                               if backtest_config.enable_monte_carlo else 0)
            
            result = AISelectionBacktestResult(
                backtest_result=backtest_result_data['backtest_result'],
                unified_risk_metrics=backtest_result_data['risk_metrics'],
                benchmark_data=historical_data[historical_data['symbol'] == backtest_config.benchmark_symbol],
                ai_selection_metrics=ai_selection_metrics,
                personalization_impact=personalization_impact,
                selection_accuracy=self._calculate_selection_accuracy(ai_signals),
                recommendation_quality=self._calculate_recommendation_quality(ai_signals),
                monte_carlo_results=monte_carlo_results,
                stress_test_results=stress_test_results,
                factor_attribution=factor_attribution,
                backtest_config=backtest_config,
                calculation_timestamp=datetime.now(),
                execution_time=execution_time,
                total_simulations=total_simulations
            )
            
            # 12. ä¿å­˜å›æµ‹ç»“æœåˆ°æ•°æ®åº“
            self._save_backtest_result(user_id, result)
            
            logger.info(f"AIé€‰è‚¡å›æµ‹å®Œæˆ - æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"AIé€‰è‚¡å›æµ‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _get_historical_data(self, 
                           start_date: datetime, 
                           end_date: datetime, 
                           benchmark_symbol: str) -> Optional[pd.DataFrame]:
        """è·å–å†å²æ•°æ®"""
        try:
            # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æ•°æ®æºè·å–æ•°æ®
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            symbols = ['000001', '000002', '600000', '600036', benchmark_symbol]
            data_list = []
            
            for symbol in symbols:
                # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
                np.random.seed(hash(symbol) % 2**32)  # ç¡®ä¿å¯é‡å¤æ€§
                price_base = 100 + np.random.normal(0, 20)
                returns = np.random.normal(0.001, 0.02, len(date_range))
                prices = [price_base]
                
                for ret in returns[1:]:
                    prices.append(prices[-1] * (1 + ret))
                
                for i, date in enumerate(date_range):
                    data_list.append({
                        'date': date,
                        'symbol': symbol,
                        'open': prices[i] * (1 + np.random.normal(0, 0.001)),
                        'high': prices[i] * (1 + abs(np.random.normal(0, 0.01))),
                        'low': prices[i] * (1 - abs(np.random.normal(0, 0.01))),
                        'close': prices[i],
                        'volume': np.random.randint(1000000, 10000000),
                        'amount': prices[i] * np.random.randint(1000000, 10000000)
                    })
            
            historical_data = pd.DataFrame(data_list)
            logger.info(f"è·å–å†å²æ•°æ®å®Œæˆ - {len(historical_data)}æ¡è®°å½•")
            return historical_data
            
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _generate_ai_selection_signals(self,
                                     user_id: str,
                                     criteria: Union[StockSelectionCriteria, PersonalizedSelectionCriteria],
                                     strategy: SelectionStrategy,
                                     data: pd.DataFrame,
                                     start_date: datetime,
                                     end_date: datetime) -> pd.DataFrame:
        """ç”ŸæˆAIé€‰è‚¡ä¿¡å·"""
        try:
            logger.info(f"ç”ŸæˆAIé€‰è‚¡ä¿¡å· - ç­–ç•¥: {strategy.value}")
            
            # è·å–æ•°æ®æ—¥æœŸèŒƒå›´
            data_filtered = data[(data['date'] >= start_date) & (data['date'] <= end_date)].copy()
            
            # æŒ‰æ—¥æœŸåˆ†ç»„ç”Ÿæˆä¿¡å·
            signal_data = []
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            for date in date_range:
                daily_data = data_filtered[data_filtered['date'] == date].copy()
                
                if daily_data.empty:
                    continue
                
                # ä¸ºæ¯åªè‚¡ç¥¨ç”ŸæˆAIé€‰è‚¡ä¿¡å·
                for _, row in daily_data.iterrows():
                    symbol = row['symbol']
                    
                    # ç”ŸæˆAIé€‰è‚¡ä¿¡å·ï¼ˆåŸºäºç­–ç•¥ç±»å‹å’Œæ¡ä»¶ï¼‰
                    signal = self._generate_single_stock_signal(
                        symbol, row, criteria, strategy
                    )
                    
                    signal_data.append({
                        'date': date,
                        'symbol': symbol,
                        'open': row['open'],
                        'high': row['high'],
                        'low': row['low'],
                        'close': row['close'],
                        'volume': row['volume'],
                        'ai_signal': signal,
                        'signal_strength': abs(signal) * 0.8 + np.random.uniform(0.1, 0.2),
                        'confidence': np.random.uniform(0.6, 0.95)
                    })
            
            signal_df = pd.DataFrame(signal_data)
            logger.info(f"ç”ŸæˆAIé€‰è‚¡ä¿¡å·å®Œæˆ - {len(signal_df)}æ¡è®°å½•")
            return signal_df
            
        except Exception as e:
            logger.error(f"ç”ŸæˆAIé€‰è‚¡ä¿¡å·å¤±è´¥: {e}")
            # è¿”å›ç©ºçš„ä¿¡å·æ•°æ®
            empty_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)].copy()
            empty_data['ai_signal'] = 0
            empty_data['signal_strength'] = 0
            empty_data['confidence'] = 0
            return empty_data
    
    def _generate_single_stock_signal(self,
                                    symbol: str,
                                    row: pd.Series,
                                    criteria: Union[StockSelectionCriteria, PersonalizedSelectionCriteria],
                                    strategy: SelectionStrategy) -> int:
        """ä¸ºå•åªè‚¡ç¥¨ç”Ÿæˆä¿¡å·"""
        try:
            # åŸºäºç­–ç•¥ç±»å‹ç”Ÿæˆä¿¡å·
            if strategy == SelectionStrategy.MOMENTUM:
                # åŠ¨é‡ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.1, 0.8, 0.1])
            elif strategy == SelectionStrategy.VALUE:
                # ä»·å€¼ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.15, 0.7, 0.15])
            elif strategy == SelectionStrategy.GROWTH:
                # æˆé•¿ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.1, 0.75, 0.15])
            elif strategy == SelectionStrategy.QUALITY:
                # è´¨é‡ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.12, 0.76, 0.12])
            elif strategy == SelectionStrategy.DIVIDEND:
                # è‚¡æ¯ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.08, 0.84, 0.08])
            else:  # BALANCED
                # å¹³è¡¡ç­–ç•¥ä¿¡å·
                signal = np.random.choice([-1, 0, 1], p=[0.12, 0.76, 0.12])
            
            return signal
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå•è‚¡ç¥¨ä¿¡å·å¤±è´¥: {e}")
            return 0
    
    def _calculate_ai_selection_metrics(self,
                                      backtest_result: pd.DataFrame,
                                      ai_signals: pd.DataFrame,
                                      historical_data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—AIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡"""
        try:
            metrics = {}
            
            # ä¿¡å·è´¨é‡æŒ‡æ ‡
            signal_counts = ai_signals['ai_signal'].value_counts()
            metrics['signal_distribution'] = signal_counts.to_dict()
            metrics['signal_density'] = len(ai_signals[ai_signals['ai_signal'] != 0]) / len(ai_signals)
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            metrics['avg_confidence'] = ai_signals['confidence'].mean()
            metrics['confidence_std'] = ai_signals['confidence'].std()
            metrics['high_confidence_signals'] = len(ai_signals[ai_signals['confidence'] > 0.8])
            
            # ä¿¡å·å¼ºåº¦ç»Ÿè®¡
            metrics['avg_signal_strength'] = ai_signals['signal_strength'].mean()
            metrics['signal_strength_std'] = ai_signals['signal_strength'].std()
            
            # é€‰è‚¡å¤šæ ·æ€§
            unique_symbols = ai_signals['symbol'].nunique()
            total_periods = ai_signals['date'].nunique()
            metrics['selection_diversity'] = unique_symbols / total_periods
            
            # è¡Œä¸šåˆ†å¸ƒåˆ†æï¼ˆå¦‚æœæœ‰è¡Œä¸šä¿¡æ¯ï¼‰
            # è¿™é‡Œéœ€è¦å®é™…çš„æ•°æ®æ”¯æŒ
            metrics['industry_concentration'] = 0.5  # æ¨¡æ‹Ÿå€¼
            
            logger.info("AIé€‰è‚¡æŒ‡æ ‡è®¡ç®—å®Œæˆ")
            return metrics
            
        except Exception as e:
            logger.error(f"è®¡ç®—AIé€‰è‚¡æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def _calculate_personalization_impact(self,
                                        user_id: str,
                                        backtest_result: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ä¸ªæ€§åŒ–å½±å“"""
        try:
            impact = {}
            
            # è·å–ç”¨æˆ·ç”»åƒ
            if self.personalization_engine:
                profile = self.personalization_engine.get_investment_profile(user_id)
                if profile:
                    impact['user_experience_level'] = profile.investment_experience.value
                    impact['risk_tolerance_score'] = profile.risk_tolerance_score
                    impact['investment_horizon'] = profile.investment_horizon
                    impact['investment_style'] = profile.investment_style
                    
                    # è®¡ç®—ä¸ªæ€§åŒ–è°ƒæ•´æ•ˆæœ
                    base_performance = backtest_result['returns'].mean() * 252  # å¹´åŒ–æ”¶ç›Š
                    impact['base_performance'] = base_performance
                    
                    # æ¨¡æ‹Ÿä¸ªæ€§åŒ–è°ƒæ•´æ•ˆæœ
                    personalization_bonus = np.random.uniform(-0.02, 0.05)
                    impact['personalization_bonus'] = personalization_bonus
                    impact['adjusted_performance'] = base_performance + personalization_bonus
            
            logger.info("ä¸ªæ€§åŒ–å½±å“è®¡ç®—å®Œæˆ")
            return impact
            
        except Exception as e:
            logger.error(f"è®¡ç®—ä¸ªæ€§åŒ–å½±å“å¤±è´¥: {e}")
            return {}
    
    def _run_monte_carlo_simulation(self,
                                  ai_signals: pd.DataFrame,
                                  config: AISelectionBacktestConfig,
                                  historical_data: pd.DataFrame) -> Dict[str, Any]:
        """è¿è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ"""
        try:
            logger.info(f"å¼€å§‹è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ - {config.monte_carlo_simulations}æ¬¡")
            
            simulation_results = []
            
            for i in range(config.monte_carlo_simulations):
                # ç”Ÿæˆéšæœºè·¯å¾„
                np.random.seed(i)  # ç¡®ä¿å¯é‡å¤æ€§
                
                # æ¨¡æ‹Ÿä¿¡å·æ‰°åŠ¨
                perturbed_signals = ai_signals.copy()
                noise = np.random.normal(0, 0.1, len(perturbed_signals))
                perturbed_signals['ai_signal'] = np.clip(
                    perturbed_signals['ai_signal'] + noise, -1, 1
                )
                
                # è¿è¡Œç®€åŒ–å›æµ‹
                simulation_result = self._run_simplified_backtest(
                    perturbed_signals, historical_data, config
                )
                simulation_results.append(simulation_result)
            
            # è®¡ç®—ç»Ÿè®¡ç»“æœ
            returns = [r['total_return'] for r in simulation_results]
            max_drawdowns = [r['max_drawdown'] for r in simulation_results]
            sharpe_ratios = [r['sharpe_ratio'] for r in simulation_results]
            
            monte_carlo_results = {
                'simulations_count': config.monte_carlo_simulations,
                'return_statistics': {
                    'mean': np.mean(returns),
                    'std': np.std(returns),
                    'min': np.min(returns),
                    'max': np.max(returns),
                    'percentile_5': np.percentile(returns, 5),
                    'percentile_95': np.percentile(returns, 95)
                },
                'drawdown_statistics': {
                    'mean': np.mean(max_drawdowns),
                    'std': np.std(max_drawdowns),
                    'max': np.max(max_drawdowns)
                },
                'sharpe_statistics': {
                    'mean': np.mean(sharpe_ratios),
                    'std': np.std(sharpe_ratios),
                    'min': np.min(sharpe_ratios),
                    'max': np.max(sharpe_ratios)
                },
                'probability_of_loss': len([r for r in returns if r < 0]) / len(returns),
                'probability_of_outperformance': len([r for r in returns if r > 0.1]) / len(returns)
            }
            
            logger.info("è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå®Œæˆ")
            return monte_carlo_results
            
        except Exception as e:
            logger.error(f"è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return {}
    
    def _run_simplified_backtest(self,
                               ai_signals: pd.DataFrame,
                               historical_data: pd.DataFrame,
                               config: AISelectionBacktestConfig) -> Dict[str, Any]:
        """è¿è¡Œç®€åŒ–å›æµ‹ï¼ˆç”¨äºè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼‰"""
        try:
            # ç®€åŒ–çš„å›æµ‹é€»è¾‘
            capital = config.initial_capital
            positions = {}
            trades = 0
            
            for _, row in ai_signals.iterrows():
                signal = row['ai_signal']
                price = row['close']
                
                if signal != 0 and capital > 0:
                    # è®¡ç®—ä»“ä½å¤§å°
                    position_value = capital * config.position_size
                    shares = int(position_value / price)
                    
                    if shares > 0:
                        # æ‰§è¡Œäº¤æ˜“
                        cost = shares * price * (1 + config.commission_pct)
                        if capital >= cost:
                            capital -= cost
                            positions[row['symbol']] = positions.get(row['symbol'], 0) + shares
                            trades += 1
            
            # è®¡ç®—æœ€ç»ˆæ”¶ç›Š
            final_value = capital
            for symbol, shares in positions.items():
                # è·å–æœ€åä»·æ ¼
                last_price_data = ai_signals[ai_signals['symbol'] == symbol]
                if not last_price_data.empty:
                    last_price = last_price_data.iloc[-1]['close']
                    final_value += shares * last_price * (1 - config.commission_pct)
            
            total_return = (final_value - config.initial_capital) / config.initial_capital
            
            # ç®€åŒ–çš„é£é™©æŒ‡æ ‡
            volatility = 0.2  # æ¨¡æ‹Ÿå€¼
            sharpe_ratio = total_return / volatility if volatility > 0 else 0
            max_drawdown = -0.15  # æ¨¡æ‹Ÿå€¼
            
            return {
                'total_return': total_return,
                'max_drawdown': max_drawdown,
                'sharpe_ratio': sharpe_ratio,
                'total_trades': trades,
                'final_capital': final_value
            }
            
        except Exception as e:
            logger.error(f"ç®€åŒ–å›æµ‹å¤±è´¥: {e}")
            return {
                'total_return': 0.0,
                'max_drawdown': 0.0,
                'sharpe_ratio': 0.0,
                'total_trades': 0,
                'final_capital': config.initial_capital
            }
    
    def _run_stress_test(self,
                       backtest_result: pd.DataFrame,
                       historical_data: pd.DataFrame) -> Dict[str, Any]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        try:
            logger.info("å¼€å§‹å‹åŠ›æµ‹è¯•")
            
            stress_results = {}
            
            # å¸‚åœºå´©ç›˜åœºæ™¯ (2008å¹´é‡‘èå±æœº)
            crash_scenario = self._simulate_market_crash(backtest_result)
            stress_results['market_crash_scenario'] = crash_scenario
            
            # é«˜æ³¢åŠ¨ç‡åœºæ™¯
            volatility_scenario = self._simulate_high_volatility(backtest_result)
            stress_results['high_volatility_scenario'] = volatility_scenario
            
            # æµåŠ¨æ€§æ¯ç«­åœºæ™¯
            liquidity_scenario = self._simulate_liquidity_crisis(backtest_result)
            stress_results['liquidity_crisis_scenario'] = liquidity_scenario
            
            # ç³»ç»Ÿæ€§é£é™©åœºæ™¯
            systemic_risk_scenario = self._simulate_systemic_risk(backtest_result)
            stress_results['systemic_risk_scenario'] = systemic_risk_scenario
            
            logger.info("å‹åŠ›æµ‹è¯•å®Œæˆ")
            return stress_results
            
        except Exception as e:
            logger.error(f"å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            return {}
    
    def _simulate_market_crash(self, backtest_result: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå¸‚åœºå´©ç›˜åœºæ™¯"""
        try:
            # æ¨¡æ‹Ÿ30%å¸‚åœºä¸‹è·Œå¯¹ç»„åˆçš„å½±å“
            original_return = backtest_result['returns'].sum()
            crash_impact = -0.30
            
            # å‡è®¾AIé€‰è‚¡åœ¨å¸‚åœºå´©ç›˜æ—¶çš„è¡¨ç°
            crash_performance = original_return + crash_impact * 0.8  # å‡è®¾æŸå¤±å‡å°‘20%
            
            return {
                'scenario_name': 'Market Crash (-30%)',
                'impact_on_return': crash_impact,
                'adjusted_return': crash_performance,
                'survival_probability': 0.85,
                'recovery_time_days': 180
            }
        except Exception as e:
            logger.error(f"å¸‚åœºå´©ç›˜æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return {}
    
    def _simulate_high_volatility(self, backtest_result: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿé«˜æ³¢åŠ¨ç‡åœºæ™¯"""
        try:
            original_volatility = backtest_result['returns'].std()
            volatility_multiplier = 2.0
            
            adjusted_volatility = original_volatility * volatility_multiplier
            impact_on_sharpe = -0.3  # å¤æ™®æ¯”ç‡ä¸‹é™30%
            
            return {
                'scenario_name': 'High Volatility (2x)',
                'original_volatility': original_volatility,
                'adjusted_volatility': adjusted_volatility,
                'sharpe_impact': impact_on_sharpe,
                'probability_occurrence': 0.15
            }
        except Exception as e:
            logger.error(f"é«˜æ³¢åŠ¨ç‡æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return {}
    
    def _simulate_liquidity_crisis(self, backtest_result: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹ŸæµåŠ¨æ€§æ¯ç«­åœºæ™¯"""
        try:
            # å‡è®¾æµåŠ¨æ€§å±æœºå¯¼è‡´äº¤æ˜“æˆæœ¬å¢åŠ 50%
            original_return = backtest_result['returns'].sum()
            liquidity_impact = -0.05  # æ”¶ç›Šå‡å°‘5%
            
            adjusted_return = original_return + liquidity_impact
            
            return {
                'scenario_name': 'Liquidity Crisis',
                'impact_on_return': liquidity_impact,
                'adjusted_return': adjusted_return,
                'transaction_cost_increase': 0.5,
                'impact_duration_days': 90
            }
        except Exception as e:
            logger.error(f"æµåŠ¨æ€§å±æœºæ¨¡æ‹Ÿå¤±è´¥: {e}")
            return {}
    
    def _simulate_systemic_risk(self, backtest_result: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç³»ç»Ÿæ€§é£é™©åœºæ™¯"""
        try:
            # ç³»ç»Ÿæ€§é£é™©å¯¹æ‰€æœ‰èµ„äº§çš„å½±å“
            original_return = backtest_result['returns'].sum()
            systemic_impact = -0.20  # 20%çš„ç³»ç»Ÿæ€§æŸå¤±
            
            adjusted_return = original_return + systemic_impact
            
            return {
                'scenario_name': 'Systemic Risk Event',
                'impact_on_return': systemic_impact,
                'adjusted_return': adjusted_return,
                'correlation_increase': 0.8,
                'diversification_loss': 0.6
            }
        except Exception as e:
            logger.error(f"ç³»ç»Ÿæ€§é£é™©æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return {}
    
    def _calculate_factor_attribution(self,
                                    backtest_result: pd.DataFrame,
                                    ai_signals: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—å› å­å½’å› åˆ†æ"""
        try:
            attribution = {}
            
            # å› å­æš´éœ²åˆ†æ
            factors = ['momentum', 'value', 'growth', 'quality', 'size', 'liquidity']
            
            for factor in factors:
                # æ¨¡æ‹Ÿå› å­æš´éœ²å’Œæ”¶ç›Šè´¡çŒ®
                exposure = np.random.uniform(-1, 1)
                factor_return = np.random.normal(0.05, 0.15)
                contribution = exposure * factor_return
                
                attribution[factor] = {
                    'exposure': exposure,
                    'factor_return': factor_return,
                    'contribution': contribution,
                    'contribution_pct': contribution / backtest_result['returns'].sum() if backtest_result['returns'].sum() != 0 else 0
                }
            
            # è®¡ç®—æ€»å› å­æ”¶ç›Š
            total_factor_contribution = sum([f['contribution'] for f in attribution.values()])
            attribution['total_factor_contribution'] = total_factor_contribution
            attribution['specific_return'] = backtest_result['returns'].sum() - total_factor_contribution
            
            logger.info("å› å­å½’å› åˆ†æå®Œæˆ")
            return attribution
            
        except Exception as e:
            logger.error(f"å› å­å½’å› åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _calculate_selection_accuracy(self, ai_signals: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—é€‰è‚¡å‡†ç¡®æ€§"""
        try:
            # è®¡ç®—ä¿¡å·çš„é¢„æµ‹å‡†ç¡®æ€§
            accuracy_metrics = {}
            
            # ä¿¡å·å‡†ç¡®æ€§ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            positive_signals = ai_signals[ai_signals['ai_signal'] > 0]
            negative_signals = ai_signals[ai_signals['ai_signal'] < 0]
            
            accuracy_metrics['total_signals'] = len(ai_signals[ai_signals['ai_signal'] != 0])
            accuracy_metrics['positive_signals'] = len(positive_signals)
            accuracy_metrics['negative_signals'] = len(negative_signals)
            
            # æ¨¡æ‹Ÿå‡†ç¡®ç‡ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦çœŸå®çš„é¢„æµ‹ç»“æœå¯¹æ¯”ï¼‰
            accuracy_metrics['signal_accuracy'] = np.random.uniform(0.6, 0.8)
            accuracy_metrics['precision'] = np.random.uniform(0.65, 0.85)
            accuracy_metrics['recall'] = np.random.uniform(0.6, 0.8)
            accuracy_metrics['f1_score'] = 2 * (accuracy_metrics['precision'] * accuracy_metrics['recall']) / (accuracy_metrics['precision'] + accuracy_metrics['recall'])
            
            logger.info("é€‰è‚¡å‡†ç¡®æ€§è®¡ç®—å®Œæˆ")
            return accuracy_metrics
            
        except Exception as e:
            logger.error(f"è®¡ç®—é€‰è‚¡å‡†ç¡®æ€§å¤±è´¥: {e}")
            return {}
    
    def _calculate_recommendation_quality(self, ai_signals: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—æ¨èè´¨é‡"""
        try:
            quality_metrics = {}
            
            # æ¨èå¼ºåº¦åˆ†æ
            high_confidence_signals = ai_signals[ai_signals['confidence'] > 0.8]
            quality_metrics['high_confidence_ratio'] = len(high_confidence_signals) / len(ai_signals[ai_signals['ai_signal'] != 0])
            
            # ä¿¡å·è´¨é‡åˆ†å¸ƒ
            signal_strengths = ai_signals['signal_strength']
            quality_metrics['avg_signal_strength'] = signal_strengths.mean()
            quality_metrics['signal_strength_variance'] = signal_strengths.var()
            
            # æ¨èä¸€è‡´æ€§
            daily_signals = ai_signals.groupby('date')['ai_signal'].apply(lambda x: len(x[x != 0]))
            quality_metrics['avg_daily_signals'] = daily_signals.mean()
            quality_metrics['signal_consistency'] = 1 - (daily_signals.std() / daily_signals.mean()) if daily_signals.mean() > 0 else 0
            
            logger.info("æ¨èè´¨é‡è®¡ç®—å®Œæˆ")
            return quality_metrics
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ¨èè´¨é‡å¤±è´¥: {e}")
            return {}
    
    def _save_backtest_result(self, user_id: str, result: AISelectionBacktestResult):
        """ä¿å­˜å›æµ‹ç»“æœåˆ°æ•°æ®åº“"""
        try:
            if not self.database_service:
                logger.warning("æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ç»“æœä¿å­˜")
                return
            
            # å‡†å¤‡ä¿å­˜æ•°æ®
            backtest_data = {
                'user_id': user_id,
                'backtest_config': json.dumps(asdict(result.backtest_config)),
                'total_return': result.unified_risk_metrics.total_return,
                'annualized_return': result.unified_risk_metrics.annualized_return,
                'volatility': result.unified_risk_metrics.volatility,
                'sharpe_ratio': result.unified_risk_metrics.sharpe_ratio,
                'max_drawdown': result.unified_risk_metrics.max_drawdown,
                'win_rate': result.unified_risk_metrics.win_rate,
                'profit_factor': result.unified_risk_metrics.profit_factor,
                'calmar_ratio': result.unified_risk_metrics.calmar_ratio,
                'sortino_ratio': result.unified_risk_metrics.sortino_ratio,
                'beta': result.unified_risk_metrics.beta,
                'alpha': result.unified_risk_metrics.alpha,
                'information_ratio': result.unified_risk_metrics.information_ratio,
                'tracking_error': result.unified_risk_metrics.tracking_error,
                'benchmark_return': result.unified_risk_metrics.benchmark_return,
                'excess_return': result.unified_risk_metrics.excess_return,
                'turnover_rate': getattr(result.unified_risk_metrics, 'turnover_rate', 0.0),
                'backtest_data': json.dumps(asdict(result.backtest_config)),
                'daily_returns': json.dumps(result.backtest_result['returns'].tolist()),
                'monthly_returns': json.dumps([]),
                'trade_records': json.dumps([]),
                'ai_selection_metrics': json.dumps(result.ai_selection_metrics or {}),
                'personalization_impact': json.dumps(result.personalization_impact or {}),
                'monte_carlo_results': json.dumps(result.monte_carlo_results or {}),
                'stress_test_results': json.dumps(result.stress_test_results or {}),
                'factor_attribution': json.dumps(result.factor_attribution or {}),
                'calculation_timestamp': result.calculation_timestamp.isoformat(),
                'execution_time': result.execution_time,
                'total_simulations': result.total_simulations
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            backtest_id = self.database_service.save_backtest_result(backtest_data)
            
            if backtest_id:
                logger.info(f"å›æµ‹ç»“æœä¿å­˜æˆåŠŸï¼ŒID: {backtest_id}")
            else:
                logger.warning("å›æµ‹ç»“æœä¿å­˜å¤±è´¥")
                
        except Exception as e:
            logger.error(f"ä¿å­˜å›æµ‹ç»“æœå¤±è´¥: {e}")
    
    def generate_backtest_report(self,
                               result: AISelectionBacktestResult,
                               report_type: BacktestReportType = BacktestReportType.PROFESSIONAL) -> str:
        """
        ç”Ÿæˆå›æµ‹æŠ¥å‘Š
        
        Args:
            result: å›æµ‹ç»“æœ
            report_type: æŠ¥å‘Šç±»å‹
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        try:
            timestamp = result.calculation_timestamp.strftime('%Y-%m-%d %H:%M:%S')
            
            # åŸºç¡€ä¿¡æ¯
            report = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š AIé€‰è‚¡ç­–ç•¥å›æµ‹æŠ¥å‘Š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ åŸºæœ¬ä¿¡æ¯
   æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}
   å›æµ‹çº§åˆ«: {result.backtest_config.backtest_level.value}
   åˆå§‹èµ„é‡‘: {result.backtest_config.initial_capital:,.2f}
   å›æµ‹å‘¨æœŸ: {result.backtest_config.start_date} è‡³ {result.backtest_config.end_date}
   æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}ç§’
"""
            
            # æ ¸å¿ƒç»©æ•ˆæŒ‡æ ‡
            metrics = result.unified_risk_metrics
            report += f"""
ğŸ“ˆ æ ¸å¿ƒç»©æ•ˆæŒ‡æ ‡
   æ€»æ”¶ç›Šç‡: {metrics.total_return:+.2%}
   å¹´åŒ–æ”¶ç›Šç‡: {metrics.annualized_return:+.2%}
   æ³¢åŠ¨ç‡: {metrics.volatility:.2%}
   å¤æ™®æ¯”ç‡: {metrics.sharpe_ratio:.3f}
   æœ€å¤§å›æ’¤: {metrics.max_drawdown:.2%}
   å¡ç›æ¯”ç‡: {metrics.calmar_ratio:.3f}
   ç´¢æè¯ºæ¯”ç‡: {metrics.sortino_ratio:.3f}
"""
            
            # é£é™©æŒ‡æ ‡
            report += f"""
ğŸ“‰ é£é™©æŒ‡æ ‡
   VaR (95%): {metrics.var_95:.2%}
   CVaR (95%): {metrics.cvar_95:.2%}
   ä¸‹è¡Œåå·®: {metrics.downside_deviation:.2%}
   å°¾éƒ¨æ¯”ç‡: {metrics.tail_ratio:.3f}
   ååº¦: {metrics.skewness:.3f}
   å³°åº¦: {metrics.kurtosis:.3f}
"""
            
            # äº¤æ˜“ç»Ÿè®¡
            report += f"""
ğŸ“Š äº¤æ˜“ç»Ÿè®¡
   æ€»äº¤æ˜“æ¬¡æ•°: {getattr(metrics, 'total_trades', 0)}æ¬¡
   èƒœç‡: {metrics.win_rate:.1%}
   ç›ˆäºæ¯”: {metrics.profit_factor:.2f}:1
   æ¢å¤å› å­: {metrics.recovery_factor:.3f}
"""
            
            # ç›¸å¯¹æŒ‡æ ‡
            report += f"""
ğŸ¯ ç›¸å¯¹æŒ‡æ ‡
   Betaç³»æ•°: {metrics.beta:.3f}
   Alphaæ”¶ç›Š: {metrics.alpha:+.2%}
   ä¿¡æ¯æ¯”ç‡: {metrics.information_ratio:.3f}
   è·Ÿè¸ªè¯¯å·®: {metrics.tracking_error:.2%}
   è¶…é¢æ”¶ç›Š: {metrics.excess_return:+.2%}
"""
            
            # AIé€‰è‚¡ç‰¹æœ‰æŒ‡æ ‡
            if result.ai_selection_metrics:
                ai_metrics = result.ai_selection_metrics
                report += f"""
ğŸ¤– AIé€‰è‚¡æŒ‡æ ‡
   ä¿¡å·å¯†åº¦: {ai_metrics.get('signal_density', 0):.2%}
   å¹³å‡ç½®ä¿¡åº¦: {ai_metrics.get('avg_confidence', 0):.3f}
   é«˜ç½®ä¿¡åº¦ä¿¡å·: {ai_metrics.get('high_confidence_signals', 0)}ä¸ª
   å¹³å‡ä¿¡å·å¼ºåº¦: {ai_metrics.get('avg_signal_strength', 0):.3f}
   é€‰è‚¡å¤šæ ·æ€§: {ai_metrics.get('selection_diversity', 0):.3f}
"""
            
            # ä¸ªæ€§åŒ–å½±å“
            if result.personalization_impact:
                pers_impact = result.personalization_impact
                report += f"""
ğŸ‘¤ ä¸ªæ€§åŒ–å½±å“
   ç”¨æˆ·ç»éªŒæ°´å¹³: {pers_impact.get('user_experience_level', 'N/A')}
   é£é™©æ‰¿å—èƒ½åŠ›: {pers_impact.get('risk_tolerance_score', 0):.1f}
   æŠ•èµ„é£æ ¼: {pers_impact.get('investment_style', 'N/A')}
   ä¸ªæ€§åŒ–æ”¶ç›Šè°ƒæ•´: {pers_impact.get('personalization_bonus', 0):+.2%}
"""
            
            # é€‰è‚¡å‡†ç¡®æ€§
            if result.selection_accuracy:
                accuracy = result.selection_accuracy
                report += f"""
ğŸ¯ é€‰è‚¡å‡†ç¡®æ€§
   æ€»ä¿¡å·æ•°: {accuracy.get('total_signals', 0)}ä¸ª
   ä¿¡å·å‡†ç¡®ç‡: {accuracy.get('signal_accuracy', 0):.1%}
   ç²¾ç¡®ç‡: {accuracy.get('precision', 0):.1%}
   å¬å›ç‡: {accuracy.get('recall', 0):.1%}
   F1å¾—åˆ†: {accuracy.get('f1_score', 0):.3f}
"""
            
            # æ¨èè´¨é‡
            if result.recommendation_quality:
                quality = result.recommendation_quality
                report += f"""
â­ æ¨èè´¨é‡
   é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {quality.get('high_confidence_ratio', 0):.1%}
   å¹³å‡ä¿¡å·å¼ºåº¦: {quality.get('avg_signal_strength', 0):.3f}
   ä¿¡å·ä¸€è‡´æ€§: {quality.get('signal_consistency', 0):.3f}
   æ—¥å‡ä¿¡å·æ•°: {quality.get('avg_daily_signals', 0):.1f}
"""
            
            # è’™ç‰¹å¡æ´›ç»“æœ
            if result.monte_carlo_results:
                mc_results = result.monte_carlo_results
                report += f"""
ğŸ”„ è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ ({mc_results.get('simulations_count', 0)}æ¬¡)
   æ”¶ç›Šå‡å€¼: {mc_results.get('return_statistics', {}).get('mean', 0):+.2%}
   æ”¶ç›Šæ ‡å‡†å·®: {mc_results.get('return_statistics', {}).get('std', 0):.2%}
   5%åˆ†ä½æ•°: {mc_results.get('return_statistics', {}).get('percentile_5', 0):+.2%}
   95%åˆ†ä½æ•°: {mc_results.get('return_statistics', {}).get('percentile_95', 0):+.2%}
   äºæŸæ¦‚ç‡: {mc_results.get('probability_of_loss', 0):.1%}
   è¶…è¶Šæ¦‚ç‡: {mc_results.get('probability_of_outperformance', 0):.1%}
"""
            
            # å‹åŠ›æµ‹è¯•ç»“æœ
            if result.stress_test_results:
                stress_results = result.stress_test_results
                report += f"""
âš¡ å‹åŠ›æµ‹è¯•ç»“æœ
"""
                for scenario_name, scenario_data in stress_results.items():
                    if isinstance(scenario_data, dict) and 'scenario_name' in scenario_data:
                        report += f"""
   {scenario_data['scenario_name']}:
     æ”¶ç›Šå½±å“: {scenario_data.get('impact_on_return', 0):+.2%}
     è°ƒæ•´åæ”¶ç›Š: {scenario_data.get('adjusted_return', 0):+.2%}
"""
            
            # å› å­å½’å› 
            if result.factor_attribution:
                factor_attr = result.factor_attribution
                report += f"""
ğŸ” å› å­å½’å› åˆ†æ
   æ€»å› å­è´¡çŒ®: {factor_attr.get('total_factor_contribution', 0):+.2%}
   ç‰¹æœ‰æ”¶ç›Š: {factor_attr.get('specific_return', 0):+.2%}
"""
                
                for factor, data in factor_attr.items():
                    if isinstance(data, dict) and 'contribution' in data:
                        report += f"""
   {factor.title()}å› å­:
     æš´éœ²åº¦: {data.get('exposure', 0):.3f}
     å› å­æ”¶ç›Š: {data.get('factor_return', 0):+.2%}
     è´¡çŒ®åº¦: {data.get('contribution', 0):+.2%}
"""
            
            # æŠ¥å‘Šç»“å°¾
            report += f"""
âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ | ç”Ÿæˆæ—¶é—´: {timestamp}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
            
            logger.info(f"å›æµ‹æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œç±»å‹: {report_type.value}")
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›æµ‹æŠ¥å‘Šå¤±è´¥: {e}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def batch_backtest(self,
                      user_id: str,
                      criteria_list: List[StockSelectionCriteria],
                      strategy_list: List[SelectionStrategy],
                      start_date: datetime,
                      end_date: datetime,
                      personalized: bool = True,
                      max_workers: int = 4) -> List[AISelectionBacktestResult]:
        """
        æ‰¹é‡å›æµ‹
        
        Args:
            user_id: ç”¨æˆ·ID
            criteria_list: é€‰è‚¡æ ‡å‡†åˆ—è¡¨
            strategy_list: ç­–ç•¥åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            personalized: æ˜¯å¦ä½¿ç”¨ä¸ªæ€§åŒ–
            max_workers: æœ€å¤§å¹¶è¡Œæ•°
            
        Returns:
            å›æµ‹ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å›æµ‹ - ç”¨æˆ·: {user_id}, ç»„åˆæ•°: {len(criteria_list) * len(strategy_list)}")
            
            # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
            test_combinations = []
            for criteria in criteria_list:
                for strategy in strategy_list:
                    test_combinations.append((criteria, strategy))
            
            results = []
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_combination = {
                    executor.submit(
                        self.run_backtest,
                        user_id,
                        criteria,
                        strategy,
                        start_date,
                        end_date,
                        personalized
                    ): (criteria, strategy)
                    for criteria, strategy in test_combinations
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_combination):
                    combination = future_to_combination[future]
                    try:
                        result = future.result()
                        results.append(result)
                        logger.info(f"ç»„åˆå›æµ‹å®Œæˆ: {combination[1].value}")
                    except Exception as e:
                        logger.error(f"ç»„åˆå›æµ‹å¤±è´¥ {combination}: {e}")
            
            logger.info(f"æ‰¹é‡å›æµ‹å®Œæˆ - æˆåŠŸ: {len(results)}, å¤±è´¥: {len(test_combinations) - len(results)}")
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å›æµ‹å¤±è´¥: {e}")
            return []
    
    def compare_backtest_results(self,
                               results: List[AISelectionBacktestResult],
                               comparison_metrics: List[str] = None) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªå›æµ‹ç»“æœ
        
        Args:
            results: å›æµ‹ç»“æœåˆ—è¡¨
            comparison_metrics: æ¯”è¾ƒæŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            æ¯”è¾ƒç»“æœå­—å…¸
        """
        try:
            if not results:
                return {}
            
            # é»˜è®¤æ¯”è¾ƒæŒ‡æ ‡
            if comparison_metrics is None:
                comparison_metrics = [
                    'total_return', 'annualized_return', 'sharpe_ratio', 
                    'max_drawdown', 'win_rate', 'profit_factor'
                ]
            
            comparison = {}
            
            for metric in comparison_metrics:
                values = []
                for i, result in enumerate(results):
                    if hasattr(result.unified_risk_metrics, metric):
                        values.append(getattr(result.unified_risk_metrics, metric))
                    else:
                        values.append(0.0)
                
                if values:
                    comparison[metric] = {
                        'values': values,
                        'best_index': np.argmax(values) if metric in ['total_return', 'annualized_return', 'sharpe_ratio', 'win_rate', 'profit_factor'] else np.argmin(values),
                        'worst_index': np.argmin(values) if metric in ['total_return', 'annualized_return', 'sharpe_ratio', 'win_rate', 'profit_factor'] else np.argmax(values),
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'median': np.median(values)
                    }
            
            # æ’ååˆ†æ
            rankings = {}
            for metric in comparison_metrics:
                if metric in comparison:
                    values = comparison[metric]['values']
                    rankings[metric] = sorted(range(len(values)), key=lambda i: values[i], reverse=(metric in ['total_return', 'annualized_return', 'sharpe_ratio', 'win_rate', 'profit_factor']))
            
            comparison['rankings'] = rankings
            
            logger.info("å›æµ‹ç»“æœæ¯”è¾ƒå®Œæˆ")
            return comparison
            
        except Exception as e:
            logger.error(f"æ¯”è¾ƒå›æµ‹ç»“æœå¤±è´¥: {e}")
            return {}
    
    def optimize_backtest_parameters(self,
                                   user_id: str,
                                   base_criteria: StockSelectionCriteria,
                                   strategy: SelectionStrategy,
                                   start_date: datetime,
                                   end_date: datetime,
                                   parameter_grid: Dict[str, List],
                                   optimization_metric: str = 'sharpe_ratio',
                                   cv_folds: int = 3) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å›æµ‹å‚æ•°
        
        Args:
            user_id: ç”¨æˆ·ID
            base_criteria: åŸºç¡€é€‰è‚¡æ ‡å‡†
            strategy: é€‰è‚¡ç­–ç•¥
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            parameter_grid: å‚æ•°ç½‘æ ¼
            optimization_metric: ä¼˜åŒ–æŒ‡æ ‡
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹å‚æ•°ä¼˜åŒ– - æŒ‡æ ‡: {optimization_metric}, ç½‘æ ¼å¤§å°: {len(parameter_grid)}")
            
            # ç”Ÿæˆå‚æ•°ç»„åˆ
            from itertools import product
            
            param_names = list(parameter_grid.keys())
            param_values = list(parameter_grid.values())
            param_combinations = [
                dict(zip(param_names, combo))
                for combo in product(*param_values)
            ]
            
            optimization_results = []
            
            # ä¸ºæ¯ä¸ªå‚æ•°ç»„åˆè¿è¡Œå›æµ‹
            for params in param_combinations:
                # åˆ›å»ºå›æµ‹é…ç½®
                backtest_config = AISelectionBacktestConfig()
                
                # åº”ç”¨å‚æ•°
                for param_name, param_value in params.items():
                    if hasattr(backtest_config, param_name):
                        setattr(backtest_config, param_name, param_value)
                
                # è¿è¡Œå›æµ‹
                result = self.run_backtest(
                    user_id=user_id,
                    stock_selection_criteria=base_criteria,
                    selection_strategy=strategy,
                    start_date=start_date,
                    end_date=end_date,
                    personalized=True,
                    backtest_config=backtest_config
                )
                
                # è®°å½•ä¼˜åŒ–æŒ‡æ ‡
                optimization_value = getattr(result.unified_risk_metrics, optimization_metric, 0.0)
                optimization_results.append({
                    'parameters': params,
                    'result': result,
                    'optimization_value': optimization_value
                })
            
            # æ’åºå¹¶è¿”å›æœ€ä½³å‚æ•°
            optimization_results.sort(key=lambda x: x['optimization_value'], reverse=True)
            
            best_result = optimization_results[0]
            
            optimization_summary = {
                'best_parameters': best_result['parameters'],
                'best_value': best_result['optimization_value'],
                'all_results': optimization_results,
                'optimization_metric': optimization_metric,
                'total_combinations': len(param_combinations)
            }
            
            logger.info(f"å‚æ•°ä¼˜åŒ–å®Œæˆ - æœ€ä½³{optimization_metric}: {best_result['optimization_value']:.4f}")
            return optimization_summary
            
        except Exception as e:
            logger.error(f"å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return {}
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        try:
            with self._cache_lock:
                self._backtest_cache.clear()
                self._performance_cache.clear()
            logger.info("å›æµ‹ç¼“å­˜æ¸…é™¤å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")