from loguru import logger
"""
æ¿å—èµ„é‡‘æµæ•°æ®æœåŠ¡

æ­¤æ¨¡å—æä¾›ç»Ÿä¸€çš„æ¿å—èµ„é‡‘æµæ•°æ®è®¿é—®æ¥å£ï¼Œæ”¯æŒå¤šæ•°æ®æºåˆ‡æ¢ã€
æ•°æ®ç¼“å­˜ã€å¼‚æ­¥åŠ è½½ç­‰åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ”¯æŒå¤šæ•°æ®æºï¼ˆAkShareã€ä¸œæ–¹è´¢å¯Œç­‰ï¼‰
- ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œæ¥å£
- æ•°æ®ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
- å¼‚æ­¥æ•°æ®åŠ è½½
- é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
"""

import asyncio
import threading
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass

import pandas as pd
from PyQt5.QtCore import QObject, pyqtSignal, QTimer

from .unified_data_manager import UnifiedDataManager


@dataclass
class SectorFlowConfig:
    """æ¿å—èµ„é‡‘æµæœåŠ¡é…ç½®"""
    cache_duration_minutes: int = 5  # ç¼“å­˜æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    auto_refresh_interval_minutes: int = 10  # è‡ªåŠ¨åˆ·æ–°é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    max_concurrent_requests: int = 3  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    request_timeout_seconds: int = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    enable_cache: bool = True  # å¯ç”¨ç¼“å­˜
    enable_auto_refresh: bool = True  # å¯ç”¨è‡ªåŠ¨åˆ·æ–°
    fallback_data_source: str = "akshare"  # é™çº§æ•°æ®æº


class SectorFundFlowService(QObject):
    """æ¿å—èµ„é‡‘æµæ•°æ®æœåŠ¡"""

    # ä¿¡å·å®šä¹‰
    data_updated = pyqtSignal(object)  # æ•°æ®æ›´æ–°ä¿¡å·
    error_occurred = pyqtSignal(str)  # é”™è¯¯ä¿¡å·
    source_changed = pyqtSignal(str)  # æ•°æ®æºå˜æ›´ä¿¡å·

    def __init__(self, data_manager: Optional[UnifiedDataManager] = None,
                 config: Optional[SectorFlowConfig] = None):
        """
        åˆå§‹åŒ–æ¿å—èµ„é‡‘æµæœåŠ¡

        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
            config: æœåŠ¡é…ç½®
            # log_manager: å·²è¿ç§»åˆ°Loguruæ—¥å¿—ç³»ç»Ÿ
        """
        super().__init__()

        # ä½¿ç”¨æ•°æ®ç®¡ç†å™¨å·¥å‚è·å–æ­£ç¡®çš„DataManagerå®ä¾‹
        if data_manager is None:
            try:
                from utils.manager_factory import get_manager_factory, get_data_manager
                factory = get_manager_factory()
                self.data_manager = get_data_manager()
            except ImportError:
                # é™çº§åˆ°ç›´æ¥å¯¼å…¥
                from .unified_data_manager import get_unified_data_manager
                self.data_manager = get_unified_data_manager()
        else:
            self.data_manager = data_manager
        self.config = config or SectorFlowConfig()
        # çº¯Loguruæ¶æ„ï¼Œç§»é™¤log_managerä¾èµ–

        # ç¼“å­˜ç®¡ç†
        self._cache: Dict[str, Any] = {}
        self._cache_timestamps: Dict[str, datetime] = {}
        self._cache_lock = threading.RLock()

        # å¼‚æ­¥æ‰§è¡Œå™¨
        self._executor = ThreadPoolExecutor(max_workers=self.config.max_concurrent_requests)

        # è‡ªåŠ¨åˆ·æ–°å®šæ—¶å™¨
        self._refresh_timer = QTimer()
        self._refresh_timer.timeout.connect(self._auto_refresh)

        self._is_initialized = False
        self._current_source = None
        self._available_sources = {}  # å¯ç”¨æ•°æ®æºæ³¨å†Œè¡¨
        self._optimal_sources = []    # æœ€ä¼˜æ•°æ®æºåˆ—è¡¨

    def initialize(self) -> bool:
        """åˆå§‹åŒ–æœåŠ¡"""
        try:
            logger.info("åˆå§‹åŒ–æ¿å—èµ„é‡‘æµæœåŠ¡...")
            import time
            start_time = time.time()

            # æ£€æŸ¥æ•°æ®ç®¡ç†å™¨
            logger.info("æ£€æŸ¥æ•°æ®ç®¡ç†å™¨çŠ¶æ€...")
            if self.data_manager:
                logger.info("æ•°æ®ç®¡ç†å™¨å¯ç”¨")
            else:
                logger.warning("æ•°æ®ç®¡ç†å™¨ä¸å¯ç”¨")

            # å¯åŠ¨è‡ªåŠ¨åˆ·æ–°
            logger.info("é…ç½®è‡ªåŠ¨åˆ·æ–°è®¾ç½®...")
            if self.config.enable_auto_refresh:
                refresh_start = time.time()
                self._start_auto_refresh()
                refresh_time = time.time()
                logger.info(f" è‡ªåŠ¨åˆ·æ–°å¯åŠ¨å®Œæˆï¼Œè€—æ—¶: {(refresh_time - refresh_start):.2f}ç§’")
            else:
                logger.info("â„¹ è‡ªåŠ¨åˆ·æ–°å·²ç¦ç”¨")

            # æ™ºèƒ½æ£€æµ‹æ¿å—èµ„é‡‘æµæ•°æ®æº
            self._detect_optimal_data_sources()

            self._is_initialized = True
            logger.info(f" æ¿å—èµ„é‡‘æµæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            return True

        except Exception as e:
            logger.error(f" æ¿å—èµ„é‡‘æµæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def cleanup(self) -> None:
        """æ¸…ç†æœåŠ¡èµ„æº"""
        try:
            logger.info("æ¸…ç†æ¿å—èµ„é‡‘æµæœåŠ¡...")

            # åœæ­¢è‡ªåŠ¨åˆ·æ–°
            self._refresh_timer.stop()

            # å…³é—­æ‰§è¡Œå™¨
            self._executor.shutdown(wait=True)

            # æ¸…ç†ç¼“å­˜
            with self._cache_lock:
                self._cache.clear()
                self._cache_timestamps.clear()

            logger.info("æ¿å—èµ„é‡‘æµæœåŠ¡æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f" æ¸…ç†æ¿å—èµ„é‡‘æµæœåŠ¡å¤±è´¥: {e}")

    def get_sector_flow_rank(self, indicator: str = "ä»Šæ—¥", force_refresh: bool = False) -> pd.DataFrame:
        """è·å–æ¿å—èµ„é‡‘æµæ’è¡Œ

        Args:
            indicator: æ—¶é—´å‘¨æœŸï¼ˆä»Šæ—¥ã€3æ—¥ã€5æ—¥ã€10æ—¥ã€20æ—¥ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜

        Returns:
            pd.DataFrame: æ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®
        """
        cache_key = f"sector_flow_rank_{indicator}"

        try:
            # æ£€æŸ¥ç¼“å­˜
            if not force_refresh and self._is_cache_valid(cache_key):
                logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„æ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®: {indicator}")
                return self._get_from_cache(cache_key)

            logger.info(f"è·å–æ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®: {indicator}")

            # ä½¿ç”¨æ™ºèƒ½æ•°æ®æºé€‰æ‹©è·å–æ•°æ®
            df = self._get_data_with_smart_routing(indicator)

            if not df.empty:
                # æ•°æ®æ ‡å‡†åŒ–å¤„ç†
                df = self._standardize_sector_flow_data(df)

                # æ›´æ–°ç¼“å­˜
                self._update_cache(cache_key, df)

                logger.info(f"æ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®è·å–æˆåŠŸ: {len(df)} æ¡è®°å½•")
                self.data_updated.emit({'type': 'sector_flow_rank', 'data': df})

                return df
            else:
                logger.warning("æœªè·å–åˆ°æ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"[ERROR] è·å–æ¿å—èµ„é‡‘æµæ’è¡Œå¤±è´¥: {e}")
            self.error_occurred.emit(f"è·å–æ¿å—èµ„é‡‘æµæ’è¡Œå¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def get_sector_flow_summary(self, symbol: str, indicator: str = "ä»Šæ—¥") -> pd.DataFrame:
        """è·å–æ¿å—èµ„é‡‘æµæ±‡æ€»

        Args:
            symbol: æ¿å—åç§°
            indicator: æ—¶é—´å‘¨æœŸ

        Returns:
            pd.DataFrame: æ¿å—èµ„é‡‘æµæ±‡æ€»æ•°æ®
        """
        try:
            df = self.data_manager.get_sector_fund_flow_summary(symbol, indicator)
            logger.info(f" æ¿å—èµ„é‡‘æµæ±‡æ€»è·å–æˆåŠŸ: {symbol}, {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f" è·å–æ¿å—èµ„é‡‘æµæ±‡æ€»å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_sector_flow_history(self, symbol: str, period: str = "è¿‘6æœˆ") -> pd.DataFrame:
        """è·å–æ¿å—å†å²èµ„é‡‘æµ

        Args:
            symbol: æ¿å—åç§°
            period: æ—¶é—´å‘¨æœŸ

        Returns:
            pd.DataFrame: æ¿å—å†å²èµ„é‡‘æµæ•°æ®
        """
        try:
            df = self.data_manager.get_sector_fund_flow_hist(symbol, period)
            logger.info(f" æ¿å—å†å²èµ„é‡‘æµè·å–æˆåŠŸ: {symbol}, {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f" è·å–æ¿å—å†å²èµ„é‡‘æµå¤±è´¥: {e}")
            return pd.DataFrame()

    def get_concept_flow_history(self, symbol: str, period: str = "è¿‘6æœˆ") -> pd.DataFrame:
        """è·å–æ¦‚å¿µå†å²èµ„é‡‘æµ

        Args:
            symbol: æ¦‚å¿µåç§°
            period: æ—¶é—´å‘¨æœŸ

        Returns:
            pd.DataFrame: æ¦‚å¿µå†å²èµ„é‡‘æµæ•°æ®
        """
        try:
            df = self.data_manager.get_concept_fund_flow_hist(symbol, period)
            logger.info(f" æ¦‚å¿µå†å²èµ„é‡‘æµè·å–æˆåŠŸ: {symbol}, {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f" è·å–æ¦‚å¿µå†å²èµ„é‡‘æµå¤±è´¥: {e}")
            return pd.DataFrame()

    def switch_data_source(self, source: str) -> bool:
        """åˆ‡æ¢æ•°æ®æº

        Args:
            source: æ•°æ®æºåç§°

        Returns:
            bool: æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        """
        try:
            logger.info(f" åˆ‡æ¢æ•°æ®æºåˆ°: {source}")

            # åˆ‡æ¢æ•°æ®ç®¡ç†å™¨çš„æ•°æ®æº
            self.data_manager.set_data_source(source)
            self._current_source = source

            # æ¸…ç†ç¼“å­˜
            self._clear_cache()

            logger.info(f" æ•°æ®æºåˆ‡æ¢æˆåŠŸ: {source}")
            self.source_changed.emit(source)

            return True

        except Exception as e:
            logger.error(f" æ•°æ®æºåˆ‡æ¢å¤±è´¥: {e}")
            self.error_occurred.emit(f"æ•°æ®æºåˆ‡æ¢å¤±è´¥: {str(e)}")
            return False

    def _standardize_sector_flow_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ¿å—èµ„é‡‘æµæ•°æ®æ ¼å¼"""
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if df is None or not isinstance(df, pd.DataFrame):
                logger.warning(f"æ— æ•ˆçš„è¾“å…¥æ•°æ®ç±»å‹: {type(df)}")
                return pd.DataFrame()
            
            if df.empty:
                logger.warning("è¾“å…¥æ•°æ®ä¸ºç©º")
                return df
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'æ¿å—': 'sector_name',
                'ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢': 'main_net_inflow',
                'ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”': 'main_net_inflow_ratio',
                'ä»Šæ—¥æ•£æˆ·å‡€æµå…¥-å‡€é¢': 'retail_net_inflow',
                'ä»Šæ—¥æ•£æˆ·å‡€æµå…¥-å‡€å æ¯”': 'retail_net_inflow_ratio',
                'ä»Šæ—¥æ¶¨è·Œå¹…': 'change_pct'
            }

            # é‡å‘½ååˆ—
            for old_col, new_col in column_mapping.items():
                if old_col in df.columns:
                    df = df.rename(columns={old_col: new_col})

            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_columns = ['main_net_inflow', 'main_net_inflow_ratio',
                               'retail_net_inflow', 'retail_net_inflow_ratio', 'change_pct']

            for col in numeric_columns:
                if col in df.columns:
                    # ç¡®ä¿åˆ—æ˜¯Seriesè€Œä¸æ˜¯DataFrame
                    col_data = df[col]
                    if isinstance(col_data, pd.DataFrame):
                        logger.warning(f"åˆ— {col} æ˜¯DataFrameè€Œä¸æ˜¯Seriesï¼Œè·³è¿‡è½¬æ¢")
                        continue
                    
                    # å®‰å…¨çš„ç±»å‹è½¬æ¢
                    try:
                        df[col] = pd.to_numeric(col_data, errors='coerce')
                    except Exception as conv_err:
                        logger.warning(f"åˆ— {col} è½¬æ¢å¤±è´¥: {conv_err}")

            return df

        except Exception as e:
            logger.warning(f"æ•°æ®æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return df

    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.config.enable_cache:
            return False

        with self._cache_lock:
            if cache_key not in self._cache_timestamps:
                return False

            cache_time = self._cache_timestamps[cache_key]
            cache_duration = timedelta(minutes=self.config.cache_duration_minutes)

            return datetime.now() - cache_time < cache_duration

    def _get_from_cache(self, cache_key: str) -> Any:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        with self._cache_lock:
            return self._cache.get(cache_key)

    def _update_cache(self, cache_key: str, data: Any) -> None:
        """æ›´æ–°ç¼“å­˜"""
        if self.config.enable_cache:
            with self._cache_lock:
                self._cache[cache_key] = data
                self._cache_timestamps[cache_key] = datetime.now()

    def _clear_cache(self) -> None:
        """æ¸…ç†ç¼“å­˜"""
        with self._cache_lock:
            self._cache.clear()
            self._cache_timestamps.clear()
        logger.info("ç¼“å­˜å·²æ¸…ç†")

    def _start_auto_refresh(self) -> None:
        """å¯åŠ¨è‡ªåŠ¨åˆ·æ–°"""
        if self.config.auto_refresh_interval_minutes > 0:
            interval_ms = self.config.auto_refresh_interval_minutes * 60 * 1000
            self._refresh_timer.start(interval_ms)
            logger.info(f" å¯åŠ¨è‡ªåŠ¨åˆ·æ–°ï¼Œé—´éš” {self.config.auto_refresh_interval_minutes} åˆ†é’Ÿ")

    def _auto_refresh(self) -> None:
        """è‡ªåŠ¨åˆ·æ–°æ•°æ®ï¼ˆåå°çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡UIçº¿ç¨‹ï¼‰"""
        try:
            logger.info("[TIME] è°ƒåº¦è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡...")
            # å°†å®é™…åˆ·æ–°ä»»åŠ¡æ”¾å…¥çº¿ç¨‹æ± ï¼Œé¿å…åœ¨Qtå®šæ—¶å™¨å›è°ƒï¼ˆä¸»çº¿ç¨‹ï¼‰ä¸­æ‰§è¡Œé‡IO/CPUå·¥ä½œ
            self._executor.submit(self._run_auto_refresh_task)
        except Exception as e:
            logger.error(f" è‡ªåŠ¨åˆ·æ–°è°ƒåº¦å¤±è´¥: {e}")

    def _run_auto_refresh_task(self) -> None:
        """å®é™…çš„è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ"""
        try:
            # è¿™é‡Œç›´æ¥è°ƒç”¨ç°æœ‰æ–¹æ³•å³å¯ï¼›è¯¥æ–¹æ³•å†…éƒ¨ä¼šé€šè¿‡Qtä¿¡å·é€šçŸ¥æ•°æ®æ›´æ–°
            self.get_sector_flow_rank(force_refresh=True)
        except Exception as e:
            logger.error(f" è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

    def _detect_optimal_data_sources(self) -> None:
        """æ™ºèƒ½æ£€æµ‹æ¿å—èµ„é‡‘æµæ•°æ®çš„æœ€ä¼˜æ•°æ®æº"""
        try:
            logger.info("å¼€å§‹æ£€æµ‹æ¿å—èµ„é‡‘æµæ•°æ®æº...")

            # é‡ç½®æ•°æ®æºæ³¨å†Œè¡¨
            self._available_sources.clear()
            self._optimal_sources.clear()

            # 1. æ£€æŸ¥TETæ¡†æ¶å¯ç”¨æ€§
            if hasattr(self.data_manager, 'tet_enabled') and self.data_manager.tet_enabled:
                logger.info("TETæ¡†æ¶å¯ç”¨ï¼Œæ£€æµ‹æ’ä»¶åŒ–æ•°æ®æº...")
                self._detect_tet_data_sources()

            # 2. æ£€æŸ¥ä¼ ç»Ÿæ•°æ®æº
            logger.info("æ£€æµ‹ä¼ ç»Ÿæ•°æ®æº...")
            self._detect_legacy_data_sources()

            # 3. æ ¹æ®å¥åº·çŠ¶æ€å’ŒåŠŸèƒ½æ”¯æŒæ’åº
            self._rank_data_sources()

            # 4. è¾“å‡ºæ£€æµ‹ç»“æœ
            self._log_detection_results()

        except Exception as e:
            logger.error(f"[ERROR] æ•°æ®æºæ£€æµ‹å¤±è´¥: {e}")
            # è®¾ç½®é»˜è®¤çš„é™çº§æ–¹æ¡ˆ
            self._set_fallback_sources()

    def _detect_tet_data_sources(self) -> None:
        """æ£€æµ‹TETæ¡†æ¶ä¸­æ”¯æŒSECTOR_FUND_FLOWçš„æ•°æ®æº"""
        try:
            if not (hasattr(self.data_manager, 'tet_pipeline') and self.data_manager.tet_pipeline):
                logger.warning("TETç®¡é“ä¸å¯ç”¨")
                return

            from ..plugin_types import DataType

            # è·å–TETè·¯ç”±å™¨ä¸­çš„æ•°æ®æº
            tet_pipeline = self.data_manager.tet_pipeline
            if hasattr(tet_pipeline, 'router') and tet_pipeline.router:
                router = tet_pipeline.router

                # åˆ›å»ºè·¯ç”±è¯·æ±‚å¯¹è±¡ç”¨äºè·å–å¯ç”¨æ•°æ®æº
                from core.data_source_router import RoutingRequest
                from core.plugin_types import AssetType, DataType

                routing_request = RoutingRequest(
                    asset_type=AssetType.STOCK,
                    data_type=DataType.SECTOR_FUND_FLOW,
                    symbol="",  # æ¿å—èµ„é‡‘æµä¸éœ€è¦å…·ä½“è‚¡ç¥¨ä»£ç 
                    priority=0,
                    timeout_ms=5000
                )

                # æ£€æŸ¥æ¯ä¸ªæ³¨å†Œçš„æ•°æ®æº
                for source_id in router.get_available_sources(routing_request):
                    try:
                        # æ£€æŸ¥æ˜¯å¦æ”¯æŒSECTOR_FUND_FLOW
                        supports_fund_flow = self._check_source_supports_fund_flow(source_id, router)
                        if supports_fund_flow:
                            health_score = self._get_source_health_score(source_id, router)
                            self._available_sources[source_id] = {
                                'type': 'tet_plugin',
                                'health_score': health_score,
                                'supports_fund_flow': True,
                                'router': router
                            }
                            logger.info(f"å‘ç°TETæ•°æ®æº: {source_id} (å¥åº·åº¦: {health_score:.2f})")
                        else:
                            logger.debug(f"ğŸ”¶ æ•°æ®æº {source_id} ä¸æ”¯æŒæ¿å—èµ„é‡‘æµ")
                    except Exception as e:
                        logger.warning(f" æ£€æµ‹æ•°æ®æº {source_id} å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"[ERROR] TETæ•°æ®æºæ£€æµ‹å¤±è´¥: {e}")

    def _detect_legacy_data_sources(self) -> None:
        """æ£€æµ‹ä¼ ç»Ÿæ•°æ®æºçš„æ¿å—èµ„é‡‘æµæ”¯æŒ"""
        try:
            # æ£€æŸ¥æ•°æ®ç®¡ç†å™¨ä¸­çš„ä¼ ç»Ÿæ•°æ®æº
            if hasattr(self.data_manager, '_data_sources'):
                for source_id, source_instance in self.data_manager._data_sources.items():
                    if source_instance is not None:
                        try:
                            # æ£€æŸ¥æ˜¯å¦æœ‰æ¿å—èµ„é‡‘æµç›¸å…³æ–¹æ³•
                            supports_fund_flow = self._check_legacy_source_supports_fund_flow(source_id, source_instance)
                            if supports_fund_flow:
                                health_score = self._test_legacy_source_health(source_id, source_instance)
                                self._available_sources[source_id] = {
                                    'type': 'legacy',
                                    'health_score': health_score,
                                    'supports_fund_flow': True,
                                    'instance': source_instance
                                }
                                logger.info(f"å‘ç°ä¼ ç»Ÿæ•°æ®æº: {source_id} (å¥åº·åº¦: {health_score:.2f})")
                            else:
                                logger.debug(f"ğŸ”¶ ä¼ ç»Ÿæ•°æ®æº {source_id} ä¸æ”¯æŒæ¿å—èµ„é‡‘æµ")
                        except Exception as e:
                            logger.warning(f" æ£€æµ‹ä¼ ç»Ÿæ•°æ®æº {source_id} å¤±è´¥: {e}")

            # ç‰¹åˆ«æ£€æŸ¥HIkyuuï¼ˆæ˜ç¡®æ ‡æ³¨ä¸æ”¯æŒæ¿å—èµ„é‡‘æµï¼‰
            if 'hikyuu' in self._available_sources:
                self._available_sources['hikyuu']['supports_fund_flow'] = False
                self._available_sources['hikyuu']['note'] = 'HIkyuuä¸“æ³¨Kçº¿æ•°æ®ï¼Œä¸æ”¯æŒæ¿å—èµ„é‡‘æµ'
                logger.info("â„¹ï¸ HIkyuuæ•°æ®æºï¼šä¸“æ³¨Kçº¿æ•°æ®ï¼Œä¸é€‚ç”¨äºæ¿å—èµ„é‡‘æµ")

        except Exception as e:
            logger.error(f"[ERROR] ä¼ ç»Ÿæ•°æ®æºæ£€æµ‹å¤±è´¥: {e}")

    def _check_source_supports_fund_flow(self, source_id: str, router) -> bool:
        """æ£€æŸ¥TETæ•°æ®æºæ˜¯å¦æ”¯æŒæ¿å—èµ„é‡‘æµ"""
        try:
            from ..plugin_types import DataType, AssetType

            # è·å–æ•°æ®æºå®ä¾‹
            source_instance = router.get_data_source(source_id)
            if not source_instance:
                return False

            # æ£€æŸ¥æ’ä»¶ä¿¡æ¯ä¸­çš„æ”¯æŒæ•°æ®ç±»å‹
            if hasattr(source_instance, 'plugin_info'):
                plugin_info = source_instance.plugin_info
                if hasattr(plugin_info, 'supported_data_types'):
                    return DataType.SECTOR_FUND_FLOW in plugin_info.supported_data_types

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–¹æ³•
            method_names = ['get_sector_fund_flow_data', 'get_fund_flow', 'get_sector_flow']
            for method_name in method_names:
                if hasattr(source_instance, method_name):
                    return True

            return False

        except Exception as e:
            logger.debug(f"æ£€æŸ¥æ•°æ®æº {source_id} æ”¯æŒæƒ…å†µæ—¶å‡ºé”™: {e}")
            return False

    def _check_legacy_source_supports_fund_flow(self, source_id: str, source_instance) -> bool:
        """æ£€æŸ¥ä¼ ç»Ÿæ•°æ®æºæ˜¯å¦æ”¯æŒæ¿å—èµ„é‡‘æµ"""
        try:
            # AkShare ç›¸å…³æ£€æŸ¥
            if 'akshare' in source_id.lower():
                return True

            # ä¸œæ–¹è´¢å¯Œç›¸å…³æ£€æŸ¥
            if 'eastmoney' in source_id.lower():
                return True

            # æ£€æŸ¥æ˜¯å¦æœ‰æ¿å—èµ„é‡‘æµç›¸å…³æ–¹æ³•
            fund_flow_methods = [
                'get_sector_fund_flow',
                'get_fund_flow',
                'stock_sector_fund_flow_rank',
                'sector_fund_flow'
            ]

            for method_name in fund_flow_methods:
                if hasattr(source_instance, method_name):
                    return True

            return False

        except Exception as e:
            logger.debug(f"æ£€æŸ¥ä¼ ç»Ÿæ•°æ®æº {source_id} æ”¯æŒæƒ…å†µæ—¶å‡ºé”™: {e}")
            return False

    def _get_source_health_score(self, source_id: str, router) -> float:
        """è·å–TETæ•°æ®æºå¥åº·è¯„åˆ†"""
        try:
            # è·å–æ•°æ®æºæŒ‡æ ‡
            if hasattr(router, 'get_source_metrics'):
                metrics = router.get_source_metrics(source_id)
                if metrics:
                    return metrics.health_score

            # å°è¯•ç®€å•çš„å¥åº·æ£€æŸ¥
            source_instance = router.get_data_source(source_id)
            if source_instance and hasattr(source_instance, 'health_check'):
                result = source_instance.health_check()
                if hasattr(result, 'is_healthy') and result.is_healthy:
                    return 1.0
                else:
                    return 0.3

            return 0.5  # é»˜è®¤ä¸­ç­‰å¥åº·åº¦

        except Exception as e:
            logger.debug(f"è·å–æ•°æ®æº {source_id} å¥åº·åº¦å¤±è´¥: {e}")
            return 0.1

    def _test_legacy_source_health(self, source_id: str, source_instance) -> float:
        """æµ‹è¯•ä¼ ç»Ÿæ•°æ®æºå¥åº·çŠ¶æ€"""
        try:
            # å°è¯•è¿æ¥æµ‹è¯•
            if hasattr(source_instance, 'test_connection'):
                if source_instance.test_connection():
                    return 0.8
                else:
                    return 0.2

            # ç®€å•å¯ç”¨æ€§æ£€æŸ¥
            if source_instance is not None:
                return 0.6

            return 0.1

        except Exception as e:
            logger.debug(f"æµ‹è¯•ä¼ ç»Ÿæ•°æ®æº {source_id} å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            return 0.1

    def _rank_data_sources(self) -> None:
        """æ ¹æ®å¥åº·çŠ¶æ€å’ŒåŠŸèƒ½æ”¯æŒå¯¹æ•°æ®æºè¿›è¡Œæ’åº"""
        try:
            # è¿‡æ»¤æ”¯æŒæ¿å—èµ„é‡‘æµçš„æ•°æ®æº
            fund_flow_sources = {
                source_id: info for source_id, info in self._available_sources.items()
                if info.get('supports_fund_flow', False)
            }

            # æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼šå¥åº·åº¦ + æ•°æ®æºç±»å‹æƒé‡
            def get_priority_score(item):
                source_id, info = item
                health_score = info['health_score']
                type_weight = 1.0 if info['type'] == 'tet_plugin' else 0.8  # TETæ’ä»¶ä¼˜å…ˆ

                # ç‰¹æ®ŠåŠ æƒ
                if 'akshare' in source_id.lower():
                    type_weight += 0.3  # AkShareæ˜¯æ¿å—èµ„é‡‘æµçš„ä¸“ä¸šæ•°æ®æº

                return health_score * type_weight

            # æ’åºå¹¶ä¿å­˜
            sorted_sources = sorted(fund_flow_sources.items(), key=get_priority_score, reverse=True)
            self._optimal_sources = [source_id for source_id, _ in sorted_sources]

        except Exception as e:
            logger.error(f"[ERROR] æ•°æ®æºæ’åºå¤±è´¥: {e}")

    def _log_detection_results(self) -> None:
        """è¾“å‡ºæ•°æ®æºæ£€æµ‹ç»“æœ"""
        try:
            logger.info("æ¿å—èµ„é‡‘æµæ•°æ®æºæ£€æµ‹ç»“æœ:")
            logger.info(f"   æ€»è®¡å‘ç°æ•°æ®æº: {len(self._available_sources)} ä¸ª")

            fund_flow_count = sum(1 for info in self._available_sources.values()
                                  if info.get('supports_fund_flow', False))
            logger.info(f"   æ”¯æŒæ¿å—èµ„é‡‘æµ: {fund_flow_count} ä¸ª")

            if self._optimal_sources:
                logger.info("[AWARD] æ¨èæ•°æ®æºä¼˜å…ˆçº§æ’åº:")
                for i, source_id in enumerate(self._optimal_sources[:3], 1):
                    info = self._available_sources[source_id]
                    logger.info(f"   {i}. {source_id} (å¥åº·åº¦: {info['health_score']:.2f}, ç±»å‹: {info['type']})")

                # è®¾ç½®å½“å‰æœ€ä¼˜æ•°æ®æº
                self._current_source = self._optimal_sources[0]
                logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æº: {self._current_source}")
            else:
                logger.warning("æœªå‘ç°æ”¯æŒæ¿å—èµ„é‡‘æµçš„æ•°æ®æºï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                self._current_source = "mock"

        except Exception as e:
            logger.error(f"[ERROR] è¾“å‡ºæ£€æµ‹ç»“æœå¤±è´¥: {e}")

    def _set_fallback_sources(self) -> None:
        """è®¾ç½®é™çº§æ•°æ®æºæ–¹æ¡ˆ"""
        try:
            logger.warning("è®¾ç½®é™çº§æ•°æ®æºæ–¹æ¡ˆ...")

            # å°è¯•é»˜è®¤çš„é™çº§é¡ºåº
            fallback_order = ['akshare', 'eastmoney', 'mock']

            for source_id in fallback_order:
                # æ£€æŸ¥æ•°æ®ç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰è¯¥æ•°æ®æº
                if hasattr(self.data_manager, '_data_sources'):
                    if source_id in self.data_manager._data_sources:
                        self._current_source = source_id
                        logger.info(f"é™çº§ä½¿ç”¨æ•°æ®æº: {source_id}")
                        return

            # æœ€ç»ˆé™çº§åˆ°æ¨¡æ‹Ÿæ¨¡å¼
            self._current_source = "mock"
            logger.info("â„¹ï¸ é™çº§åˆ°æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")

        except Exception as e:
            logger.error(f"[ERROR] è®¾ç½®é™çº§æ–¹æ¡ˆå¤±è´¥: {e}")
            self._current_source = "mock"

    def _get_data_with_smart_routing(self, indicator: str = "ä»Šæ—¥") -> pd.DataFrame:
        """ä½¿ç”¨æ™ºèƒ½è·¯ç”±è·å–æ¿å—èµ„é‡‘æµæ•°æ®"""
        try:
            # ä¼˜å…ˆä½¿ç”¨TETæ¡†æ¶è¿›è¡Œæ™ºèƒ½è·¯ç”±
            if self._try_tet_data_acquisition(indicator) is not None:
                return self._try_tet_data_acquisition(indicator)

            # é™çº§åˆ°æœ€ä¼˜ä¼ ç»Ÿæ•°æ®æº
            return self._try_optimal_legacy_sources(indicator)

        except Exception as e:
            logger.error(f"[ERROR] æ™ºèƒ½è·¯ç”±è·å–æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _try_tet_data_acquisition(self, indicator: str) -> Optional[pd.DataFrame]:
        """å°è¯•é€šè¿‡TETæ¡†æ¶è·å–æ•°æ®"""
        try:
            # æ£€æŸ¥TETæ¡†æ¶å¯ç”¨æ€§
            if not (hasattr(self.data_manager, 'tet_enabled') and self.data_manager.tet_enabled):
                return None

            if not (hasattr(self.data_manager, 'tet_pipeline') and self.data_manager.tet_pipeline):
                return None

            logger.info("é€šè¿‡TETæ¡†æ¶æ™ºèƒ½è·¯ç”±è·å–æ¿å—èµ„é‡‘æµæ•°æ®...")

            from ..plugin_types import AssetType, DataType
            from ..tet_data_pipeline import StandardQuery

            # åˆ›å»ºæ ‡å‡†åŒ–æŸ¥è¯¢
            query = StandardQuery(
                asset_type=AssetType.SECTOR,
                data_type=DataType.SECTOR_FUND_FLOW,
                symbol="",
                extra_params={"indicator": indicator, "limit": 50}
            )

            # é€šè¿‡TETç®¡é“å¤„ç†
            result = self.data_manager.tet_pipeline.process(query)

            if result and result.success and result.data is not None:
                if isinstance(result.data, pd.DataFrame) and not result.data.empty:
                    # è®°å½•å®é™…ä½¿ç”¨çš„æ•°æ®æº
                    actual_source = getattr(result, 'source_id', 'TETè·¯ç”±é€‰æ‹©')
                    logger.info(f"TETæ¡†æ¶æˆåŠŸè·å–æ•°æ®ï¼Œå®é™…æ•°æ®æº: {actual_source}")
                    self._current_source = actual_source
                    return result.data
                elif isinstance(result.data, list) and len(result.data) > 0:
                    df = pd.DataFrame(result.data)
                    actual_source = getattr(result, 'source_id', 'TETè·¯ç”±é€‰æ‹©')
                    logger.info(f"TETæ¡†æ¶æˆåŠŸè·å–æ•°æ®ï¼Œå®é™…æ•°æ®æº: {actual_source}")
                    self._current_source = actual_source
                    return df
                else:
                    logger.warning("TETæ¡†æ¶è¿”å›ç©ºæ•°æ®")
            else:
                logger.warning("TETæ¡†æ¶å¤„ç†å¤±è´¥")

            return None

        except Exception as e:
            logger.warning(f" TETæ¡†æ¶è·å–æ•°æ®å¤±è´¥: {e}")
            return None

    def _try_optimal_legacy_sources(self, indicator: str) -> pd.DataFrame:
        """å°è¯•ä½¿ç”¨æœ€ä¼˜ä¼ ç»Ÿæ•°æ®æºè·å–æ•°æ®"""
        try:
            logger.info("é™çº§åˆ°ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼...")

            # æŒ‰ä¼˜å…ˆçº§å°è¯•å¯ç”¨çš„æ•°æ®æº
            for source_id in self._optimal_sources:
                try:
                    source_info = self._available_sources.get(source_id)
                    if not source_info or not source_info.get('supports_fund_flow', False):
                        continue

                    logger.info(f"å°è¯•æ•°æ®æº: {source_id}")
                    df = self._get_data_from_specific_source(source_id, source_info, indicator)

                    if not df.empty:
                        logger.info(f"æˆåŠŸä» {source_id} è·å–æ•°æ®: {len(df)} æ¡è®°å½•")
                        self._current_source = source_id
                        return df
                    else:
                        logger.warning(f" æ•°æ®æº {source_id} è¿”å›ç©ºæ•°æ®")

                except Exception as e:
                    logger.warning(f" æ•°æ®æº {source_id} è·å–å¤±è´¥: {e}")
                    continue

            # æœ€åå°è¯•é€šè¿‡æ•°æ®ç®¡ç†å™¨çš„é€šç”¨æ–¹æ³•
            logger.info("å°è¯•æ•°æ®ç®¡ç†å™¨é€šç”¨æ–¹æ³•...")
            return self._fallback_to_data_manager()

        except Exception as e:
            logger.error(f"[ERROR] ä¼ ç»Ÿæ•°æ®æºè·å–å¤±è´¥: {e}")
            return pd.DataFrame()

    def _get_data_from_specific_source(self, source_id: str, source_info: Dict, indicator: str) -> pd.DataFrame:
        """ä»ç‰¹å®šæ•°æ®æºè·å–æ•°æ®"""
        try:
            if source_info['type'] == 'tet_plugin':
                # TETæ’ä»¶ç±»å‹
                router = source_info.get('router')
                if router:
                    source_instance = router.get_data_source(source_id)
                    if source_instance and hasattr(source_instance, 'get_sector_fund_flow_data'):
                        data = source_instance.get_sector_fund_flow_data("sector", indicator=indicator)
                        if isinstance(data, pd.DataFrame):
                            return data
                        elif isinstance(data, list):
                            return pd.DataFrame(data)

            elif source_info['type'] == 'legacy':
                # ä¼ ç»Ÿæ•°æ®æºç±»å‹
                source_instance = source_info.get('instance')
                if source_instance:
                    # å°è¯•å„ç§å¯èƒ½çš„æ–¹æ³•å
                    method_names = ['get_sector_fund_flow', 'get_fund_flow', 'stock_sector_fund_flow_rank']
                    for method_name in method_names:
                        if hasattr(source_instance, method_name):
                            try:
                                method = getattr(source_instance, method_name)
                                if method_name == 'stock_sector_fund_flow_rank':
                                    data = method(indicator=indicator)
                                else:
                                    data = method()

                                if isinstance(data, pd.DataFrame) and not data.empty:
                                    return data
                                elif isinstance(data, dict) and 'sector_flow_rank' in data:
                                    return data['sector_flow_rank']
                            except Exception as e:
                                logger.debug(f"æ–¹æ³• {method_name} è°ƒç”¨å¤±è´¥: {e}")
                                continue

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä»æ•°æ®æº {source_id} è·å–æ•°æ®æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()

    def _fallback_to_data_manager(self) -> pd.DataFrame:
        """é™çº§åˆ°æ•°æ®ç®¡ç†å™¨çš„é€šç”¨æ–¹æ³•"""
        try:
            fund_flow_data = self.data_manager.get_fund_flow()

            if fund_flow_data and 'sector_flow_rank' in fund_flow_data:
                df = fund_flow_data['sector_flow_rank']
                if isinstance(df, pd.DataFrame) and not df.empty:
                    self._current_source = "æ•°æ®ç®¡ç†å™¨é€šç”¨æ–¹æ³•"
                    return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"æ•°æ®ç®¡ç†å™¨é€šç”¨æ–¹æ³•å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_current_optimal_source(self) -> str:
        """è·å–å½“å‰æœ€ä¼˜æ•°æ®æº"""
        return self._current_source or "unknown"

    def get_available_sources_info(self) -> Dict[str, Any]:
        """è·å–å¯ç”¨æ•°æ®æºä¿¡æ¯"""
        return {
            'available_sources': self._available_sources,
            'optimal_sources': self._optimal_sources,
            'current_source': self._current_source
        }

    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            'is_initialized': self._is_initialized,
            'current_optimal_source': self._current_source,
            'available_sources_count': len(self._available_sources),
            'fund_flow_sources_count': sum(1 for info in self._available_sources.values()
                                           if info.get('supports_fund_flow', False)),
            'cache_enabled': self.config.enable_cache,
            'auto_refresh_enabled': self.config.enable_auto_refresh,
            'cache_size': len(self._cache) if self.config.enable_cache else 0
        }
