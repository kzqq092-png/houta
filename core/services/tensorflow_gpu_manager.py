#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TensorFlow GPUæ™ºèƒ½ç®¡ç†å™¨

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ£€æµ‹GPUç¡¬ä»¶å’Œç¯å¢ƒ
2. æ™ºèƒ½é…ç½®TensorFlow GPUæ”¯æŒ
3. æä¾›GPU/CPUè‡ªåŠ¨é€‰æ‹©æœºåˆ¶
4. ç›‘æ§GPUèµ„æºä½¿ç”¨æƒ…å†µ
5. æ•…éšœå›é€€åˆ°CPUæ¨¡å¼

ä½œè€…: FactorWeave-Quantå›¢é˜Ÿ
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import platform
import subprocess
import ctypes
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

try:
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger.warning("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GPUç®¡ç†åŠŸèƒ½")

class GPUStatus(Enum):
    """GPUçŠ¶æ€æšä¸¾"""
    UNAVAILABLE = "unavailable"          # GPUä¸å¯ç”¨
    CUDA_ERROR = "cuda_error"            # CUDAç¯å¢ƒé”™è¯¯
    DRIVER_ERROR = "driver_error"        # é©±åŠ¨é”™è¯¯
    AVAILABLE = "available"              # GPUå¯ç”¨
    CONFIGURED = "configured"            # å·²é…ç½®GPU
    TESTING = "testing"                  # æ­£åœ¨æµ‹è¯•
    READY = "ready"                      # GPUå°±ç»ª
    ERROR = "error"                      # GPUé”™è¯¯
    FALLBACK_CPU = "fallback_cpu"        # å›é€€åˆ°CPU

@dataclass
class GPUInfo:
    """GPUä¿¡æ¯æ•°æ®ç±»"""
    name: str = "Unknown"
    memory_total: int = 0
    memory_free: int = 0
    memory_used: int = 0
    compute_capability: str = "unknown"
    cuda_version: str = "unknown"
    driver_version: str = "unknown"
    status: GPUStatus = GPUStatus.UNAVAILABLE
    
class TensorFlowGPUManager:
    """TensorFlow GPUæ™ºèƒ½ç®¡ç†å™¨"""
    
    def __init__(self):
        self.gpu_info: Optional[GPUInfo] = None
        self.is_configured = False
        self.device_preference = "auto"  # auto, gpu, cpu
        self.auto_fallback_enabled = True
        self.performance_threshold = 0.5  # æ€§èƒ½é˜ˆå€¼
        
        # é…ç½®å‚æ•°
        self.config = {
            'allow_memory_growth': True,
            'memory_fraction': 0.8,
            'inter_op_threads': 4,
            'intra_op_threads': 4,
            'mixed_precision': False
        }
        
        logger.info("TensorFlow GPUç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_gpu_hardware(self) -> GPUInfo:
        """æ£€æµ‹GPUç¡¬ä»¶ä¿¡æ¯"""
        logger.info("ğŸ” å¼€å§‹GPUç¡¬ä»¶æ£€æµ‹...")
        gpu_info = GPUInfo()
        
        try:
            # å°è¯•ä½¿ç”¨pynvmlæ£€æµ‹NVIDIA GPU
            import pynvml
            logger.info("âœ… pynvmlåº“åŠ è½½æˆåŠŸ")
            
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            if device_count > 0:
                logger.info(f"ğŸ¯ æ£€æµ‹åˆ° {device_count} ä¸ªNVIDIA GPUè®¾å¤‡")
                logger.info("=" * 60)
                
                # è·å–ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                name_result = pynvml.nvmlDeviceGetName(handle)
                name = name_result.decode('utf-8') if isinstance(name_result, bytes) else str(name_result)
                
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                utilization_info = pynvml.nvmlDeviceGetUtilizationRates(handle)
                
                gpu_info.name = name
                gpu_info.memory_total = memory_info.total // 1024 // 1024
                gpu_info.memory_free = memory_info.free // 1024 // 1024
                gpu_info.status = GPUStatus.AVAILABLE
                
                logger.info(f"ğŸš€ GPUè®¾å¤‡: {name}")
                logger.info(f"ğŸ“Š æ˜¾å­˜æ€»é‡: {gpu_info.memory_total:,} MB")
                logger.info(f"ğŸ’¾ å¯ç”¨æ˜¾å­˜: {gpu_info.memory_free:,} MB")
                logger.info(f"ğŸ“ˆ å½“å‰ä½¿ç”¨ç‡: {utilization_info.gpu}%")
                logger.info(f"ğŸ’¿ æ˜¾å­˜ä½¿ç”¨ç‡: {utilization_info.memory}%")
                logger.info("=" * 60)
                
                pynvml.nvmlShutdown()
                
                # æ€§èƒ½è¯„ä¼°
                if gpu_info.memory_total >= 8000:
                    logger.info("ğŸ‰ é«˜æ€§èƒ½GPUé…ç½®ï¼Œé€‚åˆæ·±åº¦å­¦ä¹ è®­ç»ƒ")
                elif gpu_info.memory_total >= 4000:
                    logger.info("ğŸ‘ ä¸­ç­‰æ€§èƒ½GPUï¼Œé€‚åˆæ¨¡å‹æ¨ç†å’Œå°è§„æ¨¡è®­ç»ƒ")
                elif gpu_info.memory_total >= 2000:
                    logger.info("âš¡ å…¥é—¨çº§GPUï¼Œé€‚åˆæ¨¡å‹æ¨ç†")
                else:
                    logger.info("âš ï¸ æ˜¾å­˜è¾ƒå°‘ï¼Œå»ºè®®ä½¿ç”¨CPUæ¨¡å¼")
                
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°NVIDIA GPUè®¾å¤‡")
                logger.info("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥GPUé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
                gpu_info.status = GPUStatus.UNAVAILABLE
                
        except ImportError:
            logger.warning("âŒ pynvmlåº“æœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU")
            logger.info("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼špip install nvidia-ml-py3")
            gpu_info.status = GPUStatus.UNAVAILABLE
        except Exception as e:
            logger.error(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
            logger.info("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥NVIDIAé©±åŠ¨å’ŒCUDAå®‰è£…")
            gpu_info.status = GPUStatus.ERROR
            
        return gpu_info
    
    def _detect_cuda_version(self) -> str:
        """æ£€æµ‹CUDAç‰ˆæœ¬"""
        try:
            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            cuda_path = os.environ.get('CUDA_PATH')
            if cuda_path:
                version_file = Path(cuda_path) / "version.txt"
                if version_file.exists():
                    return version_file.read_text().strip()
            
            # å°è¯•ä»nvccè·å–
            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                output = result.stdout
                # æå–ç‰ˆæœ¬ä¿¡æ¯
                lines = output.split('\n')
                for line in lines:
                    if 'release' in line.lower():
                        version = line.split('release')[1].strip().split(',')[0]
                        return version
            
            return "unknown"
        except Exception:
            return "unknown"
    
    def _detect_cuda_windows(self) -> Optional[Dict[str, str]]:
        """Windowsç³»ç»ŸCUDAæ£€æµ‹"""
        try:
            # å°è¯•åŠ è½½CUDAåº“
            cuda_lib = ctypes.WinDLL("cudart64_110.dll")
            cuda_version = cuda_lib.cudaGetErrorString(0)  # æµ‹è¯•åº“åŠ è½½
            return {'cuda_version': '11.0'}  # ç®€åŒ–ç‰ˆæœ¬æ£€æµ‹
        except Exception:
            return None
    
    def verify_cuda_environment(self) -> bool:
        """éªŒè¯CUDAç¯å¢ƒ"""
        logger.info("éªŒè¯CUDAç¯å¢ƒ...")
        
        try:
            # 1. æ£€æŸ¥CUDAåº“
            cuda_libraries = [
                "cudart64_110.dll",  # Windows
                "cudart.so.11.0",    # Linux
                "cudnn64_8.dll",     # Windows
                "libcudnn.so.8",     # Linux
            ]
            
            for lib in cuda_libraries:
                try:
                    if platform.system() == "Windows":
                        ctypes.WinDLL(lib)
                        logger.info(f"âœ… CUDAåº“åŠ è½½æˆåŠŸ: {lib}")
                        break
                    else:
                        ctypes.CDLL(lib)
                        logger.info(f"âœ… CUDAåº“åŠ è½½æˆåŠŸ: {lib}")
                        break
                except (OSError, FileNotFoundError):
                    continue
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°CUDAåº“")
                return False
            
            # 2. æ£€æŸ¥TensorFlowæ„å»ºä¿¡æ¯
            if TENSORFLOW_AVAILABLE:
                build_info = tf.sysconfig.get_build_info()
                cuda_version = build_info.get("cuda_version", "unknown")
                cudnn_version = build_info.get("cudnn_version", "unknown")
                
                logger.info(f"TensorFlowæ„å»ºä¿¡æ¯:")
                logger.info(f"  CUDAç‰ˆæœ¬: {cuda_version}")
                logger.info(f"  cuDNNç‰ˆæœ¬: {cudnn_version}")
                
                # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
                if cuda_version != "unknown" and cudnn_version != "unknown":
                    logger.info("âœ… TensorFlow CUDAç¯å¢ƒéªŒè¯é€šè¿‡")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ CUDAç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
            return False
    
    def configure_tensorflow_gpu(self) -> bool:
        """é…ç½®TensorFlow GPUæ”¯æŒ"""
        if not TENSORFLOW_AVAILABLE:
            logger.error("âŒ TensorFlowæœªå®‰è£…ï¼Œæ— æ³•é…ç½®GPU")
            logger.info("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼špip install tensorflow")
            return False
        
        logger.info("ğŸš€ å¼€å§‹é…ç½®TensorFlow GPU...")
        logger.info("=" * 60)
        
        try:
            # 1. åˆ—å‡ºç‰©ç†è®¾å¤‡
            gpus = tf.config.list_physical_devices('GPU')
            logger.info(f"ğŸ“Š TensorFlowæ£€æµ‹åˆ° {len(gpus)} ä¸ªç‰©ç†GPUè®¾å¤‡")
            
            if len(gpus) == 0:
                logger.warning("âš ï¸ TensorFlowæœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
                logger.info("ğŸ’¡ å¯èƒ½åŸå› ï¼š")
                logger.info("   1. GPUé©±åŠ¨æœªæ­£ç¡®å®‰è£…")
                logger.info("   2. CUDAç‰ˆæœ¬ä¸å…¼å®¹")
                logger.info("   3. cuDNNåº“ç¼ºå¤±")
                self.gpu_info.status = GPUStatus.UNAVAILABLE
                return False
            
            logger.info("ğŸ¯ å¼€å§‹é…ç½®GPUè®¾å¤‡...")
            # 2. é…ç½®GPUè®¾å¤‡
            for i, gpu in enumerate(gpus):
                logger.info(f"  âš™ï¸ é…ç½®GPUè®¾å¤‡ {i}: {gpu.name}")
                
                # è®¾ç½®æ˜¾å­˜å¢é•¿
                if self.config['allow_memory_growth']:
                    tf.config.experimental.set_memory_growth(gpu, True)
                    logger.info(f"    âœ… å¯ç”¨æ˜¾å­˜å¢é•¿")
                
                # è®¾ç½®æ˜¾å­˜é™åˆ¶
                if self.config['memory_fraction'] < 1.0:
                    memory_limit = int(
                        self.gpu_info.memory_total * self.config['memory_fraction']
                    ) if self.gpu_info.memory_total > 0 else 1024
                    
                    tf.config.set_logical_device_configuration(
                        gpu,
                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]
                    )
                    logger.info(f"    âœ… è®¾ç½®æ˜¾å­˜é™åˆ¶: {self.config['memory_fraction']*100}% ({memory_limit}MB)")
                else:
                    logger.info(f"    â„¹ï¸ ä½¿ç”¨å®Œæ•´æ˜¾å­˜: {self.gpu_info.memory_total}MB")
            
            # 3. é…ç½®å¹¶è¡Œçº¿ç¨‹
            if self.config['inter_op_threads'] > 0:
                tf.config.threading.set_inter_op_parallelism_threads(self.config['inter_op_threads'])
                logger.info(f"    âœ… è®¾ç½®inter_op_threads: {self.config['inter_op_threads']}")
            
            if self.config['intra_op_threads'] > 0:
                tf.config.threading.set_intra_op_parallelism_threads(self.config['intra_op_threads'])
                logger.info(f"    âœ… è®¾ç½®intra_op_threads: {self.config['intra_op_threads']}")
            
            # 4. æ··åˆç²¾åº¦è®­ç»ƒ
            if self.config['mixed_precision']:
                try:
                    tf.keras.mixed_precision.set_global_policy('mixed_float16')
                    logger.info("    âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåŠ é€Ÿè®¡ç®—ï¼‰")
                except Exception as e:
                    logger.warning(f"    âš ï¸ æ··åˆç²¾åº¦è®¾ç½®å¤±è´¥: {e}")
            
            # 5. è®¾ç½®å¯è§è®¾å¤‡
            visible_devices = [f"GPU:{i}" for i in range(len(gpus))]
            tf.config.set_visible_devices(visible_devices, 'GPU')
            
            self.is_configured = True
            self.gpu_info.status = GPUStatus.CONFIGURED
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ TensorFlow GPUé…ç½®å®Œæˆ")
            logger.info(f"ğŸ’¡ å¯ç”¨GPUè®¾å¤‡: {len(gpus)}")
            logger.info(f"âš¡ å†…å­˜ç®¡ç†: {'æ˜¾å­˜å¢é•¿' if self.config['allow_memory_growth'] else 'å›ºå®šæ˜¾å­˜'}")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ TensorFlow GPUé…ç½®å¤±è´¥: {e}")
            self.gpu_info.status = GPUStatus.ERROR
            return False
    
    def test_gpu_computation(self) -> Tuple[bool, float]:
        """æµ‹è¯•GPUè®¡ç®—æ€§èƒ½"""
        if not TENSORFLOW_AVAILABLE or not self.is_configured:
            return False, 0.0
        
        logger.info("ğŸ§ª å¼€å§‹GPUè®¡ç®—æ€§èƒ½æµ‹è¯•...")
        logger.info("=" * 60)
        
        try:
            # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
            logger.info("ğŸ“‹ åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
            with tf.device('/GPU:0'):
                model = tf.keras.Sequential([
                    tf.keras.layers.Dense(1000, activation='relu', input_shape=(100,)),
                    tf.keras.layers.Dense(500, activation='relu'),
                    tf.keras.layers.Dense(1, activation='sigmoid')
                ])
                
                model.compile(optimizer='adam', loss='binary_crossentropy')
                
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                logger.info("ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
                x_train = tf.random.normal([1000, 100])
                y_train = tf.random.uniform([1000, 1])
                
                # GPUè®­ç»ƒæµ‹è¯•
                logger.info("ğŸš€ GPUè®­ç»ƒæµ‹è¯•å¼€å§‹...")
                start_time = time.time()
                history = model.fit(x_train, y_train, epochs=5, verbose=0)
                gpu_time = time.time() - start_time
                
                logger.info(f"âœ… GPUè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {gpu_time:.2f}ç§’")
                logger.info(f"ğŸ“ˆ æœ€ç»ˆæŸå¤±: {history.history['loss'][-1]:.4f}")
            
            # CPUå¯¹æ¯”æµ‹è¯•
            logger.info("ğŸ–¥ï¸ CPUè®­ç»ƒæµ‹è¯•å¼€å§‹...")
            with tf.device('/CPU:0'):
                model = tf.keras.Sequential([
                    tf.keras.layers.Dense(1000, activation='relu', input_shape=(100,)),
                    tf.keras.layers.Dense(500, activation='relu'),
                    tf.keras.layers.Dense(1, activation='sigmoid')
                ])
                
                model.compile(optimizer='adam', loss='binary_crossentropy')
                
                start_time = time.time()
                history = model.fit(x_train, y_train, epochs=5, verbose=0)
                cpu_time = time.time() - start_time
                
                logger.info(f"âœ… CPUè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {cpu_time:.2f}ç§’")
                logger.info(f"ğŸ“ˆ æœ€ç»ˆæŸå¤±: {history.history['loss'][-1]:.4f}")
            
            speedup = cpu_time / gpu_time
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
            logger.info(f"   CPUæ—¶é—´: {cpu_time:.2f}ç§’")
            logger.info(f"   GPUæ—¶é—´: {gpu_time:.2f}ç§’")
            logger.info(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1.2:
                logger.info("ğŸ‰ GPUæ€§èƒ½æµ‹è¯•é€šè¿‡ï¼Œæ˜¾è‘—æå‡è®¡ç®—é€Ÿåº¦")
                if speedup > 5.0:
                    logger.info("ğŸš€ å“è¶Šæ€§èƒ½ï¼GPUåŠ é€Ÿæ•ˆæœä¼˜ç§€")
                elif speedup > 2.0:
                    logger.info("ğŸ‘ è‰¯å¥½æ€§èƒ½ï¼ŒGPUåŠ é€Ÿæ•ˆæœæ˜æ˜¾")
                else:
                    logger.info("âœ… ä¸€èˆ¬æ€§èƒ½ï¼ŒGPUä»æœ‰åŠ é€Ÿæ•ˆæœ")
                
                self.gpu_info.status = GPUStatus.READY
                return True, speedup
            elif speedup > 1.0:
                logger.warning("âš ï¸ GPUåŠ é€Ÿæ•ˆæœå¾®å¼±ï¼Œå»ºè®®æ£€æŸ¥GPUé…ç½®")
                logger.info("ğŸ’¡ å¯èƒ½åŸå› ï¼šGPUè´Ÿè½½è¿‡é«˜æˆ–å†…å­˜ä¸è¶³")
                return False, speedup
            else:
                logger.warning("âš ï¸ GPUæ€§èƒ½ä¸å¦‚CPUï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
                logger.info("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAç¯å¢ƒ")
                return False, speedup
                
        except Exception as e:
            logger.error(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            logger.info("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥TensorFlow GPUé…ç½®å’Œä¾èµ–åº“")
            return False, 0.0
    
    def auto_detect_and_configure(self) -> bool:
        """è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®GPU"""
        logger.info("ğŸš€ [TensorFlow GPUç®¡ç†å™¨] å¼€å§‹è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®")
        logger.info("=" * 80)
        logger.info("ğŸ¯ æ™ºèƒ½GPUç®¡ç†å™¨ - æ­£åœ¨ä¸ºæ‚¨ä¼˜åŒ–TensorFlowæ€§èƒ½")
        logger.info("=" * 80)
        
        try:
            # 1. æ£€æµ‹GPUç¡¬ä»¶
            logger.info("ğŸ” [æ­¥éª¤ 1/4] æ­£åœ¨æ£€æµ‹GPUç¡¬ä»¶...")
            self.gpu_info = self.detect_gpu_hardware()
            
            # 2. éªŒè¯CUDAç¯å¢ƒ
            logger.info("ğŸ”§ [æ­¥éª¤ 2/4] æ­£åœ¨éªŒè¯CUDAç¯å¢ƒ...")
            cuda_ok = self.verify_cuda_environment()
            if not cuda_ok:
                logger.error("âŒ [CUDAéªŒè¯] CUDAç¯å¢ƒéªŒè¯å¤±è´¥")
                logger.info("ğŸ’¡ [å»ºè®®] è¯·å®‰è£…CUDA Toolkitå’ŒcuDNNåº“")
                if self.auto_fallback_enabled:
                    logger.info("ğŸ”„ [å›é€€] å¯ç”¨è‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
                    self.gpu_info.status = GPUStatus.FALLBACK_CPU
                    return False
                return False
            
            # 3. é…ç½®TensorFlow GPU
            logger.info("âš™ï¸ [æ­¥éª¤ 3/4] æ­£åœ¨é…ç½®TensorFlow GPU...")
            config_ok = self.configure_tensorflow_gpu()
            if not config_ok:
                logger.error("âŒ [é…ç½®å¤±è´¥] TensorFlow GPUé…ç½®å¤±è´¥")
                logger.info("ğŸ’¡ [å»ºè®®] è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒTensorFlow GPUç‰ˆæœ¬")
                if self.auto_fallback_enabled:
                    logger.info("ğŸ”„ [å›é€€] å¯ç”¨è‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
                    self.gpu_info.status = GPUStatus.FALLBACK_CPU
                    return False
                return False
            
            # 4. æµ‹è¯•GPUæ€§èƒ½
            logger.info("ğŸ§ª [æ­¥éª¤ 4/4] æ­£åœ¨æµ‹è¯•GPUæ€§èƒ½...")
            test_ok, speedup = self.test_gpu_computation()
            if not test_ok:
                logger.warning("âš ï¸ [æ€§èƒ½æµ‹è¯•] GPUæ€§èƒ½æµ‹è¯•æœªé€šè¿‡")
                logger.info("ğŸ’¡ [åŸå› ] GPUå¯èƒ½è´Ÿè½½è¿‡é«˜æˆ–é…ç½®ä¸æ­£ç¡®")
                if self.auto_fallback_enabled:
                    logger.info("ğŸ”„ [å›é€€] å¯ç”¨è‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
                    self.gpu_info.status = GPUStatus.FALLBACK_CPU
                    return False
                return False
            
            # æˆåŠŸå®Œæˆ
            logger.info("=" * 80)
            logger.info("ğŸ‰ [æˆåŠŸ] GPUè‡ªåŠ¨é…ç½®å®Œæˆï¼")
            logger.info(f"ğŸš€ [åŠ é€Ÿ] æ£€æµ‹åˆ°è®¾å¤‡: {self.gpu_info.name}")
            logger.info(f"âš¡ [æ€§èƒ½] åŠ é€Ÿæ¯”: {speedup:.2f}x")
            logger.info(f"ğŸ’¾ [å†…å­˜] GPUæ˜¾å­˜: {self.gpu_info.memory_total:,}MB")
            logger.info("=" * 80)
            logger.info("âœ… [å°±ç»ª] TensorFlowç°å·²ä½¿ç”¨GPUåŠ é€Ÿ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ [å¼‚å¸¸] è‡ªåŠ¨GPUé…ç½®å¤±è´¥: {e}")
            logger.info("ğŸ’¡ [å»ºè®®] è¯·æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå’Œä¾èµ–å®‰è£…")
            if self.auto_fallback_enabled:
                logger.info("ğŸ”„ [å›é€€] å¯ç”¨è‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
                self.gpu_info.status = GPUStatus.FALLBACK_CPU
            return False
    
    def get_device_strategy(self) -> str:
        """è·å–è®¾å¤‡ç­–ç•¥"""
        if self.device_preference == "gpu":
            return "/GPU:0"
        elif self.device_preference == "cpu":
            return "/CPU:0"
        else:
            # è‡ªåŠ¨é€‰æ‹©
            if self.gpu_info and self.gpu_info.status == GPUStatus.READY:
                return "/GPU:0"
            else:
                return "/CPU:0"
    
    def get_status_info(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€ä¿¡æ¯"""
        return {
            'gpu_info': {
                'name': self.gpu_info.name if self.gpu_info else 'Unknown',
                'status': self.gpu_info.status.value if self.gpu_info else 'unknown',
                'memory_total': self.gpu_info.memory_total if self.gpu_info else 0,
                'memory_free': self.gpu_info.memory_free if self.gpu_info else 0,
                'cuda_version': self.gpu_info.cuda_version if self.gpu_info else 'unknown',
            },
            'tensorflow_info': {
                'available': TENSORFLOW_AVAILABLE,
                'version': tf.__version__ if TENSORFLOW_AVAILABLE else 'unknown',
                'configured': self.is_configured,
            },
            'device_strategy': self.get_device_strategy(),
            'auto_fallback': self.auto_fallback_enabled,
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†GPUèµ„æº
            if self.is_configured:
                tf.config.set_visible_devices([], 'GPU')
                logger.info("GPUèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"GPUèµ„æºæ¸…ç†è­¦å‘Š: {e}")

# å…¨å±€GPUç®¡ç†å™¨å®ä¾‹
_gpu_manager = None

def get_gpu_manager() -> TensorFlowGPUManager:
    """è·å–å…¨å±€GPUç®¡ç†å™¨å®ä¾‹"""
    global _gpu_manager
    if _gpu_manager is None:
        _gpu_manager = TensorFlowGPUManager()
    return _gpu_manager

def auto_configure_gpu() -> bool:
    """ä¾¿æ·å‡½æ•°ï¼šè‡ªåŠ¨é…ç½®GPU"""
    manager = get_gpu_manager()
    return manager.auto_detect_and_configure()

def get_device_for_training() -> str:
    """è·å–è®­ç»ƒè®¾å¤‡"""
    manager = get_gpu_manager()
    return manager.get_device_strategy()

def print_gpu_status():
    """æ‰“å°GPUçŠ¶æ€ä¿¡æ¯"""
    manager = get_gpu_manager()
    status = manager.get_status_info()
    
    print("=" * 80)
    print("ğŸ“Š [TensorFlow GPUçŠ¶æ€æŠ¥å‘Š]")
    print("=" * 80)
    
    # GPUä¿¡æ¯
    if status['gpu_info']['name'] != 'Unknown':
        print(f"ğŸš€ GPUè®¾å¤‡: {status['gpu_info']['name']}")
        print(f"ğŸ“ˆ çŠ¶æ€: {status['gpu_info']['status']}")
        print(f"ğŸ’¾ æ˜¾å­˜æ€»é‡: {status['gpu_info']['memory_total']:,} MB")
        print(f"ğŸ’¿ å¯ç”¨æ˜¾å­˜: {status['gpu_info']['memory_free']:,} MB")
        print(f"ğŸ”§ CUDAç‰ˆæœ¬: {status['gpu_info']['cuda_version']}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
    
    # TensorFlowä¿¡æ¯
    if status['tensorflow_info']['available']:
        print(f"âœ… TensorFlowç‰ˆæœ¬: {status['tensorflow_info']['version']}")
        print(f"âš™ï¸ GPUé…ç½®çŠ¶æ€: {'å·²é…ç½®' if status['tensorflow_info']['configured'] else 'æœªé…ç½®'}")
    else:
        print("âŒ TensorFlowæœªå®‰è£…")
    
    # å½“å‰ç­–ç•¥
    device = status['device_strategy']
    if device == '/GPU:0':
        print(f"ğŸ¯ å½“å‰ç­–ç•¥: GPUåŠ é€Ÿæ¨¡å¼ âš¡")
    else:
        print(f"ğŸ–¥ï¸ å½“å‰ç­–ç•¥: CPUæ¨¡å¼")
    
    print(f"ğŸ”„ è‡ªåŠ¨å›é€€: {'å¯ç”¨' if status['auto_fallback'] else 'ç¦ç”¨'}")
    print("=" * 80)

if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    print("=== TensorFlow GPUç®¡ç†å™¨æµ‹è¯• ===")
    
    # è‡ªåŠ¨é…ç½®
    success = auto_configure_gpu()
    print(f"GPUé…ç½®ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # æ˜¾ç¤ºçŠ¶æ€
    print_gpu_status()
    
    # æ¸…ç†èµ„æº
    manager = get_gpu_manager()
    manager.cleanup()