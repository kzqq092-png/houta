#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡æ¨¡å—

åœ¨ç°æœ‰åˆ†å¸ƒå¼æœåŠ¡åŸºç¡€ä¸Šå¢åŠ æ™ºèƒ½åŒ–åŠŸèƒ½ï¼š
1. æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œä»»åŠ¡è°ƒåº¦
2. è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤
3. åŠ¨æ€èŠ‚ç‚¹ç®¡ç†å’Œæ‰©ç¼©å®¹
4. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
5. å®‰å…¨æ€§å’Œå¯é æ€§å¢å¼º
6. åˆ†å¸ƒå¼ç¼“å­˜å’ŒçŠ¶æ€åŒæ­¥
7. æ™ºèƒ½èµ„æºåˆ†é…
"""

import asyncio
import hashlib
import hmac
import json
import socket
import ssl
import threading
import time
import uuid
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Tuple, Set
import numpy as np
import psutil
from loguru import logger

from .distributed_service import DistributedService, NodeInfo, DistributedTask, NodeDiscovery, TaskScheduler
from ..events import EventBus, get_event_bus


class NodeStatus(Enum):
    """èŠ‚ç‚¹çŠ¶æ€æšä¸¾"""
    UNKNOWN = "unknown"
    ACTIVE = "active"
    INACTIVE = "inactive"
    BUSY = "busy"
    OVERLOADED = "overloaded"
    FAILED = "failed"
    MAINTENANCE = "maintenance"


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    BACKGROUND = 5


class LoadBalancingStrategy(Enum):
    """è´Ÿè½½å‡è¡¡ç­–ç•¥"""
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    RESOURCE_BASED = "resource_based"
    INTELLIGENT = "intelligent"


@dataclass
class EnhancedNodeInfo(NodeInfo):
    """å¢å¼ºç‰ˆèŠ‚ç‚¹ä¿¡æ¯"""
    # æ€§èƒ½æŒ‡æ ‡
    response_time: float = 0.0
    success_rate: float = 1.0
    error_count: int = 0
    completed_tasks: int = 0

    # èµ„æºç›‘æ§
    disk_usage: float = 0.0
    network_io: float = 0.0
    gpu_usage: float = 0.0
    gpu_memory: float = 0.0

    # å¥åº·çŠ¶æ€
    health_score: float = 1.0
    last_health_check: Optional[datetime] = None
    consecutive_failures: int = 0

    # è´Ÿè½½å‡è¡¡æƒé‡
    weight: float = 1.0
    current_load: float = 0.0
    max_concurrent_tasks: int = 10

    # å®‰å…¨ä¿¡æ¯
    security_token: Optional[str] = None
    last_auth_time: Optional[datetime] = None

    # åœ°ç†ä½ç½®å’Œç½‘ç»œ
    region: str = "default"
    availability_zone: str = "default"
    network_latency: float = 0.0

    def calculate_health_score(self) -> float:
        """è®¡ç®—èŠ‚ç‚¹å¥åº·åˆ†æ•°"""
        factors = {
            'cpu_usage': max(0, 1 - self.cpu_usage / 100),
            'memory_usage': max(0, 1 - self.memory_usage / 100),
            'success_rate': self.success_rate,
            'response_time': max(0, 1 - min(self.response_time / 1000, 1)),
            'consecutive_failures': max(0, 1 - self.consecutive_failures / 10)
        }

        # åŠ æƒå¹³å‡
        weights = {
            'cpu_usage': 0.2,
            'memory_usage': 0.2,
            'success_rate': 0.3,
            'response_time': 0.2,
            'consecutive_failures': 0.1
        }

        self.health_score = sum(factors[k] * weights[k] for k in factors)
        return self.health_score

    def is_healthy(self) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¥åº·"""
        return (self.health_score > 0.5 and
                self.consecutive_failures < 5 and
                self.status in [NodeStatus.ACTIVE.value, NodeStatus.BUSY.value])


@dataclass
class EnhancedDistributedTask(DistributedTask):
    """å¢å¼ºç‰ˆåˆ†å¸ƒå¼ä»»åŠ¡"""
    # ä»»åŠ¡å±æ€§
    retry_count: int = 0
    max_retries: int = 3
    timeout: int = 300  # ç§’
    dependencies: List[str] = field(default_factory=list)

    # èµ„æºéœ€æ±‚
    cpu_requirement: float = 1.0
    memory_requirement: int = 512  # MB
    gpu_requirement: bool = False

    # è°ƒåº¦ä¿¡æ¯
    preferred_nodes: List[str] = field(default_factory=list)
    excluded_nodes: List[str] = field(default_factory=list)
    affinity_rules: Dict[str, Any] = field(default_factory=dict)

    # ç›‘æ§ä¿¡æ¯
    execution_time: float = 0.0
    resource_usage: Dict[str, float] = field(default_factory=dict)
    checkpoint_data: Optional[Dict[str, Any]] = None

    def can_retry(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•"""
        return self.retry_count < self.max_retries

    def is_expired(self) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…æ—¶"""
        if self.start_time and self.status == "running":
            elapsed = (datetime.now() - self.start_time).total_seconds()
            return elapsed > self.timeout
        return False


class IntelligentLoadBalancer:
    """æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨ - ä¼˜åŒ–ç‰ˆ"""

    def __init__(self, strategy: LoadBalancingStrategy = LoadBalancingStrategy.INTELLIGENT):
        self.strategy = strategy
        self.node_weights: Dict[str, float] = {}
        self.node_connections: Dict[str, int] = defaultdict(int)
        self.round_robin_index = 0
        self.performance_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        
        # ä¼˜åŒ–å¢å¼ºåŠŸèƒ½
        self.node_scores: Dict[str, float] = defaultdict(float)
        self.task_completion_times: Dict[str, deque] = defaultdict(lambda: deque(maxlen=50))
        self.node_failure_counts: Dict[str, int] = defaultdict(int)
        self.node_success_rates: Dict[str, float] = defaultdict(lambda: 1.0)
        self.adaptive_weights = {
            'resource': 0.25,
            'performance': 0.25,
            'latency': 0.20,
            'affinity': 0.15,
            'reliability': 0.10,
            'load_trend': 0.05
        }
        
        # è´Ÿè½½é¢„æµ‹
        self.load_predictions: Dict[str, float] = defaultdict(float)
        self.last_update_time = datetime.now()
        
        logger.info("æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨å·²ä¼˜åŒ–åˆå§‹åŒ–")

    def select_node(self, nodes: List[EnhancedNodeInfo], task: EnhancedDistributedTask) -> Optional[EnhancedNodeInfo]:
        """é€‰æ‹©æœ€é€‚åˆçš„èŠ‚ç‚¹"""
        if not nodes:
            return None

        # è¿‡æ»¤å¥åº·èŠ‚ç‚¹
        healthy_nodes = [node for node in nodes if node.is_healthy()]
        if not healthy_nodes:
            logger.warning("æ²¡æœ‰å¥åº·çš„èŠ‚ç‚¹å¯ç”¨")
            return None

        # è¿‡æ»¤æ»¡è¶³èµ„æºéœ€æ±‚çš„èŠ‚ç‚¹
        suitable_nodes = self._filter_by_requirements(healthy_nodes, task)
        if not suitable_nodes:
            logger.warning("æ²¡æœ‰æ»¡è¶³èµ„æºéœ€æ±‚çš„èŠ‚ç‚¹")
            return None

        # æ ¹æ®ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹
        if self.strategy == LoadBalancingStrategy.ROUND_ROBIN:
            return self._round_robin_select(suitable_nodes)
        elif self.strategy == LoadBalancingStrategy.LEAST_CONNECTIONS:
            return self._least_connections_select(suitable_nodes)
        elif self.strategy == LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN:
            return self._weighted_round_robin_select(suitable_nodes)
        elif self.strategy == LoadBalancingStrategy.RESOURCE_BASED:
            return self._resource_based_select(suitable_nodes)
        else:  # INTELLIGENT
            return self._intelligent_select(suitable_nodes, task)

    def _filter_by_requirements(self, nodes: List[EnhancedNodeInfo], task: EnhancedDistributedTask) -> List[EnhancedNodeInfo]:
        """æ ¹æ®èµ„æºéœ€æ±‚è¿‡æ»¤èŠ‚ç‚¹"""
        suitable_nodes = []

        for node in nodes:
            # æ£€æŸ¥CPUéœ€æ±‚
            available_cpu = (100 - node.cpu_usage) / 100
            if available_cpu < task.cpu_requirement:
                continue

            # æ£€æŸ¥å†…å­˜éœ€æ±‚
            available_memory = (node.memory_total - node.memory_total * node.memory_usage / 100) / 1024 / 1024
            if available_memory < task.memory_requirement:
                continue

            # æ£€æŸ¥GPUéœ€æ±‚
            if task.gpu_requirement and node.gpu_usage > 90:
                continue

            # æ£€æŸ¥å¹¶å‘ä»»åŠ¡æ•°
            if node.task_count >= node.max_concurrent_tasks:
                continue

            # æ£€æŸ¥èŠ‚ç‚¹åå¥½
            if task.preferred_nodes and node.node_id not in task.preferred_nodes:
                continue

            # æ£€æŸ¥æ’é™¤èŠ‚ç‚¹
            if node.node_id in task.excluded_nodes:
                continue

            suitable_nodes.append(node)

        return suitable_nodes

    def _round_robin_select(self, nodes: List[EnhancedNodeInfo]) -> EnhancedNodeInfo:
        """è½®è¯¢é€‰æ‹©"""
        node = nodes[self.round_robin_index % len(nodes)]
        self.round_robin_index += 1
        return node

    def _least_connections_select(self, nodes: List[EnhancedNodeInfo]) -> EnhancedNodeInfo:
        """æœ€å°‘è¿æ¥é€‰æ‹©"""
        return min(nodes, key=lambda n: self.node_connections[n.node_id])

    def _weighted_round_robin_select(self, nodes: List[EnhancedNodeInfo]) -> EnhancedNodeInfo:
        """åŠ æƒè½®è¯¢é€‰æ‹©"""
        # è®¡ç®—æƒé‡æ€»å’Œ
        total_weight = sum(node.weight for node in nodes)
        if total_weight == 0:
            return nodes[0]

        # ç”Ÿæˆéšæœºæ•°é€‰æ‹©
        import random
        rand = random.uniform(0, total_weight)
        current_weight = 0

        for node in nodes:
            current_weight += node.weight
            if rand <= current_weight:
                return node

        return nodes[-1]

    def _resource_based_select(self, nodes: List[EnhancedNodeInfo]) -> EnhancedNodeInfo:
        """åŸºäºèµ„æºçš„é€‰æ‹©"""
        def resource_score(node: EnhancedNodeInfo) -> float:
            cpu_score = (100 - node.cpu_usage) / 100
            memory_score = (100 - node.memory_usage) / 100
            load_score = max(0, 1 - node.current_load)
            return (cpu_score + memory_score + load_score) / 3

        return max(nodes, key=resource_score)

    def _intelligent_select(self, nodes: List[EnhancedNodeInfo], task: EnhancedDistributedTask) -> EnhancedNodeInfo:
        """æ™ºèƒ½é€‰æ‹©ï¼ˆä¼˜åŒ–ç‰ˆ - ç»¼åˆå¤šä¸ªå› ç´ ï¼‰"""
        def intelligent_score(node: EnhancedNodeInfo) -> float:
            # åŸºç¡€èµ„æºåˆ†æ•°
            resource_score = self._calculate_resource_score(node)

            # æ€§èƒ½å†å²åˆ†æ•°
            performance_score = self._calculate_performance_score(node)

            # ç½‘ç»œå»¶è¿Ÿåˆ†æ•°
            latency_score = max(0, 1 - node.network_latency / 1000)

            # ä»»åŠ¡äº²å’Œæ€§åˆ†æ•°
            affinity_score = self._calculate_affinity_score(node, task)
            
            # å¯é æ€§åˆ†æ•°ï¼ˆæ–°å¢ï¼‰
            reliability_score = self._calculate_reliability_score(node)
            
            # è´Ÿè½½è¶‹åŠ¿åˆ†æ•°ï¼ˆæ–°å¢ï¼‰
            load_trend_score = self._calculate_load_trend_score(node)

            # ä½¿ç”¨è‡ªé€‚åº”æƒé‡
            total_score = (
                resource_score * self.adaptive_weights['resource'] +
                performance_score * self.adaptive_weights['performance'] +
                latency_score * self.adaptive_weights['latency'] +
                affinity_score * self.adaptive_weights['affinity'] +
                reliability_score * self.adaptive_weights['reliability'] +
                load_trend_score * self.adaptive_weights['load_trend']
            )

            # æ›´æ–°èŠ‚ç‚¹è¯„åˆ†å†å²
            self.node_scores[node.node_id] = total_score
            
            return total_score

        # é€‰æ‹©æœ€é«˜åˆ†èŠ‚ç‚¹
        best_node = max(nodes, key=intelligent_score)
        
        # è®°å½•é€‰æ‹©å†³ç­–
        logger.debug(f"æ™ºèƒ½é€‰æ‹©èŠ‚ç‚¹ {best_node.node_id}ï¼Œè¯„åˆ†: {self.node_scores[best_node.node_id]:.3f}")
        
        return best_node

    def _calculate_resource_score(self, node: EnhancedNodeInfo) -> float:
        """è®¡ç®—èµ„æºåˆ†æ•°"""
        cpu_score = (100 - node.cpu_usage) / 100
        memory_score = (100 - node.memory_usage) / 100
        load_score = max(0, 1 - node.current_load)
        return (cpu_score + memory_score + load_score) / 3

    def _calculate_reliability_score(self, node: EnhancedNodeInfo) -> float:
        """è®¡ç®—å¯é æ€§åˆ†æ•°ï¼ˆæ–°å¢ï¼‰"""
        try:
            # åŸºäºæˆåŠŸç‡è®¡ç®—
            success_rate = self.node_success_rates.get(node.node_id, 1.0)
            
            # åŸºäºæ•…éšœæ¬¡æ•°è®¡ç®—
            failure_count = self.node_failure_counts.get(node.node_id, 0)
            failure_penalty = max(0, 1 - failure_count * 0.1)  # æ¯æ¬¡æ•…éšœå‡å°‘0.1åˆ†
            
            # åŸºäºèŠ‚ç‚¹çŠ¶æ€è®¡ç®—
            status_score = 1.0
            if node.status == NodeStatus.OVERLOADED:
                status_score = 0.3
            elif node.status == NodeStatus.BUSY:
                status_score = 0.7
            elif node.status == NodeStatus.FAILED:
                status_score = 0.0
            
            # ç»¼åˆå¯é æ€§åˆ†æ•°
            reliability_score = (success_rate * 0.5 + failure_penalty * 0.3 + status_score * 0.2)
            
            return max(0.0, min(1.0, reliability_score))
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¯é æ€§åˆ†æ•°å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰å¯é æ€§
    
    def _calculate_load_trend_score(self, node: EnhancedNodeInfo) -> float:
        """è®¡ç®—è´Ÿè½½è¶‹åŠ¿åˆ†æ•°ï¼ˆæ–°å¢ï¼‰"""
        try:
            # è·å–å†å²è´Ÿè½½æ•°æ®
            history = self.performance_history.get(node.node_id, deque())
            
            if len(history) < 3:
                return 0.5  # æ•°æ®ä¸è¶³ï¼Œè¿”å›ä¸­ç­‰åˆ†æ•°
            
            # è®¡ç®—è´Ÿè½½è¶‹åŠ¿
            recent_loads = [record.get('load', 0) for record in list(history)[-5:]]
            if len(recent_loads) < 2:
                return 0.5
            
            # è®¡ç®—è´Ÿè½½å˜åŒ–è¶‹åŠ¿
            load_trend = np.polyfit(range(len(recent_loads)), recent_loads, 1)[0]
            
            # è´Ÿè½½ä¸‹é™è¶‹åŠ¿å¾—é«˜åˆ†ï¼Œä¸Šå‡è¶‹åŠ¿å¾—ä½åˆ†
            if load_trend <= 0:  # è´Ÿè½½ä¸‹é™æˆ–ç¨³å®š
                trend_score = min(1.0, 1.0 + load_trend)  # load_trendä¸ºè´Ÿå€¼
            else:  # è´Ÿè½½ä¸Šå‡
                trend_score = max(0.0, 1.0 - load_trend * 2)
            
            # ç»“åˆå½“å‰è´Ÿè½½æ°´å¹³
            current_load_score = max(0, 1 - node.current_load)
            
            # ç»¼åˆåˆ†æ•°
            final_score = (trend_score * 0.6 + current_load_score * 0.4)
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            logger.error(f"è®¡ç®—è´Ÿè½½è¶‹åŠ¿åˆ†æ•°å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_performance_score(self, node: EnhancedNodeInfo) -> float:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        history = self.performance_history[node.node_id]
        if not history:
            return 0.5  # é»˜è®¤åˆ†æ•°

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´å’ŒæˆåŠŸç‡
        avg_response_time = np.mean([h['response_time'] for h in history])
        avg_success_rate = np.mean([h['success_rate'] for h in history])

        response_score = max(0, 1 - avg_response_time / 1000)
        success_score = avg_success_rate

        return (response_score + success_score) / 2

    def _calculate_affinity_score(self, node: EnhancedNodeInfo, task: EnhancedDistributedTask) -> float:
        """è®¡ç®—ä»»åŠ¡äº²å’Œæ€§åˆ†æ•°"""
        score = 0.5  # é»˜è®¤åˆ†æ•°

        # æ£€æŸ¥åœ°ç†ä½ç½®äº²å’Œæ€§
        if 'region' in task.affinity_rules:
            if node.region == task.affinity_rules['region']:
                score += 0.3

        # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹äº²å’Œæ€§
        if 'node_type' in task.affinity_rules:
            if node.node_type == task.affinity_rules['node_type']:
                score += 0.2

        return min(1.0, score)

    def update_node_performance(self, node_id: str, response_time: float, success: bool):
        """æ›´æ–°èŠ‚ç‚¹æ€§èƒ½æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        performance_data = {
            'response_time': response_time,
            'success_rate': 1.0 if success else 0.0,
            'timestamp': datetime.now(),
            'load': 0.5  # é»˜è®¤è´Ÿè½½ï¼Œå®é™…åº”ä»èŠ‚ç‚¹è·å–
        }
        
        self.performance_history[node_id].append(performance_data)
        
        # è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µ
        self.record_task_completion(node_id, "unknown", response_time, success)
        
        # æ›´æ–°è´Ÿè½½é¢„æµ‹
        self._update_load_prediction(node_id, performance_data)
    
    def record_task_completion(self, node_id: str, task_id: str, completion_time: float, success: bool):
        """è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µï¼ˆæ–°å¢ï¼‰"""
        try:
            # è®°å½•å®Œæˆæ—¶é—´
            self.task_completion_times[node_id].append(completion_time)
            
            # æ›´æ–°æˆåŠŸç‡
            if node_id not in self.node_success_rates:
                self.node_success_rates[node_id] = 1.0 if success else 0.0
            else:
                # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æˆåŠŸç‡
                alpha = 0.1  # å­¦ä¹ ç‡
                current_rate = self.node_success_rates[node_id]
                new_rate = success * 1.0
                self.node_success_rates[node_id] = alpha * new_rate + (1 - alpha) * current_rate
            
            # æ›´æ–°æ•…éšœè®¡æ•°
            if not success:
                self.node_failure_counts[node_id] += 1
            
            logger.debug(f"è®°å½•ä»»åŠ¡å®Œæˆ: èŠ‚ç‚¹{node_id}, ä»»åŠ¡{task_id}, æˆåŠŸ:{success}, è€—æ—¶:{completion_time:.2f}s")
            
        except Exception as e:
            logger.error(f"è®°å½•ä»»åŠ¡å®Œæˆå¤±è´¥: {e}")
    
    def _update_load_prediction(self, node_id: str, performance_data: Dict[str, Any]):
        """æ›´æ–°è´Ÿè½½é¢„æµ‹ï¼ˆæ–°å¢ï¼‰"""
        try:
            current_load = performance_data.get('load', 0)
            
            # ç®€å•çš„çº¿æ€§é¢„æµ‹ï¼ˆå¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰
            history = list(self.performance_history[node_id])
            if len(history) >= 3:
                recent_loads = [record.get('load', 0) for record in history[-5:]]
                if len(recent_loads) >= 2:
                    # è®¡ç®—è´Ÿè½½å˜åŒ–è¶‹åŠ¿
                    trend = np.polyfit(range(len(recent_loads)), recent_loads, 1)[0]
                    
                    # é¢„æµ‹æœªæ¥5åˆ†é’Ÿçš„è´Ÿè½½
                    predicted_load = current_load + trend * 5
                    self.load_predictions[node_id] = max(0, min(1, predicted_load))
            
        except Exception as e:
            logger.error(f"æ›´æ–°è´Ÿè½½é¢„æµ‹å¤±è´¥: {e}")
    
    def adapt_balancing_strategy(self):
        """è‡ªé€‚åº”è°ƒæ•´è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼ˆæ–°å¢ï¼‰"""
        try:
            # åˆ†æå½“å‰æ€§èƒ½è¡¨ç°
            total_tasks = sum(len(times) for times in self.task_completion_times.values())
            
            if total_tasks < 10:
                return  # æ•°æ®ä¸è¶³ï¼Œä¸è¿›è¡Œè°ƒæ•´
            
            # è®¡ç®—å„èŠ‚ç‚¹çš„å¹³å‡å®Œæˆæ—¶é—´
            avg_completion_times = {}
            for node_id, times in self.task_completion_times.items():
                if times:
                    avg_completion_times[node_id] = np.mean(list(times))
            
            if not avg_completion_times:
                return
            
            # åˆ†ææ€§èƒ½å·®å¼‚
            completion_times = list(avg_completion_times.values())
            cv = np.std(completion_times) / np.mean(completion_times) if np.mean(completion_times) > 0 else 0
            
            # æ ¹æ®æ€§èƒ½å·®å¼‚è°ƒæ•´æƒé‡
            if cv > 0.3:  # æ€§èƒ½å·®å¼‚è¾ƒå¤§
                # å¢åŠ æ€§èƒ½æƒé‡ï¼Œå‡å°‘èµ„æºæƒé‡
                self.adaptive_weights['performance'] = min(0.4, self.adaptive_weights['performance'] + 0.05)
                self.adaptive_weights['resource'] = max(0.15, self.adaptive_weights['resource'] - 0.05)
            elif cv < 0.1:  # æ€§èƒ½å·®å¼‚è¾ƒå°
                # å¢åŠ èµ„æºæƒé‡ï¼Œå‡å°‘æ€§èƒ½æƒé‡
                self.adaptive_weights['resource'] = min(0.4, self.adaptive_weights['resource'] + 0.05)
                self.adaptive_weights['performance'] = max(0.15, self.adaptive_weights['performance'] - 0.05)
            
            # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
            total_weight = sum(self.adaptive_weights.values())
            for key in self.adaptive_weights:
                self.adaptive_weights[key] /= total_weight
            
            logger.debug(f"è‡ªé€‚åº”æƒé‡è°ƒæ•´: {self.adaptive_weights}")
            
        except Exception as e:
            logger.error(f"è‡ªé€‚åº”ç­–ç•¥è°ƒæ•´å¤±è´¥: {e}")
    
    def get_load_balancer_statistics(self) -> Dict[str, Any]:
        """è·å–è´Ÿè½½å‡è¡¡å™¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰"""
        try:
            return {
                'strategy': self.strategy.value,
                'adaptive_weights': self.adaptive_weights.copy(),
                'node_scores': dict(self.node_scores),
                'node_success_rates': dict(self.node_success_rates),
                'node_failure_counts': dict(self.node_failure_counts),
                'load_predictions': dict(self.load_predictions),
                'total_nodes_tracked': len(self.performance_history),
                'total_tasks_completed': sum(len(times) for times in self.task_completion_times.values()),
                'last_update': self.last_update_time.isoformat()
            }
        except Exception as e:
            logger.error(f"è·å–è´Ÿè½½å‡è¡¡å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}


class FailoverManager:
    """æ•…éšœè½¬ç§»ç®¡ç†å™¨"""

    def __init__(self, max_failures: int = 5, recovery_timeout: int = 300):
        self.max_failures = max_failures
        self.recovery_timeout = recovery_timeout
        self.failed_nodes: Dict[str, datetime] = {}
        self.recovery_tasks: Dict[str, asyncio.Task] = {}

    def handle_node_failure(self, node_id: str, error: Exception):
        """å¤„ç†èŠ‚ç‚¹æ•…éšœ"""
        logger.error(f"èŠ‚ç‚¹ {node_id} å‘ç”Ÿæ•…éšœ: {error}")

        # è®°å½•æ•…éšœæ—¶é—´
        self.failed_nodes[node_id] = datetime.now()

        # å¯åŠ¨æ¢å¤ä»»åŠ¡
        if node_id not in self.recovery_tasks:
            self.recovery_tasks[node_id] = asyncio.create_task(
                self._recovery_worker(node_id)
            )

    async def _recovery_worker(self, node_id: str):
        """èŠ‚ç‚¹æ¢å¤å·¥ä½œçº¿ç¨‹"""
        try:
            await asyncio.sleep(self.recovery_timeout)

            # å°è¯•æ¢å¤èŠ‚ç‚¹
            if await self._attempt_node_recovery(node_id):
                logger.info(f"èŠ‚ç‚¹ {node_id} æ¢å¤æˆåŠŸ")
                self.failed_nodes.pop(node_id, None)
            else:
                logger.warning(f"èŠ‚ç‚¹ {node_id} æ¢å¤å¤±è´¥")

        except Exception as e:
            logger.error(f"èŠ‚ç‚¹æ¢å¤è¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            self.recovery_tasks.pop(node_id, None)

    async def _attempt_node_recovery(self, node_id: str) -> bool:
        """å°è¯•æ¢å¤èŠ‚ç‚¹"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„èŠ‚ç‚¹æ¢å¤é€»è¾‘
            # ä¾‹å¦‚ï¼šé‡å¯æœåŠ¡ã€é‡æ–°è¿æ¥ç­‰
            return True
        except Exception:
            return False

    def is_node_failed(self, node_id: str) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¤„äºæ•…éšœçŠ¶æ€"""
        return node_id in self.failed_nodes


class SecurityManager:
    """å®‰å…¨ç®¡ç†å™¨"""

    def __init__(self, secret_key: str = None):
        self.secret_key = secret_key or self._generate_secret_key()
        self.node_tokens: Dict[str, str] = {}
        self.token_expiry: Dict[str, datetime] = {}

    def _generate_secret_key(self) -> str:
        """ç”Ÿæˆå¯†é’¥"""
        return hashlib.sha256(str(uuid.uuid4()).encode()).hexdigest()

    def generate_node_token(self, node_id: str) -> str:
        """ä¸ºèŠ‚ç‚¹ç”Ÿæˆè®¤è¯ä»¤ç‰Œ"""
        timestamp = str(int(time.time()))
        message = f"{node_id}:{timestamp}"
        signature = hmac.new(
            self.secret_key.encode(),
            message.encode(),
            hashlib.sha256
        ).hexdigest()

        token = f"{message}:{signature}"
        self.node_tokens[node_id] = token
        self.token_expiry[node_id] = datetime.now() + timedelta(hours=24)

        return token

    def verify_node_token(self, node_id: str, token: str) -> bool:
        """éªŒè¯èŠ‚ç‚¹ä»¤ç‰Œ"""
        try:
            # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ
            if node_id in self.token_expiry:
                if datetime.now() > self.token_expiry[node_id]:
                    return False

            # è§£æä»¤ç‰Œ
            parts = token.split(':')
            if len(parts) != 3:
                return False

            received_node_id, timestamp, signature = parts
            if received_node_id != node_id:
                return False

            # éªŒè¯ç­¾å
            message = f"{received_node_id}:{timestamp}"
            expected_signature = hmac.new(
                self.secret_key.encode(),
                message.encode(),
                hashlib.sha256
            ).hexdigest()

            return hmac.compare_digest(signature, expected_signature)

        except Exception as e:
            logger.error(f"ä»¤ç‰ŒéªŒè¯å¤±è´¥: {e}")
            return False


class EnhancedDistributedService(DistributedService):
    """å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡"""

    def __init__(self,
                 discovery_port: int = 8888,
                 load_balancing_strategy: LoadBalancingStrategy = LoadBalancingStrategy.INTELLIGENT,
                 enable_security: bool = True,
                 enable_monitoring: bool = True):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡

        Args:
            discovery_port: èŠ‚ç‚¹å‘ç°ç«¯å£
            load_balancing_strategy: è´Ÿè½½å‡è¡¡ç­–ç•¥
            enable_security: æ˜¯å¦å¯ç”¨å®‰å…¨åŠŸèƒ½
            enable_monitoring: æ˜¯å¦å¯ç”¨ç›‘æ§åŠŸèƒ½
        """
        super().__init__(discovery_port)

        # å¢å¼ºç»„ä»¶
        self.load_balancer = IntelligentLoadBalancer(load_balancing_strategy)
        self.failover_manager = FailoverManager()
        self.security_manager = SecurityManager() if enable_security else None

        # èŠ‚ç‚¹ç®¡ç†
        self.enhanced_nodes: Dict[str, EnhancedNodeInfo] = {}
        self.node_health_checks: Dict[str, threading.Timer] = {}

        # ä»»åŠ¡ç®¡ç†
        self.enhanced_tasks: Dict[str, EnhancedDistributedTask] = {}
        self.task_dependencies: Dict[str, Set[str]] = defaultdict(set)

        # ç›‘æ§å’Œç»Ÿè®¡
        self.enable_monitoring = enable_monitoring
        self.performance_metrics: Dict[str, Any] = {}
        self.event_bus = get_event_bus()

        # é…ç½®
        self.config = {
            'health_check_interval': 30,
            'task_timeout': 300,
            'max_retries': 3,
            'auto_scaling_enabled': True,
            'min_nodes': 1,
            'max_nodes': 10
        }

        logger.info("å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def start_service(self):
        """å¯åŠ¨å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡"""
        super().start_service()

        # å¯åŠ¨å¥åº·æ£€æŸ¥
        self._start_health_monitoring()

        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        if self.enable_monitoring:
            self._start_performance_monitoring()

        # å¯åŠ¨è‡ªåŠ¨æ‰©ç¼©å®¹
        if self.config['auto_scaling_enabled']:
            self._start_auto_scaling()

        logger.info("å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡å·²å¯åŠ¨")

    def start(self):
        """å¯åŠ¨åˆ†å¸ƒå¼æœåŠ¡ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return self.start_service()

    def stop(self):
        """åœæ­¢åˆ†å¸ƒå¼æœåŠ¡"""
        try:
            logger.info("ğŸ›‘ åœæ­¢å¢å¼ºåˆ†å¸ƒå¼æœåŠ¡...")

            # åœæ­¢åŸºç¡€åˆ†å¸ƒå¼æœåŠ¡
            if hasattr(self, '_base_service') and self._base_service:
                self._base_service.stop_service()

            # åœæ­¢æ€§èƒ½ç›‘æ§
            if hasattr(self, '_performance_monitor_active'):
                self._performance_monitor_active = False

            # åœæ­¢è´Ÿè½½å‡è¡¡
            if hasattr(self, '_load_balancer_active'):
                self._load_balancer_active = False

            # æ¸…ç†èµ„æº
            if hasattr(self, '_task_queue'):
                self._task_queue.clear()

            logger.info("âœ… å¢å¼ºåˆ†å¸ƒå¼æœåŠ¡å·²åœæ­¢")
            return True

        except Exception as e:
            logger.error(f"åœæ­¢å¢å¼ºåˆ†å¸ƒå¼æœåŠ¡å¤±è´¥: {e}")
            return False

    def submit_task(self, task: Dict[str, Any]) -> bool:
        """
        æäº¤ä»»åŠ¡åˆ°åˆ†å¸ƒå¼ç³»ç»Ÿ

        Args:
            task: ä»»åŠ¡ä¿¡æ¯å­—å…¸

        Returns:
            æäº¤æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ“¤ æäº¤åˆ†å¸ƒå¼ä»»åŠ¡: {task.get('name', 'unknown')}")

            # éªŒè¯ä»»åŠ¡æ ¼å¼
            if not isinstance(task, dict) or 'name' not in task:
                logger.error("æ— æ•ˆçš„ä»»åŠ¡æ ¼å¼")
                return False

            # æ·»åŠ ä»»åŠ¡å…ƒæ•°æ®
            task_with_metadata = {
                **task,
                'submit_time': datetime.now().isoformat(),
                'status': 'submitted',
                'priority': task.get('priority', 'normal'),
                'node_id': self.node_id if hasattr(self, 'node_id') else 'local'
            }

            # æ¨¡æ‹Ÿä»»åŠ¡æäº¤åˆ°é˜Ÿåˆ—
            if not hasattr(self, '_task_queue'):
                self._task_queue = []

            self._task_queue.append(task_with_metadata)

            logger.info(f"âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task['name']}")
            return True

        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            return False

    def stop_service(self):
        """åœæ­¢å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡"""
        # åœæ­¢å¥åº·æ£€æŸ¥
        for timer in self.node_health_checks.values():
            timer.cancel()

        super().stop_service()
        logger.info("å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡å·²åœæ­¢")

    def submit_enhanced_task(self,
                             task_type: str,
                             task_data: Dict[str, Any],
                             priority: TaskPriority = TaskPriority.NORMAL,
                             cpu_requirement: float = 1.0,
                             memory_requirement: int = 512,
                             gpu_requirement: bool = False,
                             timeout: int = 300,
                             dependencies: List[str] = None,
                             affinity_rules: Dict[str, Any] = None) -> str:
        """æäº¤å¢å¼ºç‰ˆä»»åŠ¡"""
        task_id = str(uuid.uuid4())

        task = EnhancedDistributedTask(
            task_id=task_id,
            task_type=task_type,
            task_data=task_data,
            priority=priority.value,
            cpu_requirement=cpu_requirement,
            memory_requirement=memory_requirement,
            gpu_requirement=gpu_requirement,
            timeout=timeout,
            dependencies=dependencies or [],
            affinity_rules=affinity_rules or {}
        )

        self.enhanced_tasks[task_id] = task

        # å¤„ç†ä»»åŠ¡ä¾èµ–
        if dependencies:
            for dep_id in dependencies:
                self.task_dependencies[dep_id].add(task_id)

        # å°è¯•è°ƒåº¦ä»»åŠ¡
        self._schedule_enhanced_task(task)

        logger.info(f"æäº¤å¢å¼ºç‰ˆä»»åŠ¡: {task_id} ({task_type})")
        return task_id

    def _schedule_enhanced_task(self, task: EnhancedDistributedTask):
        """è°ƒåº¦å¢å¼ºç‰ˆä»»åŠ¡"""
        try:
            # æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
            if not self._check_task_dependencies(task):
                logger.debug(f"ä»»åŠ¡ {task.task_id} ä¾èµ–æœªæ»¡è¶³ï¼Œç­‰å¾…è°ƒåº¦")
                return

            # é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹
            available_nodes = [node for node in self.enhanced_nodes.values()
                               if not self.failover_manager.is_node_failed(node.node_id)]

            selected_node = self.load_balancer.select_node(available_nodes, task)

            if selected_node:
                self._assign_task_to_node(task, selected_node)
            else:
                logger.warning(f"æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹æ‰§è¡Œä»»åŠ¡ {task.task_id}")
                task.status = "pending"

        except Exception as e:
            logger.error(f"è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}")
            task.status = "failed"
            task.error_message = str(e)

    def _check_task_dependencies(self, task: EnhancedDistributedTask) -> bool:
        """æ£€æŸ¥ä»»åŠ¡ä¾èµ–æ˜¯å¦æ»¡è¶³"""
        for dep_id in task.dependencies:
            if dep_id in self.enhanced_tasks:
                dep_task = self.enhanced_tasks[dep_id]
                if dep_task.status != "completed":
                    return False
            else:
                # ä¾èµ–ä»»åŠ¡ä¸å­˜åœ¨
                return False
        return True

    def _assign_task_to_node(self, task: EnhancedDistributedTask, node: EnhancedNodeInfo):
        """å°†ä»»åŠ¡åˆ†é…ç»™èŠ‚ç‚¹"""
        try:
            task.assigned_node = node.node_id
            task.status = "assigned"
            task.start_time = datetime.now()

            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            node.task_count += 1
            node.current_load = min(1.0, node.task_count / node.max_concurrent_tasks)

            # å‘é€ä»»åŠ¡åˆ°èŠ‚ç‚¹ï¼ˆè¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„é€šä¿¡é€»è¾‘ï¼‰
            self._send_task_to_node(task, node)

            logger.info(f"ä»»åŠ¡ {task.task_id} å·²åˆ†é…ç»™èŠ‚ç‚¹ {node.node_id}")

        except Exception as e:
            logger.error(f"åˆ†é…ä»»åŠ¡å¤±è´¥: {e}")
            task.status = "failed"
            task.error_message = str(e)

            # å°è¯•æ•…éšœè½¬ç§»
            self._handle_task_failure(task, e)

    def _send_task_to_node(self, task: EnhancedDistributedTask, node: EnhancedNodeInfo):
        """å‘é€ä»»åŠ¡åˆ°èŠ‚ç‚¹"""
        # è¿™é‡Œå®ç°å…·ä½“çš„ä»»åŠ¡å‘é€é€»è¾‘
        # å¯ä»¥ä½¿ç”¨HTTPã€gRPCã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰æ–¹å¼
        pass

    def _handle_task_failure(self, task: EnhancedDistributedTask, error: Exception):
        """å¤„ç†ä»»åŠ¡å¤±è´¥"""
        if task.can_retry():
            task.retry_count += 1
            task.status = "pending"
            logger.info(f"ä»»åŠ¡ {task.task_id} é‡è¯• ({task.retry_count}/{task.max_retries})")

            # é‡æ–°è°ƒåº¦
            self._schedule_enhanced_task(task)
        else:
            task.status = "failed"
            task.error_message = str(error)
            logger.error(f"ä»»åŠ¡ {task.task_id} æœ€ç»ˆå¤±è´¥: {error}")

    def _start_health_monitoring(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        def health_check_worker():
            while self.running:
                try:
                    for node_id, node in self.enhanced_nodes.items():
                        self._check_node_health(node)
                    time.sleep(self.config['health_check_interval'])
                except Exception as e:
                    logger.error(f"å¥åº·æ£€æŸ¥é”™è¯¯: {e}")
                    time.sleep(5)

        health_thread = threading.Thread(target=health_check_worker, daemon=True)
        health_thread.start()

    def _check_node_health(self, node: EnhancedNodeInfo):
        """æ£€æŸ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        try:
            # æ›´æ–°å¥åº·åˆ†æ•°
            node.calculate_health_score()
            node.last_health_check = datetime.now()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ•…éšœè½¬ç§»
            if not node.is_healthy():
                self.failover_manager.handle_node_failure(
                    node.node_id,
                    Exception(f"èŠ‚ç‚¹å¥åº·åˆ†æ•°è¿‡ä½: {node.health_score}")
                )

        except Exception as e:
            logger.error(f"æ£€æŸ¥èŠ‚ç‚¹ {node.node_id} å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            node.consecutive_failures += 1

    def _start_performance_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        def performance_monitor_worker():
            while self.running:
                try:
                    self._collect_performance_metrics()
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡
                except Exception as e:
                    logger.error(f"æ€§èƒ½ç›‘æ§é”™è¯¯: {e}")
                    time.sleep(60)

        perf_thread = threading.Thread(target=performance_monitor_worker, daemon=True)
        perf_thread.start()

    def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            metrics = {
                'timestamp': datetime.now().isoformat(),
                'total_nodes': len(self.enhanced_nodes),
                'active_nodes': len([n for n in self.enhanced_nodes.values() if n.is_healthy()]),
                'total_tasks': len(self.enhanced_tasks),
                'running_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "running"]),
                'completed_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "completed"]),
                'failed_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "failed"]),
                'average_response_time': self._calculate_average_response_time(),
                'system_load': self._calculate_system_load()
            }

            self.performance_metrics = metrics

            # å‘é€æ€§èƒ½äº‹ä»¶
            if self.event_bus:
                from core.events.event_bus import BaseEvent
                metrics_event = BaseEvent('distributed_service_metrics', metrics)
                self.event_bus.publish(metrics_event)

        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")

    def _calculate_average_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        completed_tasks = [t for t in self.enhanced_tasks.values()
                           if t.status == "completed" and t.execution_time > 0]

        if not completed_tasks:
            return 0.0

        return sum(t.execution_time for t in completed_tasks) / len(completed_tasks)

    def _calculate_system_load(self) -> float:
        """è®¡ç®—ç³»ç»Ÿè´Ÿè½½"""
        if not self.enhanced_nodes:
            return 0.0

        total_load = sum(node.current_load for node in self.enhanced_nodes.values())
        return total_load / len(self.enhanced_nodes)

    def _start_auto_scaling(self):
        """å¯åŠ¨è‡ªåŠ¨æ‰©ç¼©å®¹"""
        def auto_scaling_worker():
            while self.running:
                try:
                    self._check_scaling_conditions()
                    time.sleep(120)  # æ¯2åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨æ‰©ç¼©å®¹é”™è¯¯: {e}")
                    time.sleep(120)

        scaling_thread = threading.Thread(target=auto_scaling_worker, daemon=True)
        scaling_thread.start()

    def _check_scaling_conditions(self):
        """æ£€æŸ¥æ‰©ç¼©å®¹æ¡ä»¶"""
        try:
            active_nodes = [n for n in self.enhanced_nodes.values() if n.is_healthy()]
            pending_tasks = [t for t in self.enhanced_tasks.values() if t.status == "pending"]

            # æ‰©å®¹æ¡ä»¶ï¼šå¾…å¤„ç†ä»»åŠ¡è¿‡å¤šæˆ–ç³»ç»Ÿè´Ÿè½½è¿‡é«˜
            if (len(pending_tasks) > len(active_nodes) * 2 or
                    self._calculate_system_load() > 0.8):

                if len(active_nodes) < self.config['max_nodes']:
                    logger.info("è§¦å‘è‡ªåŠ¨æ‰©å®¹æ¡ä»¶")
                    self._trigger_scale_out()

            # ç¼©å®¹æ¡ä»¶ï¼šç³»ç»Ÿè´Ÿè½½è¿‡ä½ä¸”èŠ‚ç‚¹æ•°é‡è¶…è¿‡æœ€å°å€¼
            elif (self._calculate_system_load() < 0.3 and
                  len(active_nodes) > self.config['min_nodes']):

                logger.info("è§¦å‘è‡ªåŠ¨ç¼©å®¹æ¡ä»¶")
                self._trigger_scale_in()

        except Exception as e:
            logger.error(f"æ£€æŸ¥æ‰©ç¼©å®¹æ¡ä»¶å¤±è´¥: {e}")

    def _trigger_scale_out(self):
        """è§¦å‘æ‰©å®¹"""
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„æ‰©å®¹é€»è¾‘
        # ä¾‹å¦‚ï¼šå¯åŠ¨æ–°çš„å®¹å™¨ã€è™šæ‹Ÿæœºç­‰
        logger.info("æ‰§è¡Œæ‰©å®¹æ“ä½œ")

    def _trigger_scale_in(self):
        """è§¦å‘ç¼©å®¹"""
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ç¼©å®¹é€»è¾‘
        # ä¾‹å¦‚ï¼šå…³é—­ç©ºé—²èŠ‚ç‚¹
        logger.info("æ‰§è¡Œç¼©å®¹æ“ä½œ")

    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            'service_running': self.running,
            'total_nodes': len(self.enhanced_nodes),
            'healthy_nodes': len([n for n in self.enhanced_nodes.values() if n.is_healthy()]),
            'total_tasks': len(self.enhanced_tasks),
            'pending_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "pending"]),
            'running_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "running"]),
            'completed_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "completed"]),
            'failed_tasks': len([t for t in self.enhanced_tasks.values() if t.status == "failed"]),
            'performance_metrics': self.performance_metrics,
            'load_balancing_strategy': self.load_balancer.strategy.value,
            'security_enabled': self.security_manager is not None,
            'monitoring_enabled': self.enable_monitoring
        }


# å…¨å±€å®ä¾‹
_enhanced_distributed_service: Optional[EnhancedDistributedService] = None


def get_enhanced_distributed_service() -> EnhancedDistributedService:
    """è·å–å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡å®ä¾‹"""
    global _enhanced_distributed_service
    if _enhanced_distributed_service is None:
        _enhanced_distributed_service = EnhancedDistributedService()
    return _enhanced_distributed_service


def initialize_enhanced_distributed_service(
    discovery_port: int = 8888,
    load_balancing_strategy: LoadBalancingStrategy = LoadBalancingStrategy.INTELLIGENT,
    enable_security: bool = True,
    enable_monitoring: bool = True,
    auto_start: bool = False
) -> EnhancedDistributedService:
    """åˆå§‹åŒ–å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡"""
    global _enhanced_distributed_service

    _enhanced_distributed_service = EnhancedDistributedService(
        discovery_port=discovery_port,
        load_balancing_strategy=load_balancing_strategy,
        enable_security=enable_security,
        enable_monitoring=enable_monitoring
    )

    if auto_start:
        _enhanced_distributed_service.start_service()

    logger.info("å¢å¼ºç‰ˆåˆ†å¸ƒå¼æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    return _enhanced_distributed_service
