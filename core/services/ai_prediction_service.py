#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AIé¢„æµ‹æœåŠ¡ - ç»Ÿä¸€çš„æœºå™¨å­¦ä¹ é¢„æµ‹æœåŠ¡

æä¾›ï¼š
1. å½¢æ€é¢„æµ‹
2. è¶‹åŠ¿é¢„æµ‹  
3. æƒ…ç»ªé¢„æµ‹
4. ä»·æ ¼é¢„æµ‹
5. é£é™©é¢„æµ‹
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
import json
import pickle
import os
import hashlib
from pathlib import Path
import traceback
from enum import Enum
from dataclasses import dataclass

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ æ¨¡å—
try:
    from models.deep_learning import build_deep_learning_model, TENSORFLOW_AVAILABLE
    from models.model_evaluation import evaluate_ml_model
    DL_AVAILABLE = True
except ImportError:
    DL_AVAILABLE = False
    TENSORFLOW_AVAILABLE = False

from core.services.base_service import BaseService

logger = logging.getLogger(__name__)

# æ·»åŠ æ¨¡å‹ç±»å‹æ˜ å°„å­—å…¸
MODEL_TYPE_DISPLAY_NAMES = {
    'deep_learning': 'æ·±åº¦å­¦ä¹ ',
    'statistical': 'ç»Ÿè®¡æ¨¡å‹',
    'rule_based': 'è§„åˆ™æ¨¡å‹',
    'ensemble': 'é›†æˆæ¨¡å‹',
    'pattern_analysis': 'å½¢æ€åˆ†æ',
    'pattern_analysis_fallback': 'å½¢æ€åˆ†æï¼ˆåå¤‡ï¼‰',
    'fallback': 'åå¤‡æ¨¡å‹'
}


def get_model_display_name(model_type: str) -> str:
    """è·å–æ¨¡å‹ç±»å‹çš„ä¸­æ–‡æ˜¾ç¤ºåç§°"""
    return MODEL_TYPE_DISPLAY_NAMES.get(model_type, model_type)


class AIModelType:
    """AIæ¨¡å‹ç±»å‹"""
    DEEP_LEARNING = "deep_learning"
    ENSEMBLE = "ensemble"
    STATISTICAL = "statistical"
    RULE_BASED = "rule_based"


class PredictionType:
    """é¢„æµ‹ç±»å‹"""
    PATTERN = "pattern"      # å½¢æ€é¢„æµ‹
    TREND = "trend"         # è¶‹åŠ¿é¢„æµ‹
    SENTIMENT = "sentiment"  # æƒ…ç»ªé¢„æµ‹
    PRICE = "price"         # ä»·æ ¼é¢„æµ‹
    RISK = "risk"           # é£é™©é¢„æµ‹


class AIPredictionService(BaseService):
    """AIé¢„æµ‹æœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ–AIé¢„æµ‹æœåŠ¡"""
        super().__init__()

        # ä»æ•°æ®åº“åŠ è½½é…ç½®
        self._load_config_from_database()

        # æ¨¡å‹ç¼“å­˜
        self._models = {}
        self._predictions_cache = {}
        self._last_update = {}

        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()

    def _load_config_from_database(self):
        """ä»æ•°æ®åº“åŠ è½½é…ç½®"""
        try:
            from db.models.ai_config_models import get_ai_config_manager
            config_manager = get_ai_config_manager()

            # åŠ è½½å„ç§é…ç½®
            self.model_config = config_manager.get_config('model_config') or {
                'enabled': True,
                'model_type': AIModelType.ENSEMBLE,
                'confidence_threshold': 0.7,
                'prediction_horizon': 5,
                'feature_window': 20,
                'cache_size': 1000,
                'model_update_interval': 24
            }

            self.validation_config = config_manager.get_config('validation') or {
                'min_data_points': 10,
                'max_prediction_horizon': 30,
                'max_data_rows': 10000,
                'required_columns': ['open', 'high', 'low', 'close']
            }

            self.feature_config = config_manager.get_config('feature_config') or {
                'technical_indicators': True,
                'pattern_features': True,
                'volume_features': True,
                'price_features': True,
                'volatility_features': True
            }

            self.cache_config = config_manager.get_config('cache_config') or {
                'enable_cache': True,
                'cache_ttl': 300,
                'max_cache_size': 1000
            }

            self.logging_config = config_manager.get_config('logging') or {
                'log_predictions': True,
                'log_level': 'INFO',
                'detailed_errors': True
            }

            logger.info("âœ… AIé¢„æµ‹é…ç½®å·²ä»æ•°æ®åº“åŠ è½½")

        except Exception as e:
            logger.warning(f"ä»æ•°æ®åº“åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.model_config = {
                'enabled': True,
                'model_type': AIModelType.ENSEMBLE,
                'confidence_threshold': 0.7,
                'prediction_horizon': 5,
                'feature_window': 20,
                'cache_size': 1000,
                'model_update_interval': 24
            }

            self.validation_config = {
                'min_data_points': 10,
                'max_prediction_horizon': 30,
                'max_data_rows': 10000,
                'required_columns': ['open', 'high', 'low', 'close']
            }

            self.feature_config = {
                'technical_indicators': True,
                'pattern_features': True,
                'volume_features': True,
                'price_features': True,
                'volatility_features': True
            }

            self.cache_config = {
                'enable_cache': True,
                'cache_ttl': 300,
                'max_cache_size': 1000
            }

            self.logging_config = {
                'log_predictions': True,
                'log_level': 'INFO',
                'detailed_errors': True
            }

    def _validate_kdata(self, kdata: pd.DataFrame) -> bool:
        """
        éªŒè¯Kçº¿æ•°æ®æ ¼å¼å’Œå†…å®¹

        Args:
            kdata: Kçº¿æ•°æ®DataFrame

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡

        Raises:
            ValueError: æ•°æ®æ ¼å¼é”™è¯¯
            TypeError: æ•°æ®ç±»å‹é”™è¯¯
        """
        required_columns = ['open', 'high', 'low', 'close']

        # æ£€æŸ¥åŸºç¡€æ ¼å¼
        if kdata is None or kdata.empty:
            logger.warning("Kçº¿æ•°æ®ä¸ºç©º")
            return False

        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_columns = [col for col in required_columns if col not in kdata.columns]
        if missing_columns:
            raise ValueError(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")

        # æ£€æŸ¥æ•°æ®ç±»å‹
        for col in required_columns:
            if not pd.api.types.is_numeric_dtype(kdata[col]):
                raise TypeError(f"åˆ— {col} å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ï¼Œå½“å‰ç±»å‹: {kdata[col].dtype}")

        # æ£€æŸ¥ç©ºå€¼
        null_counts = kdata[required_columns].isnull().sum()
        if null_counts.any():
            logger.warning(f"Kçº¿æ•°æ®åŒ…å«ç©ºå€¼: {null_counts[null_counts > 0].to_dict()}")

        # æ£€æŸ¥æ•°æ®åˆç†æ€§
        invalid_high_low = (kdata['high'] < kdata['low']).sum()
        if invalid_high_low > 0:
            raise ValueError(f"å‘ç° {invalid_high_low} æ¡è®°å½•çš„é«˜ä»·ä½äºä½ä»·")

        # æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
        for col in required_columns:
            if (kdata[col] <= 0).any():
                raise ValueError(f"åˆ— {col} åŒ…å«éæ­£æ•°å€¼")

        # æ£€æŸ¥æ•°æ®å¤§å°é™åˆ¶
        max_rows = 10000  # é™åˆ¶æœ€å¤§è¡Œæ•°
        if len(kdata) > max_rows:
            logger.warning(f"æ•°æ®è¡Œæ•°({len(kdata)})è¶…è¿‡å»ºè®®æœ€å¤§å€¼({max_rows})")

        return True

    def _generate_cache_key(self, kdata: pd.DataFrame, method: str, **kwargs) -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„ç¼“å­˜é”®

        Args:
            kdata: Kçº¿æ•°æ®
            method: æ–¹æ³•åç§°
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        try:
            # åŸºç¡€ä¿¡æ¯
            basic_info = f"{method}_{kdata.shape[0]}_{kdata.shape[1]}"

            # æ—¶é—´èŒƒå›´ä¿¡æ¯
            if hasattr(kdata.index, 'min') and hasattr(kdata.index, 'max'):
                try:
                    time_info = f"_{kdata.index.min()}_{kdata.index.max()}"
                except Exception:
                    time_info = f"_{len(kdata)}"
            else:
                time_info = f"_{len(kdata)}"

            # æ•°æ®å†…å®¹æ‘˜è¦
            if len(kdata) > 0:
                try:
                    first_row_sum = float(kdata.iloc[0][['open', 'high', 'low', 'close']].sum())
                    last_row_sum = float(kdata.iloc[-1][['open', 'high', 'low', 'close']].sum())
                    content_info = f"_{first_row_sum:.2f}_{last_row_sum:.2f}"
                except Exception:
                    content_info = "_default"
            else:
                content_info = "_empty"

            # é¢å¤–å‚æ•°
            kwargs_str = "_".join(f"{k}_{v}" for k, v in sorted(kwargs.items()))
            if kwargs_str:
                kwargs_str = f"_{kwargs_str}"

            # ç”Ÿæˆæœ€ç»ˆçš„ç¼“å­˜é”®
            cache_content = f"{basic_info}{time_info}{content_info}{kwargs_str}"
            cache_key = hashlib.md5(cache_content.encode('utf-8')).hexdigest()[:16]

            return f"{method}_{cache_key}"

        except Exception as e:
            logger.warning(f"ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é”®")
            return f"{method}_default_{datetime.now().timestamp()}"

    def _initialize_models(self):
        """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
        try:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆ–åˆ›å»ºæ–°æ¨¡å‹
            model_dir = Path("models/trained")
            model_dir.mkdir(exist_ok=True)

            if DL_AVAILABLE:
                logger.info("âœ… æ·±åº¦å­¦ä¹ æ¨¡å—å¯ç”¨ï¼Œåˆå§‹åŒ–AIé¢„æµ‹æ¨¡å‹")
                self._load_or_create_models()
            else:
                logger.warning("âš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç»Ÿè®¡æ¨¡å‹")
                self._initialize_statistical_models()

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self._initialize_fallback_models()

    def _load_or_create_models(self):
        """åŠ è½½æˆ–åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            model_path = Path(f"models/trained/{pred_type}_model.h5")
            if model_path.exists():
                try:
                    # å°è¯•åŠ è½½TensorFlowæ¨¡å‹
                    if TENSORFLOW_AVAILABLE:
                        import tensorflow as tf

                        # éªŒè¯æ¨¡å‹æ–‡ä»¶
                        if model_path.stat().st_size == 0:
                            logger.warning(f"{pred_type}æ¨¡å‹æ–‡ä»¶ä¸ºç©º")
                            self._models[pred_type] = None
                            continue

                        # åŠ è½½æ¨¡å‹å¹¶éªŒè¯
                        model = tf.keras.models.load_model(str(model_path))

                        # åŸºç¡€æ¨¡å‹éªŒè¯
                        if not hasattr(model, 'predict'):
                            logger.warning(f"{pred_type}æ¨¡å‹ç¼ºå°‘predictæ–¹æ³•")
                            self._models[pred_type] = None
                            continue

                        self._models[pred_type] = model
                        logger.info(f"âœ… åŠ è½½{pred_type}æ·±åº¦å­¦ä¹ æ¨¡å‹æˆåŠŸ")
                    else:
                        # å¦‚æœæ²¡æœ‰TensorFlowï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ç®€åŒ–æ¨¡å‹
                        try:
                            with open(model_path, 'r', encoding='utf-8') as f:
                                model_data = json.load(f)
                                if model_data.get('model_type') == 'simplified':
                                    self._models[pred_type] = model_data
                                    logger.info(f"âœ… åŠ è½½{pred_type}ç®€åŒ–æ¨¡å‹")
                                else:
                                    raise ValueError("Not a simplified model")
                        except Exception:
                            self._models[pred_type] = None
                            logger.warning(f"âš ï¸ æ— æ³•è¯†åˆ«{pred_type}æ¨¡å‹æ ¼å¼")

                except Exception as e:
                    # å›é€€ï¼šå°è¯•åŠ è½½ä¸ºç®€åŒ–æ¨¡å‹
                    try:
                        with open(model_path, 'r', encoding='utf-8') as f:
                            model_data = json.load(f)
                            if model_data.get('model_type') == 'simplified':
                                self._models[pred_type] = model_data
                                logger.info(f"âœ… åŠ è½½{pred_type}ç®€åŒ–æ¨¡å‹ï¼ˆå›é€€æ¨¡å¼ï¼‰")
                            else:
                                raise ValueError("Not a simplified model")
                    except Exception:
                        logger.warning(f"âš ï¸ åŠ è½½{pred_type}æ¨¡å‹å¤±è´¥: {e}")
                        self._models[pred_type] = None
            else:
                # æ ‡è®°éœ€è¦è®­ç»ƒ
                self._models[pred_type] = None
                logger.warning(f"âš ï¸ åŠ è½½{pred_type}æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·¯å¾„: {model_path}")

    def _initialize_statistical_models(self):
        """åˆå§‹åŒ–ç»Ÿè®¡æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–ç»Ÿè®¡é¢„æµ‹æ¨¡å‹")
        # ä½¿ç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³•ä½œä¸ºåå¤‡
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            self._models[pred_type] = "statistical"

    def _initialize_fallback_models(self):
        """åˆå§‹åŒ–åå¤‡æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–è§„åˆ™åŸºç¡€æ¨¡å‹")
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            self._models[pred_type] = "rule_based"

    def predict_patterns(self, kdata: pd.DataFrame, patterns: List[Dict]) -> Dict[str, Any]:
        """
        é¢„æµ‹å½¢æ€ä¿¡å·

        Args:
            kdata: Kçº¿æ•°æ®
            patterns: æ£€æµ‹åˆ°çš„å½¢æ€åˆ—è¡¨

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿—å¼€å§‹ ===
        logger.info("="*80)
        logger.info("ğŸš€ AIé¢„æµ‹æœåŠ¡ - predict_patterns å¼€å§‹")
        logger.info(f"ğŸ“Š è¾“å…¥æ•°æ®: Kçº¿é•¿åº¦={len(kdata)}, å½¢æ€æ•°é‡={len(patterns)}")
        logger.info(f"ğŸ§  å½“å‰æ¨¡å‹é…ç½®: {self.model_config}")
        logger.info(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {self.model_config.get('model_type', 'N/A')}")
        logger.info("="*80)
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not self._validate_kdata(kdata):
                return self._get_fallback_pattern_prediction()

            if not patterns or not isinstance(patterns, list):
                logger.warning("å½¢æ€åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹")
                patterns = []

            # éªŒè¯æ¯ä¸ªå½¢æ€çš„ç»“æ„
            valid_patterns = []
            for i, pattern in enumerate(patterns):
                if not isinstance(pattern, dict):
                    logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                    continue

                # æ£€æŸ¥å¿…è¦å­—æ®µï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„å­—æ®µå
                has_name = any(key in pattern for key in ['name', 'pattern_name', 'pattern_type'])
                if not has_name:
                    logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œç¼ºå°‘åç§°å­—æ®µï¼Œè·³è¿‡")
                    continue

                # è§„èŒƒåŒ–å­—æ®µåï¼Œç¡®ä¿æœ‰nameå­—æ®µä¾›åç»­ä½¿ç”¨
                if 'name' not in pattern:
                    if 'pattern_name' in pattern:
                        pattern['name'] = pattern['pattern_name']
                    elif 'pattern_type' in pattern:
                        pattern['name'] = pattern['pattern_type']

                valid_patterns.append(pattern)

            # ç”¨æœ‰æ•ˆçš„å½¢æ€æ›¿æ¢åŸå§‹åˆ—è¡¨
            patterns = valid_patterns
            logger.info(f"å½¢æ€æ•°æ®éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆå½¢æ€æ•°é‡: {len(patterns)}/{len(valid_patterns)}")

            cache_key = self._generate_cache_key(kdata, "predict_patterns", patterns=len(patterns))
            if cache_key in self._predictions_cache:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å½¢æ€é¢„æµ‹ç»“æœ: {cache_key}")
                return self._predictions_cache[cache_key]

            prediction = self._generate_pattern_prediction(kdata, patterns)
            self._predictions_cache[cache_key] = prediction
            return prediction

        except Exception as e:
            logger.error(f"å½¢æ€é¢„æµ‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self._get_fallback_pattern_prediction()

    def predict_trend(self, kdata: pd.DataFrame, timeframe: int = 5) -> Dict[str, Any]:
        """
        è¶‹åŠ¿é¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            timeframe: é¢„æµ‹æ—¶é—´æ¡†æ¶ï¼ˆå¤©æ•°ï¼‰

        Returns:
            è¶‹åŠ¿é¢„æµ‹ç»“æœ
        """
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not self._validate_kdata(kdata):
                raise ValueError("æ— æ•ˆçš„Kçº¿æ•°æ®")

            # å‚æ•°éªŒè¯
            if not isinstance(timeframe, int) or timeframe < 1 or timeframe > 30:
                raise ValueError("é¢„æµ‹æ—¶é—´æ¡†æ¶å¿…é¡»åœ¨1-30å¤©ä¹‹é—´")

            if len(kdata) < timeframe * 2:
                raise ValueError(f"æ•°æ®é•¿åº¦({len(kdata)})ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{timeframe * 2}ä¸ªæ•°æ®ç‚¹")

            features = self._extract_trend_features(kdata)
            model = self._models.get(PredictionType.TREND)

            if model and model != "rule_based" and model != "statistical":
                # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
                prediction = self._predict_with_dl_model(model, features, PredictionType.TREND)
                if prediction:  # ç¡®ä¿é¢„æµ‹ç»“æœä¸ä¸ºNone
                    return prediction

            if model == "statistical":
                # ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹
                prediction = self._predict_with_statistical_model(features, PredictionType.TREND)
                if prediction:
                    return prediction

            # ä½¿ç”¨è§„åˆ™æ¨¡å‹ä½œä¸ºåå¤‡
            prediction = self._predict_with_rules(kdata, PredictionType.TREND)
            return prediction

        except Exception as e:
            logger.error(f"è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_trend_prediction()

    def predict_sentiment(self, kdata: pd.DataFrame, market_data: Dict = None) -> Dict[str, Any]:
        """
        æƒ…ç»ªé¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            market_data: å¸‚åœºæ•°æ®

        Returns:
            æƒ…ç»ªé¢„æµ‹ç»“æœ
        """
        try:
            features = self._extract_sentiment_features(kdata, market_data)
            model = self._models.get(PredictionType.SENTIMENT)

            if model and model != "rule_based" and model != "statistical":
                prediction = self._predict_with_dl_model(model, features, PredictionType.SENTIMENT)
            elif model == "statistical":
                prediction = self._predict_with_statistical_model(features, PredictionType.SENTIMENT)
            else:
                prediction = self._predict_sentiment_with_rules(kdata, market_data)

            return prediction

        except Exception as e:
            logger.error(f"æƒ…ç»ªé¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_sentiment_prediction()

    def predict_price(self, kdata: pd.DataFrame, horizon: int = 5) -> Dict[str, Any]:
        """
        ä»·æ ¼é¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰

        Returns:
            ä»·æ ¼é¢„æµ‹ç»“æœ
        """
        try:
            features = self._extract_price_features(kdata)
            model = self._models.get(PredictionType.PRICE)

            if model and model != "rule_based" and model != "statistical":
                # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
                prediction = self._predict_with_dl_model(model, features, PredictionType.PRICE)
                if prediction:  # ç¡®ä¿é¢„æµ‹ç»“æœä¸ä¸ºNone
                    return prediction

            if model == "statistical":
                # ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹
                prediction = self._predict_with_statistical_model(features, PredictionType.PRICE)
                if prediction:
                    return prediction

            # ä½¿ç”¨è§„åˆ™æ¨¡å‹ä½œä¸ºåå¤‡
            prediction = self._predict_price_with_rules(kdata, horizon)
            return prediction

        except Exception as e:
            logger.error(f"ä»·æ ¼é¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_price_prediction()

    def assess_risk(self, kdata: pd.DataFrame, predictions: Dict = None) -> Dict[str, Any]:
        """
        é£é™©è¯„ä¼°

        Args:
            kdata: Kçº¿æ•°æ®
            predictions: å…¶ä»–é¢„æµ‹ç»“æœ

        Returns:
            é£é™©è¯„ä¼°ç»“æœ
        """
        try:
            # è®¡ç®—å„ç§é£é™©æŒ‡æ ‡
            volatility_risk = self._calculate_volatility_risk(kdata)
            technical_risk = self._calculate_technical_risk(kdata)
            market_risk = self._calculate_market_risk(kdata)

            # ç»¼åˆé£é™©è¯„ä¼°
            overall_risk = self._calculate_overall_risk(
                volatility_risk, technical_risk, market_risk, predictions
            )

            return {
                'overall_risk': overall_risk,
                'volatility_risk': volatility_risk,
                'technical_risk': technical_risk,
                'market_risk': market_risk,
                'risk_level': self._categorize_risk(overall_risk),
                'risk_factors': self._identify_risk_factors(kdata),
                'recommendations': self._get_risk_recommendations(overall_risk)
            }

        except Exception as e:
            logger.error(f"é£é™©è¯„ä¼°å¤±è´¥: {e}")
            return self._get_fallback_risk_assessment()

    def _generate_pattern_prediction(self, kdata: pd.DataFrame, patterns: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå½¢æ€é¢„æµ‹"""
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿— ===
        logger.info("ğŸ”§ _generate_pattern_prediction å¼€å§‹")
        logger.info(f"ğŸ“Š å½¢æ€æ•°é‡: {len(patterns)}")

        if not patterns:
            logger.warning("âš ï¸ å½¢æ€åˆ—è¡¨ä¸ºç©ºï¼Œè°ƒç”¨ _predict_without_patterns")
            logger.info(f"ğŸ§  å³å°†ä½¿ç”¨æ¨¡å‹ç±»å‹: {self.model_config.get('model_type', 'N/A')}")
            result = self._predict_without_patterns(kdata)
            logger.info(f"âœ… _predict_without_patterns è¿”å›ç»“æœ: {result}")
            return result
        # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        # éªŒè¯æ¯ä¸ªå½¢æ€çš„ç»“æ„
        valid_patterns = []
        for i, pattern in enumerate(patterns):
            if not isinstance(pattern, dict):
                logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                continue

            # æ£€æŸ¥å¿…è¦å­—æ®µï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„å­—æ®µå
            has_name = any(key in pattern for key in ['name', 'pattern_name', 'pattern_type'])
            if not has_name:
                logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œç¼ºå°‘åç§°å­—æ®µï¼Œè·³è¿‡")
                continue

            # è§„èŒƒåŒ–å­—æ®µåï¼Œç¡®ä¿æœ‰nameå­—æ®µä¾›åç»­ä½¿ç”¨
            if 'name' not in pattern:
                if 'pattern_name' in pattern:
                    pattern['name'] = pattern['pattern_name']
                elif 'pattern_type' in pattern:
                    pattern['name'] = pattern['pattern_type']

            valid_patterns.append(pattern)

        logger.info(f"æœ‰æ•ˆå½¢æ€æ•°é‡: {len(valid_patterns)}")

        if not valid_patterns:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å½¢æ€æ•°æ®ï¼Œä½¿ç”¨æ— å½¢æ€é¢„æµ‹")
            return self._predict_without_patterns(kdata)

        # === å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œä¸åŒçš„å½¢æ€é¢„æµ‹ ===
        model_type = self.model_config.get('model_type', AIModelType.ENSEMBLE)
        logger.info(f"ğŸ¯ æœ‰å½¢æ€çš„é¢„æµ‹ï¼Œä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")

        # åˆ†æå½¢æ€ä¿¡å·å¼ºåº¦
        buy_signals = [p for p in valid_patterns if p.get('signal_type') == 'bullish']
        sell_signals = [p for p in valid_patterns if p.get('signal_type') == 'bearish']

        # è®¡ç®—åŸºç¡€å½¢æ€ç»Ÿè®¡
        pattern_analysis = {
            'total_patterns': len(valid_patterns),
            'bullish_signals': len(buy_signals),
            'bearish_signals': len(sell_signals),
            'avg_confidence': np.mean([p.get('confidence', 0.5) for p in valid_patterns])
        }

        # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œä¸åŒçš„é¢„æµ‹å¤„ç†
        try:
            if model_type == AIModelType.DEEP_LEARNING:
                logger.info("ğŸ¤– ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_deep_learning(kdata, valid_patterns, pattern_analysis)
            elif model_type == AIModelType.STATISTICAL:
                logger.info("ğŸ“Š ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_statistical(kdata, valid_patterns, pattern_analysis)
            elif model_type == AIModelType.RULE_BASED:
                logger.info("ğŸ“ ä½¿ç”¨è§„åˆ™æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_rule_based(kdata, valid_patterns, pattern_analysis)
            else:  # ENSEMBLE
                logger.info("ğŸ”„ ä½¿ç”¨é›†æˆæ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_ensemble(kdata, valid_patterns, pattern_analysis)

            # æ·»åŠ å½¢æ€åˆ†æä¿¡æ¯
            result.update({
                'pattern_count': len(valid_patterns),
                'bullish_signals': len(buy_signals),
                'bearish_signals': len(sell_signals),
                'prediction_type': PredictionType.PATTERN,
                'timestamp': datetime.now().isoformat()
            })

            logger.info(f"âœ… å½¢æ€é¢„æµ‹å®Œæˆ:")
            logger.info(f"   ğŸ“ˆ æ–¹å‘: {result.get('direction', 'N/A')}")
            logger.info(f"   ğŸ¯ ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            logger.info(f"   ğŸ§  æ¨¡å‹ç±»å‹: {result.get('model_type', 'N/A')}")

            return result

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ç‰¹å®šå½¢æ€é¢„æµ‹å¤±è´¥ ({model_type}): {e}")
            logger.error(traceback.format_exc())
            # é™çº§åˆ°é€šç”¨å½¢æ€åˆ†æ
            return self._fallback_pattern_analysis(valid_patterns, buy_signals, sell_signals, pattern_analysis)

    def _predict_without_patterns(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """å½“å½¢æ€åˆ—è¡¨ä¸ºç©ºæ—¶ï¼Œæ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹"""
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿— ===
        logger.info("ğŸ¯ _predict_without_patterns å¼€å§‹æ‰§è¡Œ")
        model_type = self.model_config.get('model_type', AIModelType.ENSEMBLE)
        logger.info(f"ğŸ§  ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f"ğŸ“‹ å®Œæ•´æ¨¡å‹é…ç½®: {self.model_config}")
        # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„æµ‹æ–¹æ³•
            if model_type == AIModelType.DEEP_LEARNING:
                logger.info("ğŸ¤– è°ƒç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_deep_learning(kdata)
                result['model_path'] = 'deep_learning_without_patterns'
            elif model_type == AIModelType.STATISTICAL:
                logger.info("ğŸ“Š è°ƒç”¨ç»Ÿè®¡æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_statistical_method(kdata)
                result['model_path'] = 'statistical_without_patterns'
            elif model_type == AIModelType.RULE_BASED:
                logger.info("ğŸ“ è°ƒç”¨è§„åˆ™æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_rule_based_method(kdata)
                result['model_path'] = 'rule_based_without_patterns'
            else:  # ENSEMBLE
                logger.info("ğŸ”„ è°ƒç”¨é›†æˆæ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_ensemble_method(kdata)
                result['model_path'] = 'ensemble_without_patterns'

            # === è°ƒè¯•æ—¥å¿—ï¼šé¢„æµ‹ç»“æœ ===
            logger.info(f"âœ… {model_type} é¢„æµ‹å®Œæˆ:")
            logger.info(f"   ğŸ“ˆ æ–¹å‘: {result.get('direction', 'N/A')}")
            logger.info(f"   ğŸ¯ ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            logger.info(f"   ğŸ·ï¸ æ¨¡å‹ç±»å‹: {result.get('model_type', 'N/A')}")
            logger.info(f"   ğŸ›£ï¸ æ¨¡å‹è·¯å¾„: {result.get('model_path', 'N/A')}")
            # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

            return result

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹é¢„æµ‹å¤±è´¥ ({model_type}): {e}")
            logger.error(traceback.format_exc())
            # è¿”å›åå¤‡é¢„æµ‹
            return self._get_fallback_pattern_prediction()

    def _extract_pattern_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–ç”¨äºæ— å½¢æ€é¢„æµ‹çš„æŠ€æœ¯ç‰¹å¾"""
        features = []
        close_prices = kdata['close'].values
        high_prices = kdata['high'].values
        low_prices = kdata['low'].values
        volumes = kdata.get('volume', pd.Series([1]*len(kdata))).values

        # ä»·æ ¼ç‰¹å¾
        ma5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else close_prices[-1]
        ma10 = np.mean(close_prices[-10:]) if len(close_prices) >= 10 else close_prices[-1]
        ma20 = np.mean(close_prices[-20:]) if len(close_prices) >= 20 else close_prices[-1]

        features.extend([
            close_prices[-1] / ma5 - 1,  # ç›¸å¯¹5æ—¥å‡çº¿
            close_prices[-1] / ma10 - 1,  # ç›¸å¯¹10æ—¥å‡çº¿
            close_prices[-1] / ma20 - 1,  # ç›¸å¯¹20æ—¥å‡çº¿
            ma5 / ma20 - 1 if ma20 != 0 else 0,  # çŸ­æœŸè¶‹åŠ¿
        ])

        # æ³¢åŠ¨ç‡ç‰¹å¾
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-6:]) / close_prices[-6:-1]
            volatility = np.std(returns) if len(returns) > 1 else 0
            features.append(volatility)
        else:
            features.append(0)

        # æˆäº¤é‡ç‰¹å¾
        if len(volumes) >= 5:
            vol_ma5 = np.mean(volumes[-5:])
            vol_ma20 = np.mean(volumes[-20:]) if len(volumes) >= 20 else vol_ma5
            vol_ratio = volumes[-1] / vol_ma5 - 1 if vol_ma5 != 0 else 0
            features.append(vol_ratio)
        else:
            features.append(0)

        return np.array(features)

    def _predict_with_deep_learning(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹"""
        logger.info("ğŸ¤– === æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # æå–ç‰¹å¾
            features = self._extract_pattern_features(kdata)
            logger.info(f"ğŸ” ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(features)}")

            # æ¨¡æ‹Ÿæ·±åº¦å­¦ä¹ é¢„æµ‹ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„DLæ¨¡å‹ï¼‰
            prediction_strength = np.mean([
                features.get('price_momentum', 0.5),
                features.get('volume_strength', 0.5),
                features.get('volatility_signal', 0.5)
            ])

            # æ·»åŠ ä¸€äº›éšæœºæ€§æ¨¡æ‹Ÿç¥ç»ç½‘ç»œçš„å¤æ‚æ€§
            random_factor = np.random.normal(0, 0.1)
            adjusted_strength = np.clip(prediction_strength + random_factor, 0, 1)

            if adjusted_strength > 0.6:
                direction = "ä¸Šæ¶¨"
                confidence = 0.65 + (adjusted_strength - 0.6) * 0.3
            elif adjusted_strength < 0.4:
                direction = "ä¸‹è·Œ"
                confidence = 0.65 + (0.4 - adjusted_strength) * 0.3
            else:
                direction = "éœ‡è¡"
                confidence = 0.55 + abs(adjusted_strength - 0.5) * 0.2

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'deep_learning',
                'prediction_type': PredictionType.PATTERN,
                'features_used': len(features),
                'dl_strength': prediction_strength,
                'random_factor': random_factor
            }

            logger.info(f"ğŸ¤– æ·±åº¦å­¦ä¹ é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f"âŒ æ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_statistical_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """ç»Ÿè®¡æ¨¡å‹é¢„æµ‹"""
        logger.info("ğŸ“Š === ç»Ÿè®¡æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            features = self._extract_pattern_features(kdata)
            logger.info(f"ğŸ“ˆ ç»Ÿè®¡ç‰¹å¾æå–å®Œæˆ")

            # åŸºäºZ-scoreçš„ç»Ÿè®¡åˆ†æ
            price_zscore = features.get('price_zscore', 0)
            volume_zscore = features.get('volume_zscore', 0)

            # ç»Ÿè®¡å†³ç­–è§„åˆ™
            if price_zscore > 1.5 and volume_zscore > 0.5:
                direction = "ä¸Šæ¶¨"
                confidence = 0.70 + min(abs(price_zscore) * 0.1, 0.25)
            elif price_zscore < -1.5 and volume_zscore > 0.5:
                direction = "ä¸‹è·Œ"
                confidence = 0.70 + min(abs(price_zscore) * 0.1, 0.25)
            else:
                direction = "éœ‡è¡"
                confidence = 0.60 + abs(price_zscore) * 0.05

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'statistical',
                'prediction_type': PredictionType.PATTERN,
                'price_zscore': price_zscore,
                'volume_zscore': volume_zscore,
                'features_used': len(features)
            }

            logger.info(f"ğŸ“Š ç»Ÿè®¡æ¨¡å‹é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_rule_based_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """è§„åˆ™æ¨¡å‹é¢„æµ‹"""
        logger.info("ğŸ“ === è§„åˆ™æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            features = self._extract_pattern_features(kdata)
            logger.info(f"âš™ï¸ è§„åˆ™ç‰¹å¾æå–å®Œæˆ")

            # å¤šé‡æŠ€æœ¯æŒ‡æ ‡è§„åˆ™
            signals = []

            # è§„åˆ™1: å‡çº¿ä¿¡å·
            if features.get('ma_signal', 0) > 0.5:
                signals.append(('bullish', 0.8))
            elif features.get('ma_signal', 0) < -0.5:
                signals.append(('bearish', 0.8))

            # è§„åˆ™2: æˆäº¤é‡ä¿¡å·
            if features.get('volume_strength', 0) > 0.7:
                signals.append(('bullish', 0.6))

            # è§„åˆ™3: æ³¢åŠ¨ç‡ä¿¡å·
            if features.get('volatility_signal', 0) > 0.6:
                signals.append(('bearish', 0.7))

            # ç»¼åˆåˆ¤æ–­
            bullish_weight = sum(w for s, w in signals if s == 'bullish')
            bearish_weight = sum(w for s, w in signals if s == 'bearish')

            if bullish_weight > bearish_weight and bullish_weight > 0.5:
                direction = "ä¸Šæ¶¨"
                confidence = 0.75 + min(bullish_weight - bearish_weight, 0.2)
            elif bearish_weight > bullish_weight and bearish_weight > 0.5:
                direction = "ä¸‹è·Œ"
                confidence = 0.75 + min(bearish_weight - bullish_weight, 0.2)
            else:
                direction = "éœ‡è¡"
                confidence = 0.65

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'rule_based',
                'prediction_type': PredictionType.PATTERN,
                'signals_count': len(signals),
                'bullish_weight': bullish_weight,
                'bearish_weight': bearish_weight,
                'features_used': len(features)
            }

            logger.info(f"ğŸ“ è§„åˆ™æ¨¡å‹é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f"âŒ è§„åˆ™æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_ensemble_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """é›†æˆæ¨¡å‹é¢„æµ‹"""
        logger.info("ğŸ”„ === é›†æˆæ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # è°ƒç”¨æ‰€æœ‰å­æ¨¡å‹
            logger.info("ğŸ¤– è°ƒç”¨æ·±åº¦å­¦ä¹ å­æ¨¡å‹...")
            dl_result = self._predict_with_deep_learning(kdata)

            logger.info("ğŸ“Š è°ƒç”¨ç»Ÿè®¡æ¨¡å‹å­æ¨¡å‹...")
            stat_result = self._predict_with_statistical_method(kdata)

            logger.info("ğŸ“ è°ƒç”¨è§„åˆ™æ¨¡å‹å­æ¨¡å‹...")
            rule_result = self._predict_with_rule_based_method(kdata)

            # åŠ æƒæŠ•ç¥¨
            models = [
                (dl_result, 0.4),      # æ·±åº¦å­¦ä¹ æƒé‡40%
                (stat_result, 0.35),   # ç»Ÿè®¡æ¨¡å‹æƒé‡35%
                (rule_result, 0.25)    # è§„åˆ™æ¨¡å‹æƒé‡25%
            ]

            direction_votes = {'ä¸Šæ¶¨': 0, 'ä¸‹è·Œ': 0, 'éœ‡è¡': 0}
            total_confidence = 0
            total_weight = 0

            for result, weight in models:
                direction = result.get('direction', 'éœ‡è¡')
                confidence = result.get('confidence', 0.5)

                direction_votes[direction] += weight * confidence
                total_confidence += weight * confidence
                total_weight += weight

            # ç¡®å®šæœ€ç»ˆæ–¹å‘
            final_direction = max(direction_votes.items(), key=lambda x: x[1])[0]
            final_confidence = total_confidence / total_weight

            result = {
                'direction': final_direction,
                'confidence': final_confidence,
                'model_type': 'ensemble',
                'prediction_type': PredictionType.PATTERN,
                'sub_models': {
                    'deep_learning': dl_result,
                    'statistical': stat_result,
                    'rule_based': rule_result
                },
                'vote_weights': direction_votes
            }

            logger.info(f"ğŸ”„ é›†æˆæ¨¡å‹é¢„æµ‹ç»“æœ: {final_direction}, ç½®ä¿¡åº¦: {final_confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _extract_trend_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–è¶‹åŠ¿é¢„æµ‹ç‰¹å¾"""
        features = []

        # ä»·æ ¼ç‰¹å¾
        close_prices = kdata['close'].values
        features.extend([
            np.mean(close_prices[-5:]) / np.mean(close_prices[-20:]),  # çŸ­æœŸå‡çº¿æ¯”ç‡
            np.std(close_prices[-20:]) / np.mean(close_prices[-20:]),  # æ³¢åŠ¨ç‡
            (close_prices[-1] - close_prices[-5]) / close_prices[-5],  # 5æ—¥æ¶¨å¹…
            (close_prices[-1] - close_prices[-20]) / close_prices[-20]  # 20æ—¥æ¶¨å¹…
        ])

        # æˆäº¤é‡ç‰¹å¾
        if 'volume' in kdata.columns:
            volumes = kdata['volume'].values
            features.extend([
                np.mean(volumes[-5:]) / np.mean(volumes[-20:]),  # æˆäº¤é‡æ¯”ç‡
                np.std(volumes[-20:]) / np.mean(volumes[-20:])   # æˆäº¤é‡æ³¢åŠ¨
            ])

        return np.array(features)

    def _extract_sentiment_features(self, kdata: pd.DataFrame, market_data: Dict = None) -> np.ndarray:
        """æå–æƒ…ç»ªé¢„æµ‹ç‰¹å¾"""
        features = []

        # æŠ€æœ¯æƒ…ç»ªç‰¹å¾
        close_prices = kdata['close'].values
        high_prices = kdata['high'].values
        low_prices = kdata['low'].values

        # RSIè¿‘ä¼¼è®¡ç®—
        price_changes = np.diff(close_prices[-21:])
        gains = np.where(price_changes > 0, price_changes, 0)
        losses = np.where(price_changes < 0, -price_changes, 0)
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        rsi = 100 - (100 / (1 + avg_gain / (avg_loss + 1e-8)))

        features.extend([
            rsi / 100,  # æ ‡å‡†åŒ–RSI
            len([1 for i in range(-10, 0) if close_prices[i] > close_prices[i-1]]) / 10,  # ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹
            np.mean(high_prices[-10:] - close_prices[-10:]) / np.mean(close_prices[-10:]),  # ä¸Šå½±çº¿æ¯”ä¾‹
            np.mean(close_prices[-10:] - low_prices[-10:]) / np.mean(close_prices[-10:])   # ä¸‹å½±çº¿æ¯”ä¾‹
        ])

        return np.array(features)

    def _extract_price_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–ä»·æ ¼é¢„æµ‹ç‰¹å¾"""
        features = []

        # OHLCVç‰¹å¾
        for col in ['open', 'high', 'low', 'close']:
            if col in kdata.columns:
                values = kdata[col].values[-20:]
                features.extend([
                    np.mean(values),
                    np.std(values),
                    values[-1] / values[0] - 1  # 20æ—¥æ”¶ç›Šç‡
                ])

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        close_prices = kdata['close'].values
        ma5 = np.mean(close_prices[-5:])
        ma10 = np.mean(close_prices[-10:])
        ma20 = np.mean(close_prices[-20:])

        features.extend([
            close_prices[-1] / ma5 - 1,
            close_prices[-1] / ma10 - 1,
            close_prices[-1] / ma20 - 1,
            ma5 / ma20 - 1
        ])

        return np.array(features)

    def _predict_with_dl_model(self, model, features, prediction_type):
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç®€åŒ–æ¨¡å‹
            if isinstance(model, dict) and model.get('model_type') == 'simplified':
                return self._predict_with_simplified_model(model, features, prediction_type)

            # å¦åˆ™ä½¿ç”¨TensorFlowæ¨¡å‹
            if TENSORFLOW_AVAILABLE and hasattr(model, 'predict'):
                # è·å–æ¨¡å‹æœŸæœ›çš„è¾“å…¥å½¢çŠ¶
                expected_input_dim = model.input_shape[-1] if hasattr(model, 'input_shape') else len(features)

                # è°ƒæ•´ç‰¹å¾ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹
                if len(features) != expected_input_dim:
                    logger.info(f"è°ƒæ•´ç‰¹å¾ç»´åº¦: {len(features)} -> {expected_input_dim}")
                    if len(features) < expected_input_dim:
                        # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œç”¨å‡å€¼å¡«å……
                        features = np.pad(features, (0, expected_input_dim - len(features)),
                                          mode='constant', constant_values=np.mean(features))
                    else:
                        # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œæˆªå–å‰Nä¸ª
                        features = features[:expected_input_dim]

                prediction = model.predict(features.reshape(1, -1), verbose=0)
                confidence = float(np.max(prediction))
                predicted_class = int(np.argmax(prediction))

                # æ ¹æ®é¢„æµ‹ç±»å‹è¿”å›ç»“æœ
                return self._format_prediction_result(predicted_class, confidence, prediction_type)
            else:
                raise ValueError("Invalid model type for deep learning prediction")

        except Exception as e:
            logger.warning(f"æ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›åå¤‡é¢„æµ‹ç»“æœ
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'dl_model_fallback',
                'timestamp': datetime.now().isoformat()
            }

    def _predict_with_simplified_model(self, model, features, prediction_type):
        """ä½¿ç”¨ç®€åŒ–æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            # ç®€åŒ–é¢„æµ‹é€»è¾‘ï¼šåŸºäºç‰¹å¾å’Œæ¨¡å‹æƒé‡
            model_info = model.get('model_info', {})
            expected_input_dim = model_info.get('input_features', len(features))

            # è°ƒæ•´ç‰¹å¾ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹
            if len(features) != expected_input_dim:
                logger.info(f"ç®€åŒ–æ¨¡å‹è°ƒæ•´ç‰¹å¾ç»´åº¦: {len(features)} -> {expected_input_dim}")
                if len(features) < expected_input_dim:
                    # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œç”¨å‡å€¼å¡«å……
                    features = np.pad(features, (0, expected_input_dim - len(features)),
                                      mode='constant', constant_values=np.mean(features) if len(features) > 0 else 0.0)
                else:
                    # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œæˆªå–å‰Nä¸ª
                    features = features[:expected_input_dim]

            # ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œç®€å•çš„çº¿æ€§ç»„åˆé¢„æµ‹
            weights = model.get('weights', {})
            layer1_weights = np.array(weights.get('layer1', np.random.randn(expected_input_dim, 64)))

            # ç¡®ä¿æƒé‡ç»´åº¦åŒ¹é…
            if layer1_weights.shape[0] != len(features):
                layer1_weights = np.resize(layer1_weights, (len(features), 64))

            # ç®€åŒ–çš„å‰å‘ä¼ æ’­
            try:
                hidden = np.tanh(np.dot(features, layer1_weights))
                output = np.mean(hidden) + 0.5  # ç®€åŒ–è¾“å‡º
            except Exception:
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„ç‰¹å¾å¹³å‡
                output = np.mean(features) + 0.5 if len(features) > 0 else 0.5

            # ç”Ÿæˆé¢„æµ‹ç»“æœ
            confidence = min(max(abs(output - 0.5) * 2, 0.3), 0.9)  # é™åˆ¶ç½®ä¿¡åº¦èŒƒå›´
            predicted_class = 1 if abs(output - 0.5) < 0.1 else (2 if output > 0.5 else 0)

            return self._format_prediction_result(predicted_class, confidence, prediction_type)

        except Exception as e:
            logger.warning(f"ç®€åŒ–æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›åå¤‡é¢„æµ‹ç»“æœ
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'simplified_model_fallback',
                'timestamp': datetime.now().isoformat()
            }

    def _format_prediction_result(self, predicted_class, confidence, prediction_type):
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœ"""
        class_names = {
            PredictionType.PATTERN: ['ä¸‹é™å½¢æ€', 'éœ‡è¡å½¢æ€', 'ä¸Šå‡å½¢æ€'],
            PredictionType.TREND: ['ä¸‹è·Œè¶‹åŠ¿', 'æ¨ªç›˜è¶‹åŠ¿', 'ä¸Šæ¶¨è¶‹åŠ¿'],
            PredictionType.SENTIMENT: ['æ‚²è§‚æƒ…ç»ª', 'ä¸­æ€§æƒ…ç»ª', 'ä¹è§‚æƒ…ç»ª'],
            PredictionType.PRICE: ['ä»·æ ¼ä¸‹è·Œ', 'ä»·æ ¼å¹³ç¨³', 'ä»·æ ¼ä¸Šæ¶¨']
        }

        direction_map = {
            0: 'ä¸‹è·Œ',
            1: 'éœ‡è¡',
            2: 'ä¸Šæ¶¨'
        }

        class_list = class_names.get(prediction_type, ['ä¸‹è·Œ', 'éœ‡è¡', 'ä¸Šæ¶¨'])
        predicted_label = class_list[predicted_class] if predicted_class < len(class_list) else class_list[1]
        direction = direction_map.get(predicted_class, 'éœ‡è¡')

        return {
            'direction': direction,
            'confidence': confidence,
            'predicted_class': predicted_class,
            'predicted_label': predicted_label,
            'model_type': 'ai_model',
            'timestamp': datetime.now().isoformat()
        }

    def _predict_with_statistical_model(self, features: np.ndarray, pred_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹é¢„æµ‹"""
        # ç®€å•çš„ç»Ÿè®¡æ–¹æ³•
        feature_mean = np.mean(features)
        feature_std = np.std(features)

        if feature_mean > feature_std:
            direction = "ä¸Šæ¶¨" if pred_type == PredictionType.TREND else "ä¹è§‚"
            confidence = 0.6
        elif feature_mean < -feature_std:
            direction = "ä¸‹è·Œ" if pred_type == PredictionType.TREND else "æ‚²è§‚"
            confidence = 0.6
        else:
            direction = "éœ‡è¡" if pred_type == PredictionType.TREND else "ä¸­æ€§"
            confidence = 0.5

        return {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'statistical'
        }

    def _predict_with_rules(self, kdata: pd.DataFrame, pred_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨è§„åˆ™æ¨¡å‹é¢„æµ‹"""
        # å¦‚æœæ²¡æœ‰æä¾›kdataï¼Œè¿”å›é»˜è®¤é¢„æµ‹
        if kdata is None or kdata.empty:
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'rule_based_fallback'
            }

        try:
            close_prices = kdata['close'].values

            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(close_prices) < 10:
                return {
                    'direction': 'éœ‡è¡',
                    'confidence': 0.5,
                    'model_type': 'rule_based_insufficient_data'
                }

            # ç®€å•çš„æŠ€æœ¯åˆ†æè§„åˆ™
            ma5 = np.mean(close_prices[-5:])
            ma10 = np.mean(close_prices[-10:])
            current_price = close_prices[-1]

            if current_price > ma5 > ma10:
                direction = "ä¸Šæ¶¨"
                confidence = 0.65
            elif current_price < ma5 < ma10:
                direction = "ä¸‹è·Œ"
                confidence = 0.65
            else:
                direction = "éœ‡è¡"
                confidence = 0.5

            return {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'rule_based'
            }
        except Exception as e:
            logger.warning(f"è§„åˆ™é¢„æµ‹å¤±è´¥: {e}")
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'rule_based_error'
            }

    def _predict_sentiment_with_rules(self, kdata: pd.DataFrame, market_data: Dict = None) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„æƒ…ç»ªé¢„æµ‹"""
        return self._predict_with_rules(kdata, PredictionType.SENTIMENT)

    def _predict_price_with_rules(self, kdata: pd.DataFrame, horizon: int) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ä»·æ ¼é¢„æµ‹"""
        current_price = float(kdata['close'].iloc[-1])
        close_prices = kdata['close'].values

        # è®¡ç®—è¶‹åŠ¿
        trend = np.polyfit(range(len(close_prices[-10:])), close_prices[-10:], 1)[0]

        # é¢„æµ‹ä»·æ ¼èŒƒå›´
        if trend > 0:
            target_low = current_price * 1.01
            target_high = current_price * 1.05
            direction = "ä¸Šæ¶¨"
        elif trend < 0:
            target_low = current_price * 0.95
            target_high = current_price * 0.99
            direction = "ä¸‹è·Œ"
        else:
            target_low = current_price * 0.98
            target_high = current_price * 1.02
            direction = "éœ‡è¡"

        return {
            'direction': direction,
            'current_price': current_price,
            'target_low': target_low,
            'target_high': target_high,
            'target_range': f"{target_low:.2f} - {target_high:.2f}",
            'horizon_days': horizon,
            'confidence': 0.6,
            'model_type': 'rule_based'
        }

    def _calculate_volatility_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—æ³¢åŠ¨ç‡é£é™©"""
        returns = kdata['close'].pct_change().dropna()
        volatility = returns.std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
        return min(volatility * 5, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1

    def _calculate_technical_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—æŠ€æœ¯é¢é£é™©"""
        close_prices = kdata['close'].values

        # è®¡ç®—æœ€å¤§å›æ’¤
        peak = np.maximum.accumulate(close_prices)
        drawdown = (close_prices - peak) / peak
        max_drawdown = abs(np.min(drawdown))

        return min(max_drawdown * 2, 1.0)

    def _calculate_market_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—å¸‚åœºé£é™©"""
        # ç®€åŒ–çš„å¸‚åœºé£é™©è¯„ä¼°
        volumes = kdata['volume'].values if 'volume' in kdata.columns else np.ones(len(kdata))
        vol_ratio = np.std(volumes[-10:]) / np.mean(volumes[-10:])
        return min(vol_ratio * 0.5, 1.0)

    def _calculate_overall_risk(self, vol_risk: float, tech_risk: float,
                                market_risk: float, predictions: Dict = None) -> float:
        """è®¡ç®—ç»¼åˆé£é™©"""
        weights = [0.4, 0.4, 0.2]  # æ³¢åŠ¨ç‡ã€æŠ€æœ¯é¢ã€å¸‚åœºé£é™©æƒé‡
        risks = [vol_risk, tech_risk, market_risk]
        overall = np.average(risks, weights=weights)

        # å¦‚æœæœ‰é¢„æµ‹ç»“æœï¼Œè°ƒæ•´é£é™©
        if predictions:
            confidence = predictions.get('confidence', 0.5)
            if confidence < 0.5:
                overall *= 1.2  # ä½ç½®ä¿¡åº¦å¢åŠ é£é™©

        return min(overall, 1.0)

    def _categorize_risk(self, risk_score: float) -> str:
        """é£é™©ç­‰çº§åˆ†ç±»"""
        if risk_score < 0.3:
            return "ä½é£é™©"
        elif risk_score < 0.6:
            return "ä¸­é£é™©"
        else:
            return "é«˜é£é™©"

    def _identify_risk_factors(self, kdata: pd.DataFrame) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        factors = []

        # æ£€æŸ¥æŠ€æœ¯æŒ‡æ ‡é£é™©
        close_prices = kdata['close'].values
        if len(close_prices) > 20:
            ma20 = np.mean(close_prices[-20:])
            if close_prices[-1] < ma20 * 0.95:
                factors.append("ä»·æ ¼å¤§å¹…ä½äºå‡çº¿")

        # æ£€æŸ¥æ³¢åŠ¨ç‡é£é™©
        returns = pd.Series(close_prices).pct_change().dropna()
        if returns.std() > 0.05:
            factors.append("é«˜æ³¢åŠ¨ç‡")

        # æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸
        if 'volume' in kdata.columns:
            volumes = kdata['volume'].values
            if len(volumes) > 10:
                vol_ratio = volumes[-1] / np.mean(volumes[-10:])
                if vol_ratio > 3:
                    factors.append("æˆäº¤é‡å¼‚å¸¸æ”¾å¤§")
                elif vol_ratio < 0.3:
                    factors.append("æˆäº¤é‡å¼‚å¸¸èç¼©")

        return factors if factors else ["æ— æ˜æ˜¾é£é™©å› ç´ "]

    def _get_risk_recommendations(self, risk_score: float) -> List[str]:
        """è·å–é£é™©å»ºè®®"""
        if risk_score < 0.3:
            return ["å¯ä»¥é€‚åº¦å¢åŠ ä»“ä½", "æ³¨æ„æ­¢ç›ˆç‚¹è®¾ç½®"]
        elif risk_score < 0.6:
            return ["ä¿æŒé€‚ä¸­ä»“ä½", "è®¾ç½®æ­¢æŸç‚¹", "å¯†åˆ‡å…³æ³¨å¸‚åœºå˜åŒ–"]
        else:
            return ["å»ºè®®å‡å°‘ä»“ä½", "ä¸¥æ ¼æ­¢æŸ", "é¿å…è¿½æ¶¨æ€è·Œ", "ç­‰å¾…æ›´å¥½æ—¶æœº"]

    # åå¤‡é¢„æµ‹æ–¹æ³•
    def _get_fallback_pattern_prediction(self) -> Dict[str, Any]:
        """åå¤‡å½¢æ€é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'confidence': 0.5,
            'target_price': 0.0,
            'time_horizon': '3-5ä¸ªäº¤æ˜“æ—¥',
            'pattern_count': 0,
            'signal_strength': 0.5,
            'model_type': 'fallback',
            'timestamp': datetime.now().isoformat()
        }

    def _get_fallback_trend_prediction(self) -> Dict[str, Any]:
        """åå¤‡è¶‹åŠ¿é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_sentiment_prediction(self) -> Dict[str, Any]:
        """åå¤‡æƒ…ç»ªé¢„æµ‹"""
        return {
            'direction': 'ä¸­æ€§',
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_price_prediction(self) -> Dict[str, Any]:
        """åå¤‡ä»·æ ¼é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'current_price': 0.0,
            'target_low': 0.0,
            'target_high': 0.0,
            'target_range': 'N/A',
            'horizon_days': 5,
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_risk_assessment(self) -> Dict[str, Any]:
        """åå¤‡é£é™©è¯„ä¼°"""
        return {
            'overall_risk': 0.5,
            'volatility_risk': 0.5,
            'technical_risk': 0.5,
            'market_risk': 0.5,
            'risk_level': 'ä¸­é£é™©',
            'risk_factors': ['æ•°æ®ä¸è¶³'],
            'recommendations': ['è°¨æ…æ“ä½œ', 'å……åˆ†å‡†å¤‡']
        }

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'available_models': list(self._models.keys()),
            'model_types': {k: type(v).__name__ for k, v in self._models.items()},
            'deep_learning_available': DL_AVAILABLE,
            'tensorflow_available': TENSORFLOW_AVAILABLE,
            'config': self.model_config,
            'cache_size': len(self._predictions_cache)
        }

    def get_model_type_display_name(self, model_type: str) -> str:
        """è·å–æ¨¡å‹ç±»å‹çš„æ˜¾ç¤ºåç§°"""
        display_names = {
            'ensemble': 'é›†æˆæ¨¡å‹',
            'deep_learning': 'æ·±åº¦å­¦ä¹ ',
            'statistical': 'ç»Ÿè®¡æ¨¡å‹',
            'rule_based': 'è§„åˆ™æ¨¡å‹'
        }
        return display_names.get(model_type, model_type)

    def validate_model_type(self, model_type: str) -> bool:
        """éªŒè¯æ¨¡å‹ç±»å‹æ˜¯å¦æœ‰æ•ˆ"""
        valid_types = [AIModelType.ENSEMBLE, AIModelType.DEEP_LEARNING,
                       AIModelType.STATISTICAL, AIModelType.RULE_BASED]
        return model_type in valid_types

    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            self._load_config_from_database()
            logger.info("AIé¢„æµ‹æœåŠ¡é…ç½®å·²é‡æ–°åŠ è½½")
        except Exception as e:
            logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")

    def get_current_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰æœ‰æ•ˆé…ç½®"""
        return {
            'model_config': self.model_config,
            'validation_config': self.validation_config,
            'feature_config': self.feature_config,
            'cache_config': self.cache_config,
            'logging_config': self.logging_config
        }

    def clear_cache(self):
        """æ¸…ç†é¢„æµ‹ç¼“å­˜"""
        self._predictions_cache.clear()
        logger.info("é¢„æµ‹ç¼“å­˜å·²æ¸…ç†")

    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.model_config.update(new_config)
        logger.info(f"é…ç½®å·²æ›´æ–°: {new_config}")

    def dispose(self):
        """æ¸…ç†èµ„æº"""
        self.clear_cache()
        self._models.clear()
        logger.info("AIé¢„æµ‹æœåŠ¡å·²æ¸…ç†")

    def _predict_with_patterns_deep_learning(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("ğŸ¤– === æ·±åº¦å­¦ä¹ å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # æå–å½¢æ€ç‰¹å¾
        pattern_features = self._extract_pattern_features_from_patterns(patterns)
        kdata_features = self._extract_pattern_features(kdata)

        # ç¡®ä¿kdata_featuresæ˜¯å­—å…¸ç±»å‹
        if isinstance(kdata_features, np.ndarray):
            # å¦‚æœè¿”å›çš„æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºé»˜è®¤å­—å…¸
            kdata_features = {
                'price_momentum': 0.5,
                'volume_strength': 0.5,
                'volatility_signal': 0.5,
                'ma_signal': 0,
                'price_zscore': 0,
                'volume_zscore': 0
            }

        # ç»“åˆå½¢æ€å’ŒKçº¿ç‰¹å¾
        combined_strength = (
            pattern_analysis['avg_confidence'] * 0.6 +
            kdata_features.get('price_momentum', 0.5) * 0.4
        )

        # æ·±åº¦å­¦ä¹ çš„å¤æ‚æ€§æ¨¡æ‹Ÿ
        signal_bias = pattern_analysis['bullish_signals'] - pattern_analysis['bearish_signals']
        normalized_bias = signal_bias / max(pattern_analysis['total_patterns'], 1)

        # æ·»åŠ ç¥ç»ç½‘ç»œçš„éçº¿æ€§
        neural_factor = np.tanh(normalized_bias * 2) * 0.3
        final_strength = np.clip(combined_strength + neural_factor, 0, 1)

        if final_strength > 0.65:
            direction = "ä¸Šæ¶¨"
            confidence = 0.70 + (final_strength - 0.65) * 0.25
        elif final_strength < 0.35:
            direction = "ä¸‹è·Œ"
            confidence = 0.70 + (0.35 - final_strength) * 0.25
        else:
            direction = "éœ‡è¡"
            confidence = 0.60 + abs(final_strength - 0.5) * 0.3

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'deep_learning',
            'model_path': 'deep_learning_with_patterns',
            'pattern_strength': combined_strength,
            'neural_factor': neural_factor,
            'signal_bias': signal_bias
        }

        logger.info(f"ğŸ¤– æ·±åº¦å­¦ä¹ å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        return result

    def _predict_with_patterns_statistical(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """ç»Ÿè®¡æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("ğŸ“Š === ç»Ÿè®¡æ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # ç»Ÿè®¡åˆ†ææ–¹æ³•
        pattern_confidence_std = np.std([p.get('confidence', 0.5) for p in patterns])
        signal_ratio = pattern_analysis['bullish_signals'] / max(pattern_analysis['total_patterns'], 1)

        # åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        if pattern_analysis['total_patterns'] > 10:
            # å¤§æ ·æœ¬ç»Ÿè®¡åˆ†æ
            z_score = (signal_ratio - 0.5) / (pattern_confidence_std + 0.1)

            if z_score > 1.0 and signal_ratio > 0.6:
                direction = "ä¸Šæ¶¨"
                confidence = 0.75 + min(abs(z_score) * 0.1, 0.2)
            elif z_score < -1.0 and signal_ratio < 0.4:
                direction = "ä¸‹è·Œ"
                confidence = 0.75 + min(abs(z_score) * 0.1, 0.2)
            else:
                direction = "éœ‡è¡"
                confidence = 0.65 + abs(z_score) * 0.05
        else:
            # å°æ ·æœ¬ç»Ÿè®¡åˆ†æ
            if signal_ratio > 0.7:
                direction = "ä¸Šæ¶¨"
                confidence = 0.68 + signal_ratio * 0.2
            elif signal_ratio < 0.3:
                direction = "ä¸‹è·Œ"
                confidence = 0.68 + (1 - signal_ratio) * 0.2
            else:
                direction = "éœ‡è¡"
                confidence = 0.62

            z_score = (signal_ratio - 0.5) * 2

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'statistical',
            'model_path': 'statistical_with_patterns',
            'z_score': z_score,
            'signal_ratio': signal_ratio,
            'confidence_std': pattern_confidence_std
        }

        logger.info(f"ğŸ“Š ç»Ÿè®¡æ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        return result

    def _predict_with_patterns_rule_based(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """è§„åˆ™æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("ğŸ“ === è§„åˆ™æ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        rules_score = 0
        rules_applied = []

        # è§„åˆ™1: å¼ºåŠ¿å½¢æ€æ¯”ä¾‹
        bullish_ratio = pattern_analysis['bullish_signals'] / max(pattern_analysis['total_patterns'], 1)
        if bullish_ratio > 0.6:
            rules_score += 2
            rules_applied.append("å¼ºåŠ¿çœ‹æ¶¨å½¢æ€å æ¯”é«˜")
        elif bullish_ratio < 0.3:
            rules_score -= 2
            rules_applied.append("å¼ºåŠ¿çœ‹è·Œå½¢æ€å æ¯”é«˜")

        # è§„åˆ™2: å½¢æ€å¯†åº¦
        pattern_density = pattern_analysis['total_patterns'] / len(kdata)
        if pattern_density > 0.05:  # 5%ä»¥ä¸Šå¯†åº¦
            rules_score += 1
            rules_applied.append("å½¢æ€å¯†åº¦è¾ƒé«˜")

        # è§„åˆ™3: å¹³å‡ç½®ä¿¡åº¦
        if pattern_analysis['avg_confidence'] > 0.8:
            rules_score += 1
            rules_applied.append("å½¢æ€ç½®ä¿¡åº¦é«˜")
        elif pattern_analysis['avg_confidence'] < 0.5:
            rules_score -= 1
            rules_applied.append("å½¢æ€ç½®ä¿¡åº¦ä½")

        # è§„åˆ™4: ä¿¡å·ä¸€è‡´æ€§
        signal_consistency = abs(pattern_analysis['bullish_signals'] - pattern_analysis['bearish_signals'])
        if signal_consistency > pattern_analysis['total_patterns'] * 0.3:
            rules_score += 1
            rules_applied.append("ä¿¡å·æ–¹å‘ä¸€è‡´æ€§é«˜")

        # æ ¹æ®è§„åˆ™å¾—åˆ†åˆ¤æ–­
        if rules_score >= 3:
            direction = "ä¸Šæ¶¨"
            confidence = 0.80 + min(rules_score - 3, 2) * 0.05
        elif rules_score <= -2:
            direction = "ä¸‹è·Œ"
            confidence = 0.78 + min(abs(rules_score) - 2, 2) * 0.06
        else:
            direction = "éœ‡è¡"
            confidence = 0.72 - abs(rules_score) * 0.02

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'rule_based',
            'model_path': 'rule_based_with_patterns',
            'rules_score': rules_score,
            'rules_applied': rules_applied,
            'pattern_density': pattern_density
        }

        logger.info(f"ğŸ“ è§„åˆ™æ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        logger.info(f"ğŸ“ åº”ç”¨è§„åˆ™: {rules_applied}")
        return result

    def _predict_with_patterns_ensemble(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """é›†æˆæ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("ğŸ”„ === é›†æˆæ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # è°ƒç”¨æ‰€æœ‰å­æ¨¡å‹
        dl_result = self._predict_with_patterns_deep_learning(kdata, patterns, pattern_analysis)
        stat_result = self._predict_with_patterns_statistical(kdata, patterns, pattern_analysis)
        rule_result = self._predict_with_patterns_rule_based(kdata, patterns, pattern_analysis)

        # é›†æˆåŠ æƒæŠ•ç¥¨
        models = [
            (dl_result, 0.45),    # æ·±åº¦å­¦ä¹ æƒé‡45%
            (stat_result, 0.30),  # ç»Ÿè®¡æ¨¡å‹æƒé‡30%
            (rule_result, 0.25)   # è§„åˆ™æ¨¡å‹æƒé‡25%
        ]

        direction_votes = {'ä¸Šæ¶¨': 0, 'ä¸‹è·Œ': 0, 'éœ‡è¡': 0}
        total_confidence = 0
        total_weight = 0

        for result, weight in models:
            direction = result.get('direction', 'éœ‡è¡')
            confidence = result.get('confidence', 0.5)

            direction_votes[direction] += weight * confidence
            total_confidence += weight * confidence
            total_weight += weight

        final_direction = max(direction_votes.items(), key=lambda x: x[1])[0]
        final_confidence = total_confidence / total_weight

        result = {
            'direction': final_direction,
            'confidence': final_confidence,
            'model_type': 'ensemble',
            'model_path': 'ensemble_with_patterns',
            'sub_models': {
                'deep_learning': dl_result,
                'statistical': stat_result,
                'rule_based': rule_result
            },
            'vote_weights': direction_votes
        }

        logger.info(f"ğŸ”„ é›†æˆæ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {final_direction}, ç½®ä¿¡åº¦: {final_confidence:.3f}")
        return result

    def _extract_pattern_features_from_patterns(self, patterns: List[Dict]) -> Dict[str, float]:
        """ä»å½¢æ€åˆ—è¡¨ä¸­æå–ç‰¹å¾"""
        if not patterns:
            return {}

        # è®¡ç®—å½¢æ€ç»Ÿè®¡ç‰¹å¾
        confidences = [p.get('confidence', 0.5) for p in patterns]
        signal_types = [p.get('signal_type', 'neutral') for p in patterns]

        return {
            'avg_confidence': np.mean(confidences),
            'confidence_std': np.std(confidences),
            'bullish_ratio': signal_types.count('bullish') / len(signal_types),
            'bearish_ratio': signal_types.count('bearish') / len(signal_types),
            'pattern_count': len(patterns),
            'max_confidence': np.max(confidences),
            'min_confidence': np.min(confidences)
        }

    def _fallback_pattern_analysis(self, valid_patterns: List[Dict], buy_signals: List[Dict], sell_signals: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """é™çº§åå¤‡å½¢æ€åˆ†æ"""
        logger.warning("âš ï¸ ä½¿ç”¨åå¤‡å½¢æ€åˆ†æ")

        # åŸºäºå½¢æ€ä¿¡å·å¼ºåº¦çš„ç®€å•é¢„æµ‹
        if len(buy_signals) > len(sell_signals):
            direction = "ä¸Šæ¶¨"
            confidence = min(pattern_analysis['avg_confidence'] + 0.1, 0.95)
        elif len(sell_signals) > len(buy_signals):
            direction = "ä¸‹è·Œ"
            confidence = min(pattern_analysis['avg_confidence'] + 0.1, 0.95)
        else:
            direction = "éœ‡è¡"
            confidence = pattern_analysis['avg_confidence']

        return {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'pattern_analysis_fallback',
            'model_path': 'fallback_pattern_analysis',
            'prediction_type': PredictionType.PATTERN
        }
