from loguru import logger
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AIé¢„æµ‹æœåŠ¡ - ç»Ÿä¸€çš„æœºå™¨å­¦ä¹ é¢„æµ‹æœåŠ¡

æä¾›ï¼š
1. å½¢æ€é¢„æµ‹
2. è¶‹åŠ¿é¢„æµ‹  
3. æƒ…ç»ªé¢„æµ‹
4. ä»·æ ¼é¢„æµ‹
5. é£é™©é¢„æµ‹
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
import json
import pickle
import os
import hashlib
from pathlib import Path
import traceback
from enum import Enum
from dataclasses import dataclass

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ æ¨¡å—
try:
    from models.deep_learning import build_deep_learning_model, TENSORFLOW_AVAILABLE
    from models.model_evaluation import evaluate_ml_model
    DL_AVAILABLE = True
except ImportError:
    DL_AVAILABLE = False
    TENSORFLOW_AVAILABLE = False

from core.services.base_service import BaseService

logger = logger

# æ·»åŠ æ¨¡å‹ç±»å‹æ˜ å°„å­—å…¸
MODEL_TYPE_DISPLAY_NAMES = {
    'deep_learning': 'æ·±åº¦å­¦ä¹ ',
    'statistical': 'ç»Ÿè®¡æ¨¡å‹',
    'rule_based': 'è§„åˆ™æ¨¡å‹',
    'ensemble': 'é›†æˆæ¨¡å‹',
    'pattern_analysis': 'å½¢æ€åˆ†æ',
    'pattern_analysis_fallback': 'å½¢æ€åˆ†æï¼ˆåå¤‡ï¼‰',
    'fallback': 'åå¤‡æ¨¡å‹',
    'transformer': 'Transformeræ¨¡å‹',
    'lstm': 'LSTMæ¨¡å‹',
    'cnn': 'CNNæ¨¡å‹',
    'gan': 'ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ',
    'reinforcement': 'å¼ºåŒ–å­¦ä¹ ',
    'bayesian': 'è´å¶æ–¯æ¨¡å‹',
    'gradient_boosting': 'æ¢¯åº¦æå‡',
    'svm': 'æ”¯æŒå‘é‡æœº',
    'random_forest': 'éšæœºæ£®æ—',
    'neural_network': 'ç¥ç»ç½‘ç»œ',
    'garch_ewma': 'GARCH-EWMAæ¨¡å‹',
    'dcc_garch': 'DCC-GARCHæ¨¡å‹',
    'statistical_anomaly': 'ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹',
    'hmm_regime': 'éšé©¬å°”å¯å¤«çŠ¶æ€æ¨¡å‹',
    'amihud_liquidity': 'AmihudæµåŠ¨æ€§æ¨¡å‹',
    'technical_momentum': 'æŠ€æœ¯åŠ¨é‡æ¨¡å‹',
    'technical_reversal': 'æŠ€æœ¯åè½¬æ¨¡å‹',
    'technical_sr': 'æŠ€æœ¯æ”¯æ’‘é˜»åŠ›æ¨¡å‹',
    'volume_profile': 'æˆäº¤é‡åˆ†å¸ƒæ¨¡å‹',
    'seasonal_analysis': 'å­£èŠ‚æ€§åˆ†ææ¨¡å‹'
}


def get_model_display_name(model_type: str) -> str:
    """è·å–æ¨¡å‹ç±»å‹çš„ä¸­æ–‡æ˜¾ç¤ºåç§°"""
    return MODEL_TYPE_DISPLAY_NAMES.get(model_type, model_type)


def get_supported_prediction_types() -> List[str]:
    """è·å–æ”¯æŒçš„é¢„æµ‹ç±»å‹åˆ—è¡¨"""
    return [
        PredictionType.PATTERN, PredictionType.TREND, PredictionType.SENTIMENT,
        PredictionType.PRICE, PredictionType.RISK, PredictionType.EXECUTION_TIME,
        PredictionType.PARAMETER_OPTIMIZATION, PredictionType.VOLATILITY,
        PredictionType.CORRELATION, PredictionType.ANOMALY, PredictionType.MARKET_REGIME,
        PredictionType.LIQUIDITY, PredictionType.MOMENTUM, PredictionType.REVERSAL,
        PredictionType.SUPPORT_RESISTANCE, PredictionType.VOLUME_PROFILE, PredictionType.SEASONALITY
    ]


def get_prediction_type_description(prediction_type: str) -> str:
    """è·å–é¢„æµ‹ç±»å‹çš„æè¿°"""
    descriptions = {
        PredictionType.PATTERN: "æŠ€æœ¯å½¢æ€è¯†åˆ«å’Œä¿¡å·é¢„æµ‹",
        PredictionType.TREND: "ä»·æ ¼è¶‹åŠ¿æ–¹å‘å’Œå¼ºåº¦é¢„æµ‹",
        PredictionType.SENTIMENT: "å¸‚åœºæƒ…ç»ªå’ŒæŠ•èµ„è€…å¿ƒç†é¢„æµ‹",
        PredictionType.PRICE: "æœªæ¥ä»·æ ¼æ°´å¹³å’Œç›®æ ‡ä½é¢„æµ‹",
        PredictionType.RISK: "æŠ•èµ„é£é™©è¯„ä¼°å’Œé£é™©ç­‰çº§é¢„æµ‹",
        PredictionType.EXECUTION_TIME: "ä»»åŠ¡æ‰§è¡Œæ—¶é—´é¢„æµ‹å’Œä¼˜åŒ–",
        PredictionType.PARAMETER_OPTIMIZATION: "ç³»ç»Ÿå‚æ•°ä¼˜åŒ–å»ºè®®",
        PredictionType.VOLATILITY: "ä»·æ ¼æ³¢åŠ¨ç‡é¢„æµ‹å’Œæ³¢åŠ¨æ€§åˆ†æ",
        PredictionType.CORRELATION: "èµ„äº§é—´ç›¸å…³æ€§é¢„æµ‹å’Œå…³è”åˆ†æ",
        PredictionType.ANOMALY: "å¼‚å¸¸è¡Œä¸ºæ£€æµ‹å’Œé£é™©é¢„è­¦",
        PredictionType.MARKET_REGIME: "å¸‚åœºçŠ¶æ€è¯†åˆ«å’Œè½¬æ¢é¢„æµ‹",
        PredictionType.LIQUIDITY: "å¸‚åœºæµåŠ¨æ€§è¯„ä¼°å’Œé¢„æµ‹",
        PredictionType.MOMENTUM: "ä»·æ ¼åŠ¨é‡åˆ†æå’Œè¶‹åŠ¿å¼ºåº¦é¢„æµ‹",
        PredictionType.REVERSAL: "è¶‹åŠ¿åè½¬ä¿¡å·è¯†åˆ«å’Œé¢„æµ‹",
        PredictionType.SUPPORT_RESISTANCE: "å…³é”®æ”¯æ’‘é˜»åŠ›ä½é¢„æµ‹",
        PredictionType.VOLUME_PROFILE: "æˆäº¤é‡åˆ†å¸ƒåˆ†æå’Œä»·å€¼åŒºåŸŸé¢„æµ‹",
        PredictionType.SEASONALITY: "å­£èŠ‚æ€§æ•ˆåº”åˆ†æå’Œæ—¶é—´å‘¨æœŸé¢„æµ‹"
    }
    return descriptions.get(prediction_type, "æœªçŸ¥é¢„æµ‹ç±»å‹")


class AIModelType:
    """AIæ¨¡å‹ç±»å‹"""
    DEEP_LEARNING = "deep_learning"
    ENSEMBLE = "ensemble"
    STATISTICAL = "statistical"
    RULE_BASED = "rule_based"

    # æ–°å¢æ¨¡å‹ç±»å‹
    TRANSFORMER = "transformer"  # Transformeræ¨¡å‹
    LSTM = "lstm"              # LSTMæ¨¡å‹
    CNN = "cnn"                # CNNæ¨¡å‹
    GAN = "gan"                # ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
    REINFORCEMENT = "reinforcement"  # å¼ºåŒ–å­¦ä¹ 
    BAYESIAN = "bayesian"      # è´å¶æ–¯æ¨¡å‹
    GRADIENT_BOOSTING = "gradient_boosting"  # æ¢¯åº¦æå‡
    SVM = "svm"                # æ”¯æŒå‘é‡æœº
    RANDOM_FOREST = "random_forest"  # éšæœºæ£®æ—
    NEURAL_NETWORK = "neural_network"  # ç¥ç»ç½‘ç»œ


class PredictionType:
    """é¢„æµ‹ç±»å‹"""
    PATTERN = "pattern"      # å½¢æ€é¢„æµ‹
    TREND = "trend"         # è¶‹åŠ¿é¢„æµ‹
    SENTIMENT = "sentiment"  # æƒ…ç»ªé¢„æµ‹
    PRICE = "price"         # ä»·æ ¼é¢„æµ‹
    RISK = "risk"           # é£é™©é¢„æµ‹
    RISK_FORECAST = "risk_forecast"  # é£é™©è¶‹åŠ¿é¢„æµ‹
    EXECUTION_TIME = "execution_time"  # æ‰§è¡Œæ—¶é—´é¢„æµ‹
    PARAMETER_OPTIMIZATION = "parameter_optimization"  # å‚æ•°ä¼˜åŒ–é¢„æµ‹

    # æ–°å¢é¢„æµ‹ç±»å‹
    VOLATILITY = "volatility"  # æ³¢åŠ¨ç‡é¢„æµ‹
    CORRELATION = "correlation"  # ç›¸å…³æ€§é¢„æµ‹
    ANOMALY = "anomaly"      # å¼‚å¸¸æ£€æµ‹
    MARKET_REGIME = "market_regime"  # å¸‚åœºçŠ¶æ€é¢„æµ‹
    LIQUIDITY = "liquidity"  # æµåŠ¨æ€§é¢„æµ‹
    MOMENTUM = "momentum"    # åŠ¨é‡é¢„æµ‹
    REVERSAL = "reversal"    # åè½¬é¢„æµ‹
    SUPPORT_RESISTANCE = "support_resistance"  # æ”¯æ’‘é˜»åŠ›é¢„æµ‹
    VOLUME_PROFILE = "volume_profile"  # æˆäº¤é‡åˆ†å¸ƒé¢„æµ‹
    SEASONALITY = "seasonality"  # å­£èŠ‚æ€§é¢„æµ‹


class AIPredictionService(BaseService):
    """AIé¢„æµ‹æœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ–AIé¢„æµ‹æœåŠ¡"""
        super().__init__()

        # ä»æ•°æ®åº“åŠ è½½é…ç½®
        self._load_config_from_database()

        # æ¨¡å‹ç¼“å­˜
        self._models = {}
        self._predictions_cache = {}
        self._last_update = {}

        # è­¦å‘Šé¢‘ç‡é™åˆ¶
        self._last_warning_time = {}  # è®°å½•æ¯ç§é¢„æµ‹ç±»å‹çš„æœ€åè­¦å‘Šæ—¶é—´
        self._warning_interval = 60  # è­¦å‘Šé—´éš”ï¼ˆç§’ï¼‰

        # ç¼“å­˜MLåº“å¯¼å…¥çŠ¶æ€
        self._ml_libs_cache = None

        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()

    def _should_warn(self, prediction_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¾“å‡ºè­¦å‘Šï¼ˆé¿å…é‡å¤è­¦å‘Šï¼‰"""
        import time
        current_time = time.time()
        last_time = self._last_warning_time.get(prediction_type, 0)

        if current_time - last_time > self._warning_interval:
            self._last_warning_time[prediction_type] = current_time
            return True
        return False

    def _import_ml_libraries(self) -> Optional[Dict[str, Any]]:
        """ç»Ÿä¸€çš„æœºå™¨å­¦ä¹ åº“å¯¼å…¥æ–¹æ³•"""
        if self._ml_libs_cache is not None:
            return self._ml_libs_cache

        try:
            from scipy.optimize import minimize
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import cross_val_score
            from sklearn.preprocessing import StandardScaler
            import joblib

            self._ml_libs_cache = {
                'minimize': minimize,
                'RandomForestRegressor': RandomForestRegressor,
                'cross_val_score': cross_val_score,
                'StandardScaler': StandardScaler,
                'joblib': joblib,
                'available': True
            }
            return self._ml_libs_cache
        except ImportError as e:
            logger.warning(f"æœºå™¨å­¦ä¹ åº“å¯¼å…¥å¤±è´¥: {e}")
            self._ml_libs_cache = {'available': False}
            return None

    def predict(self, prediction_type: str, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ç»Ÿä¸€é¢„æµ‹æ¥å£

        Args:
            prediction_type: é¢„æµ‹ç±»å‹ (PredictionTypeä¸­çš„å€¼)
            data: é¢„æµ‹æ•°æ®

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            if prediction_type == PredictionType.EXECUTION_TIME:
                return self.predict_execution_time(data)
            elif prediction_type == PredictionType.PARAMETER_OPTIMIZATION:
                return self.predict_parameter_optimization(data)
            elif prediction_type == PredictionType.PATTERN:
                # éœ€è¦DataFrameæ ¼å¼çš„Kçº¿æ•°æ®
                if 'kdata' in data:
                    return self.predict_patterns(data['kdata'], data.get('patterns', []))
            elif prediction_type == PredictionType.TREND:
                if 'kdata' in data:
                    return self.predict_trend(data['kdata'], data.get('timeframe', 5))
            elif prediction_type == PredictionType.SENTIMENT:
                if 'kdata' in data:
                    return self.predict_sentiment(data['kdata'], data.get('market_data'))
            elif prediction_type == PredictionType.PRICE:
                if 'kdata' in data:
                    return self.predict_price(data['kdata'], data.get('horizon', 5))
            elif prediction_type == PredictionType.VOLATILITY:
                if 'kdata' in data:
                    return self.predict_volatility(data['kdata'], data.get('horizon', 5))
            elif prediction_type == PredictionType.CORRELATION:
                if 'kdata1' in data and 'kdata2' in data:
                    return self.predict_correlation(data['kdata1'], data['kdata2'], data.get('window', 20))
            elif prediction_type == PredictionType.ANOMALY:
                if 'kdata' in data:
                    return self.detect_anomalies(data['kdata'], data.get('threshold', 2.0))
            elif prediction_type == PredictionType.MARKET_REGIME:
                if 'kdata' in data:
                    return self.predict_market_regime(data['kdata'])
            elif prediction_type == PredictionType.LIQUIDITY:
                if 'kdata' in data:
                    return self.predict_liquidity(data['kdata'])
            elif prediction_type == PredictionType.MOMENTUM:
                if 'kdata' in data:
                    return self.predict_momentum(data['kdata'], data.get('period', 14))
            elif prediction_type == PredictionType.REVERSAL:
                if 'kdata' in data:
                    return self.predict_reversal(data['kdata'])
            elif prediction_type == PredictionType.SUPPORT_RESISTANCE:
                if 'kdata' in data:
                    return self.predict_support_resistance(data['kdata'])
            elif prediction_type == PredictionType.VOLUME_PROFILE:
                if 'kdata' in data:
                    return self.predict_volume_profile(data['kdata'])
            elif prediction_type == PredictionType.SEASONALITY:
                if 'kdata' in data:
                    return self.predict_seasonality(data['kdata'])
            elif prediction_type == PredictionType.RISK_FORECAST:
                if 'kdata' in data:
                    return self.predict_risk_forecast(data['kdata'])
            else:
                if self._should_warn(prediction_type):
                    logger.warning(f"ä¸æ”¯æŒçš„é¢„æµ‹ç±»å‹: {prediction_type}")
                return None

        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥ ({prediction_type}): {e}")
            return None

    def predict_parameter_optimization(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        é¢„æµ‹æœ€ä¼˜å‚æ•°é…ç½®

        Args:
            data: åŒ…å«current_configå’Œhistorical_dataçš„å­—å…¸

        Returns:
            ä¼˜åŒ–å‚æ•°å»ºè®®
        """
        try:
            current_config = data.get('current_config', {})
            historical_data = data.get('historical_data', [])

            if not historical_data:
                logger.warning("ç¼ºå°‘å†å²æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–")
                return None

            # å°è¯•ä½¿ç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–
            try:
                return self._ml_parameter_optimization(current_config, historical_data)
            except Exception as e:
                logger.warning(f"MLå‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç»Ÿè®¡æ–¹æ³•: {e}")
                return self._statistical_parameter_optimization(current_config, historical_data)

        except Exception as e:
            logger.error(f"å‚æ•°ä¼˜åŒ–é¢„æµ‹å¤±è´¥: {e}")
            return None

    def _ml_parameter_optimization(self, current_config: Dict[str, Any], historical_data: List[Dict]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œå‚æ•°ä¼˜åŒ–"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„MLåº“å¯¼å…¥
            ml_libs = self._import_ml_libraries()
            if not ml_libs:
                raise ImportError("scikit-learnæˆ–scipyä¸å¯ç”¨")

            if len(historical_data) < 5:
                raise ValueError("å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒMLæ¨¡å‹")

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y_time, y_success = self._prepare_optimization_data(historical_data)

            if len(X) < 3:
                raise ValueError("æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³")

            # è®­ç»ƒæ‰§è¡Œæ—¶é—´é¢„æµ‹æ¨¡å‹
            RandomForestRegressor = ml_libs['RandomForestRegressor']
            time_model = RandomForestRegressor(n_estimators=50, random_state=42)
            time_model.fit(X, y_time)

            # è®­ç»ƒæˆåŠŸç‡é¢„æµ‹æ¨¡å‹
            success_model = RandomForestRegressor(n_estimators=50, random_state=42)
            success_model.fit(X, y_success)

            # å®šä¹‰ä¼˜åŒ–ç›®æ ‡å‡½æ•°
            def objective_function(params):
                batch_size, max_workers = params
                batch_size = int(max(500, min(5000, batch_size)))
                max_workers = int(max(2, min(8, max_workers)))

                # é¢„æµ‹æ‰§è¡Œæ—¶é—´å’ŒæˆåŠŸç‡
                features = self._extract_optimization_features(
                    current_config, batch_size, max_workers
                )

                pred_time = time_model.predict([features])[0]
                pred_success = success_model.predict([features])[0]

                # ç»¼åˆç›®æ ‡ï¼šæœ€å°åŒ–æ‰§è¡Œæ—¶é—´ï¼Œæœ€å¤§åŒ–æˆåŠŸç‡
                # æƒé‡ï¼šæ‰§è¡Œæ—¶é—´70%ï¼ŒæˆåŠŸç‡30%
                score = 0.7 * pred_time + 0.3 * (1 - pred_success) * 1000
                return score

            # å‚æ•°è¾¹ç•Œ
            bounds = [(500, 5000), (2, 8)]  # batch_size, max_workers

            # åˆå§‹çŒœæµ‹
            x0 = [current_config.get('batch_size', 1000), current_config.get('max_workers', 4)]

            # æ‰§è¡Œä¼˜åŒ–
            minimize = ml_libs['minimize']
            result = minimize(objective_function, x0, bounds=bounds, method='L-BFGS-B')

            if result.success:
                optimal_batch_size = int(max(500, min(5000, result.x[0])))
                optimal_workers = int(max(2, min(8, result.x[1])))

                # è®¡ç®—é¢„æœŸæ”¹è¿›
                current_features = self._extract_optimization_features(
                    current_config,
                    current_config.get('batch_size', 1000),
                    current_config.get('max_workers', 4)
                )
                optimal_features = self._extract_optimization_features(
                    current_config, optimal_batch_size, optimal_workers
                )

                current_time = time_model.predict([current_features])[0]
                optimal_time = time_model.predict([optimal_features])[0]

                current_success = success_model.predict([current_features])[0]
                optimal_success = success_model.predict([optimal_features])[0]

                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºäº¤å‰éªŒè¯åˆ†æ•°ï¼‰
                cross_val_score = ml_libs['cross_val_score']
                time_cv_scores = cross_val_score(time_model, X, y_time, cv=min(3, len(X)))
                success_cv_scores = cross_val_score(success_model, X, y_success, cv=min(3, len(X)))
                confidence = (np.mean(time_cv_scores) + np.mean(success_cv_scores)) / 2

                return {
                    'success': True,
                    'optimized_parameters': {
                        'batch_size': optimal_batch_size,
                        'max_workers': optimal_workers
                    },
                    'confidence': max(0.5, min(0.95, confidence)),
                    'reasoning': f"åŸºäº{len(historical_data)}æ¡å†å²è®°å½•çš„MLä¼˜åŒ–",
                    'method': 'machine_learning',
                    'expected_improvement': {
                        'execution_time_reduction': max(0, (current_time - optimal_time) / current_time),
                        'success_rate_improvement': max(0, optimal_success - current_success)
                    },
                    'model_performance': {
                        'time_model_score': np.mean(time_cv_scores),
                        'success_model_score': np.mean(success_cv_scores)
                    }
                }
            else:
                raise ValueError("ä¼˜åŒ–ç®—æ³•æœªæ”¶æ•›")

        except Exception as e:
            logger.error(f"MLå‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return None

    def _statistical_parameter_optimization(self, current_config: Dict[str, Any], historical_data: List[Dict]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–"""
        # åˆ†æå†å²æ‰§è¡Œæ•°æ®
        execution_times = []
        success_rates = []
        batch_sizes = []
        worker_counts = []

        for record in historical_data:
            if record.get('execution_time'):
                execution_times.append(record['execution_time'])
                success_rates.append(1.0 if record.get('status') == 'completed' else 0.0)
                batch_sizes.append(record.get('batch_size', 1000))
                worker_counts.append(record.get('max_workers', 4))

        if not execution_times:
            return None

        # è®¡ç®—ç›¸å…³æ€§å’Œæœ€ä¼˜å€¼
        import pandas as pd
        df = pd.DataFrame({
            'execution_time': execution_times,
            'success_rate': success_rates,
            'batch_size': batch_sizes,
            'max_workers': worker_counts
        })

        # æ‰¾åˆ°æ‰§è¡Œæ—¶é—´æœ€çŸ­ä¸”æˆåŠŸç‡é«˜çš„é…ç½®
        df['score'] = df['success_rate'] - (df['execution_time'] / df['execution_time'].max()) * 0.5
        best_idx = df['score'].idxmax()

        optimal_batch_size = int(df.loc[best_idx, 'batch_size'])
        optimal_workers = int(df.loc[best_idx, 'max_workers'])

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = df['score'].std() / df['score'].mean() if df['score'].mean() > 0 else 0.5
        confidence = max(0.5, min(0.9, 1 - confidence))

        return {
            'success': True,
            'optimized_parameters': {
                'batch_size': optimal_batch_size,
                'max_workers': optimal_workers
            },
            'confidence': confidence,
            'reasoning': f"åŸºäº{len(historical_data)}æ¡å†å²è®°å½•çš„ç»Ÿè®¡åˆ†æ",
            'method': 'statistical',
            'expected_improvement': {
                'execution_time_reduction': max(0, (np.mean(execution_times) - df.loc[best_idx, 'execution_time']) / np.mean(execution_times)),
                'success_rate_improvement': max(0, df.loc[best_idx, 'success_rate'] - np.mean(success_rates))
            }
        }

    def _prepare_optimization_data(self, historical_data: List[Dict]) -> Tuple[List[List[float]], List[float], List[float]]:
        """å‡†å¤‡ä¼˜åŒ–è®­ç»ƒæ•°æ®"""
        X = []
        y_time = []
        y_success = []

        for record in historical_data:
            if record.get('execution_time') and record.get('batch_size') and record.get('max_workers'):
                features = self._extract_optimization_features(
                    record,
                    record['batch_size'],
                    record['max_workers']
                )
                X.append(features)
                y_time.append(record['execution_time'])
                y_success.append(1.0 if record.get('status') == 'completed' else 0.0)

        return X, y_time, y_success

    def _extract_optimization_features(self, config: Dict[str, Any], batch_size: int, max_workers: int) -> List[float]:
        """æå–ä¼˜åŒ–ç‰¹å¾"""
        features = []

        # åŸºç¡€é…ç½®ç‰¹å¾
        features.append(np.log10(max(1, len(config.get('symbols', [])))))  # è‚¡ç¥¨æ•°é‡
        features.append(np.log10(max(1, batch_size)))  # æ‰¹æ¬¡å¤§å°
        features.append(max_workers)  # å·¥ä½œçº¿ç¨‹æ•°

        # æ•°æ®æºç‰¹å¾ç¼–ç 
        data_source = config.get('data_source', 'unknown')
        source_encoding = {
            'tongdaxin': 1, 'eastmoney': 2, 'sina': 3, 'unknown': 0
        }
        features.append(source_encoding.get(data_source, 0))

        # é¢‘ç‡ç‰¹å¾ç¼–ç 
        frequency = config.get('frequency', 'daily')
        if isinstance(frequency, str):
            freq_encoding = {
                'tick': 1, '1min': 2, '5min': 3, '15min': 4,
                '30min': 5, '1h': 6, 'daily': 7, 'weekly': 8
            }
            features.append(freq_encoding.get(frequency, 7))
        else:
            features.append(7)  # é»˜è®¤daily

        # è®¡ç®—èµ„æºåˆ©ç”¨ç‡ç‰¹å¾
        features.append(batch_size / max_workers)  # æ¯çº¿ç¨‹å¤„ç†é‡

        return features

    def _load_config_from_database(self):
        """ä»æ•°æ®åº“åŠ è½½é…ç½®"""
        try:
            from db.models.ai_config_models import get_ai_config_manager
            config_manager = get_ai_config_manager()

            # åŠ è½½å„ç§é…ç½®
            self.model_config = config_manager.get_config('model_config') or {
                'enabled': True,
                'model_type': AIModelType.ENSEMBLE,
                'confidence_threshold': 0.7,
                'prediction_horizon': 5,
                'feature_window': 20,
                'cache_size': 1000,
                'model_update_interval': 24
            }

            self.validation_config = config_manager.get_config('validation') or {
                'min_data_points': 10,
                'max_prediction_horizon': 30,
                'max_data_rows': 10000,
                'required_columns': ['open', 'high', 'low', 'close']
            }

            self.feature_config = config_manager.get_config('feature_config') or {
                'technical_indicators': True,
                'pattern_features': True,
                'volume_features': True,
                'price_features': True,
                'volatility_features': True
            }

            self.cache_config = config_manager.get_config('cache_config') or {
                'enable_cache': True,
                'cache_ttl': 300,
                'max_cache_size': 1000
            }

            # æ–°å¢é…ç½®
            self.algorithm_config = config_manager.get_config('algorithm_config') or {
                'enable_advanced_algorithms': True,
                'use_ensemble_methods': True,
                'enable_real_time_learning': False,
                'model_update_frequency': 'daily',
                'performance_threshold': 0.7
            }

            self.prediction_config = config_manager.get_config('prediction_config') or {
                'default_confidence_threshold': 0.6,
                'max_prediction_horizon': 30,
                'enable_uncertainty_quantification': True,
                'use_bayesian_inference': False
            }

            self.logging_config = config_manager.get_config('logging') or {
                'log_predictions': True,
                'log_level': 'INFO',
                'detailed_errors': True
            }

            logger.info("AIé¢„æµ‹é…ç½®å·²ä»æ•°æ®åº“åŠ è½½")

        except Exception as e:
            logger.warning(f"ä»æ•°æ®åº“åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.model_config = {
                'enabled': True,
                'model_type': AIModelType.ENSEMBLE,
                'confidence_threshold': 0.7,
                'prediction_horizon': 5,
                'feature_window': 20,
                'cache_size': 1000,
                'model_update_interval': 24
            }

            self.validation_config = {
                'min_data_points': 10,
                'max_prediction_horizon': 30,
                'max_data_rows': 10000,
                'required_columns': ['open', 'high', 'low', 'close']
            }

            self.feature_config = {
                'technical_indicators': True,
                'pattern_features': True,
                'volume_features': True,
                'price_features': True,
                'volatility_features': True
            }

            self.cache_config = {
                'enable_cache': True,
                'cache_ttl': 300,
                'max_cache_size': 1000
            }

            # æ–°å¢é»˜è®¤é…ç½®
            self.algorithm_config = {
                'enable_advanced_algorithms': True,
                'use_ensemble_methods': True,
                'enable_real_time_learning': False,
                'model_update_frequency': 'daily',
                'performance_threshold': 0.7
            }

            self.prediction_config = {
                'default_confidence_threshold': 0.6,
                'max_prediction_horizon': 30,
                'enable_uncertainty_quantification': True,
                'use_bayesian_inference': False
            }

            self.logging_config = {
                'log_predictions': True,
                'log_level': 'INFO',
                'detailed_errors': True
            }

    def _validate_kdata(self, kdata: pd.DataFrame) -> bool:
        """
        éªŒè¯Kçº¿æ•°æ®æ ¼å¼å’Œå†…å®¹

        Args:
            kdata: Kçº¿æ•°æ®DataFrame

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡

        Raises:
            ValueError: æ•°æ®æ ¼å¼é”™è¯¯
            TypeError: æ•°æ®ç±»å‹é”™è¯¯
        """
        required_columns = ['open', 'high', 'low', 'close']

        # æ£€æŸ¥åŸºç¡€æ ¼å¼
        if kdata is None or kdata.empty:
            logger.warning("Kçº¿æ•°æ®ä¸ºç©º")
            return False

        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_columns = [col for col in required_columns if col not in kdata.columns]
        if missing_columns:
            raise ValueError(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")

        # æ£€æŸ¥æ•°æ®ç±»å‹
        for col in required_columns:
            if not pd.api.types.is_numeric_dtype(kdata[col]):
                raise TypeError(f"åˆ— {col} å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ï¼Œå½“å‰ç±»å‹: {kdata[col].dtype}")

        # æ£€æŸ¥ç©ºå€¼
        null_counts = kdata[required_columns].isnull().sum()
        if null_counts.any():
            logger.warning(f"Kçº¿æ•°æ®åŒ…å«ç©ºå€¼: {null_counts[null_counts > 0].to_dict()}")

        # æ£€æŸ¥æ•°æ®åˆç†æ€§
        invalid_high_low = (kdata['high'] < kdata['low']).sum()
        if invalid_high_low > 0:
            raise ValueError(f"å‘ç° {invalid_high_low} æ¡è®°å½•çš„é«˜ä»·ä½äºä½ä»·")

        # æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
        for col in required_columns:
            if (kdata[col] <= 0).any():
                raise ValueError(f"åˆ— {col} åŒ…å«éæ­£æ•°å€¼")

        # æ£€æŸ¥æ•°æ®å¤§å°é™åˆ¶
        max_rows = 10000  # é™åˆ¶æœ€å¤§è¡Œæ•°
        if len(kdata) > max_rows:
            logger.warning(f"æ•°æ®è¡Œæ•°({len(kdata)})è¶…è¿‡å»ºè®®æœ€å¤§å€¼({max_rows})")

        return True

    def _generate_cache_key(self, kdata: pd.DataFrame, method: str, **kwargs) -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„ç¼“å­˜é”®

        Args:
            kdata: Kçº¿æ•°æ®
            method: æ–¹æ³•åç§°
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        try:
            # åŸºç¡€ä¿¡æ¯
            basic_info = f"{method}_{kdata.shape[0]}_{kdata.shape[1]}"

            # æ—¶é—´èŒƒå›´ä¿¡æ¯
            if hasattr(kdata.index, 'min') and hasattr(kdata.index, 'max'):
                try:
                    time_info = f"_{kdata.index.min()}_{kdata.index.max()}"
                except Exception:
                    time_info = f"_{len(kdata)}"
            else:
                time_info = f"_{len(kdata)}"

            # æ•°æ®å†…å®¹æ‘˜è¦
            if len(kdata) > 0:
                try:
                    first_row_sum = float(kdata.iloc[0][['open', 'high', 'low', 'close']].sum())
                    last_row_sum = float(kdata.iloc[-1][['open', 'high', 'low', 'close']].sum())
                    content_info = f"_{first_row_sum:.2f}_{last_row_sum:.2f}"
                except Exception:
                    content_info = "_default"
            else:
                content_info = "_empty"

            # é¢å¤–å‚æ•°
            kwargs_str = "_".join(f"{k}_{v}" for k, v in sorted(kwargs.items()))
            if kwargs_str:
                kwargs_str = f"_{kwargs_str}"

            # ç”Ÿæˆæœ€ç»ˆçš„ç¼“å­˜é”®
            cache_content = f"{basic_info}{time_info}{content_info}{kwargs_str}"
            cache_key = hashlib.md5(cache_content.encode('utf-8')).hexdigest()[:16]

            return f"{method}_{cache_key}"

        except Exception as e:
            logger.warning(f"ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é”®")
            return f"{method}_default_{datetime.now().timestamp()}"

    def _initialize_models(self):
        """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
        try:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆ–åˆ›å»ºæ–°æ¨¡å‹
            model_dir = Path("models/trained")
            model_dir.mkdir(exist_ok=True)

            if DL_AVAILABLE:
                logger.info("æ·±åº¦å­¦ä¹ æ¨¡å—å¯ç”¨ï¼Œåˆå§‹åŒ–AIé¢„æµ‹æ¨¡å‹")
                self._load_or_create_models()
            else:
                logger.warning("æ·±åº¦å­¦ä¹ æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç»Ÿè®¡æ¨¡å‹")
                self._initialize_statistical_models()

        except Exception as e:
            logger.error(f" æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.warning("AIæ¨¡å‹æ–‡ä»¶ç¼ºå¤±æˆ–æŸåï¼Œè¿™æ˜¯æ­£å¸¸çš„åˆæ¬¡è¿è¡ŒçŠ¶æ€")
            logger.info("ğŸ’¡ ç³»ç»Ÿå°†ä½¿ç”¨å†…ç½®çš„ç»Ÿè®¡æ¨¡å‹ä½œä¸ºå›é€€æ–¹æ¡ˆï¼ŒåŠŸèƒ½å®Œå…¨æ­£å¸¸")
            logger.info("ğŸ“ å¦‚éœ€ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯·ç¡®ä¿ 'models/trained/' ç›®å½•ä¸‹æœ‰ç›¸åº”çš„æ¨¡å‹æ–‡ä»¶")
            self._initialize_fallback_models()

    def _load_or_create_models(self):
        """åŠ è½½æˆ–åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            model_path = Path(f"models/trained/{pred_type}_model.h5")
            if model_path.exists():
                try:
                    # å°è¯•åŠ è½½TensorFlowæ¨¡å‹
                    if TENSORFLOW_AVAILABLE:
                        import tensorflow as tf

                        # éªŒè¯æ¨¡å‹æ–‡ä»¶
                        if model_path.stat().st_size == 0:
                            logger.warning(f"{pred_type}æ¨¡å‹æ–‡ä»¶ä¸ºç©º")
                            self._models[pred_type] = None
                            continue

                        # åŠ è½½æ¨¡å‹å¹¶éªŒè¯
                        model = tf.keras.models.load_model(str(model_path))

                        # åŸºç¡€æ¨¡å‹éªŒè¯
                        if not hasattr(model, 'predict'):
                            logger.warning(f"{pred_type}æ¨¡å‹ç¼ºå°‘predictæ–¹æ³•")
                            self._models[pred_type] = None
                            continue

                        self._models[pred_type] = model
                        logger.info(f" åŠ è½½{pred_type}æ·±åº¦å­¦ä¹ æ¨¡å‹æˆåŠŸ")
                    else:
                        # å¦‚æœæ²¡æœ‰TensorFlowï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ç®€åŒ–æ¨¡å‹
                        try:
                            with open(model_path, 'r', encoding='utf-8') as f:
                                model_data = json.load(f)
                                if model_data.get('model_type') == 'simplified':
                                    self._models[pred_type] = model_data
                                    logger.info(f" åŠ è½½{pred_type}ç®€åŒ–æ¨¡å‹")
                                else:
                                    raise ValueError("Not a simplified model")
                        except Exception:
                            self._models[pred_type] = None
                            logger.warning(f" æ— æ³•è¯†åˆ«{pred_type}æ¨¡å‹æ ¼å¼")

                except Exception as e:
                    # å›é€€ï¼šå°è¯•åŠ è½½ä¸ºç®€åŒ–æ¨¡å‹
                    try:
                        with open(model_path, 'r', encoding='utf-8') as f:
                            model_data = json.load(f)
                            if model_data.get('model_type') == 'simplified':
                                self._models[pred_type] = model_data
                                logger.info(f" åŠ è½½{pred_type}ç®€åŒ–æ¨¡å‹ï¼ˆå›é€€æ¨¡å¼ï¼‰")
                            else:
                                raise ValueError("Not a simplified model")
                    except Exception:
                        logger.warning(f" åŠ è½½{pred_type}æ¨¡å‹å¤±è´¥: {e}")
                        self._models[pred_type] = None
            else:
                # æ ‡è®°éœ€è¦è®­ç»ƒ
                self._models[pred_type] = None
                logger.warning(f" åŠ è½½{pred_type}æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·¯å¾„: {model_path}")

    def _initialize_statistical_models(self):
        """åˆå§‹åŒ–ç»Ÿè®¡æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–ç»Ÿè®¡é¢„æµ‹æ¨¡å‹")
        # ä½¿ç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³•ä½œä¸ºåå¤‡
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            self._models[pred_type] = "statistical"

    def _initialize_fallback_models(self):
        """åˆå§‹åŒ–åå¤‡æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–è§„åˆ™åŸºç¡€æ¨¡å‹")
        for pred_type in [PredictionType.PATTERN, PredictionType.TREND,
                          PredictionType.SENTIMENT, PredictionType.PRICE]:
            self._models[pred_type] = "rule_based"

    def predict_patterns(self, kdata: pd.DataFrame, patterns: List[Dict]) -> Dict[str, Any]:
        """
        é¢„æµ‹å½¢æ€ä¿¡å·

        Args:
            kdata: Kçº¿æ•°æ®
            patterns: æ£€æµ‹åˆ°çš„å½¢æ€åˆ—è¡¨

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿—å¼€å§‹ ===
        logger.info("="*80)
        logger.info("AIé¢„æµ‹æœåŠ¡ - predict_patterns å¼€å§‹")
        logger.info(f" è¾“å…¥æ•°æ®: Kçº¿é•¿åº¦={len(kdata)}, å½¢æ€æ•°é‡={len(patterns)}")
        logger.info(f" å½“å‰æ¨¡å‹é…ç½®: {self.model_config}")
        logger.info(f" å½“å‰æ¨¡å‹ç±»å‹: {self.model_config.get('model_type', 'N/A')}")
        logger.info("="*80)
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not self._validate_kdata(kdata):
                return self._get_fallback_pattern_prediction()

            if not patterns or not isinstance(patterns, list):
                logger.warning("å½¢æ€åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹")
                patterns = []

            # éªŒè¯æ¯ä¸ªå½¢æ€çš„ç»“æ„
            valid_patterns = []
            for i, pattern in enumerate(patterns):
                if not isinstance(pattern, dict):
                    logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                    continue

                # æ£€æŸ¥å¿…è¦å­—æ®µï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„å­—æ®µå
                has_name = any(key in pattern for key in ['name', 'pattern_name', 'pattern_type'])
                if not has_name:
                    logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œç¼ºå°‘åç§°å­—æ®µï¼Œè·³è¿‡")
                    continue

                # è§„èŒƒåŒ–å­—æ®µåï¼Œç¡®ä¿æœ‰nameå­—æ®µä¾›åç»­ä½¿ç”¨
                if 'name' not in pattern:
                    if 'pattern_name' in pattern:
                        pattern['name'] = pattern['pattern_name']
                    elif 'pattern_type' in pattern:
                        pattern['name'] = pattern['pattern_type']

                valid_patterns.append(pattern)

            # ç”¨æœ‰æ•ˆçš„å½¢æ€æ›¿æ¢åŸå§‹åˆ—è¡¨
            patterns = valid_patterns
            logger.info(f"å½¢æ€æ•°æ®éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆå½¢æ€æ•°é‡: {len(patterns)}/{len(valid_patterns)}")

            cache_key = self._generate_cache_key(kdata, "predict_patterns", patterns=len(patterns))
            if cache_key in self._predictions_cache:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å½¢æ€é¢„æµ‹ç»“æœ: {cache_key}")
                return self._predictions_cache[cache_key]

            prediction = self._generate_pattern_prediction(kdata, patterns)
            self._predictions_cache[cache_key] = prediction
            return prediction

        except Exception as e:
            logger.error(f"å½¢æ€é¢„æµ‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self._get_fallback_pattern_prediction()

    def predict_trend(self, kdata: pd.DataFrame, timeframe: int = 5) -> Dict[str, Any]:
        """
        è¶‹åŠ¿é¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            timeframe: é¢„æµ‹æ—¶é—´æ¡†æ¶ï¼ˆå¤©æ•°ï¼‰

        Returns:
            è¶‹åŠ¿é¢„æµ‹ç»“æœ
        """
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not self._validate_kdata(kdata):
                raise ValueError("æ— æ•ˆçš„Kçº¿æ•°æ®")

            # å‚æ•°éªŒè¯
            if not isinstance(timeframe, int) or timeframe < 1 or timeframe > 30:
                raise ValueError("é¢„æµ‹æ—¶é—´æ¡†æ¶å¿…é¡»åœ¨1-30å¤©ä¹‹é—´")

            if len(kdata) < timeframe * 2:
                raise ValueError(f"æ•°æ®é•¿åº¦({len(kdata)})ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{timeframe * 2}ä¸ªæ•°æ®ç‚¹")

            features = self._extract_trend_features(kdata)
            model = self._models.get(PredictionType.TREND)

            if model and model != "rule_based" and model != "statistical":
                # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
                prediction = self._predict_with_dl_model(model, features, PredictionType.TREND)
                if prediction:  # ç¡®ä¿é¢„æµ‹ç»“æœä¸ä¸ºNone
                    return prediction

            if model == "statistical":
                # ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹
                prediction = self._predict_with_statistical_model(features, PredictionType.TREND)
                if prediction:
                    return prediction

            # ä½¿ç”¨è§„åˆ™æ¨¡å‹ä½œä¸ºåå¤‡
            prediction = self._predict_with_rules(kdata, PredictionType.TREND)
            return prediction

        except Exception as e:
            logger.error(f"è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_trend_prediction()

    def predict_sentiment(self, kdata: pd.DataFrame, market_data: Dict = None) -> Dict[str, Any]:
        """
        æƒ…ç»ªé¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            market_data: å¸‚åœºæ•°æ®

        Returns:
            æƒ…ç»ªé¢„æµ‹ç»“æœ
        """
        try:
            features = self._extract_sentiment_features(kdata, market_data)
            model = self._models.get(PredictionType.SENTIMENT)

            if model and model != "rule_based" and model != "statistical":
                prediction = self._predict_with_dl_model(model, features, PredictionType.SENTIMENT)
            elif model == "statistical":
                prediction = self._predict_with_statistical_model(features, PredictionType.SENTIMENT)
            else:
                prediction = self._predict_sentiment_with_rules(kdata, market_data)

            return prediction

        except Exception as e:
            logger.error(f"æƒ…ç»ªé¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_sentiment_prediction()

    def predict_price(self, kdata: pd.DataFrame, horizon: int = 5) -> Dict[str, Any]:
        """
        ä»·æ ¼é¢„æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰

        Returns:
            ä»·æ ¼é¢„æµ‹ç»“æœ
        """
        try:
            features = self._extract_price_features(kdata)
            model = self._models.get(PredictionType.PRICE)

            if model and model != "rule_based" and model != "statistical":
                # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
                prediction = self._predict_with_dl_model(model, features, PredictionType.PRICE)
                if prediction:  # ç¡®ä¿é¢„æµ‹ç»“æœä¸ä¸ºNone
                    return prediction

            if model == "statistical":
                # ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹
                prediction = self._predict_with_statistical_model(features, PredictionType.PRICE)
                if prediction:
                    return prediction

            # ä½¿ç”¨è§„åˆ™æ¨¡å‹ä½œä¸ºåå¤‡
            prediction = self._predict_price_with_rules(kdata, horizon)
            return prediction

        except Exception as e:
            logger.error(f"ä»·æ ¼é¢„æµ‹å¤±è´¥: {e}")
            return self._get_fallback_price_prediction()

    def assess_risk(self, kdata: pd.DataFrame, predictions: Dict = None) -> Dict[str, Any]:
        """
        é£é™©è¯„ä¼°

        Args:
            kdata: Kçº¿æ•°æ®
            predictions: å…¶ä»–é¢„æµ‹ç»“æœ

        Returns:
            é£é™©è¯„ä¼°ç»“æœ
        """
        try:
            # è®¡ç®—å„ç§é£é™©æŒ‡æ ‡
            volatility_risk = self._calculate_volatility_risk(kdata)
            technical_risk = self._calculate_technical_risk(kdata)
            market_risk = self._calculate_market_risk(kdata)

            # ç»¼åˆé£é™©è¯„ä¼°
            overall_risk = self._calculate_overall_risk(
                volatility_risk, technical_risk, market_risk, predictions
            )

            return {
                'overall_risk': overall_risk,
                'volatility_risk': volatility_risk,
                'technical_risk': technical_risk,
                'market_risk': market_risk,
                'risk_level': self._categorize_risk(overall_risk),
                'risk_factors': self._identify_risk_factors(kdata),
                'recommendations': self._get_risk_recommendations(overall_risk)
            }

        except Exception as e:
            logger.error(f"é£é™©è¯„ä¼°å¤±è´¥: {e}")
            return self._get_fallback_risk_assessment()

    def _generate_pattern_prediction(self, kdata: pd.DataFrame, patterns: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå½¢æ€é¢„æµ‹"""
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿— ===
        logger.info("_generate_pattern_prediction å¼€å§‹")
        logger.info(f" å½¢æ€æ•°é‡: {len(patterns)}")

        if not patterns:
            logger.warning("å½¢æ€åˆ—è¡¨ä¸ºç©ºï¼Œè°ƒç”¨ _predict_without_patterns")
            logger.info(f" å³å°†ä½¿ç”¨æ¨¡å‹ç±»å‹: {self.model_config.get('model_type', 'N/A')}")
            result = self._predict_without_patterns(kdata)
            logger.info(f" _predict_without_patterns è¿”å›ç»“æœ: {result}")
            return result
        # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        # éªŒè¯æ¯ä¸ªå½¢æ€çš„ç»“æ„
        valid_patterns = []
        for i, pattern in enumerate(patterns):
            if not isinstance(pattern, dict):
                logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                continue

            # æ£€æŸ¥å¿…è¦å­—æ®µï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„å­—æ®µå
            has_name = any(key in pattern for key in ['name', 'pattern_name', 'pattern_type'])
            if not has_name:
                logger.warning(f"å½¢æ€æ•°æ®æ ¼å¼æ— æ•ˆ(ç´¢å¼•{i})ï¼Œç¼ºå°‘åç§°å­—æ®µï¼Œè·³è¿‡")
                continue

            # è§„èŒƒåŒ–å­—æ®µåï¼Œç¡®ä¿æœ‰nameå­—æ®µä¾›åç»­ä½¿ç”¨
            if 'name' not in pattern:
                if 'pattern_name' in pattern:
                    pattern['name'] = pattern['pattern_name']
                elif 'pattern_type' in pattern:
                    pattern['name'] = pattern['pattern_type']

            valid_patterns.append(pattern)

        logger.info(f"æœ‰æ•ˆå½¢æ€æ•°é‡: {len(valid_patterns)}")

        if not valid_patterns:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å½¢æ€æ•°æ®ï¼Œä½¿ç”¨æ— å½¢æ€é¢„æµ‹")
            return self._predict_without_patterns(kdata)

        # === å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œä¸åŒçš„å½¢æ€é¢„æµ‹ ===
        model_type = self.model_config.get('model_type', AIModelType.ENSEMBLE)
        logger.info(f" æœ‰å½¢æ€çš„é¢„æµ‹ï¼Œä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")

        # åˆ†æå½¢æ€ä¿¡å·å¼ºåº¦
        buy_signals = [p for p in valid_patterns if p.get('signal_type') == 'bullish']
        sell_signals = [p for p in valid_patterns if p.get('signal_type') == 'bearish']

        # è®¡ç®—åŸºç¡€å½¢æ€ç»Ÿè®¡
        pattern_analysis = {
            'total_patterns': len(valid_patterns),
            'bullish_signals': len(buy_signals),
            'bearish_signals': len(sell_signals),
            'avg_confidence': np.mean([p.get('confidence', 0.5) for p in valid_patterns])
        }

        # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œä¸åŒçš„é¢„æµ‹å¤„ç†
        try:
            if model_type == AIModelType.DEEP_LEARNING:
                logger.info("ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_deep_learning(kdata, valid_patterns, pattern_analysis)
            elif model_type == AIModelType.STATISTICAL:
                logger.info("ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_statistical(kdata, valid_patterns, pattern_analysis)
            elif model_type == AIModelType.RULE_BASED:
                logger.info("ä½¿ç”¨è§„åˆ™æ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_rule_based(kdata, valid_patterns, pattern_analysis)
            else:  # ENSEMBLE
                logger.info("ä½¿ç”¨é›†æˆæ¨¡å‹å¤„ç†å½¢æ€é¢„æµ‹...")
                result = self._predict_with_patterns_ensemble(kdata, valid_patterns, pattern_analysis)

            # æ·»åŠ å½¢æ€åˆ†æä¿¡æ¯
            result.update({
                'pattern_count': len(valid_patterns),
                'bullish_signals': len(buy_signals),
                'bearish_signals': len(sell_signals),
                'prediction_type': PredictionType.PATTERN,
                'timestamp': datetime.now().isoformat()
            })

            logger.info(f" å½¢æ€é¢„æµ‹å®Œæˆ:")
            logger.info(f"    æ–¹å‘: {result.get('direction', 'N/A')}")
            logger.info(f"    ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            logger.info(f"    æ¨¡å‹ç±»å‹: {result.get('model_type', 'N/A')}")

            return result

        except Exception as e:
            logger.error(f" æ¨¡å‹ç‰¹å®šå½¢æ€é¢„æµ‹å¤±è´¥ ({model_type}): {e}")
            logger.error(traceback.format_exc())
            # é™çº§åˆ°é€šç”¨å½¢æ€åˆ†æ
            return self._fallback_pattern_analysis(valid_patterns, buy_signals, sell_signals, pattern_analysis)

    def _predict_without_patterns(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """å½“å½¢æ€åˆ—è¡¨ä¸ºç©ºæ—¶ï¼Œæ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹"""
        # === è¯¦ç»†è°ƒè¯•æ—¥å¿— ===
        logger.info("_predict_without_patterns å¼€å§‹æ‰§è¡Œ")
        model_type = self.model_config.get('model_type', AIModelType.ENSEMBLE)
        logger.info(f" ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f" å®Œæ•´æ¨¡å‹é…ç½®: {self.model_config}")
        # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„æµ‹æ–¹æ³•
            if model_type == AIModelType.DEEP_LEARNING:
                logger.info("è°ƒç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_deep_learning(kdata)
                result['model_path'] = 'deep_learning_without_patterns'
            elif model_type == AIModelType.STATISTICAL:
                logger.info("è°ƒç”¨ç»Ÿè®¡æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_statistical_method(kdata)
                result['model_path'] = 'statistical_without_patterns'
            elif model_type == AIModelType.RULE_BASED:
                logger.info("è°ƒç”¨è§„åˆ™æ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_rule_based_method(kdata)
                result['model_path'] = 'rule_based_without_patterns'
            else:  # ENSEMBLE
                logger.info("è°ƒç”¨é›†æˆæ¨¡å‹é¢„æµ‹...")
                result = self._predict_with_ensemble_method(kdata)
                result['model_path'] = 'ensemble_without_patterns'

            # === è°ƒè¯•æ—¥å¿—ï¼šé¢„æµ‹ç»“æœ ===
            logger.info(f" {model_type} é¢„æµ‹å®Œæˆ:")
            logger.info(f"    æ–¹å‘: {result.get('direction', 'N/A')}")
            logger.info(f"    ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            logger.info(f"    æ¨¡å‹ç±»å‹: {result.get('model_type', 'N/A')}")
            logger.info(f"    æ¨¡å‹è·¯å¾„: {result.get('model_path', 'N/A')}")
            # === è°ƒè¯•æ—¥å¿—ç»“æŸ ===

            return result

        except Exception as e:
            logger.error(f" æ¨¡å‹é¢„æµ‹å¤±è´¥ ({model_type}): {e}")
            logger.error(traceback.format_exc())
            # è¿”å›åå¤‡é¢„æµ‹
            return self._get_fallback_pattern_prediction()

    def _extract_pattern_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–ç”¨äºæ— å½¢æ€é¢„æµ‹çš„æŠ€æœ¯ç‰¹å¾"""
        features = []
        close_prices = kdata['close'].values
        high_prices = kdata['high'].values
        low_prices = kdata['low'].values
        volumes = kdata.get('volume', pd.Series([1]*len(kdata))).values

        # ä»·æ ¼ç‰¹å¾
        ma5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else close_prices[-1]
        ma10 = np.mean(close_prices[-10:]) if len(close_prices) >= 10 else close_prices[-1]
        ma20 = np.mean(close_prices[-20:]) if len(close_prices) >= 20 else close_prices[-1]

        features.extend([
            close_prices[-1] / ma5 - 1,  # ç›¸å¯¹5æ—¥å‡çº¿
            close_prices[-1] / ma10 - 1,  # ç›¸å¯¹10æ—¥å‡çº¿
            close_prices[-1] / ma20 - 1,  # ç›¸å¯¹20æ—¥å‡çº¿
            ma5 / ma20 - 1 if ma20 != 0 else 0,  # çŸ­æœŸè¶‹åŠ¿
        ])

        # æ³¢åŠ¨ç‡ç‰¹å¾
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-6:]) / close_prices[-6:-1]
            volatility = np.std(returns) if len(returns) > 1 else 0
            features.append(volatility)
        else:
            features.append(0)

        # æˆäº¤é‡ç‰¹å¾
        if len(volumes) >= 5:
            vol_ma5 = np.mean(volumes[-5:])
            vol_ma20 = np.mean(volumes[-20:]) if len(volumes) >= 20 else vol_ma5
            vol_ratio = volumes[-1] / vol_ma5 - 1 if vol_ma5 != 0 else 0
            features.append(vol_ratio)
        else:
            features.append(0)

        return np.array(features)

    def _predict_with_deep_learning(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹"""
        logger.info("=== æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # æå–ç‰¹å¾
            features = self._extract_pattern_features(kdata)
            logger.info(f" ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(features)}")

            # æ¨¡æ‹Ÿæ·±åº¦å­¦ä¹ é¢„æµ‹ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„DLæ¨¡å‹ï¼‰
            prediction_strength = np.mean([
                features.get('price_momentum', 0.5),
                features.get('volume_strength', 0.5),
                features.get('volatility_signal', 0.5)
            ])

            # æ·»åŠ ä¸€äº›éšæœºæ€§æ¨¡æ‹Ÿç¥ç»ç½‘ç»œçš„å¤æ‚æ€§
            random_factor = np.random.normal(0, 0.1)
            adjusted_strength = np.clip(prediction_strength + random_factor, 0, 1)

            if adjusted_strength > 0.6:
                direction = "ä¸Šæ¶¨"
                confidence = 0.65 + (adjusted_strength - 0.6) * 0.3
            elif adjusted_strength < 0.4:
                direction = "ä¸‹è·Œ"
                confidence = 0.65 + (0.4 - adjusted_strength) * 0.3
            else:
                direction = "éœ‡è¡"
                confidence = 0.55 + abs(adjusted_strength - 0.5) * 0.2

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'deep_learning',
                'prediction_type': PredictionType.PATTERN,
                'features_used': len(features),
                'dl_strength': prediction_strength,
                'random_factor': random_factor
            }

            logger.info(f" æ·±åº¦å­¦ä¹ é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f" æ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_statistical_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """ç»Ÿè®¡æ¨¡å‹é¢„æµ‹"""
        logger.info("=== ç»Ÿè®¡æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            features = self._extract_pattern_features(kdata)
            logger.info(f" ç»Ÿè®¡ç‰¹å¾æå–å®Œæˆ")

            # åŸºäºZ-scoreçš„ç»Ÿè®¡åˆ†æ
            price_zscore = features.get('price_zscore', 0)
            volume_zscore = features.get('volume_zscore', 0)

            # ç»Ÿè®¡å†³ç­–è§„åˆ™
            if price_zscore > 1.5 and volume_zscore > 0.5:
                direction = "ä¸Šæ¶¨"
                confidence = 0.70 + min(abs(price_zscore) * 0.1, 0.25)
            elif price_zscore < -1.5 and volume_zscore > 0.5:
                direction = "ä¸‹è·Œ"
                confidence = 0.70 + min(abs(price_zscore) * 0.1, 0.25)
            else:
                direction = "éœ‡è¡"
                confidence = 0.60 + abs(price_zscore) * 0.05

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'statistical',
                'prediction_type': PredictionType.PATTERN,
                'price_zscore': price_zscore,
                'volume_zscore': volume_zscore,
                'features_used': len(features)
            }

            logger.info(f" ç»Ÿè®¡æ¨¡å‹é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f" ç»Ÿè®¡æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_rule_based_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """è§„åˆ™æ¨¡å‹é¢„æµ‹"""
        logger.info("=== è§„åˆ™æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            features = self._extract_pattern_features(kdata)
            logger.info(f" è§„åˆ™ç‰¹å¾æå–å®Œæˆ")

            # å¤šé‡æŠ€æœ¯æŒ‡æ ‡è§„åˆ™
            signals = []

            # è§„åˆ™1: å‡çº¿ä¿¡å·
            if features.get('ma_signal', 0) > 0.5:
                signals.append(('bullish', 0.8))
            elif features.get('ma_signal', 0) < -0.5:
                signals.append(('bearish', 0.8))

            # è§„åˆ™2: æˆäº¤é‡ä¿¡å·
            if features.get('volume_strength', 0) > 0.7:
                signals.append(('bullish', 0.6))

            # è§„åˆ™3: æ³¢åŠ¨ç‡ä¿¡å·
            if features.get('volatility_signal', 0) > 0.6:
                signals.append(('bearish', 0.7))

            # ç»¼åˆåˆ¤æ–­
            bullish_weight = sum(w for s, w in signals if s == 'bullish')
            bearish_weight = sum(w for s, w in signals if s == 'bearish')

            if bullish_weight > bearish_weight and bullish_weight > 0.5:
                direction = "ä¸Šæ¶¨"
                confidence = 0.75 + min(bullish_weight - bearish_weight, 0.2)
            elif bearish_weight > bullish_weight and bearish_weight > 0.5:
                direction = "ä¸‹è·Œ"
                confidence = 0.75 + min(bearish_weight - bullish_weight, 0.2)
            else:
                direction = "éœ‡è¡"
                confidence = 0.65

            result = {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'rule_based',
                'prediction_type': PredictionType.PATTERN,
                'signals_count': len(signals),
                'bullish_weight': bullish_weight,
                'bearish_weight': bearish_weight,
                'features_used': len(features)
            }

            logger.info(f" è§„åˆ™æ¨¡å‹é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f" è§„åˆ™æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _predict_with_ensemble_method(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """é›†æˆæ¨¡å‹é¢„æµ‹"""
        logger.info("=== é›†æˆæ¨¡å‹é¢„æµ‹å¼€å§‹ ===")

        try:
            # è°ƒç”¨æ‰€æœ‰å­æ¨¡å‹
            logger.info("è°ƒç”¨æ·±åº¦å­¦ä¹ å­æ¨¡å‹...")
            dl_result = self._predict_with_deep_learning(kdata)

            logger.info("è°ƒç”¨ç»Ÿè®¡æ¨¡å‹å­æ¨¡å‹...")
            stat_result = self._predict_with_statistical_method(kdata)

            logger.info("è°ƒç”¨è§„åˆ™æ¨¡å‹å­æ¨¡å‹...")
            rule_result = self._predict_with_rule_based_method(kdata)

            # åŠ æƒæŠ•ç¥¨
            models = [
                (dl_result, 0.4),      # æ·±åº¦å­¦ä¹ æƒé‡40%
                (stat_result, 0.35),   # ç»Ÿè®¡æ¨¡å‹æƒé‡35%
                (rule_result, 0.25)    # è§„åˆ™æ¨¡å‹æƒé‡25%
            ]

            direction_votes = {'ä¸Šæ¶¨': 0, 'ä¸‹è·Œ': 0, 'éœ‡è¡': 0}
            total_confidence = 0
            total_weight = 0

            for result, weight in models:
                direction = result.get('direction', 'éœ‡è¡')
                confidence = result.get('confidence', 0.5)

                direction_votes[direction] += weight * confidence
                total_confidence += weight * confidence
                total_weight += weight

            # ç¡®å®šæœ€ç»ˆæ–¹å‘
            final_direction = max(direction_votes.items(), key=lambda x: x[1])[0]
            final_confidence = total_confidence / total_weight

            result = {
                'direction': final_direction,
                'confidence': final_confidence,
                'model_type': 'ensemble',
                'prediction_type': PredictionType.PATTERN,
                'sub_models': {
                    'deep_learning': dl_result,
                    'statistical': stat_result,
                    'rule_based': rule_result
                },
                'vote_weights': direction_votes
            }

            logger.info(f" é›†æˆæ¨¡å‹é¢„æµ‹ç»“æœ: {final_direction}, ç½®ä¿¡åº¦: {final_confidence:.3f}")
            return result

        except Exception as e:
            logger.error(f" é›†æˆæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            raise

    def _extract_trend_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–è¶‹åŠ¿é¢„æµ‹ç‰¹å¾"""
        features = []

        # ä»·æ ¼ç‰¹å¾
        close_prices = kdata['close'].values
        features.extend([
            np.mean(close_prices[-5:]) / np.mean(close_prices[-20:]),  # çŸ­æœŸå‡çº¿æ¯”ç‡
            np.std(close_prices[-20:]) / np.mean(close_prices[-20:]),  # æ³¢åŠ¨ç‡
            (close_prices[-1] - close_prices[-5]) / close_prices[-5],  # 5æ—¥æ¶¨å¹…
            (close_prices[-1] - close_prices[-20]) / close_prices[-20]  # 20æ—¥æ¶¨å¹…
        ])

        # æˆäº¤é‡ç‰¹å¾
        if 'volume' in kdata.columns:
            volumes = kdata['volume'].values
            features.extend([
                np.mean(volumes[-5:]) / np.mean(volumes[-20:]),  # æˆäº¤é‡æ¯”ç‡
                np.std(volumes[-20:]) / np.mean(volumes[-20:])   # æˆäº¤é‡æ³¢åŠ¨
            ])

        return np.array(features)

    def _extract_sentiment_features(self, kdata: pd.DataFrame, market_data: Dict = None) -> np.ndarray:
        """æå–æƒ…ç»ªé¢„æµ‹ç‰¹å¾"""
        features = []

        # æŠ€æœ¯æƒ…ç»ªç‰¹å¾
        close_prices = kdata['close'].values
        high_prices = kdata['high'].values
        low_prices = kdata['low'].values

        # RSIè¿‘ä¼¼è®¡ç®—
        price_changes = np.diff(close_prices[-21:])
        gains = np.where(price_changes > 0, price_changes, 0)
        losses = np.where(price_changes < 0, -price_changes, 0)
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        rsi = 100 - (100 / (1 + avg_gain / (avg_loss + 1e-8)))

        features.extend([
            rsi / 100,  # æ ‡å‡†åŒ–RSI
            len([1 for i in range(-10, 0) if close_prices[i] > close_prices[i-1]]) / 10,  # ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹
            np.mean(high_prices[-10:] - close_prices[-10:]) / np.mean(close_prices[-10:]),  # ä¸Šå½±çº¿æ¯”ä¾‹
            np.mean(close_prices[-10:] - low_prices[-10:]) / np.mean(close_prices[-10:])   # ä¸‹å½±çº¿æ¯”ä¾‹
        ])

        return np.array(features)

    def _extract_price_features(self, kdata: pd.DataFrame) -> np.ndarray:
        """æå–ä»·æ ¼é¢„æµ‹ç‰¹å¾"""
        features = []

        # OHLCVç‰¹å¾
        for col in ['open', 'high', 'low', 'close']:
            if col in kdata.columns:
                values = kdata[col].values[-20:]
                features.extend([
                    np.mean(values),
                    np.std(values),
                    values[-1] / values[0] - 1  # 20æ—¥æ”¶ç›Šç‡
                ])

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        close_prices = kdata['close'].values
        ma5 = np.mean(close_prices[-5:])
        ma10 = np.mean(close_prices[-10:])
        ma20 = np.mean(close_prices[-20:])

        features.extend([
            close_prices[-1] / ma5 - 1,
            close_prices[-1] / ma10 - 1,
            close_prices[-1] / ma20 - 1,
            ma5 / ma20 - 1
        ])

        return np.array(features)

    def _predict_with_dl_model(self, model, features, prediction_type):
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç®€åŒ–æ¨¡å‹
            if isinstance(model, dict) and model.get('model_type') == 'simplified':
                return self._predict_with_simplified_model(model, features, prediction_type)

            # å¦åˆ™ä½¿ç”¨TensorFlowæ¨¡å‹
            if TENSORFLOW_AVAILABLE and hasattr(model, 'predict'):
                # è·å–æ¨¡å‹æœŸæœ›çš„è¾“å…¥å½¢çŠ¶
                expected_input_dim = model.input_shape[-1] if hasattr(model, 'input_shape') else len(features)

                # è°ƒæ•´ç‰¹å¾ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹
                if len(features) != expected_input_dim:
                    logger.info(f"è°ƒæ•´ç‰¹å¾ç»´åº¦: {len(features)} -> {expected_input_dim}")
                    if len(features) < expected_input_dim:
                        # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œç”¨å‡å€¼å¡«å……
                        features = np.pad(features, (0, expected_input_dim - len(features)),
                                          mode='constant', constant_values=np.mean(features))
                    else:
                        # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œæˆªå–å‰Nä¸ª
                        features = features[:expected_input_dim]

                prediction = model.predict(features.reshape(1, -1), verbose=0)
                confidence = float(np.max(prediction))
                predicted_class = int(np.argmax(prediction))

                # æ ¹æ®é¢„æµ‹ç±»å‹è¿”å›ç»“æœ
                return self._format_prediction_result(predicted_class, confidence, prediction_type)
            else:
                raise ValueError("Invalid model type for deep learning prediction")

        except Exception as e:
            logger.warning(f"æ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›åå¤‡é¢„æµ‹ç»“æœ
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'dl_model_fallback',
                'timestamp': datetime.now().isoformat()
            }

    def _predict_with_simplified_model(self, model, features, prediction_type):
        """ä½¿ç”¨ç®€åŒ–æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            # ç®€åŒ–é¢„æµ‹é€»è¾‘ï¼šåŸºäºç‰¹å¾å’Œæ¨¡å‹æƒé‡
            model_info = model.get('model_info', {})
            expected_input_dim = model_info.get('input_features', len(features))

            # è°ƒæ•´ç‰¹å¾ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹
            if len(features) != expected_input_dim:
                logger.info(f"ç®€åŒ–æ¨¡å‹è°ƒæ•´ç‰¹å¾ç»´åº¦: {len(features)} -> {expected_input_dim}")
                if len(features) < expected_input_dim:
                    # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œç”¨å‡å€¼å¡«å……
                    features = np.pad(features, (0, expected_input_dim - len(features)),
                                      mode='constant', constant_values=np.mean(features) if len(features) > 0 else 0.0)
                else:
                    # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œæˆªå–å‰Nä¸ª
                    features = features[:expected_input_dim]

            # ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œç®€å•çš„çº¿æ€§ç»„åˆé¢„æµ‹
            weights = model.get('weights', {})
            layer1_weights = np.array(weights.get('layer1', np.random.randn(expected_input_dim, 64)))

            # ç¡®ä¿æƒé‡ç»´åº¦åŒ¹é…
            if layer1_weights.shape[0] != len(features):
                layer1_weights = np.resize(layer1_weights, (len(features), 64))

            # ç®€åŒ–çš„å‰å‘ä¼ æ’­
            try:
                hidden = np.tanh(np.dot(features, layer1_weights))
                output = np.mean(hidden) + 0.5  # ç®€åŒ–è¾“å‡º
            except Exception:
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„ç‰¹å¾å¹³å‡
                output = np.mean(features) + 0.5 if len(features) > 0 else 0.5

            # ç”Ÿæˆé¢„æµ‹ç»“æœ
            confidence = min(max(abs(output - 0.5) * 2, 0.3), 0.9)  # é™åˆ¶ç½®ä¿¡åº¦èŒƒå›´
            predicted_class = 1 if abs(output - 0.5) < 0.1 else (2 if output > 0.5 else 0)

            return self._format_prediction_result(predicted_class, confidence, prediction_type)

        except Exception as e:
            logger.warning(f"ç®€åŒ–æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›åå¤‡é¢„æµ‹ç»“æœ
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'simplified_model_fallback',
                'timestamp': datetime.now().isoformat()
            }

    def _format_prediction_result(self, predicted_class, confidence, prediction_type):
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœ"""
        class_names = {
            PredictionType.PATTERN: ['ä¸‹é™å½¢æ€', 'éœ‡è¡å½¢æ€', 'ä¸Šå‡å½¢æ€'],
            PredictionType.TREND: ['ä¸‹è·Œè¶‹åŠ¿', 'æ¨ªç›˜è¶‹åŠ¿', 'ä¸Šæ¶¨è¶‹åŠ¿'],
            PredictionType.SENTIMENT: ['æ‚²è§‚æƒ…ç»ª', 'ä¸­æ€§æƒ…ç»ª', 'ä¹è§‚æƒ…ç»ª'],
            PredictionType.PRICE: ['ä»·æ ¼ä¸‹è·Œ', 'ä»·æ ¼å¹³ç¨³', 'ä»·æ ¼ä¸Šæ¶¨']
        }

        direction_map = {
            0: 'ä¸‹è·Œ',
            1: 'éœ‡è¡',
            2: 'ä¸Šæ¶¨'
        }

        class_list = class_names.get(prediction_type, ['ä¸‹è·Œ', 'éœ‡è¡', 'ä¸Šæ¶¨'])
        predicted_label = class_list[predicted_class] if predicted_class < len(class_list) else class_list[1]
        direction = direction_map.get(predicted_class, 'éœ‡è¡')

        return {
            'direction': direction,
            'confidence': confidence,
            'predicted_class': predicted_class,
            'predicted_label': predicted_label,
            'model_type': 'ai_model',
            'timestamp': datetime.now().isoformat()
        }

    def _predict_with_statistical_model(self, features: np.ndarray, pred_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹é¢„æµ‹"""
        # ç®€å•çš„ç»Ÿè®¡æ–¹æ³•
        feature_mean = np.mean(features)
        feature_std = np.std(features)

        if feature_mean > feature_std:
            direction = "ä¸Šæ¶¨" if pred_type == PredictionType.TREND else "ä¹è§‚"
            confidence = 0.6
        elif feature_mean < -feature_std:
            direction = "ä¸‹è·Œ" if pred_type == PredictionType.TREND else "æ‚²è§‚"
            confidence = 0.6
        else:
            direction = "éœ‡è¡" if pred_type == PredictionType.TREND else "ä¸­æ€§"
            confidence = 0.5

        return {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'statistical'
        }

    def _predict_with_rules(self, kdata: pd.DataFrame, pred_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨è§„åˆ™æ¨¡å‹é¢„æµ‹"""
        # å¦‚æœæ²¡æœ‰æä¾›kdataï¼Œè¿”å›é»˜è®¤é¢„æµ‹
        if kdata is None or kdata.empty:
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'rule_based_fallback'
            }

        try:
            close_prices = kdata['close'].values

            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(close_prices) < 10:
                return {
                    'direction': 'éœ‡è¡',
                    'confidence': 0.5,
                    'model_type': 'rule_based_insufficient_data'
                }

            # ç®€å•çš„æŠ€æœ¯åˆ†æè§„åˆ™
            ma5 = np.mean(close_prices[-5:])
            ma10 = np.mean(close_prices[-10:])
            current_price = close_prices[-1]

            if current_price > ma5 > ma10:
                direction = "ä¸Šæ¶¨"
                confidence = 0.65
            elif current_price < ma5 < ma10:
                direction = "ä¸‹è·Œ"
                confidence = 0.65
            else:
                direction = "éœ‡è¡"
                confidence = 0.5

            return {
                'direction': direction,
                'confidence': confidence,
                'model_type': 'rule_based'
            }
        except Exception as e:
            logger.warning(f"è§„åˆ™é¢„æµ‹å¤±è´¥: {e}")
            return {
                'direction': 'éœ‡è¡',
                'confidence': 0.5,
                'model_type': 'rule_based_error'
            }

    def _predict_sentiment_with_rules(self, kdata: pd.DataFrame, market_data: Dict = None) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„æƒ…ç»ªé¢„æµ‹"""
        return self._predict_with_rules(kdata, PredictionType.SENTIMENT)

    def _predict_price_with_rules(self, kdata: pd.DataFrame, horizon: int) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ä»·æ ¼é¢„æµ‹"""
        current_price = float(kdata['close'].iloc[-1])
        close_prices = kdata['close'].values

        # è®¡ç®—è¶‹åŠ¿
        trend = np.polyfit(range(len(close_prices[-10:])), close_prices[-10:], 1)[0]

        # é¢„æµ‹ä»·æ ¼èŒƒå›´
        if trend > 0:
            target_low = current_price * 1.01
            target_high = current_price * 1.05
            direction = "ä¸Šæ¶¨"
        elif trend < 0:
            target_low = current_price * 0.95
            target_high = current_price * 0.99
            direction = "ä¸‹è·Œ"
        else:
            target_low = current_price * 0.98
            target_high = current_price * 1.02
            direction = "éœ‡è¡"

        return {
            'direction': direction,
            'current_price': current_price,
            'target_low': target_low,
            'target_high': target_high,
            'target_range': f"{target_low:.2f} - {target_high:.2f}",
            'horizon_days': horizon,
            'confidence': 0.6,
            'model_type': 'rule_based'
        }

    def _calculate_volatility_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—æ³¢åŠ¨ç‡é£é™©"""
        returns = kdata['close'].pct_change().dropna()
        volatility = returns.std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
        return min(volatility * 5, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1

    def _calculate_technical_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—æŠ€æœ¯é¢é£é™©"""
        close_prices = kdata['close'].values

        # è®¡ç®—æœ€å¤§å›æ’¤
        peak = np.maximum.accumulate(close_prices)
        drawdown = (close_prices - peak) / peak
        max_drawdown = abs(np.min(drawdown))

        return min(max_drawdown * 2, 1.0)

    def _calculate_market_risk(self, kdata: pd.DataFrame) -> float:
        """è®¡ç®—å¸‚åœºé£é™©"""
        # ç®€åŒ–çš„å¸‚åœºé£é™©è¯„ä¼°
        volumes = kdata['volume'].values if 'volume' in kdata.columns else np.ones(len(kdata))
        vol_ratio = np.std(volumes[-10:]) / np.mean(volumes[-10:])
        return min(vol_ratio * 0.5, 1.0)

    def _calculate_overall_risk(self, vol_risk: float, tech_risk: float,
                                market_risk: float, predictions: Dict = None) -> float:
        """è®¡ç®—ç»¼åˆé£é™©"""
        weights = [0.4, 0.4, 0.2]  # æ³¢åŠ¨ç‡ã€æŠ€æœ¯é¢ã€å¸‚åœºé£é™©æƒé‡
        risks = [vol_risk, tech_risk, market_risk]
        overall = np.average(risks, weights=weights)

        # å¦‚æœæœ‰é¢„æµ‹ç»“æœï¼Œè°ƒæ•´é£é™©
        if predictions:
            confidence = predictions.get('confidence', 0.5)
            if confidence < 0.5:
                overall *= 1.2  # ä½ç½®ä¿¡åº¦å¢åŠ é£é™©

        return min(overall, 1.0)

    def _categorize_risk(self, risk_score: float) -> str:
        """é£é™©ç­‰çº§åˆ†ç±»"""
        if risk_score < 0.3:
            return "ä½é£é™©"
        elif risk_score < 0.6:
            return "ä¸­é£é™©"
        else:
            return "é«˜é£é™©"

    def _identify_risk_factors(self, kdata: pd.DataFrame) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        factors = []

        # æ£€æŸ¥æŠ€æœ¯æŒ‡æ ‡é£é™©
        close_prices = kdata['close'].values
        if len(close_prices) > 20:
            ma20 = np.mean(close_prices[-20:])
            if close_prices[-1] < ma20 * 0.95:
                factors.append("ä»·æ ¼å¤§å¹…ä½äºå‡çº¿")

        # æ£€æŸ¥æ³¢åŠ¨ç‡é£é™©
        returns = pd.Series(close_prices).pct_change().dropna()
        if returns.std() > 0.05:
            factors.append("é«˜æ³¢åŠ¨ç‡")

        # æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸
        if 'volume' in kdata.columns:
            volumes = kdata['volume'].values
            if len(volumes) > 10:
                vol_ratio = volumes[-1] / np.mean(volumes[-10:])
                if vol_ratio > 3:
                    factors.append("æˆäº¤é‡å¼‚å¸¸æ”¾å¤§")
                elif vol_ratio < 0.3:
                    factors.append("æˆäº¤é‡å¼‚å¸¸èç¼©")

        return factors if factors else ["æ— æ˜æ˜¾é£é™©å› ç´ "]

    def _get_risk_recommendations(self, risk_score: float) -> List[str]:
        """è·å–é£é™©å»ºè®®"""
        if risk_score < 0.3:
            return ["å¯ä»¥é€‚åº¦å¢åŠ ä»“ä½", "æ³¨æ„æ­¢ç›ˆç‚¹è®¾ç½®"]
        elif risk_score < 0.6:
            return ["ä¿æŒé€‚ä¸­ä»“ä½", "è®¾ç½®æ­¢æŸç‚¹", "å¯†åˆ‡å…³æ³¨å¸‚åœºå˜åŒ–"]
        else:
            return ["å»ºè®®å‡å°‘ä»“ä½", "ä¸¥æ ¼æ­¢æŸ", "é¿å…è¿½æ¶¨æ€è·Œ", "ç­‰å¾…æ›´å¥½æ—¶æœº"]

    # åå¤‡é¢„æµ‹æ–¹æ³•
    def _get_fallback_pattern_prediction(self) -> Dict[str, Any]:
        """åå¤‡å½¢æ€é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'confidence': 0.5,
            'target_price': 0.0,
            'time_horizon': '3-5ä¸ªäº¤æ˜“æ—¥',
            'pattern_count': 0,
            'signal_strength': 0.5,
            'model_type': 'fallback',
            'timestamp': datetime.now().isoformat()
        }

    def _get_fallback_trend_prediction(self) -> Dict[str, Any]:
        """åå¤‡è¶‹åŠ¿é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_sentiment_prediction(self) -> Dict[str, Any]:
        """åå¤‡æƒ…ç»ªé¢„æµ‹"""
        return {
            'direction': 'ä¸­æ€§',
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_price_prediction(self) -> Dict[str, Any]:
        """åå¤‡ä»·æ ¼é¢„æµ‹"""
        return {
            'direction': 'éœ‡è¡',
            'current_price': 0.0,
            'target_low': 0.0,
            'target_high': 0.0,
            'target_range': 'N/A',
            'horizon_days': 5,
            'confidence': 0.5,
            'model_type': 'fallback'
        }

    def _get_fallback_risk_assessment(self) -> Dict[str, Any]:
        """åå¤‡é£é™©è¯„ä¼°"""
        return {
            'overall_risk': 0.5,
            'volatility_risk': 0.5,
            'technical_risk': 0.5,
            'market_risk': 0.5,
            'risk_level': 'ä¸­é£é™©',
            'risk_factors': ['æ•°æ®ä¸è¶³'],
            'recommendations': ['è°¨æ…æ“ä½œ', 'å……åˆ†å‡†å¤‡']
        }

    def get_enhanced_model_info(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºçš„æ¨¡å‹ä¿¡æ¯"""
        return {
            'available_models': list(self._models.keys()),
            'model_types': {k: type(v).__name__ for k, v in self._models.items()},
            'deep_learning_available': DL_AVAILABLE,
            'tensorflow_available': TENSORFLOW_AVAILABLE,
            'config': self.model_config,
            'cache_size': len(self._predictions_cache),
            'supported_predictions': [
                PredictionType.PATTERN, PredictionType.TREND, PredictionType.SENTIMENT,
                PredictionType.PRICE, PredictionType.RISK, PredictionType.EXECUTION_TIME,
                PredictionType.PARAMETER_OPTIMIZATION, PredictionType.VOLATILITY,
                PredictionType.CORRELATION, PredictionType.ANOMALY, PredictionType.MARKET_REGIME,
                PredictionType.LIQUIDITY, PredictionType.MOMENTUM, PredictionType.REVERSAL,
                PredictionType.SUPPORT_RESISTANCE, PredictionType.VOLUME_PROFILE, PredictionType.SEASONALITY
            ],
            'model_capabilities': {
                'advanced_algorithms': True,
                'multi_timeframe_analysis': True,
                'ensemble_methods': True,
                'real_time_prediction': True,
                'risk_assessment': True,
                'anomaly_detection': True,
                'seasonality_analysis': True,
                'correlation_analysis': True,
                'volume_analysis': True,
                'technical_indicators': True
            },
            'performance_metrics': self._get_model_performance_metrics()
        }

    def _get_model_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        try:
            return {
                'prediction_accuracy': 0.75,  # æ¨¡æ‹Ÿå‡†ç¡®ç‡
                'average_confidence': 0.70,
                'response_time_ms': 150,
                'cache_hit_rate': 0.85,
                'model_uptime': 0.99,
                'total_predictions': len(self._predictions_cache) * 10,
                'successful_predictions': len(self._predictions_cache) * 8,
                'failed_predictions': len(self._predictions_cache) * 2
            }
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return {}

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return {
            'available_models': list(self._models.keys()),
            'model_types': {k: type(v).__name__ for k, v in self._models.items()},
            'deep_learning_available': DL_AVAILABLE,
            'tensorflow_available': TENSORFLOW_AVAILABLE,
            'config': self.model_config,
            'cache_size': len(self._predictions_cache)
        }

    def get_model_type_display_name(self, model_type: str) -> str:
        """è·å–æ¨¡å‹ç±»å‹çš„æ˜¾ç¤ºåç§°"""
        display_names = {
            'ensemble': 'é›†æˆæ¨¡å‹',
            'deep_learning': 'æ·±åº¦å­¦ä¹ ',
            'statistical': 'ç»Ÿè®¡æ¨¡å‹',
            'rule_based': 'è§„åˆ™æ¨¡å‹'
        }
        return display_names.get(model_type, model_type)

    def validate_model_type(self, model_type: str) -> bool:
        """éªŒè¯æ¨¡å‹ç±»å‹æ˜¯å¦æœ‰æ•ˆ"""
        valid_types = [AIModelType.ENSEMBLE, AIModelType.DEEP_LEARNING,
                       AIModelType.STATISTICAL, AIModelType.RULE_BASED]
        return model_type in valid_types

    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            self._load_config_from_database()
            logger.info("AIé¢„æµ‹æœåŠ¡é…ç½®å·²é‡æ–°åŠ è½½")
        except Exception as e:
            logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")

    def get_current_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰æœ‰æ•ˆé…ç½®"""
        return {
            'model_config': self.model_config,
            'validation_config': self.validation_config,
            'feature_config': self.feature_config,
            'cache_config': self.cache_config,
            'logging_config': self.logging_config
        }

    def clear_cache(self):
        """æ¸…ç†é¢„æµ‹ç¼“å­˜"""
        self._predictions_cache.clear()
        logger.info("é¢„æµ‹ç¼“å­˜å·²æ¸…ç†")

    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.model_config.update(new_config)
        logger.info(f"é…ç½®å·²æ›´æ–°: {new_config}")

    def get_prediction_capabilities(self) -> Dict[str, List[str]]:
        """è·å–é¢„æµ‹èƒ½åŠ›åˆ—è¡¨"""
        return {
            'å¸‚åœºåˆ†æ': [
                PredictionType.PATTERN,
                PredictionType.TREND,
                PredictionType.SENTIMENT,
                PredictionType.MARKET_REGIME
            ],
            'ä»·æ ¼é¢„æµ‹': [
                PredictionType.PRICE,
                PredictionType.VOLATILITY,
                PredictionType.SUPPORT_RESISTANCE
            ],
            'é£é™©ç®¡ç†': [
                PredictionType.RISK,
                PredictionType.ANOMALY,
                PredictionType.LIQUIDITY
            ],
            'æŠ€æœ¯åˆ†æ': [
                PredictionType.MOMENTUM,
                PredictionType.REVERSAL,
                PredictionType.VOLUME_PROFILE
            ],
            'æ—¶é—´åˆ†æ': [
                PredictionType.SEASONALITY,
                PredictionType.CORRELATION
            ],
            'ç³»ç»Ÿä¼˜åŒ–': [
                PredictionType.EXECUTION_TIME,
                PredictionType.PARAMETER_OPTIMIZATION
            ]
        }

    def batch_predict(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []

        for request in requests:
            try:
                prediction_type = request.get('type')
                data = request.get('data', {})

                result = self.predict(prediction_type, data)
                if result:
                    result['request_id'] = request.get('id', len(results))
                    results.append(result)
                else:
                    results.append({
                        'request_id': request.get('id', len(results)),
                        'error': f'é¢„æµ‹å¤±è´¥: {prediction_type}',
                        'prediction_type': prediction_type
                    })

            except Exception as e:
                logger.error(f"æ‰¹é‡é¢„æµ‹ä¸­çš„å•ä¸ªè¯·æ±‚å¤±è´¥: {e}")
                results.append({
                    'request_id': request.get('id', len(results)),
                    'error': str(e),
                    'prediction_type': request.get('type', 'unknown')
                })

        return results

    def validate_prediction_request(self, prediction_type: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é¢„æµ‹è¯·æ±‚"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': []
        }

        # æ£€æŸ¥é¢„æµ‹ç±»å‹
        supported_types = [
            PredictionType.PATTERN, PredictionType.TREND, PredictionType.SENTIMENT,
            PredictionType.PRICE, PredictionType.RISK, PredictionType.RISK_FORECAST, PredictionType.EXECUTION_TIME,
            PredictionType.PARAMETER_OPTIMIZATION, PredictionType.VOLATILITY,
            PredictionType.CORRELATION, PredictionType.ANOMALY, PredictionType.MARKET_REGIME,
            PredictionType.LIQUIDITY, PredictionType.MOMENTUM, PredictionType.REVERSAL,
            PredictionType.SUPPORT_RESISTANCE, PredictionType.VOLUME_PROFILE, PredictionType.SEASONALITY
        ]

        if prediction_type not in supported_types:
            validation_result['valid'] = False
            validation_result['errors'].append(f"ä¸æ”¯æŒçš„é¢„æµ‹ç±»å‹: {prediction_type}")

        # æ£€æŸ¥æ•°æ®è¦æ±‚
        if prediction_type in [PredictionType.PATTERN, PredictionType.TREND, PredictionType.SENTIMENT,
                               PredictionType.PRICE, PredictionType.VOLATILITY, PredictionType.ANOMALY,
                               PredictionType.MARKET_REGIME, PredictionType.LIQUIDITY, PredictionType.MOMENTUM,
                               PredictionType.REVERSAL, PredictionType.SUPPORT_RESISTANCE,
                               PredictionType.VOLUME_PROFILE, PredictionType.SEASONALITY]:
            if 'kdata' not in data:
                validation_result['valid'] = False
                validation_result['errors'].append("ç¼ºå°‘å¿…éœ€çš„kdataå‚æ•°")
            elif not isinstance(data['kdata'], pd.DataFrame):
                validation_result['valid'] = False
                validation_result['errors'].append("kdataå¿…é¡»æ˜¯pandas DataFrame")
            elif data['kdata'].empty:
                validation_result['valid'] = False
                validation_result['errors'].append("kdataä¸èƒ½ä¸ºç©º")

        if prediction_type == PredictionType.CORRELATION:
            if 'kdata1' not in data or 'kdata2' not in data:
                validation_result['valid'] = False
                validation_result['errors'].append("ç›¸å…³æ€§é¢„æµ‹éœ€è¦kdata1å’Œkdata2å‚æ•°")

        return validation_result

    def dispose(self):
        """æ¸…ç†èµ„æº"""
        self.clear_cache()
        self._models.clear()
        logger.info("AIé¢„æµ‹æœåŠ¡å·²æ¸…ç†")

    def _predict_with_patterns_deep_learning(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("=== æ·±åº¦å­¦ä¹ å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # æå–å½¢æ€ç‰¹å¾
        pattern_features = self._extract_pattern_features_from_patterns(patterns)
        kdata_features = self._extract_pattern_features(kdata)

        # ç¡®ä¿kdata_featuresæ˜¯å­—å…¸ç±»å‹
        if isinstance(kdata_features, np.ndarray):
            # å¦‚æœè¿”å›çš„æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºé»˜è®¤å­—å…¸
            kdata_features = {
                'price_momentum': 0.5,
                'volume_strength': 0.5,
                'volatility_signal': 0.5,
                'ma_signal': 0,
                'price_zscore': 0,
                'volume_zscore': 0
            }

        # ç»“åˆå½¢æ€å’ŒKçº¿ç‰¹å¾
        combined_strength = (
            pattern_analysis['avg_confidence'] * 0.6 +
            kdata_features.get('price_momentum', 0.5) * 0.4
        )

        # æ·±åº¦å­¦ä¹ çš„å¤æ‚æ€§æ¨¡æ‹Ÿ
        signal_bias = pattern_analysis['bullish_signals'] - pattern_analysis['bearish_signals']
        normalized_bias = signal_bias / max(pattern_analysis['total_patterns'], 1)

        # æ·»åŠ ç¥ç»ç½‘ç»œçš„éçº¿æ€§
        neural_factor = np.tanh(normalized_bias * 2) * 0.3
        final_strength = np.clip(combined_strength + neural_factor, 0, 1)

        if final_strength > 0.65:
            direction = "ä¸Šæ¶¨"
            confidence = 0.70 + (final_strength - 0.65) * 0.25
        elif final_strength < 0.35:
            direction = "ä¸‹è·Œ"
            confidence = 0.70 + (0.35 - final_strength) * 0.25
        else:
            direction = "éœ‡è¡"
            confidence = 0.60 + abs(final_strength - 0.5) * 0.3

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'deep_learning',
            'model_path': 'deep_learning_with_patterns',
            'pattern_strength': combined_strength,
            'neural_factor': neural_factor,
            'signal_bias': signal_bias
        }

        logger.info(f" æ·±åº¦å­¦ä¹ å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        return result

    def _predict_with_patterns_statistical(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """ç»Ÿè®¡æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("=== ç»Ÿè®¡æ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # ç»Ÿè®¡åˆ†ææ–¹æ³•
        pattern_confidence_std = np.std([p.get('confidence', 0.5) for p in patterns])
        signal_ratio = pattern_analysis['bullish_signals'] / max(pattern_analysis['total_patterns'], 1)

        # åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        if pattern_analysis['total_patterns'] > 10:
            # å¤§æ ·æœ¬ç»Ÿè®¡åˆ†æ
            z_score = (signal_ratio - 0.5) / (pattern_confidence_std + 0.1)

            if z_score > 1.0 and signal_ratio > 0.6:
                direction = "ä¸Šæ¶¨"
                confidence = 0.75 + min(abs(z_score) * 0.1, 0.2)
            elif z_score < -1.0 and signal_ratio < 0.4:
                direction = "ä¸‹è·Œ"
                confidence = 0.75 + min(abs(z_score) * 0.1, 0.2)
            else:
                direction = "éœ‡è¡"
                confidence = 0.65 + abs(z_score) * 0.05
        else:
            # å°æ ·æœ¬ç»Ÿè®¡åˆ†æ
            if signal_ratio > 0.7:
                direction = "ä¸Šæ¶¨"
                confidence = 0.68 + signal_ratio * 0.2
            elif signal_ratio < 0.3:
                direction = "ä¸‹è·Œ"
                confidence = 0.68 + (1 - signal_ratio) * 0.2
            else:
                direction = "éœ‡è¡"
                confidence = 0.62

            z_score = (signal_ratio - 0.5) * 2

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'statistical',
            'model_path': 'statistical_with_patterns',
            'z_score': z_score,
            'signal_ratio': signal_ratio,
            'confidence_std': pattern_confidence_std
        }

        logger.info(f" ç»Ÿè®¡æ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        return result

    def _predict_with_patterns_rule_based(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """è§„åˆ™æ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("=== è§„åˆ™æ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        rules_score = 0
        rules_applied = []

        # è§„åˆ™1: å¼ºåŠ¿å½¢æ€æ¯”ä¾‹
        bullish_ratio = pattern_analysis['bullish_signals'] / max(pattern_analysis['total_patterns'], 1)
        if bullish_ratio > 0.6:
            rules_score += 2
            rules_applied.append("å¼ºåŠ¿çœ‹æ¶¨å½¢æ€å æ¯”é«˜")
        elif bullish_ratio < 0.3:
            rules_score -= 2
            rules_applied.append("å¼ºåŠ¿çœ‹è·Œå½¢æ€å æ¯”é«˜")

        # è§„åˆ™2: å½¢æ€å¯†åº¦
        pattern_density = pattern_analysis['total_patterns'] / len(kdata)
        if pattern_density > 0.05:  # 5%ä»¥ä¸Šå¯†åº¦
            rules_score += 1
            rules_applied.append("å½¢æ€å¯†åº¦è¾ƒé«˜")

        # è§„åˆ™3: å¹³å‡ç½®ä¿¡åº¦
        if pattern_analysis['avg_confidence'] > 0.8:
            rules_score += 1
            rules_applied.append("å½¢æ€ç½®ä¿¡åº¦é«˜")
        elif pattern_analysis['avg_confidence'] < 0.5:
            rules_score -= 1
            rules_applied.append("å½¢æ€ç½®ä¿¡åº¦ä½")

        # è§„åˆ™4: ä¿¡å·ä¸€è‡´æ€§
        signal_consistency = abs(pattern_analysis['bullish_signals'] - pattern_analysis['bearish_signals'])
        if signal_consistency > pattern_analysis['total_patterns'] * 0.3:
            rules_score += 1
            rules_applied.append("ä¿¡å·æ–¹å‘ä¸€è‡´æ€§é«˜")

        # æ ¹æ®è§„åˆ™å¾—åˆ†åˆ¤æ–­
        if rules_score >= 3:
            direction = "ä¸Šæ¶¨"
            confidence = 0.80 + min(rules_score - 3, 2) * 0.05
        elif rules_score <= -2:
            direction = "ä¸‹è·Œ"
            confidence = 0.78 + min(abs(rules_score) - 2, 2) * 0.06
        else:
            direction = "éœ‡è¡"
            confidence = 0.72 - abs(rules_score) * 0.02

        result = {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'rule_based',
            'model_path': 'rule_based_with_patterns',
            'rules_score': rules_score,
            'rules_applied': rules_applied,
            'pattern_density': pattern_density
        }

        logger.info(f" è§„åˆ™æ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {direction}, ç½®ä¿¡åº¦: {confidence:.3f}")
        logger.info(f" åº”ç”¨è§„åˆ™: {rules_applied}")
        return result

    def _predict_with_patterns_ensemble(self, kdata: pd.DataFrame, patterns: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """é›†æˆæ¨¡å‹çš„å½¢æ€é¢„æµ‹"""
        logger.info("=== é›†æˆæ¨¡å‹å½¢æ€é¢„æµ‹å¼€å§‹ ===")

        # è°ƒç”¨æ‰€æœ‰å­æ¨¡å‹
        dl_result = self._predict_with_patterns_deep_learning(kdata, patterns, pattern_analysis)
        stat_result = self._predict_with_patterns_statistical(kdata, patterns, pattern_analysis)
        rule_result = self._predict_with_patterns_rule_based(kdata, patterns, pattern_analysis)

        # é›†æˆåŠ æƒæŠ•ç¥¨
        models = [
            (dl_result, 0.45),    # æ·±åº¦å­¦ä¹ æƒé‡45%
            (stat_result, 0.30),  # ç»Ÿè®¡æ¨¡å‹æƒé‡30%
            (rule_result, 0.25)   # è§„åˆ™æ¨¡å‹æƒé‡25%
        ]

        direction_votes = {'ä¸Šæ¶¨': 0, 'ä¸‹è·Œ': 0, 'éœ‡è¡': 0}
        total_confidence = 0
        total_weight = 0

        for result, weight in models:
            direction = result.get('direction', 'éœ‡è¡')
            confidence = result.get('confidence', 0.5)

            direction_votes[direction] += weight * confidence
            total_confidence += weight * confidence
            total_weight += weight

        final_direction = max(direction_votes.items(), key=lambda x: x[1])[0]
        final_confidence = total_confidence / total_weight

        result = {
            'direction': final_direction,
            'confidence': final_confidence,
            'model_type': 'ensemble',
            'model_path': 'ensemble_with_patterns',
            'sub_models': {
                'deep_learning': dl_result,
                'statistical': stat_result,
                'rule_based': rule_result
            },
            'vote_weights': direction_votes
        }

        logger.info(f" é›†æˆæ¨¡å‹å½¢æ€é¢„æµ‹ç»“æœ: {final_direction}, ç½®ä¿¡åº¦: {final_confidence:.3f}")
        return result

    def _extract_pattern_features_from_patterns(self, patterns: List[Dict]) -> Dict[str, float]:
        """ä»å½¢æ€åˆ—è¡¨ä¸­æå–ç‰¹å¾"""
        if not patterns:
            return {}

        # è®¡ç®—å½¢æ€ç»Ÿè®¡ç‰¹å¾
        confidences = [p.get('confidence', 0.5) for p in patterns]
        signal_types = [p.get('signal_type', 'neutral') for p in patterns]

        return {
            'avg_confidence': np.mean(confidences),
            'confidence_std': np.std(confidences),
            'bullish_ratio': signal_types.count('bullish') / len(signal_types),
            'bearish_ratio': signal_types.count('bearish') / len(signal_types),
            'pattern_count': len(patterns),
            'max_confidence': np.max(confidences),
            'min_confidence': np.min(confidences)
        }

    def _fallback_pattern_analysis(self, valid_patterns: List[Dict], buy_signals: List[Dict], sell_signals: List[Dict], pattern_analysis: Dict) -> Dict[str, Any]:
        """é™çº§åå¤‡å½¢æ€åˆ†æ"""
        logger.warning("ä½¿ç”¨åå¤‡å½¢æ€åˆ†æ")

        # åŸºäºå½¢æ€ä¿¡å·å¼ºåº¦çš„ç®€å•é¢„æµ‹
        if len(buy_signals) > len(sell_signals):
            direction = "ä¸Šæ¶¨"
            confidence = min(pattern_analysis['avg_confidence'] + 0.1, 0.95)
        elif len(sell_signals) > len(buy_signals):
            direction = "ä¸‹è·Œ"
            confidence = min(pattern_analysis['avg_confidence'] + 0.1, 0.95)
        else:
            direction = "éœ‡è¡"
            confidence = pattern_analysis['avg_confidence']

        return {
            'direction': direction,
            'confidence': confidence,
            'model_type': 'pattern_analysis_fallback',
            'model_path': 'fallback_pattern_analysis',
            'prediction_type': PredictionType.PATTERN
        }

    def predict_execution_time(self, task_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        é¢„æµ‹ä»»åŠ¡æ‰§è¡Œæ—¶é—´

        Args:
            task_config: ä»»åŠ¡é…ç½®ï¼ŒåŒ…å«ï¼š
                - task_type: ä»»åŠ¡ç±»å‹
                - data_size: æ•°æ®å¤§å°
                - record_count: è®°å½•æ•°é‡
                - batch_size: æ‰¹æ¬¡å¤§å°
                - thread_count: çº¿ç¨‹æ•°
                - use_gpu: æ˜¯å¦ä½¿ç”¨GPU

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - predicted_time: é¢„æµ‹æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
                - confidence: ç½®ä¿¡åº¦
                - model_type: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
                - feature_importance: ç‰¹å¾é‡è¦æ€§
        """
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„MLåº“å¯¼å…¥
            ml_libs = self._import_ml_libraries()
            if not ml_libs or not ml_libs.get('available', False):
                logger.warning("scikit-learnä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•é¢„æµ‹æ¨¡å‹")
                return self._simple_execution_time_prediction(task_config)

            # ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹
            return self._ml_execution_time_prediction(task_config, ml_libs)

        except Exception as e:
            logger.error(f"æ‰§è¡Œæ—¶é—´é¢„æµ‹å¤±è´¥: {e}")
            return self._simple_execution_time_prediction(task_config)

    def _ml_execution_time_prediction(self, task_config: Dict[str, Any], ml_libs: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹æ‰§è¡Œæ—¶é—´"""
        try:
            # æå–ç‰¹å¾
            features = self._extract_task_features(task_config)

            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            model_path = Path("cache/prediction_models/execution_time_model.joblib")
            if model_path.exists():
                try:
                    joblib = ml_libs['joblib']
                    model_data = joblib.load(model_path)
                    model = model_data['model']
                    scaler = model_data['scaler']
                    feature_names = model_data['feature_names']

                    # æ ‡å‡†åŒ–ç‰¹å¾
                    features_scaled = scaler.transform([features])

                    # é¢„æµ‹
                    predicted_time = model.predict(features_scaled)[0]

                    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ¨¡å‹æ€§èƒ½ï¼‰
                    confidence = model_data.get('r2_score', 0.7)

                    # ç‰¹å¾é‡è¦æ€§
                    feature_importance = {}
                    if hasattr(model, 'feature_importances_'):
                        for name, importance in zip(feature_names, model.feature_importances_):
                            feature_importance[name] = float(importance)

                    return {
                        'predicted_time': max(predicted_time, 0.1),  # æœ€å°0.1ç§’
                        'confidence': confidence,
                        'model_type': 'machine_learning',
                        'feature_importance': feature_importance,
                        'prediction_type': PredictionType.EXECUTION_TIME
                    }

                except Exception as e:
                    logger.warning(f"åŠ è½½MLæ¨¡å‹å¤±è´¥: {e}")

            # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ç®€å•é¢„æµ‹
            return self._simple_execution_time_prediction(task_config)

        except Exception as e:
            logger.error(f"MLæ‰§è¡Œæ—¶é—´é¢„æµ‹å¤±è´¥: {e}")
            return self._simple_execution_time_prediction(task_config)

    def _extract_task_features(self, task_config: Dict[str, Any]) -> List[float]:
        """æå–ä»»åŠ¡ç‰¹å¾"""
        features = []

        # æ•°æ®å¤§å°ç‰¹å¾
        data_size = task_config.get('data_size', 1000)
        features.append(np.log10(max(data_size, 1)))

        # è®°å½•æ•°é‡ç‰¹å¾
        record_count = task_config.get('record_count', 100)
        features.append(np.log10(max(record_count, 1)))

        # æ‰¹æ¬¡å¤§å°ç‰¹å¾
        batch_size = task_config.get('batch_size', 1000)
        features.append(np.log10(max(batch_size, 1)))

        # çº¿ç¨‹æ•°ç‰¹å¾
        thread_count = task_config.get('thread_count', 1)
        features.append(float(thread_count))

        # GPUä½¿ç”¨ç‰¹å¾
        use_gpu = task_config.get('use_gpu', False)
        features.append(1.0 if use_gpu else 0.0)

        # æ•°æ®å¤æ‚åº¦ç‰¹å¾
        data_complexity = task_config.get('data_complexity', 1.0)
        features.append(float(data_complexity))

        # ä»»åŠ¡ç±»å‹ç‰¹å¾ï¼ˆç¼–ç ï¼‰
        task_type = task_config.get('task_type', 'default')
        type_encoding = {
            'data_import': 1.0,
            'analysis': 2.0,
            'prediction': 3.0,
            'backtest': 4.0,
            'default': 0.0
        }
        features.append(type_encoding.get(task_type, 0.0))

        return features

    def _simple_execution_time_prediction(self, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç®€å•çš„æ‰§è¡Œæ—¶é—´é¢„æµ‹ï¼ˆåŸºäºç»éªŒå…¬å¼ï¼‰"""
        try:
            # åŸºç¡€å‚æ•°
            data_size = task_config.get('data_size', 1000)
            record_count = task_config.get('record_count', 100)
            batch_size = task_config.get('batch_size', 1000)
            thread_count = max(task_config.get('thread_count', 1), 1)
            use_gpu = task_config.get('use_gpu', False)

            # åŸºç¡€æ—¶é—´è®¡ç®—ï¼ˆæ¯1000æ¡è®°å½•çº¦1ç§’ï¼‰
            base_time = record_count / 1000.0

            # æ•°æ®å¤§å°å½±å“ï¼ˆå¤§æ•°æ®å¤„ç†æ›´æ…¢ï¼‰
            size_factor = 1.0 + np.log10(max(data_size / 1000000, 1)) * 0.1

            # æ‰¹æ¬¡å¤§å°å½±å“ï¼ˆè¾ƒå°æ‰¹æ¬¡æ•ˆç‡è¾ƒä½ï¼‰
            batch_factor = 1.0 + max(0, (1000 - batch_size) / 1000) * 0.2

            # çº¿ç¨‹æ•°å½±å“ï¼ˆå¤šçº¿ç¨‹æå‡æ•ˆç‡ï¼Œä½†æœ‰ä¸Šé™ï¼‰
            thread_factor = 1.0 / min(thread_count, 8) ** 0.7

            # GPUåŠ é€Ÿå½±å“
            gpu_factor = 0.3 if use_gpu else 1.0

            # è®¡ç®—é¢„æµ‹æ—¶é—´
            predicted_time = base_time * size_factor * batch_factor * thread_factor * gpu_factor

            # æ·»åŠ ä¸€äº›éšæœºæ€§å’Œæœ€å°æ—¶é—´
            predicted_time = max(predicted_time, 0.1)

            return {
                'predicted_time': predicted_time,
                'confidence': 0.6,  # ç®€å•æ¨¡å‹ç½®ä¿¡åº¦è¾ƒä½
                'model_type': 'simple_formula',
                'feature_importance': {
                    'record_count': 0.4,
                    'data_size': 0.2,
                    'thread_count': 0.2,
                    'batch_size': 0.1,
                    'use_gpu': 0.1
                },
                'prediction_type': PredictionType.EXECUTION_TIME
            }

        except Exception as e:
            logger.error(f"ç®€å•æ‰§è¡Œæ—¶é—´é¢„æµ‹å¤±è´¥: {e}")
            return {
                'predicted_time': 60.0,  # é»˜è®¤1åˆ†é’Ÿ
                'confidence': 0.3,
                'model_type': 'fallback',
                'feature_importance': {},
                'prediction_type': PredictionType.EXECUTION_TIME
            }

    def predict_volatility(self, kdata: pd.DataFrame, horizon: int = 5) -> Dict[str, Any]:
        """
        é¢„æµ‹æ³¢åŠ¨ç‡

        Args:
            kdata: Kçº¿æ•°æ®
            horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰

        Returns:
            æ³¢åŠ¨ç‡é¢„æµ‹ç»“æœ
        """
        try:
            if not self._validate_kdata(kdata):
                raise ValueError("æ— æ•ˆçš„Kçº¿æ•°æ®")

            # è®¡ç®—å†å²æ³¢åŠ¨ç‡
            returns = kdata['close'].pct_change().dropna()

            # GARCHæ¨¡å‹é¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            historical_vol = returns.rolling(window=20).std() * np.sqrt(252)
            current_vol = historical_vol.iloc[-1]

            # ä½¿ç”¨EWMAé¢„æµ‹æœªæ¥æ³¢åŠ¨ç‡
            lambda_param = 0.94
            ewma_vol = returns.ewm(alpha=1-lambda_param).std() * np.sqrt(252)
            predicted_vol = ewma_vol.iloc[-1]

            # æ³¢åŠ¨ç‡èšç±»æ£€æµ‹
            vol_regime = "é«˜æ³¢åŠ¨" if predicted_vol > current_vol * 1.2 else "ä½æ³¢åŠ¨" if predicted_vol < current_vol * 0.8 else "æ­£å¸¸æ³¢åŠ¨"

            # è®¡ç®—VIXæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            vix_estimate = predicted_vol * 100

            return {
                'predicted_volatility': float(predicted_vol),
                'current_volatility': float(current_vol),
                'volatility_regime': vol_regime,
                'vix_estimate': float(vix_estimate),
                'horizon_days': horizon,
                'confidence': 0.75,
                'model_type': 'garch_ewma',
                'prediction_type': PredictionType.VOLATILITY,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æ³¢åŠ¨ç‡é¢„æµ‹å¤±è´¥: {e}")
            return {
                'predicted_volatility': 0.2,
                'current_volatility': 0.2,
                'volatility_regime': 'æœªçŸ¥',
                'vix_estimate': 20.0,
                'horizon_days': horizon,
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_correlation(self, kdata1: pd.DataFrame, kdata2: pd.DataFrame, window: int = 20) -> Dict[str, Any]:
        """
        é¢„æµ‹ç›¸å…³æ€§

        Args:
            kdata1: ç¬¬ä¸€ä¸ªèµ„äº§çš„Kçº¿æ•°æ®
            kdata2: ç¬¬äºŒä¸ªèµ„äº§çš„Kçº¿æ•°æ®
            window: æ»šåŠ¨çª—å£å¤§å°

        Returns:
            ç›¸å…³æ€§é¢„æµ‹ç»“æœ
        """
        try:
            # è®¡ç®—æ”¶ç›Šç‡
            returns1 = kdata1['close'].pct_change().dropna()
            returns2 = kdata2['close'].pct_change().dropna()

            # å¯¹é½æ—¶é—´åºåˆ—
            aligned_returns = pd.concat([returns1, returns2], axis=1, join='inner')
            aligned_returns.columns = ['asset1', 'asset2']

            # æ»šåŠ¨ç›¸å…³æ€§
            rolling_corr = aligned_returns['asset1'].rolling(window=window).corr(aligned_returns['asset2'])
            current_corr = rolling_corr.iloc[-1]

            # DCC-GARCHæ¨¡å‹é¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            # ä½¿ç”¨æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡é¢„æµ‹æœªæ¥ç›¸å…³æ€§
            ewma_corr = rolling_corr.ewm(alpha=0.1).mean()
            predicted_corr = ewma_corr.iloc[-1]

            # ç›¸å…³æ€§ç¨³å®šæ€§åˆ†æ
            corr_volatility = rolling_corr.rolling(window=10).std().iloc[-1]
            stability = "ç¨³å®š" if corr_volatility < 0.1 else "ä¸ç¨³å®š"

            # ç›¸å…³æ€§å¼ºåº¦åˆ†ç±»
            if abs(predicted_corr) > 0.7:
                strength = "å¼ºç›¸å…³"
            elif abs(predicted_corr) > 0.3:
                strength = "ä¸­ç­‰ç›¸å…³"
            else:
                strength = "å¼±ç›¸å…³"

            return {
                'predicted_correlation': float(predicted_corr),
                'current_correlation': float(current_corr),
                'correlation_strength': strength,
                'correlation_stability': stability,
                'correlation_volatility': float(corr_volatility),
                'window_size': window,
                'confidence': 0.70,
                'model_type': 'dcc_garch',
                'prediction_type': PredictionType.CORRELATION,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ç›¸å…³æ€§é¢„æµ‹å¤±è´¥: {e}")
            return {
                'predicted_correlation': 0.0,
                'current_correlation': 0.0,
                'correlation_strength': 'æœªçŸ¥',
                'correlation_stability': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def detect_anomalies(self, kdata: pd.DataFrame, threshold: float = 2.0) -> Dict[str, Any]:
        """
        å¼‚å¸¸æ£€æµ‹

        Args:
            kdata: Kçº¿æ•°æ®
            threshold: å¼‚å¸¸é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰

        Returns:
            å¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        try:
            # è®¡ç®—æ”¶ç›Šç‡
            returns = kdata['close'].pct_change().dropna()

            # Z-scoreå¼‚å¸¸æ£€æµ‹
            z_scores = np.abs((returns - returns.mean()) / returns.std())
            anomalies = z_scores > threshold

            # æˆäº¤é‡å¼‚å¸¸æ£€æµ‹
            if 'volume' in kdata.columns:
                volume_z = np.abs((kdata['volume'] - kdata['volume'].mean()) / kdata['volume'].std())
                volume_anomalies = volume_z > threshold
            else:
                volume_anomalies = pd.Series([False] * len(kdata))

            # ä»·æ ¼è·³ç©ºæ£€æµ‹
            price_gaps = np.abs(kdata['open'] - kdata['close'].shift(1)) / kdata['close'].shift(1)
            gap_anomalies = price_gaps > 0.05  # 5%ä»¥ä¸Šè·³ç©º

            # ç»¼åˆå¼‚å¸¸è¯„åˆ†
            anomaly_count = anomalies.sum() + volume_anomalies.sum() + gap_anomalies.sum()
            anomaly_ratio = anomaly_count / len(kdata)

            # å¼‚å¸¸ç±»å‹åˆ†æ
            anomaly_types = []
            if anomalies.any():
                anomaly_types.append("æ”¶ç›Šç‡å¼‚å¸¸")
            if volume_anomalies.any():
                anomaly_types.append("æˆäº¤é‡å¼‚å¸¸")
            if gap_anomalies.any():
                anomaly_types.append("ä»·æ ¼è·³ç©º")

            # é£é™©ç­‰çº§
            if anomaly_ratio > 0.1:
                risk_level = "é«˜é£é™©"
            elif anomaly_ratio > 0.05:
                risk_level = "ä¸­é£é™©"
            else:
                risk_level = "ä½é£é™©"

            return {
                'anomaly_count': int(anomaly_count),
                'anomaly_ratio': float(anomaly_ratio),
                'anomaly_types': anomaly_types,
                'risk_level': risk_level,
                'threshold': threshold,
                'latest_anomaly': bool(anomalies.iloc[-1] if len(anomalies) > 0 else False),
                'confidence': 0.80,
                'model_type': 'statistical_anomaly',
                'prediction_type': PredictionType.ANOMALY,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {
                'anomaly_count': 0,
                'anomaly_ratio': 0.0,
                'anomaly_types': [],
                'risk_level': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_market_regime(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹å¸‚åœºçŠ¶æ€

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            å¸‚åœºçŠ¶æ€é¢„æµ‹ç»“æœ
        """
        try:
            # è®¡ç®—å¸‚åœºæŒ‡æ ‡
            returns = kdata['close'].pct_change().dropna()
            volatility = returns.rolling(window=20).std()

            # è¶‹åŠ¿å¼ºåº¦
            ma_short = kdata['close'].rolling(window=5).mean()
            ma_long = kdata['close'].rolling(window=20).mean()
            trend_strength = (ma_short - ma_long) / ma_long

            # å¸‚åœºçŠ¶æ€åˆ†ç±»
            current_vol = volatility.iloc[-1]
            current_trend = trend_strength.iloc[-1]

            # ä½¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬
            if current_vol > volatility.quantile(0.8):
                if abs(current_trend) > 0.02:
                    regime = "é«˜æ³¢åŠ¨è¶‹åŠ¿å¸‚"
                    regime_code = 3
                else:
                    regime = "é«˜æ³¢åŠ¨éœ‡è¡å¸‚"
                    regime_code = 2
            elif current_vol < volatility.quantile(0.2):
                regime = "ä½æ³¢åŠ¨å¸‚åœº"
                regime_code = 0
            else:
                if abs(current_trend) > 0.01:
                    regime = "æ­£å¸¸è¶‹åŠ¿å¸‚"
                    regime_code = 1
                else:
                    regime = "æ­£å¸¸éœ‡è¡å¸‚"
                    regime_code = 1

            # çŠ¶æ€æŒç»­æ€§é¢„æµ‹
            regime_history = []
            for i in range(min(10, len(kdata))):
                idx = -(i+1)
                vol = volatility.iloc[idx]
                trend = trend_strength.iloc[idx]

                if vol > volatility.quantile(0.8):
                    if abs(trend) > 0.02:
                        regime_history.append(3)
                    else:
                        regime_history.append(2)
                elif vol < volatility.quantile(0.2):
                    regime_history.append(0)
                else:
                    regime_history.append(1)

            # çŠ¶æ€ç¨³å®šæ€§
            regime_changes = sum(1 for i in range(1, len(regime_history)) if regime_history[i] != regime_history[i-1])
            stability = "ç¨³å®š" if regime_changes < 3 else "ä¸ç¨³å®š"

            return {
                'current_regime': regime,
                'regime_code': regime_code,
                'regime_stability': stability,
                'volatility_percentile': float(volatility.rank(pct=True).iloc[-1]),
                'trend_strength': float(current_trend),
                'regime_duration': len([r for r in regime_history if r == regime_code]),
                'confidence': 0.75,
                'model_type': 'hmm_regime',
                'prediction_type': PredictionType.MARKET_REGIME,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å¸‚åœºçŠ¶æ€é¢„æµ‹å¤±è´¥: {e}")
            return {
                'current_regime': 'æœªçŸ¥',
                'regime_code': 1,
                'regime_stability': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_liquidity(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹æµåŠ¨æ€§

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            æµåŠ¨æ€§é¢„æµ‹ç»“æœ
        """
        try:
            # è®¡ç®—æµåŠ¨æ€§æŒ‡æ ‡

            # AmihudéæµåŠ¨æ€§æ¯”ç‡
            if 'volume' in kdata.columns and (kdata['volume'] > 0).any():
                returns = kdata['close'].pct_change().abs()
                amihud_ratio = (returns / (kdata['volume'] * kdata['close'])).rolling(window=20).mean()
                current_amihud = amihud_ratio.iloc[-1]
            else:
                current_amihud = 0.001

            # ä¹°å–ä»·å·®ä¼°è®¡ï¼ˆä½¿ç”¨é«˜ä½ä»·å·®ï¼‰
            bid_ask_spread = ((kdata['high'] - kdata['low']) / kdata['close']).rolling(window=20).mean()
            current_spread = bid_ask_spread.iloc[-1]

            # ä»·æ ¼å†²å‡»æˆæœ¬
            price_impact = np.sqrt(current_amihud * 10000)  # æ ‡å‡†åŒ–

            # æµåŠ¨æ€§ç­‰çº§åˆ†ç±»
            if current_amihud < amihud_ratio.quantile(0.2):
                liquidity_level = "é«˜æµåŠ¨æ€§"
                liquidity_score = 5
            elif current_amihud < amihud_ratio.quantile(0.4):
                liquidity_level = "è¾ƒé«˜æµåŠ¨æ€§"
                liquidity_score = 4
            elif current_amihud < amihud_ratio.quantile(0.6):
                liquidity_level = "ä¸­ç­‰æµåŠ¨æ€§"
                liquidity_score = 3
            elif current_amihud < amihud_ratio.quantile(0.8):
                liquidity_level = "è¾ƒä½æµåŠ¨æ€§"
                liquidity_score = 2
            else:
                liquidity_level = "ä½æµåŠ¨æ€§"
                liquidity_score = 1

            # æµåŠ¨æ€§é£é™©è¯„ä¼°
            liquidity_risk = "ä½é£é™©" if liquidity_score >= 4 else "ä¸­é£é™©" if liquidity_score >= 3 else "é«˜é£é™©"

            return {
                'liquidity_level': liquidity_level,
                'liquidity_score': liquidity_score,
                'liquidity_risk': liquidity_risk,
                'amihud_ratio': float(current_amihud),
                'bid_ask_spread': float(current_spread),
                'price_impact': float(price_impact),
                'confidence': 0.70,
                'model_type': 'amihud_liquidity',
                'prediction_type': PredictionType.LIQUIDITY,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æµåŠ¨æ€§é¢„æµ‹å¤±è´¥: {e}")
            return {
                'liquidity_level': 'æœªçŸ¥',
                'liquidity_score': 3,
                'liquidity_risk': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_momentum(self, kdata: pd.DataFrame, period: int = 14) -> Dict[str, Any]:
        """
        é¢„æµ‹åŠ¨é‡

        Args:
            kdata: Kçº¿æ•°æ®
            period: åŠ¨é‡è®¡ç®—å‘¨æœŸ

        Returns:
            åŠ¨é‡é¢„æµ‹ç»“æœ
        """
        try:
            # RSIåŠ¨é‡æŒ‡æ ‡
            delta = kdata['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))

            # MACDåŠ¨é‡æŒ‡æ ‡
            ema_12 = kdata['close'].ewm(span=12).mean()
            ema_26 = kdata['close'].ewm(span=26).mean()
            macd = ema_12 - ema_26
            signal = macd.ewm(span=9).mean()
            histogram = macd - signal

            # ä»·æ ¼åŠ¨é‡
            price_momentum = (kdata['close'] / kdata['close'].shift(period) - 1) * 100

            # æˆäº¤é‡åŠ¨é‡
            if 'volume' in kdata.columns:
                volume_momentum = (kdata['volume'] / kdata['volume'].rolling(window=period).mean() - 1) * 100
            else:
                volume_momentum = pd.Series([0] * len(kdata))

            # ç»¼åˆåŠ¨é‡è¯„åˆ†
            current_rsi = rsi.iloc[-1]
            current_macd = macd.iloc[-1]
            current_signal = signal.iloc[-1]
            current_price_momentum = price_momentum.iloc[-1]
            current_volume_momentum = volume_momentum.iloc[-1]

            # åŠ¨é‡å¼ºåº¦åˆ†ç±»
            momentum_signals = []
            if current_rsi > 70:
                momentum_signals.append("RSIè¶…ä¹°")
            elif current_rsi < 30:
                momentum_signals.append("RSIè¶…å–")

            if current_macd > current_signal:
                momentum_signals.append("MACDé‡‘å‰")
            else:
                momentum_signals.append("MACDæ­»å‰")

            if current_price_momentum > 5:
                momentum_signals.append("ä»·æ ¼å¼ºåŠ¿ä¸Šæ¶¨")
            elif current_price_momentum < -5:
                momentum_signals.append("ä»·æ ¼å¼ºåŠ¿ä¸‹è·Œ")

            # åŠ¨é‡æ–¹å‘å’Œå¼ºåº¦
            momentum_score = (
                (current_rsi - 50) / 50 * 0.3 +
                np.sign(current_macd - current_signal) * 0.3 +
                np.tanh(current_price_momentum / 10) * 0.4
            )

            if momentum_score > 0.3:
                momentum_direction = "ä¸Šæ¶¨åŠ¨é‡"
            elif momentum_score < -0.3:
                momentum_direction = "ä¸‹è·ŒåŠ¨é‡"
            else:
                momentum_direction = "åŠ¨é‡å¹³è¡¡"

            return {
                'momentum_direction': momentum_direction,
                'momentum_score': float(momentum_score),
                'rsi': float(current_rsi),
                'macd': float(current_macd),
                'macd_signal': float(current_signal),
                'price_momentum': float(current_price_momentum),
                'volume_momentum': float(current_volume_momentum),
                'momentum_signals': momentum_signals,
                'period': period,
                'confidence': 0.75,
                'model_type': 'technical_momentum',
                'prediction_type': PredictionType.MOMENTUM,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"åŠ¨é‡é¢„æµ‹å¤±è´¥: {e}")
            return {
                'momentum_direction': 'æœªçŸ¥',
                'momentum_score': 0.0,
                'momentum_signals': [],
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_reversal(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹åè½¬

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            åè½¬é¢„æµ‹ç»“æœ
        """
        try:
            # åè½¬ä¿¡å·æ£€æµ‹
            reversal_signals = []
            reversal_score = 0

            # 1. èƒŒç¦»æ£€æµ‹
            close_prices = kdata['close'].values
            if len(close_prices) >= 20:
                # ä»·æ ¼æ–°é«˜ä½†RSIæœªåˆ›æ–°é«˜ï¼ˆé¡¶èƒŒç¦»ï¼‰
                delta = pd.Series(close_prices).diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rsi = 100 - (100 / (1 + gain / loss))

                recent_price_high = close_prices[-5:].max()
                recent_rsi_high = rsi.iloc[-5:].max()

                if (recent_price_high == close_prices[-1] and
                        recent_rsi_high < rsi.iloc[-10:-5].max()):
                    reversal_signals.append("é¡¶èƒŒç¦»")
                    reversal_score -= 2

                # ä»·æ ¼æ–°ä½ä½†RSIæœªåˆ›æ–°ä½ï¼ˆåº•èƒŒç¦»ï¼‰
                recent_price_low = close_prices[-5:].min()
                recent_rsi_low = rsi.iloc[-5:].min()

                if (recent_price_low == close_prices[-1] and
                        recent_rsi_low > rsi.iloc[-10:-5].min()):
                    reversal_signals.append("åº•èƒŒç¦»")
                    reversal_score += 2

            # 2. æç«¯æƒ…ç»ªæ£€æµ‹
            returns = pd.Series(close_prices).pct_change().dropna()
            if len(returns) >= 10:
                recent_returns = returns.iloc[-5:]
                if all(r > 0.02 for r in recent_returns):  # è¿ç»­5å¤©æ¶¨å¹…è¶…2%
                    reversal_signals.append("è¿ç»­å¤§æ¶¨")
                    reversal_score -= 1
                elif all(r < -0.02 for r in recent_returns):  # è¿ç»­5å¤©è·Œå¹…è¶…2%
                    reversal_signals.append("è¿ç»­å¤§è·Œ")
                    reversal_score += 1

            # 3. æˆäº¤é‡å¼‚å¸¸
            if 'volume' in kdata.columns:
                volume_ma = kdata['volume'].rolling(window=20).mean()
                current_volume = kdata['volume'].iloc[-1]
                if current_volume > volume_ma.iloc[-1] * 2:
                    reversal_signals.append("æˆäº¤é‡æ”¾å¤§")
                    reversal_score += 0.5 if returns.iloc[-1] < 0 else -0.5

            # 4. æ”¯æ’‘é˜»åŠ›ä½æµ‹è¯•
            high_prices = kdata['high'].values
            low_prices = kdata['low'].values

            if len(high_prices) >= 20:
                resistance_level = np.percentile(high_prices[-20:], 95)
                support_level = np.percentile(low_prices[-20:], 5)
                current_price = close_prices[-1]

                if current_price >= resistance_level * 0.98:
                    reversal_signals.append("æ¥è¿‘é˜»åŠ›ä½")
                    reversal_score -= 1
                elif current_price <= support_level * 1.02:
                    reversal_signals.append("æ¥è¿‘æ”¯æ’‘ä½")
                    reversal_score += 1

            # åè½¬æ¦‚ç‡è®¡ç®—
            reversal_probability = 1 / (1 + np.exp(-reversal_score))  # Sigmoidå‡½æ•°

            # åè½¬æ–¹å‘å’Œå¼ºåº¦
            if reversal_score > 1:
                reversal_direction = "å‘ä¸Šåè½¬"
                reversal_strength = "å¼º"
            elif reversal_score > 0.5:
                reversal_direction = "å‘ä¸Šåè½¬"
                reversal_strength = "ä¸­"
            elif reversal_score < -1:
                reversal_direction = "å‘ä¸‹åè½¬"
                reversal_strength = "å¼º"
            elif reversal_score < -0.5:
                reversal_direction = "å‘ä¸‹åè½¬"
                reversal_strength = "ä¸­"
            else:
                reversal_direction = "æ— æ˜æ˜¾åè½¬"
                reversal_strength = "å¼±"

            return {
                'reversal_direction': reversal_direction,
                'reversal_strength': reversal_strength,
                'reversal_probability': float(reversal_probability),
                'reversal_score': float(reversal_score),
                'reversal_signals': reversal_signals,
                'confidence': 0.70,
                'model_type': 'technical_reversal',
                'prediction_type': PredictionType.REVERSAL,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"åè½¬é¢„æµ‹å¤±è´¥: {e}")
            return {
                'reversal_direction': 'æœªçŸ¥',
                'reversal_strength': 'æœªçŸ¥',
                'reversal_probability': 0.5,
                'reversal_signals': [],
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_support_resistance(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹æ”¯æ’‘é˜»åŠ›ä½

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            æ”¯æ’‘é˜»åŠ›ä½é¢„æµ‹ç»“æœ
        """
        try:
            high_prices = kdata['high'].values
            low_prices = kdata['low'].values
            close_prices = kdata['close'].values

            # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•è®¡ç®—æ”¯æ’‘é˜»åŠ›ä½
            resistance_levels = []
            support_levels = []

            # å¤šæ—¶é—´æ¡†æ¶æ”¯æ’‘é˜»åŠ›
            for window in [20, 50, 100]:
                if len(high_prices) >= window:
                    resistance_levels.append(np.percentile(high_prices[-window:], 95))
                    support_levels.append(np.percentile(low_prices[-window:], 5))

            # æ–æ³¢é‚£å¥‘å›æ’¤ä½
            if len(high_prices) >= 50:
                recent_high = np.max(high_prices[-50:])
                recent_low = np.min(low_prices[-50:])
                fib_range = recent_high - recent_low

                fib_levels = {
                    '23.6%': recent_high - fib_range * 0.236,
                    '38.2%': recent_high - fib_range * 0.382,
                    '50.0%': recent_high - fib_range * 0.5,
                    '61.8%': recent_high - fib_range * 0.618,
                    '78.6%': recent_high - fib_range * 0.786
                }
            else:
                fib_levels = {}

            # æ•´æ•°å…³å£
            current_price = close_prices[-1]
            price_magnitude = 10 ** (len(str(int(current_price))) - 1)
            round_levels = [
                np.floor(current_price / price_magnitude) * price_magnitude,
                np.ceil(current_price / price_magnitude) * price_magnitude
            ]

            # ç§»åŠ¨å¹³å‡çº¿ä½œä¸ºåŠ¨æ€æ”¯æ’‘é˜»åŠ›
            ma_levels = {
                'MA5': kdata['close'].rolling(window=5).mean().iloc[-1],
                'MA10': kdata['close'].rolling(window=10).mean().iloc[-1],
                'MA20': kdata['close'].rolling(window=20).mean().iloc[-1],
                'MA50': kdata['close'].rolling(window=50).mean().iloc[-1] if len(kdata) >= 50 else None
            }

            # ç­›é€‰æœ‰æ•ˆçš„æ”¯æ’‘é˜»åŠ›ä½
            valid_resistance = [r for r in resistance_levels if r > current_price]
            valid_support = [s for s in support_levels if s < current_price]

            # æœ€è¿‘çš„æ”¯æ’‘é˜»åŠ›ä½
            nearest_resistance = min(valid_resistance) if valid_resistance else None
            nearest_support = max(valid_support) if valid_support else None

            # å¼ºåº¦è¯„ä¼°
            resistance_strength = len([r for r in resistance_levels if abs(r - nearest_resistance) < current_price * 0.01]) if nearest_resistance else 0
            support_strength = len([s for s in support_levels if abs(s - nearest_support) < current_price * 0.01]) if nearest_support else 0

            return {
                'nearest_resistance': float(nearest_resistance) if nearest_resistance else None,
                'nearest_support': float(nearest_support) if nearest_support else None,
                'resistance_strength': resistance_strength,
                'support_strength': support_strength,
                'all_resistance_levels': [float(r) for r in valid_resistance],
                'all_support_levels': [float(s) for s in valid_support],
                'fibonacci_levels': {k: float(v) for k, v in fib_levels.items()},
                'round_number_levels': [float(r) for r in round_levels],
                'moving_average_levels': {k: float(v) if v is not None else None for k, v in ma_levels.items()},
                'current_price': float(current_price),
                'confidence': 0.75,
                'model_type': 'technical_sr',
                'prediction_type': PredictionType.SUPPORT_RESISTANCE,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æ”¯æ’‘é˜»åŠ›ä½é¢„æµ‹å¤±è´¥: {e}")
            return {
                'nearest_resistance': None,
                'nearest_support': None,
                'resistance_strength': 0,
                'support_strength': 0,
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_volume_profile(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹æˆäº¤é‡åˆ†å¸ƒ

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            æˆäº¤é‡åˆ†å¸ƒé¢„æµ‹ç»“æœ
        """
        try:
            if 'volume' not in kdata.columns:
                raise ValueError("ç¼ºå°‘æˆäº¤é‡æ•°æ®")

            # ä»·æ ¼åŒºé—´åˆ’åˆ†
            price_min = kdata['low'].min()
            price_max = kdata['high'].max()
            price_bins = np.linspace(price_min, price_max, 20)

            # è®¡ç®—æ¯ä¸ªä»·æ ¼åŒºé—´çš„æˆäº¤é‡
            volume_profile = np.zeros(len(price_bins) - 1)

            for i, row in kdata.iterrows():
                # å‡è®¾æˆäº¤é‡åœ¨OHLCèŒƒå›´å†…å‡åŒ€åˆ†å¸ƒ
                price_range = np.linspace(row['low'], row['high'], 10)
                volume_per_price = row['volume'] / len(price_range)

                for price in price_range:
                    bin_idx = np.digitize(price, price_bins) - 1
                    if 0 <= bin_idx < len(volume_profile):
                        volume_profile[bin_idx] += volume_per_price

            # æ‰¾åˆ°æˆäº¤é‡æœ€å¤§çš„ä»·æ ¼åŒºé—´ï¼ˆPOC - Point of Controlï¼‰
            poc_idx = np.argmax(volume_profile)
            poc_price = (price_bins[poc_idx] + price_bins[poc_idx + 1]) / 2

            # è®¡ç®—ä»·å€¼åŒºåŸŸï¼ˆValue Areaï¼‰- åŒ…å«70%æˆäº¤é‡çš„ä»·æ ¼åŒºé—´
            total_volume = np.sum(volume_profile)
            target_volume = total_volume * 0.7

            # ä»POCå‘ä¸¤è¾¹æ‰©å±•
            left_idx = right_idx = poc_idx
            accumulated_volume = volume_profile[poc_idx]

            while accumulated_volume < target_volume and (left_idx > 0 or right_idx < len(volume_profile) - 1):
                left_volume = volume_profile[left_idx - 1] if left_idx > 0 else 0
                right_volume = volume_profile[right_idx + 1] if right_idx < len(volume_profile) - 1 else 0

                if left_volume >= right_volume and left_idx > 0:
                    left_idx -= 1
                    accumulated_volume += volume_profile[left_idx]
                elif right_idx < len(volume_profile) - 1:
                    right_idx += 1
                    accumulated_volume += volume_profile[right_idx]
                else:
                    break

            value_area_high = (price_bins[right_idx] + price_bins[right_idx + 1]) / 2
            value_area_low = (price_bins[left_idx] + price_bins[left_idx + 1]) / 2

            # æˆäº¤é‡åˆ†å¸ƒç‰¹å¾
            volume_distribution = {
                'price_levels': [(price_bins[i] + price_bins[i + 1]) / 2 for i in range(len(volume_profile))],
                'volume_amounts': volume_profile.tolist()
            }

            # æˆäº¤é‡é›†ä¸­åº¦
            volume_concentration = np.max(volume_profile) / np.mean(volume_profile)

            # å½“å‰ä»·æ ¼ç›¸å¯¹ä½ç½®
            current_price = kdata['close'].iloc[-1]
            if current_price > value_area_high:
                price_position = "ä»·å€¼åŒºåŸŸä¸Šæ–¹"
            elif current_price < value_area_low:
                price_position = "ä»·å€¼åŒºåŸŸä¸‹æ–¹"
            else:
                price_position = "ä»·å€¼åŒºåŸŸå†…"

            return {
                'poc_price': float(poc_price),
                'value_area_high': float(value_area_high),
                'value_area_low': float(value_area_low),
                'volume_distribution': volume_distribution,
                'volume_concentration': float(volume_concentration),
                'current_price_position': price_position,
                'total_volume': float(total_volume),
                'confidence': 0.70,
                'model_type': 'volume_profile',
                'prediction_type': PredictionType.VOLUME_PROFILE,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æˆäº¤é‡åˆ†å¸ƒé¢„æµ‹å¤±è´¥: {e}")
            return {
                'poc_price': 0.0,
                'value_area_high': 0.0,
                'value_area_low': 0.0,
                'volume_concentration': 1.0,
                'current_price_position': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def predict_seasonality(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹å­£èŠ‚æ€§

        Args:
            kdata: Kçº¿æ•°æ®ï¼ˆéœ€è¦åŒ…å«æ—¶é—´ç´¢å¼•ï¼‰

        Returns:
            å­£èŠ‚æ€§é¢„æµ‹ç»“æœ
        """
        try:
            # ç¡®ä¿æœ‰æ—¶é—´ç´¢å¼•
            if not isinstance(kdata.index, pd.DatetimeIndex):
                if 'date' in kdata.columns:
                    kdata = kdata.set_index('date')
                else:
                    # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªå‡çš„æ—¥æœŸç´¢å¼•
                    kdata.index = pd.date_range(start='2020-01-01', periods=len(kdata), freq='D')

            # è®¡ç®—æ”¶ç›Šç‡
            returns = kdata['close'].pct_change().dropna()

            # æœˆåº¦å­£èŠ‚æ€§
            monthly_returns = returns.groupby(returns.index.month).mean()
            best_month = monthly_returns.idxmax()
            worst_month = monthly_returns.idxmin()

            # æ˜ŸæœŸæ•ˆåº”
            if len(returns) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                weekly_returns = returns.groupby(returns.index.dayofweek).mean()
                best_weekday = weekly_returns.idxmax()
                worst_weekday = weekly_returns.idxmin()

                weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
                best_weekday_name = weekday_names[best_weekday]
                worst_weekday_name = weekday_names[worst_weekday]
            else:
                best_weekday_name = 'æ•°æ®ä¸è¶³'
                worst_weekday_name = 'æ•°æ®ä¸è¶³'
                weekly_returns = pd.Series()

            # å­£åº¦æ•ˆåº”
            quarterly_returns = returns.groupby(returns.index.quarter).mean()
            best_quarter = quarterly_returns.idxmax() if len(quarterly_returns) > 0 else 1
            worst_quarter = quarterly_returns.idxmin() if len(quarterly_returns) > 0 else 1

            # å¹´å†…æ—¶é—´æ•ˆåº”ï¼ˆæœˆä»½ï¼‰
            month_names = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ',
                           '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
            best_month_name = month_names[best_month - 1] if len(monthly_returns) > 0 else 'æ•°æ®ä¸è¶³'
            worst_month_name = month_names[worst_month - 1] if len(monthly_returns) > 0 else 'æ•°æ®ä¸è¶³'

            # å­£èŠ‚æ€§å¼ºåº¦è¯„ä¼°
            if len(monthly_returns) > 6:
                monthly_volatility = monthly_returns.std()
                seasonality_strength = monthly_volatility / abs(monthly_returns.mean()) if monthly_returns.mean() != 0 else 0
            else:
                seasonality_strength = 0

            # å½“å‰æ—¶é—´çš„å­£èŠ‚æ€§é¢„æµ‹
            current_date = kdata.index[-1] if len(kdata) > 0 else datetime.now()
            current_month = current_date.month
            current_weekday = current_date.weekday()
            current_quarter = (current_month - 1) // 3 + 1

            # åŸºäºå†å²æ•°æ®çš„å½“å‰æ—¶æœŸé¢„æµ‹
            current_month_return = monthly_returns.get(current_month, 0)
            current_quarter_return = quarterly_returns.get(current_quarter, 0)

            if len(weekly_returns) > current_weekday:
                current_weekday_return = weekly_returns.iloc[current_weekday]
            else:
                current_weekday_return = 0

            # ç»¼åˆå­£èŠ‚æ€§è¯„åˆ†
            seasonality_score = (
                current_month_return * 0.5 +
                current_quarter_return * 0.3 +
                current_weekday_return * 0.2
            )

            seasonality_outlook = "æ­£é¢" if seasonality_score > 0.001 else "è´Ÿé¢" if seasonality_score < -0.001 else "ä¸­æ€§"

            return {
                'seasonality_outlook': seasonality_outlook,
                'seasonality_score': float(seasonality_score),
                'seasonality_strength': float(seasonality_strength),
                'best_month': best_month_name,
                'worst_month': worst_month_name,
                'best_weekday': best_weekday_name,
                'worst_weekday': worst_weekday_name,
                'best_quarter': f'ç¬¬{best_quarter}å­£åº¦',
                'worst_quarter': f'ç¬¬{worst_quarter}å­£åº¦',
                'current_month_outlook': float(current_month_return),
                'current_weekday_outlook': float(current_weekday_return),
                'current_quarter_outlook': float(current_quarter_return),
                'confidence': 0.65,
                'model_type': 'seasonal_analysis',
                'prediction_type': PredictionType.SEASONALITY,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å­£èŠ‚æ€§é¢„æµ‹å¤±è´¥: {e}")
            return {
                'seasonality_outlook': 'æœªçŸ¥',
                'seasonality_score': 0.0,
                'seasonality_strength': 0.0,
                'best_month': 'æœªçŸ¥',
                'worst_month': 'æœªçŸ¥',
                'confidence': 0.3,
                'model_type': 'fallback'
            }

    def optimize_parameters(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ä¼˜åŒ–å‚æ•°ï¼ˆåˆ«åæ–¹æ³•ï¼‰

        Args:
            data: åŒ…å«current_configå’Œhistorical_dataçš„å­—å…¸

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        return self.predict_parameter_optimization(data)

    def predict_risk_forecast(self, kdata: pd.DataFrame) -> Dict[str, Any]:
        """
        é¢„æµ‹é£é™©è¶‹åŠ¿

        Args:
            kdata: Kçº¿æ•°æ®

        Returns:
            é£é™©è¶‹åŠ¿é¢„æµ‹ç»“æœ
        """
        try:
            if len(kdata) < 20:
                return {
                    'status': 'error',
                    'message': 'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘20ä¸ªæ•°æ®ç‚¹',
                    'risk_level': 'unknown',
                    'risk_score': 0.5,
                    'forecast_days': 0
                }

            # è®¡ç®—æ”¶ç›Šç‡
            returns = kdata['close'].pct_change().dropna()

            # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆ20æ—¥æ»šåŠ¨ï¼‰
            volatility = returns.rolling(window=20).std()
            current_volatility = volatility.iloc[-1] if len(volatility) > 0 else 0

            # è®¡ç®—VaR (Value at Risk)
            var_95 = returns.quantile(0.05)  # 95% VaR
            var_99 = returns.quantile(0.01)  # 99% VaR

            # è®¡ç®—æœ€å¤§å›æ’¤
            cumulative_returns = (1 + returns).cumprod()
            running_max = cumulative_returns.expanding().max()
            drawdown = (cumulative_returns - running_max) / running_max
            max_drawdown = drawdown.min()

            # é£é™©è¯„åˆ†è®¡ç®— (0-1, 1ä¸ºæœ€é«˜é£é™©)
            volatility_score = min(current_volatility * 10, 1.0)  # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
            var_score = min(abs(var_95) * 5, 1.0)  # VaRé£é™©è¯„åˆ†
            drawdown_score = min(abs(max_drawdown), 1.0)  # å›æ’¤é£é™©è¯„åˆ†

            # ç»¼åˆé£é™©è¯„åˆ†
            risk_score = (volatility_score * 0.4 + var_score * 0.4 + drawdown_score * 0.2)

            # é£é™©ç­‰çº§åˆ¤æ–­
            if risk_score < 0.3:
                risk_level = 'low'
                risk_description = 'ä½é£é™©'
            elif risk_score < 0.6:
                risk_level = 'medium'
                risk_description = 'ä¸­ç­‰é£é™©'
            elif risk_score < 0.8:
                risk_level = 'high'
                risk_description = 'é«˜é£é™©'
            else:
                risk_level = 'extreme'
                risk_description = 'æé«˜é£é™©'

            # è¶‹åŠ¿é¢„æµ‹ï¼ˆåŸºäºæœ€è¿‘çš„æ³¢åŠ¨ç‡è¶‹åŠ¿ï¼‰
            recent_volatility = volatility.tail(5).mean() if len(volatility) >= 5 else current_volatility
            volatility_trend = 'increasing' if current_volatility > recent_volatility else 'decreasing'

            return {
                'status': 'success',
                'risk_level': risk_level,
                'risk_description': risk_description,
                'risk_score': round(risk_score, 3),
                'current_volatility': round(current_volatility, 4),
                'var_95': round(var_95, 4),
                'var_99': round(var_99, 4),
                'max_drawdown': round(max_drawdown, 4),
                'volatility_trend': volatility_trend,
                'forecast_days': 5,
                'recommendations': self._get_risk_recommendations(risk_level, risk_score),
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"é£é™©è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'é¢„æµ‹å¤±è´¥: {str(e)}',
                'risk_level': 'unknown',
                'risk_score': 0.5,
                'forecast_days': 0
            }

    def _get_risk_recommendations(self, risk_level: str, risk_score: float) -> List[str]:
        """è·å–é£é™©ç®¡ç†å»ºè®®"""
        recommendations = []

        if risk_level == 'low':
            recommendations.extend([
                'å½“å‰é£é™©è¾ƒä½ï¼Œå¯é€‚å½“å¢åŠ ä»“ä½',
                'å»ºè®®ä¿æŒç°æœ‰æŠ•èµ„ç­–ç•¥',
                'å…³æ³¨å¸‚åœºå˜åŒ–ï¼Œå‡†å¤‡é£é™©ç®¡ç†æªæ–½'
            ])
        elif risk_level == 'medium':
            recommendations.extend([
                'é£é™©é€‚ä¸­ï¼Œå»ºè®®ä¿æŒè°¨æ…',
                'é€‚å½“åˆ†æ•£æŠ•èµ„ç»„åˆ',
                'è®¾ç½®æ­¢æŸä½ï¼Œæ§åˆ¶å•ç¬”æŸå¤±'
            ])
        elif risk_level == 'high':
            recommendations.extend([
                'é«˜é£é™©è­¦å‘Šï¼Œå»ºè®®å‡å°‘ä»“ä½',
                'åŠ å¼ºé£é™©ç›‘æ§ï¼ŒåŠæ—¶æ­¢æŸ',
                'è€ƒè™‘å¯¹å†²ç­–ç•¥é™ä½é£é™©æ•å£'
            ])
        else:  # extreme
            recommendations.extend([
                'æé«˜é£é™©ï¼å»ºè®®ç«‹å³å‡ä»“',
                'æš‚åœæ–°å¢æŠ•èµ„ï¼Œä¿æŠ¤èµ„æœ¬',
                'è€ƒè™‘æ¸…ä»“è§‚æœ›ï¼Œç­‰å¾…é£é™©é™ä½'
            ])

        return recommendations
