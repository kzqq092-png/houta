#!/usr/bin/env python3
"""
Unified Analysis Service

Consolidates AnalysisManager, PatternManager, and various analysis components
into unified analysis interface with caching and performance optimization.

Created for: FactorWeave-Quant Architecture Refactoring Project
Phase: 2 - Service Consolidation
Task: 9 - Create AnalysisService consolidating analysis managers
"""

import asyncio
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
import pandas as pd
import numpy as np
from decimal import Decimal
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from loguru import logger

from ..services.base_service import BaseService
from ..events import EventBus, get_event_bus
from ..containers import ServiceContainer, get_service_container

logger = logger.bind(module=__name__)


class AnalysisType(Enum):
    """分析类型"""
    TECHNICAL = "technical"
    FUNDAMENTAL = "fundamental"
    PATTERN = "pattern"
    SENTIMENT = "sentiment"
    RISK = "risk"


class IndicatorType(Enum):
    """技术指标类型"""
    TREND = "trend"           # 趋势指标
    MOMENTUM = "momentum"     # 动量指标
    VOLATILITY = "volatility"  # 波动率指标
    VOLUME = "volume"         # 成交量指标
    SUPPORT_RESISTANCE = "support_resistance"  # 支撑阻力


class PatternType(Enum):
    """形态类型"""
    CANDLESTICK = "candlestick"     # K线形态
    CHART_PATTERN = "chart_pattern"  # 图表形态
    HARMONIC = "harmonic"           # 谐波形态
    WAVE = "wave"                   # 波浪形态


@dataclass
class AnalysisResult:
    """分析结果"""
    analysis_id: str
    symbol: str
    analysis_type: AnalysisType
    result_data: Dict[str, Any]
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class IndicatorResult:
    """技术指标结果"""
    indicator_name: str
    indicator_type: IndicatorType
    values: Dict[str, Union[float, List[float]]]
    signals: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class PatternResult:
    """形态识别结果"""
    pattern_name: str
    pattern_type: PatternType
    start_time: datetime
    end_time: datetime
    confidence: float
    description: str
    price_targets: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalysisConfig:
    """分析配置"""
    enabled_indicators: List[str] = field(default_factory=list)
    enabled_patterns: List[str] = field(default_factory=list)
    cache_enabled: bool = True
    cache_ttl_seconds: int = 300  # 5分钟缓存
    parallel_analysis: bool = True
    max_workers: int = 4


class UnifiedAnalysisService(BaseService):
    """
    统一分析服务

    整合所有分析功能，提供：
    - 技术分析指标计算
    - 形态识别和模式匹配
    - 基本面分析数据处理
    - 情绪分析数据处理
    - 风险分析和评估
    - 智能缓存和性能优化
    - 并行分析执行
    """

    def __init__(self,
                 service_container: Optional[ServiceContainer] = None,
                 event_bus: Optional[EventBus] = None):
        super().__init__()
        self.service_name = "UnifiedAnalysisService"

        # 依赖注入
        self._service_container = service_container or get_service_container()
        self._event_bus = event_bus or get_event_bus()

        # 配置
        self._config = AnalysisConfig()

        # 分析结果缓存
        self._analysis_cache: Dict[str, AnalysisResult] = {}
        self._indicator_cache: Dict[str, IndicatorResult] = {}
        self._pattern_cache: Dict[str, List[PatternResult]] = {}

        # 同步控制
        self._cache_lock = threading.RLock()
        self._analysis_lock = threading.RLock()

        # 执行器
        self._executor = ThreadPoolExecutor(
            max_workers=self._config.max_workers,
            thread_name_prefix="AnalysisService"
        )

        # 注册的分析器
        self._technical_analyzers: Dict[str, Callable] = {}
        self._pattern_analyzers: Dict[str, Callable] = {}
        self._fundamental_analyzers: Dict[str, Callable] = {}

        # 缓存清理任务
        self._cache_cleanup_task: Optional[asyncio.Task] = None

        logger.info(f"{self.service_name} initialized")

    async def _do_initialize(self) -> bool:
        """初始化分析服务"""
        try:
            logger.info("Initializing UnifiedAnalysisService...")

            # 注册内置分析器
            self._register_builtin_analyzers()

            # 启动缓存清理任务
            self._start_cache_cleanup()

            # 注册事件监听器
            self._register_event_listeners()

            # 加载分析配置
            await self._load_analysis_config()

            logger.info("UnifiedAnalysisService initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize UnifiedAnalysisService: {e}")
            return False

    def _register_builtin_analyzers(self) -> None:
        """注册内置分析器"""
        try:
            # 技术指标分析器
            self._technical_analyzers.update({
                'sma': self._calculate_sma,
                'ema': self._calculate_ema,
                'rsi': self._calculate_rsi,
                'macd': self._calculate_macd,
                'bollinger_bands': self._calculate_bollinger_bands,
                'atr': self._calculate_atr,
                'stochastic': self._calculate_stochastic,
                'williams_r': self._calculate_williams_r
            })

            # 形态识别分析器
            self._pattern_analyzers.update({
                'doji': self._detect_doji,
                'hammer': self._detect_hammer,
                'shooting_star': self._detect_shooting_star,
                'engulfing': self._detect_engulfing,
                'head_shoulders': self._detect_head_shoulders,
                'double_top': self._detect_double_top,
                'triangle': self._detect_triangle
            })

            logger.info(f"Registered {len(self._technical_analyzers)} technical analyzers")
            logger.info(f"Registered {len(self._pattern_analyzers)} pattern analyzers")

        except Exception as e:
            logger.error(f"Failed to register builtin analyzers: {e}")

    def _start_cache_cleanup(self) -> None:
        """启动缓存清理任务"""
        async def cache_cleanup_loop():
            while True:
                try:
                    await asyncio.sleep(60)  # 每分钟清理一次
                    self._cleanup_expired_cache()
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Cache cleanup error: {e}")

        self._cache_cleanup_task = asyncio.create_task(cache_cleanup_loop())

    def _cleanup_expired_cache(self) -> None:
        """清理过期缓存"""
        try:
            now = datetime.now()
            ttl = timedelta(seconds=self._config.cache_ttl_seconds)

            with self._cache_lock:
                # 清理分析结果缓存
                expired_keys = [
                    key for key, result in self._analysis_cache.items()
                    if now - result.timestamp > ttl
                ]
                for key in expired_keys:
                    del self._analysis_cache[key]

                # 清理指标缓存
                expired_keys = [
                    key for key, result in self._indicator_cache.items()
                    if now - result.timestamp > ttl
                ]
                for key in expired_keys:
                    del self._indicator_cache[key]

                if expired_keys:
                    logger.debug(f"Cleaned up {len(expired_keys)} expired cache entries")

        except Exception as e:
            logger.error(f"Cache cleanup failed: {e}")

    def _register_event_listeners(self) -> None:
        """注册事件监听器"""
        try:
            # 监听价格数据更新事件
            self._event_bus.on("market.price_update", self._on_price_update)

            # 监听分析请求事件
            self._event_bus.on("analysis.request", self._on_analysis_request)

            logger.info("Event listeners registered")

        except Exception as e:
            logger.error(f"Failed to register event listeners: {e}")

    async def _load_analysis_config(self) -> None:
        """加载分析配置"""
        try:
            # 从配置服务加载设置
            # 这里使用默认配置
            self._config.enabled_indicators = list(self._technical_analyzers.keys())
            self._config.enabled_patterns = list(self._pattern_analyzers.keys())

            logger.info("Analysis configuration loaded")

        except Exception as e:
            logger.warning(f"Failed to load analysis config: {e}")

    def _on_price_update(self, event_data: Dict[str, Any]) -> None:
        """处理价格更新事件"""
        try:
            symbol = event_data.get("symbol")
            if symbol:
                # 清除相关缓存
                self._invalidate_cache_for_symbol(symbol)

        except Exception as e:
            logger.error(f"Price update handling failed: {e}")

    def _on_analysis_request(self, event_data: Dict[str, Any]) -> None:
        """处理分析请求事件"""
        try:
            symbol = event_data.get("symbol")
            analysis_types = event_data.get("analysis_types", [])

            if symbol and analysis_types:
                # 异步执行分析
                asyncio.create_task(self._execute_async_analysis(symbol, analysis_types))

        except Exception as e:
            logger.error(f"Analysis request handling failed: {e}")

    # ================================
    # 公共API - 技术分析
    # ================================

    async def analyze_technical(self,
                                symbol: str,
                                data: pd.DataFrame,
                                indicators: Optional[List[str]] = None) -> Dict[str, IndicatorResult]:
        """执行技术分析"""
        try:
            if indicators is None:
                indicators = self._config.enabled_indicators

            # 检查缓存
            cache_key = f"technical_{symbol}_{hash(str(indicators))}"
            if self._config.cache_enabled:
                cached_result = self._get_cached_analysis(cache_key)
                if cached_result:
                    return cached_result.result_data

            # 执行技术分析
            results = {}

            if self._config.parallel_analysis and len(indicators) > 1:
                # 并行执行
                results = await self._execute_parallel_technical_analysis(symbol, data, indicators)
            else:
                # 串行执行
                for indicator in indicators:
                    if indicator in self._technical_analyzers:
                        try:
                            result = self._technical_analyzers[indicator](data)
                            results[indicator] = result
                        except Exception as e:
                            logger.error(f"Technical analysis failed for {indicator}: {e}")

            # 缓存结果
            if self._config.cache_enabled:
                analysis_result = AnalysisResult(
                    analysis_id=cache_key,
                    symbol=symbol,
                    analysis_type=AnalysisType.TECHNICAL,
                    result_data=results
                )
                self._cache_analysis_result(cache_key, analysis_result)

            # 发送完成事件
            self._event_bus.emit("analysis.technical_completed", {
                "symbol": symbol,
                "indicators": indicators,
                "results_count": len(results)
            })

            logger.info(f"Technical analysis completed for {symbol}: {len(results)} indicators")
            return results

        except Exception as e:
            logger.error(f"Technical analysis failed for {symbol}: {e}")
            return {}

    async def _execute_parallel_technical_analysis(self,
                                                   symbol: str,
                                                   data: pd.DataFrame,
                                                   indicators: List[str]) -> Dict[str, IndicatorResult]:
        """并行执行技术分析"""
        results = {}

        # 创建任务
        tasks = []
        for indicator in indicators:
            if indicator in self._technical_analyzers:
                task = self._executor.submit(self._technical_analyzers[indicator], data)
                tasks.append((indicator, task))

        # 等待完成
        for indicator, task in tasks:
            try:
                result = task.result(timeout=30)  # 30秒超时
                results[indicator] = result
            except Exception as e:
                logger.error(f"Parallel technical analysis failed for {indicator}: {e}")

        return results

    # ================================
    # 技术指标实现
    # ================================

    def _calculate_sma(self, data: pd.DataFrame, period: int = 20) -> IndicatorResult:
        """简单移动平均线"""
        sma_values = data['close'].rolling(window=period).mean()

        # 生成信号
        signals = []
        current_price = data['close'].iloc[-1]
        current_sma = sma_values.iloc[-1]

        if current_price > current_sma:
            signals.append("bullish")
        elif current_price < current_sma:
            signals.append("bearish")

        return IndicatorResult(
            indicator_name=f"SMA_{period}",
            indicator_type=IndicatorType.TREND,
            values={"sma": sma_values.tolist()},
            signals=signals
        )

    def _calculate_ema(self, data: pd.DataFrame, period: int = 20) -> IndicatorResult:
        """指数移动平均线"""
        ema_values = data['close'].ewm(span=period).mean()

        signals = []
        current_price = data['close'].iloc[-1]
        current_ema = ema_values.iloc[-1]

        if current_price > current_ema:
            signals.append("bullish")
        elif current_price < current_ema:
            signals.append("bearish")

        return IndicatorResult(
            indicator_name=f"EMA_{period}",
            indicator_type=IndicatorType.TREND,
            values={"ema": ema_values.tolist()},
            signals=signals
        )

    def _calculate_rsi(self, data: pd.DataFrame, period: int = 14) -> IndicatorResult:
        """相对强弱指数"""
        delta = data['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))

        signals = []
        current_rsi = rsi.iloc[-1]

        if current_rsi > 70:
            signals.append("overbought")
        elif current_rsi < 30:
            signals.append("oversold")

        return IndicatorResult(
            indicator_name=f"RSI_{period}",
            indicator_type=IndicatorType.MOMENTUM,
            values={"rsi": rsi.tolist()},
            signals=signals
        )

    def _calculate_macd(self, data: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9) -> IndicatorResult:
        """MACD指标"""
        ema_fast = data['close'].ewm(span=fast).mean()
        ema_slow = data['close'].ewm(span=slow).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal).mean()
        histogram = macd_line - signal_line

        signals = []
        if len(macd_line) >= 2:
            if macd_line.iloc[-1] > signal_line.iloc[-1] and macd_line.iloc[-2] <= signal_line.iloc[-2]:
                signals.append("bullish_crossover")
            elif macd_line.iloc[-1] < signal_line.iloc[-1] and macd_line.iloc[-2] >= signal_line.iloc[-2]:
                signals.append("bearish_crossover")

        return IndicatorResult(
            indicator_name=f"MACD_{fast}_{slow}_{signal}",
            indicator_type=IndicatorType.MOMENTUM,
            values={
                "macd": macd_line.tolist(),
                "signal": signal_line.tolist(),
                "histogram": histogram.tolist()
            },
            signals=signals
        )

    def _calculate_bollinger_bands(self, data: pd.DataFrame, period: int = 20, std: float = 2) -> IndicatorResult:
        """布林带"""
        sma = data['close'].rolling(window=period).mean()
        rolling_std = data['close'].rolling(window=period).std()
        upper_band = sma + (rolling_std * std)
        lower_band = sma - (rolling_std * std)

        signals = []
        current_price = data['close'].iloc[-1]
        current_upper = upper_band.iloc[-1]
        current_lower = lower_band.iloc[-1]

        if current_price > current_upper:
            signals.append("overbought")
        elif current_price < current_lower:
            signals.append("oversold")

        return IndicatorResult(
            indicator_name=f"BOLLINGER_{period}_{std}",
            indicator_type=IndicatorType.VOLATILITY,
            values={
                "upper": upper_band.tolist(),
                "middle": sma.tolist(),
                "lower": lower_band.tolist()
            },
            signals=signals
        )

    def _calculate_atr(self, data: pd.DataFrame, period: int = 14) -> IndicatorResult:
        """平均真实波动范围"""
        high_low = data['high'] - data['low']
        high_close = np.abs(data['high'] - data['close'].shift())
        low_close = np.abs(data['low'] - data['close'].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        atr = true_range.rolling(window=period).mean()

        return IndicatorResult(
            indicator_name=f"ATR_{period}",
            indicator_type=IndicatorType.VOLATILITY,
            values={"atr": atr.tolist()},
            signals=[]
        )

    def _calculate_stochastic(self, data: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> IndicatorResult:
        """随机指标"""
        lowest_low = data['low'].rolling(window=k_period).min()
        highest_high = data['high'].rolling(window=k_period).max()
        k_percent = 100 * ((data['close'] - lowest_low) / (highest_high - lowest_low))
        d_percent = k_percent.rolling(window=d_period).mean()

        signals = []
        current_k = k_percent.iloc[-1]
        current_d = d_percent.iloc[-1]

        if current_k > 80 and current_d > 80:
            signals.append("overbought")
        elif current_k < 20 and current_d < 20:
            signals.append("oversold")

        return IndicatorResult(
            indicator_name=f"STOCH_{k_period}_{d_period}",
            indicator_type=IndicatorType.MOMENTUM,
            values={
                "k": k_percent.tolist(),
                "d": d_percent.tolist()
            },
            signals=signals
        )

    def _calculate_williams_r(self, data: pd.DataFrame, period: int = 14) -> IndicatorResult:
        """威廉指标"""
        highest_high = data['high'].rolling(window=period).max()
        lowest_low = data['low'].rolling(window=period).min()
        williams_r = -100 * ((highest_high - data['close']) / (highest_high - lowest_low))

        signals = []
        current_wr = williams_r.iloc[-1]

        if current_wr > -20:
            signals.append("overbought")
        elif current_wr < -80:
            signals.append("oversold")

        return IndicatorResult(
            indicator_name=f"WILLIAMS_R_{period}",
            indicator_type=IndicatorType.MOMENTUM,
            values={"williams_r": williams_r.tolist()},
            signals=signals
        )

    # ================================
    # 公共API - 形态识别
    # ================================

    async def analyze_patterns(self,
                               symbol: str,
                               data: pd.DataFrame,
                               patterns: Optional[List[str]] = None) -> List[PatternResult]:
        """执行形态识别"""
        try:
            if patterns is None:
                patterns = self._config.enabled_patterns

            # 检查缓存
            cache_key = f"patterns_{symbol}_{hash(str(patterns))}"
            if self._config.cache_enabled:
                cached_patterns = self._get_cached_patterns(cache_key)
                if cached_patterns:
                    return cached_patterns

            # 执行形态识别
            results = []

            for pattern in patterns:
                if pattern in self._pattern_analyzers:
                    try:
                        pattern_results = self._pattern_analyzers[pattern](data)
                        if pattern_results:
                            results.extend(pattern_results)
                    except Exception as e:
                        logger.error(f"Pattern analysis failed for {pattern}: {e}")

            # 缓存结果
            if self._config.cache_enabled:
                self._cache_pattern_results(cache_key, results)

            # 发送完成事件
            self._event_bus.emit("analysis.pattern_completed", {
                "symbol": symbol,
                "patterns": patterns,
                "results_count": len(results)
            })

            logger.info(f"Pattern analysis completed for {symbol}: {len(results)} patterns found")
            return results

        except Exception as e:
            logger.error(f"Pattern analysis failed for {symbol}: {e}")
            return []

    # ================================
    # 形态识别实现
    # ================================

    def _detect_doji(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测十字星形态"""
        results = []

        for i in range(len(data)):
            if i < 1:
                continue

            row = data.iloc[i]
            open_price = row['open']
            close_price = row['close']
            high_price = row['high']
            low_price = row['low']

            # 十字星判断：实体很小，上下影线较长
            body_size = abs(close_price - open_price)
            total_range = high_price - low_price

            if total_range > 0 and body_size / total_range < 0.1:
                results.append(PatternResult(
                    pattern_name="Doji",
                    pattern_type=PatternType.CANDLESTICK,
                    start_time=data.index[i],
                    end_time=data.index[i],
                    confidence=0.8,
                    description="Doji candlestick pattern indicating indecision"
                ))

        return results

    def _detect_hammer(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测锤子形态"""
        results = []

        for i in range(len(data)):
            if i < 1:
                continue

            row = data.iloc[i]
            open_price = row['open']
            close_price = row['close']
            high_price = row['high']
            low_price = row['low']

            # 锤子形态：长下影线，短上影线，实体在上部
            body_size = abs(close_price - open_price)
            lower_shadow = min(open_price, close_price) - low_price
            upper_shadow = high_price - max(open_price, close_price)

            if (lower_shadow > body_size * 2 and
                upper_shadow < body_size * 0.5 and
                    body_size > 0):

                results.append(PatternResult(
                    pattern_name="Hammer",
                    pattern_type=PatternType.CANDLESTICK,
                    start_time=data.index[i],
                    end_time=data.index[i],
                    confidence=0.75,
                    description="Hammer pattern indicating potential reversal"
                ))

        return results

    def _detect_shooting_star(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测流星形态"""
        results = []

        for i in range(len(data)):
            if i < 1:
                continue

            row = data.iloc[i]
            open_price = row['open']
            close_price = row['close']
            high_price = row['high']
            low_price = row['low']

            # 流星形态：长上影线，短下影线，实体在下部
            body_size = abs(close_price - open_price)
            lower_shadow = min(open_price, close_price) - low_price
            upper_shadow = high_price - max(open_price, close_price)

            if (upper_shadow > body_size * 2 and
                lower_shadow < body_size * 0.5 and
                    body_size > 0):

                results.append(PatternResult(
                    pattern_name="Shooting Star",
                    pattern_type=PatternType.CANDLESTICK,
                    start_time=data.index[i],
                    end_time=data.index[i],
                    confidence=0.75,
                    description="Shooting star pattern indicating potential reversal"
                ))

        return results

    def _detect_engulfing(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测吞没形态"""
        results = []

        for i in range(1, len(data)):
            prev_row = data.iloc[i-1]
            curr_row = data.iloc[i]

            prev_open = prev_row['open']
            prev_close = prev_row['close']
            curr_open = curr_row['open']
            curr_close = curr_row['close']

            # 看涨吞没
            if (prev_close < prev_open and  # 前一天阴线
                curr_close > curr_open and  # 当天阳线
                curr_open < prev_close and  # 开盘价低于前收盘
                    curr_close > prev_open):    # 收盘价高于前开盘

                results.append(PatternResult(
                    pattern_name="Bullish Engulfing",
                    pattern_type=PatternType.CANDLESTICK,
                    start_time=data.index[i-1],
                    end_time=data.index[i],
                    confidence=0.8,
                    description="Bullish engulfing pattern"
                ))

            # 看跌吞没
            elif (prev_close > prev_open and  # 前一天阳线
                  curr_close < curr_open and  # 当天阴线
                  curr_open > prev_close and  # 开盘价高于前收盘
                  curr_close < prev_open):    # 收盘价低于前开盘

                results.append(PatternResult(
                    pattern_name="Bearish Engulfing",
                    pattern_type=PatternType.CANDLESTICK,
                    start_time=data.index[i-1],
                    end_time=data.index[i],
                    confidence=0.8,
                    description="Bearish engulfing pattern"
                ))

        return results

    def _detect_head_shoulders(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测头肩顶形态"""
        # 简化实现，实际应该更复杂
        return []

    def _detect_double_top(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测双顶形态"""
        # 简化实现，实际应该更复杂
        return []

    def _detect_triangle(self, data: pd.DataFrame) -> List[PatternResult]:
        """检测三角形形态"""
        # 简化实现，实际应该更复杂
        return []

    # ================================
    # 缓存管理
    # ================================

    def _get_cached_analysis(self, cache_key: str) -> Optional[AnalysisResult]:
        """获取缓存的分析结果"""
        with self._cache_lock:
            result = self._analysis_cache.get(cache_key)
            if result:
                # 检查是否过期
                ttl = timedelta(seconds=self._config.cache_ttl_seconds)
                if datetime.now() - result.timestamp < ttl:
                    return result
                else:
                    del self._analysis_cache[cache_key]
            return None

    def _cache_analysis_result(self, cache_key: str, result: AnalysisResult) -> None:
        """缓存分析结果"""
        with self._cache_lock:
            self._analysis_cache[cache_key] = result

    def _get_cached_patterns(self, cache_key: str) -> Optional[List[PatternResult]]:
        """获取缓存的形态结果"""
        with self._cache_lock:
            # 简化实现，实际应该检查时间戳
            return self._pattern_cache.get(cache_key)

    def _cache_pattern_results(self, cache_key: str, results: List[PatternResult]) -> None:
        """缓存形态结果"""
        with self._cache_lock:
            self._pattern_cache[cache_key] = results

    def _invalidate_cache_for_symbol(self, symbol: str) -> None:
        """清除指定代码的缓存"""
        with self._cache_lock:
            keys_to_remove = [
                key for key in self._analysis_cache.keys()
                if symbol in key
            ]
            for key in keys_to_remove:
                del self._analysis_cache[key]

            keys_to_remove = [
                key for key in self._pattern_cache.keys()
                if symbol in key
            ]
            for key in keys_to_remove:
                del self._pattern_cache[key]

    # ================================
    # 工具方法
    # ================================

    async def _execute_async_analysis(self, symbol: str, analysis_types: List[str]) -> None:
        """异步执行分析"""
        try:
            # 这里应该获取数据并执行相应分析
            # 简化实现
            logger.info(f"Executing async analysis for {symbol}: {analysis_types}")

        except Exception as e:
            logger.error(f"Async analysis failed for {symbol}: {e}")

    def update_config(self, config: AnalysisConfig) -> None:
        """更新分析配置"""
        self._config = config
        logger.info("Analysis configuration updated")

    def get_config(self) -> AnalysisConfig:
        """获取分析配置"""
        return self._config

    def register_custom_analyzer(self, name: str, analyzer: Callable, analysis_type: str = "technical") -> None:
        """注册自定义分析器"""
        if analysis_type == "technical":
            self._technical_analyzers[name] = analyzer
        elif analysis_type == "pattern":
            self._pattern_analyzers[name] = analyzer

        logger.info(f"Custom analyzer registered: {name} ({analysis_type})")

    async def shutdown(self) -> None:
        """关闭服务"""
        try:
            logger.info("Shutting down UnifiedAnalysisService...")

            # 取消缓存清理任务
            if self._cache_cleanup_task:
                self._cache_cleanup_task.cancel()
                try:
                    await self._cache_cleanup_task
                except asyncio.CancelledError:
                    pass

            # 关闭执行器
            self._executor.shutdown(wait=True)

            logger.info("UnifiedAnalysisService shutdown completed")

        except Exception as e:
            logger.error(f"Error during UnifiedAnalysisService shutdown: {e}")


# ================================
# 服务工厂函数
# ================================

_unified_analysis_service: Optional[UnifiedAnalysisService] = None


def get_unified_analysis_service(
    service_container: Optional[ServiceContainer] = None,
    event_bus: Optional[EventBus] = None
) -> UnifiedAnalysisService:
    """获取统一分析服务实例"""
    global _unified_analysis_service

    if _unified_analysis_service is None:
        _unified_analysis_service = UnifiedAnalysisService(
            service_container=service_container,
            event_bus=event_bus
        )

    return _unified_analysis_service
