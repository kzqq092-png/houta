#!/usr/bin/env python3
"""
Unified Performance Service

Consolidates performance managers into comprehensive monitoring and optimization service
with real-time metrics collection, alerting, and automatic optimization.

Created for: HIkyuu-UI Architecture Refactoring Project
Phase: 3 - Infrastructure Services
Task: 13 - Create PerformanceService for monitoring and optimization
"""

import asyncio
import threading
import time
import psutil
import gc
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from decimal import Decimal
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from collections import deque, defaultdict
import json
from loguru import logger

from ..services.base_service import BaseService
from ..events import EventBus, get_event_bus
from ..containers import ServiceContainer, get_service_container

logger = logger.bind(module=__name__)


class MetricType(Enum):
    """指标类型"""
    COUNTER = "counter"         # 计数器
    GAUGE = "gauge"            # 仪表
    HISTOGRAM = "histogram"     # 直方图
    TIMING = "timing"          # 时间测量


class AlertSeverity(Enum):
    """警报严重程度"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class OptimizationStrategy(Enum):
    """优化策略"""
    AGGRESSIVE = "aggressive"   # 激进优化
    BALANCED = "balanced"      # 平衡优化
    CONSERVATIVE = "conservative"  # 保守优化


@dataclass
class PerformanceMetric:
    """性能指标"""
    name: str
    metric_type: MetricType
    value: Union[float, int]
    unit: str = ""
    timestamp: datetime = field(default_factory=datetime.now)
    tags: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceAlert:
    """性能警报"""
    alert_id: str
    metric_name: str
    severity: AlertSeverity
    message: str
    current_value: Union[float, int]
    threshold: Union[float, int]
    timestamp: datetime = field(default_factory=datetime.now)
    resolved: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class OptimizationResult:
    """优化结果"""
    optimization_id: str
    strategy: OptimizationStrategy
    target_metric: str
    before_value: Union[float, int]
    after_value: Union[float, int]
    improvement_percent: float
    timestamp: datetime = field(default_factory=datetime.now)
    description: str = ""
    success: bool = True


@dataclass
class PerformanceThreshold:
    """性能阈值"""
    metric_name: str
    warning_threshold: Union[float, int]
    error_threshold: Union[float, int]
    critical_threshold: Union[float, int]
    enabled: bool = True


@dataclass
class PerformanceConfig:
    """性能配置"""
    collection_interval: int = 10  # 秒
    retention_days: int = 7
    auto_optimization_enabled: bool = True
    optimization_strategy: OptimizationStrategy = OptimizationStrategy.BALANCED
    alert_enabled: bool = True
    dashboard_enabled: bool = True
    metrics_enabled: List[str] = field(default_factory=list)


class UnifiedPerformanceService(BaseService):
    """
    统一性能服务

    整合所有性能监控和优化功能，提供：
    - 实时性能指标收集和监控
    - 自动化性能警报和通知
    - 智能性能优化建议和自动执行
    - 性能趋势分析和预测
    - 资源使用监控和优化
    - 性能仪表板和可视化
    - 性能基准测试和对比
    """

    def __init__(self,
                 service_container: Optional[ServiceContainer] = None,
                 event_bus: Optional[EventBus] = None):
        super().__init__()
        self.service_name = "UnifiedPerformanceService"

        # 依赖注入
        self._service_container = service_container or get_service_container()
        self._event_bus = event_bus or get_event_bus()

        # 配置
        self._config = PerformanceConfig()

        # 性能数据存储
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self._alerts: Dict[str, PerformanceAlert] = {}
        self._optimizations: List[OptimizationResult] = []
        self._thresholds: Dict[str, PerformanceThreshold] = {}

        # 同步控制
        self._metrics_lock = threading.RLock()
        self._alerts_lock = threading.RLock()
        self._optimization_lock = threading.RLock()

        # 监控任务
        self._monitor_task: Optional[asyncio.Task] = None
        self._optimization_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None

        # 执行器
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="PerformanceService")

        # 回调函数
        self._metric_callbacks: List[Callable[[PerformanceMetric], None]] = []
        self._alert_callbacks: List[Callable[[PerformanceAlert], None]] = []

        # 系统监控状态
        self._monitoring_active = False
        self._last_gc_time = time.time()
        self._baseline_metrics: Dict[str, float] = {}

        logger.info(f"{self.service_name} initialized")

    async def _do_initialize(self) -> bool:
        """初始化性能服务"""
        try:
            logger.info("Initializing UnifiedPerformanceService...")

            # 设置默认阈值
            self._setup_default_thresholds()

            # 启动监控任务
            await self._start_monitoring()

            # 启动优化任务
            if self._config.auto_optimization_enabled:
                await self._start_optimization()

            # 启动清理任务
            await self._start_cleanup()

            # 注册事件监听器
            self._register_event_listeners()

            # 收集基准指标
            await self._collect_baseline_metrics()

            logger.info("UnifiedPerformanceService initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize UnifiedPerformanceService: {e}")
            return False

    def _setup_default_thresholds(self) -> None:
        """设置默认性能阈值"""
        default_thresholds = {
            "cpu_usage": PerformanceThreshold("cpu_usage", 70, 85, 95),
            "memory_usage": PerformanceThreshold("memory_usage", 75, 90, 98),
            "disk_usage": PerformanceThreshold("disk_usage", 80, 90, 95),
            "response_time": PerformanceThreshold("response_time", 1000, 3000, 5000),
            "error_rate": PerformanceThreshold("error_rate", 0.01, 0.05, 0.1),
            "throughput": PerformanceThreshold("throughput", 100, 50, 10),
        }

        self._thresholds.update(default_thresholds)
        logger.info(f"Setup {len(default_thresholds)} default thresholds")

    async def _start_monitoring(self) -> None:
        """启动性能监控"""
        async def monitoring_loop():
            self._monitoring_active = True
            logger.info("Performance monitoring started")

            while self._monitoring_active:
                try:
                    await self._collect_system_metrics()
                    await self._collect_application_metrics()
                    await self._check_thresholds()

                    await asyncio.sleep(self._config.collection_interval)

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Monitoring loop error: {e}")
                    await asyncio.sleep(5)

            logger.info("Performance monitoring stopped")

        self._monitor_task = asyncio.create_task(monitoring_loop())

    async def _start_optimization(self) -> None:
        """启动自动优化"""
        async def optimization_loop():
            logger.info("Auto optimization started")

            while self._monitoring_active:
                try:
                    await self._run_automatic_optimization()
                    await asyncio.sleep(300)  # 每5分钟检查一次优化机会

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Optimization loop error: {e}")
                    await asyncio.sleep(60)

            logger.info("Auto optimization stopped")

        self._optimization_task = asyncio.create_task(optimization_loop())

    async def _start_cleanup(self) -> None:
        """启动数据清理"""
        async def cleanup_loop():
            while self._monitoring_active:
                try:
                    await self._cleanup_old_data()
                    await asyncio.sleep(3600)  # 每小时清理一次

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Cleanup loop error: {e}")
                    await asyncio.sleep(300)

        self._cleanup_task = asyncio.create_task(cleanup_loop())

    def _register_event_listeners(self) -> None:
        """注册事件监听器"""
        try:
            # 监听服务启动/停止事件
            self._event_bus.on("service.started", self._on_service_event)
            self._event_bus.on("service.stopped", self._on_service_event)

            # 监听性能事件
            self._event_bus.on("performance.metric", self._on_performance_metric)

            logger.info("Event listeners registered")

        except Exception as e:
            logger.error(f"Failed to register event listeners: {e}")

    async def _collect_baseline_metrics(self) -> None:
        """收集基准指标"""
        try:
            self._baseline_metrics = {
                "cpu_usage": psutil.cpu_percent(),
                "memory_usage": psutil.virtual_memory().percent,
                "startup_time": 10.0,  # 假设基准启动时间
                "response_time": 100.0  # 假设基准响应时间
            }

            logger.info(f"Baseline metrics collected: {self._baseline_metrics}")

        except Exception as e:
            logger.error(f"Failed to collect baseline metrics: {e}")

    # ================================
    # 指标收集
    # ================================

    async def _collect_system_metrics(self) -> None:
        """收集系统性能指标"""
        try:
            # CPU使用率
            cpu_percent = psutil.cpu_percent(interval=1)
            await self.record_metric("cpu_usage", MetricType.GAUGE, cpu_percent, "percent")

            # 内存使用率
            memory = psutil.virtual_memory()
            await self.record_metric("memory_usage", MetricType.GAUGE, memory.percent, "percent")
            await self.record_metric("memory_available", MetricType.GAUGE, memory.available / (1024**3), "GB")

            # 磁盘使用率
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100
            await self.record_metric("disk_usage", MetricType.GAUGE, disk_percent, "percent")

            # 网络I/O
            network = psutil.net_io_counters()
            await self.record_metric("network_bytes_sent", MetricType.COUNTER, network.bytes_sent, "bytes")
            await self.record_metric("network_bytes_recv", MetricType.COUNTER, network.bytes_recv, "bytes")

            # 进程信息
            process = psutil.Process()
            await self.record_metric("process_memory", MetricType.GAUGE, process.memory_info().rss / (1024**2), "MB")
            await self.record_metric("process_cpu", MetricType.GAUGE, process.cpu_percent(), "percent")

        except Exception as e:
            logger.error(f"Failed to collect system metrics: {e}")

    async def _collect_application_metrics(self) -> None:
        """收集应用程序性能指标"""
        try:
            # 服务容器指标
            if hasattr(self._service_container, 'get_metrics'):
                container_metrics = self._service_container.get_metrics()
                for name, value in container_metrics.items():
                    await self.record_metric(f"container_{name}", MetricType.GAUGE, value)

            # 垃圾回收指标
            current_time = time.time()
            if current_time - self._last_gc_time > 60:  # 每分钟检查一次
                gc_stats = gc.get_stats()
                if gc_stats:
                    await self.record_metric("gc_collections", MetricType.COUNTER, sum(stat['collections'] for stat in gc_stats))

                self._last_gc_time = current_time

            # 线程数量
            thread_count = threading.active_count()
            await self.record_metric("thread_count", MetricType.GAUGE, thread_count, "count")

        except Exception as e:
            logger.error(f"Failed to collect application metrics: {e}")

    async def record_metric(self,
                            name: str,
                            metric_type: MetricType,
                            value: Union[float, int],
                            unit: str = "",
                            tags: Optional[Dict[str, str]] = None) -> None:
        """记录性能指标"""
        try:
            metric = PerformanceMetric(
                name=name,
                metric_type=metric_type,
                value=value,
                unit=unit,
                tags=tags or {},
                timestamp=datetime.now()
            )

            with self._metrics_lock:
                self._metrics[name].append(metric)

            # 触发回调
            for callback in self._metric_callbacks:
                try:
                    callback(metric)
                except Exception as e:
                    logger.error(f"Metric callback error: {e}")

            # 发送事件
            self._event_bus.emit("performance.metric_recorded", {
                "name": name,
                "value": value,
                "unit": unit,
                "timestamp": metric.timestamp.isoformat()
            })

        except Exception as e:
            logger.error(f"Failed to record metric {name}: {e}")

    # ================================
    # 阈值检查和警报
    # ================================

    async def _check_thresholds(self) -> None:
        """检查性能阈值"""
        try:
            with self._metrics_lock:
                for metric_name, threshold in self._thresholds.items():
                    if not threshold.enabled:
                        continue

                    if metric_name in self._metrics and self._metrics[metric_name]:
                        latest_metric = self._metrics[metric_name][-1]
                        current_value = latest_metric.value

                        # 检查是否超过阈值
                        severity = None
                        if current_value >= threshold.critical_threshold:
                            severity = AlertSeverity.CRITICAL
                        elif current_value >= threshold.error_threshold:
                            severity = AlertSeverity.ERROR
                        elif current_value >= threshold.warning_threshold:
                            severity = AlertSeverity.WARNING

                        if severity:
                            await self._create_alert(metric_name, severity, current_value, threshold)
                        else:
                            # 检查是否需要解决现有警报
                            await self._resolve_alerts(metric_name)

        except Exception as e:
            logger.error(f"Threshold checking failed: {e}")

    async def _create_alert(self,
                            metric_name: str,
                            severity: AlertSeverity,
                            current_value: Union[float, int],
                            threshold: PerformanceThreshold) -> None:
        """创建性能警报"""
        try:
            alert_id = f"{metric_name}_{severity.value}_{int(time.time())}"

            # 检查是否已存在相同警报
            existing_alert_key = f"{metric_name}_{severity.value}"
            with self._alerts_lock:
                if existing_alert_key in self._alerts and not self._alerts[existing_alert_key].resolved:
                    return  # 避免重复警报

            alert = PerformanceAlert(
                alert_id=alert_id,
                metric_name=metric_name,
                severity=severity,
                message=f"{metric_name} exceeded {severity.value} threshold",
                current_value=current_value,
                threshold=getattr(threshold, f"{severity.value}_threshold")
            )

            with self._alerts_lock:
                self._alerts[existing_alert_key] = alert

            # 触发回调
            for callback in self._alert_callbacks:
                try:
                    callback(alert)
                except Exception as e:
                    logger.error(f"Alert callback error: {e}")

            # 发送事件
            self._event_bus.emit("performance.alert_created", {
                "alert_id": alert_id,
                "metric_name": metric_name,
                "severity": severity.value,
                "current_value": current_value,
                "message": alert.message
            })

            logger.warning(f"Performance alert: {alert.message} (current: {current_value})")

        except Exception as e:
            logger.error(f"Failed to create alert: {e}")

    async def _resolve_alerts(self, metric_name: str) -> None:
        """解决性能警报"""
        try:
            with self._alerts_lock:
                keys_to_resolve = [
                    key for key in self._alerts.keys()
                    if key.startswith(metric_name) and not self._alerts[key].resolved
                ]

                for key in keys_to_resolve:
                    alert = self._alerts[key]
                    alert.resolved = True

                    # 发送解决事件
                    self._event_bus.emit("performance.alert_resolved", {
                        "alert_id": alert.alert_id,
                        "metric_name": metric_name
                    })

                    logger.info(f"Performance alert resolved: {alert.alert_id}")

        except Exception as e:
            logger.error(f"Failed to resolve alerts: {e}")

    # ================================
    # 自动优化
    # ================================

    async def _run_automatic_optimization(self) -> None:
        """运行自动化性能优化"""
        try:
            # 检查优化机会
            optimization_opportunities = await self._identify_optimization_opportunities()

            for opportunity in optimization_opportunities:
                if await self._should_apply_optimization(opportunity):
                    result = await self._apply_optimization(opportunity)
                    if result:
                        with self._optimization_lock:
                            self._optimizations.append(result)

                        logger.info(f"Optimization applied: {result.description}")

        except Exception as e:
            logger.error(f"Automatic optimization failed: {e}")

    async def _identify_optimization_opportunities(self) -> List[Dict[str, Any]]:
        """识别优化机会"""
        opportunities = []

        try:
            with self._metrics_lock:
                # 检查内存使用优化机会
                if "memory_usage" in self._metrics and self._metrics["memory_usage"]:
                    recent_memory = [m.value for m in list(self._metrics["memory_usage"])[-10:]]
                    avg_memory = sum(recent_memory) / len(recent_memory)

                    if avg_memory > 80:
                        opportunities.append({
                            "type": "memory_optimization",
                            "metric": "memory_usage",
                            "current_value": avg_memory,
                            "strategy": "garbage_collection"
                        })

                # 检查CPU使用优化机会
                if "cpu_usage" in self._metrics and self._metrics["cpu_usage"]:
                    recent_cpu = [m.value for m in list(self._metrics["cpu_usage"])[-10:]]
                    avg_cpu = sum(recent_cpu) / len(recent_cpu)

                    if avg_cpu > 85:
                        opportunities.append({
                            "type": "cpu_optimization",
                            "metric": "cpu_usage",
                            "current_value": avg_cpu,
                            "strategy": "thread_optimization"
                        })

        except Exception as e:
            logger.error(f"Failed to identify optimization opportunities: {e}")

        return opportunities

    async def _should_apply_optimization(self, opportunity: Dict[str, Any]) -> bool:
        """判断是否应该应用优化"""
        try:
            # 根据优化策略决定
            if self._config.optimization_strategy == OptimizationStrategy.CONSERVATIVE:
                return opportunity["current_value"] > 90
            elif self._config.optimization_strategy == OptimizationStrategy.BALANCED:
                return opportunity["current_value"] > 80
            else:  # AGGRESSIVE
                return opportunity["current_value"] > 70

        except Exception as e:
            logger.error(f"Failed to determine optimization application: {e}")
            return False

    async def _apply_optimization(self, opportunity: Dict[str, Any]) -> Optional[OptimizationResult]:
        """应用性能优化"""
        try:
            optimization_id = f"opt_{int(time.time())}"
            metric_name = opportunity["metric"]
            before_value = opportunity["current_value"]

            if opportunity["strategy"] == "garbage_collection":
                # 执行垃圾回收优化
                collected = gc.collect()
                after_value = await self._get_current_metric_value(metric_name)

                if after_value is not None and after_value < before_value:
                    improvement = ((before_value - after_value) / before_value) * 100

                    return OptimizationResult(
                        optimization_id=optimization_id,
                        strategy=self._config.optimization_strategy,
                        target_metric=metric_name,
                        before_value=before_value,
                        after_value=after_value,
                        improvement_percent=improvement,
                        description=f"Garbage collection freed {collected} objects, reduced {metric_name} from {before_value:.1f}% to {after_value:.1f}%"
                    )

            elif opportunity["strategy"] == "thread_optimization":
                # 线程优化（简化实现）
                thread_count_before = threading.active_count()

                # 这里可以实现具体的线程优化逻辑
                # 目前只是记录

                return OptimizationResult(
                    optimization_id=optimization_id,
                    strategy=self._config.optimization_strategy,
                    target_metric=metric_name,
                    before_value=before_value,
                    after_value=before_value,  # 简化实现
                    improvement_percent=0,
                    description=f"Thread optimization analysis completed, {thread_count_before} threads active",
                    success=False
                )

        except Exception as e:
            logger.error(f"Failed to apply optimization: {e}")

        return None

    async def _get_current_metric_value(self, metric_name: str) -> Optional[float]:
        """获取当前指标值"""
        try:
            with self._metrics_lock:
                if metric_name in self._metrics and self._metrics[metric_name]:
                    return self._metrics[metric_name][-1].value
        except Exception as e:
            logger.error(f"Failed to get current metric value: {e}")

        return None

    # ================================
    # 数据管理
    # ================================

    async def _cleanup_old_data(self) -> None:
        """清理过期数据"""
        try:
            cutoff_time = datetime.now() - timedelta(days=self._config.retention_days)

            with self._metrics_lock:
                for metric_name, metric_deque in self._metrics.items():
                    # 清理过期指标
                    while metric_deque and metric_deque[0].timestamp < cutoff_time:
                        metric_deque.popleft()

            with self._alerts_lock:
                # 清理已解决的旧警报
                resolved_alerts = [
                    key for key, alert in self._alerts.items()
                    if alert.resolved and alert.timestamp < cutoff_time
                ]
                for key in resolved_alerts:
                    del self._alerts[key]

            logger.debug(f"Cleaned up data older than {self._config.retention_days} days")

        except Exception as e:
            logger.error(f"Data cleanup failed: {e}")

    # ================================
    # 公共API
    # ================================

    def get_metric_history(self, metric_name: str, hours: int = 24) -> List[PerformanceMetric]:
        """获取指标历史"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._metrics_lock:
            if metric_name in self._metrics:
                return [
                    metric for metric in self._metrics[metric_name]
                    if metric.timestamp >= cutoff_time
                ]

        return []

    def get_current_metrics(self) -> Dict[str, PerformanceMetric]:
        """获取当前指标"""
        current_metrics = {}

        with self._metrics_lock:
            for metric_name, metric_deque in self._metrics.items():
                if metric_deque:
                    current_metrics[metric_name] = metric_deque[-1]

        return current_metrics

    def get_active_alerts(self) -> List[PerformanceAlert]:
        """获取活跃警报"""
        with self._alerts_lock:
            return [alert for alert in self._alerts.values() if not alert.resolved]

    def get_optimization_history(self, hours: int = 24) -> List[OptimizationResult]:
        """获取优化历史"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._optimization_lock:
            return [
                opt for opt in self._optimizations
                if opt.timestamp >= cutoff_time
            ]

    def update_threshold(self, metric_name: str, threshold: PerformanceThreshold) -> None:
        """更新性能阈值"""
        self._thresholds[metric_name] = threshold
        logger.info(f"Updated threshold for {metric_name}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """获取性能摘要"""
        current_metrics = self.get_current_metrics()
        active_alerts = self.get_active_alerts()
        recent_optimizations = self.get_optimization_history(hours=1)

        return {
            "timestamp": datetime.now().isoformat(),
            "metrics_count": len(current_metrics),
            "active_alerts_count": len(active_alerts),
            "recent_optimizations_count": len(recent_optimizations),
            "monitoring_active": self._monitoring_active,
            "auto_optimization_enabled": self._config.auto_optimization_enabled,
            "key_metrics": {
                name: metric.value
                for name, metric in current_metrics.items()
                if name in ["cpu_usage", "memory_usage", "disk_usage"]
            }
        }

    def add_metric_callback(self, callback: Callable[[PerformanceMetric], None]) -> None:
        """添加指标回调"""
        self._metric_callbacks.append(callback)

    def add_alert_callback(self, callback: Callable[[PerformanceAlert], None]) -> None:
        """添加警报回调"""
        self._alert_callbacks.append(callback)

    def update_config(self, config: PerformanceConfig) -> None:
        """更新性能配置"""
        self._config = config
        logger.info("Performance configuration updated")

    def get_config(self) -> PerformanceConfig:
        """获取性能配置"""
        return self._config

    # ================================
    # 事件处理
    # ================================

    def _on_service_event(self, event_data: Dict[str, Any]) -> None:
        """处理服务事件"""
        try:
            service_name = event_data.get("service_name", "unknown")
            event_type = event_data.get("event_type", "unknown")

            # 记录服务事件指标
            asyncio.create_task(
                self.record_metric(f"service_events_{event_type}", MetricType.COUNTER, 1, tags={"service": service_name})
            )

        except Exception as e:
            logger.error(f"Service event handling failed: {e}")

    def _on_performance_metric(self, event_data: Dict[str, Any]) -> None:
        """处理性能指标事件"""
        try:
            metric_name = event_data.get("name")
            metric_value = event_data.get("value")
            metric_unit = event_data.get("unit", "")

            if metric_name and metric_value is not None:
                asyncio.create_task(
                    self.record_metric(f"external_{metric_name}", MetricType.GAUGE, metric_value, metric_unit)
                )

        except Exception as e:
            logger.error(f"Performance metric event handling failed: {e}")

    # ================================
    # 生命周期管理
    # ================================

    async def shutdown(self) -> None:
        """关闭服务"""
        try:
            logger.info("Shutting down UnifiedPerformanceService...")

            # 停止监控
            self._monitoring_active = False

            # 取消任务
            tasks_to_cancel = [
                self._monitor_task,
                self._optimization_task,
                self._cleanup_task
            ]

            for task in tasks_to_cancel:
                if task and not task.done():
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass

            # 关闭执行器
            self._executor.shutdown(wait=True)

            # 保存最终指标
            await self._save_performance_data()

            logger.info("UnifiedPerformanceService shutdown completed")

        except Exception as e:
            logger.error(f"Error during UnifiedPerformanceService shutdown: {e}")

    async def _save_performance_data(self) -> None:
        """保存性能数据"""
        try:
            # 保存关键指标到文件
            summary = self.get_performance_summary()

            save_path = "performance_summary.json"
            with open(save_path, 'w') as f:
                json.dump(summary, f, indent=2, default=str)

            logger.info("Performance data saved successfully")

        except Exception as e:
            logger.error(f"Failed to save performance data: {e}")


# ================================
# 服务工厂函数
# ================================

_unified_performance_service: Optional[UnifiedPerformanceService] = None


def get_unified_performance_service(
    service_container: Optional[ServiceContainer] = None,
    event_bus: Optional[EventBus] = None
) -> UnifiedPerformanceService:
    """获取统一性能服务实例"""
    global _unified_performance_service

    if _unified_performance_service is None:
        _unified_performance_service = UnifiedPerformanceService(
            service_container=service_container,
            event_bus=event_bus
        )

    return _unified_performance_service
