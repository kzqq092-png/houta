"""
统一数据服务模块

整合所有数据管理器功能，提供单一的数据访问接口。
将原有的UnifiedDataManager、UniPluginDataManager等整合为统一服务。
"""

import asyncio
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Callable, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, Future
import pandas as pd
import numpy as np
import sqlite3
import os
import traceback

from loguru import logger

from .base_service import BaseService
from ..events import EventBus, DataUpdateEvent, get_event_bus
from ..containers import ServiceContainer, get_service_container
from ..plugin_types import AssetType, DataType, PluginType
from ..data_source_extensions import IDataSourcePlugin, PluginInfo, HealthCheckResult
from ..tet_data_pipeline import TETDataPipeline, StandardQuery, StandardData
from ..data_source_router import DataSourceRouter, RoutingStrategy
from ..plugin_manager import PluginManager
from ..plugin_center import PluginCenter
from ..tet_router_engine import TETRouterEngine
from ..data_quality_risk_manager import DataQualityRiskManager


class DataAccessMode(Enum):
    """数据访问模式"""
    REAL_TIME = "real_time"
    CACHED = "cached"
    HYBRID = "hybrid"
    FALLBACK = "fallback"


class DataServiceStatus(Enum):
    """数据服务状态"""
    INITIALIZING = "initializing"
    READY = "ready"
    DEGRADED = "degraded"
    OFFLINE = "offline"


@dataclass
class DataRequest:
    """数据请求"""
    asset_type: AssetType
    data_type: DataType
    symbol: Optional[str] = None
    market: Optional[str] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    priority: int = 50
    quality_requirement: float = 0.8
    timeout: int = 30
    access_mode: DataAccessMode = DataAccessMode.HYBRID
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class DataResponse:
    """数据响应"""
    request_id: str
    success: bool
    data: Optional[Any] = None
    error_message: Optional[str] = None
    data_source: Optional[str] = None
    quality_score: float = 0.0
    cached: bool = False
    latency_ms: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ServiceMetrics:
    """服务指标"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    average_latency_ms: float = 0.0
    active_connections: int = 0
    data_sources_available: int = 0
    last_update: datetime = field(default_factory=datetime.now)


class UnifiedDataService(BaseService):
    """
    统一数据服务

    整合所有数据管理功能，提供统一的数据访问接口。
    替代原有的多个数据管理器，实现真正的数据管理统一化。
    """

    def __init__(self,
                 service_container: Optional[ServiceContainer] = None,
                 event_bus: Optional[EventBus] = None):
        """
        初始化统一数据服务

        Args:
            service_container: 服务容器
            event_bus: 事件总线
        """
        super().__init__(event_bus)

        self._service_container = service_container or get_service_container()

        # 核心组件
        self._data_pipeline: Optional[TETDataPipeline] = None
        self._data_router: Optional[DataSourceRouter] = None
        self._plugin_manager: Optional[PluginManager] = None
        self._plugin_center: Optional[PluginCenter] = None
        self._quality_manager: Optional[DataQualityRiskManager] = None

        # 状态管理
        self._status = DataServiceStatus.INITIALIZING
        self._metrics = ServiceMetrics()
        self._lock = threading.RLock()

        # 缓存系统
        self._cache: Dict[str, Any] = {}
        self._cache_metadata: Dict[str, datetime] = {}
        self._cache_ttl = timedelta(minutes=5)  # 默认5分钟缓存

        # 连接池和线程池
        self._thread_pool = ThreadPoolExecutor(max_workers=20, thread_name_prefix="UnifiedDataService")
        self._active_requests: Dict[str, DataRequest] = {}

        # 数据源管理
        self._available_sources: Set[str] = set()
        self._source_health: Dict[str, bool] = {}

        # 订阅者管理
        self._data_subscribers: Dict[str, List[Callable]] = {}

        logger.info("Unified data service initialized")

    def initialize(self) -> None:
        """初始化服务"""
        if self._initialized:
            return

        try:
            with self._lock:
                logger.info("Initializing unified data service...")

                # 初始化核心组件
                self._initialize_core_components()

                # 初始化数据源
                self._initialize_data_sources()

                # 启动后台任务
                self._start_background_tasks()

                self._status = DataServiceStatus.READY
                self._initialized = True

                logger.info("✅ Unified data service initialized successfully")

        except Exception as e:
            self._status = DataServiceStatus.OFFLINE
            logger.error(f"Failed to initialize unified data service: {e}")
            logger.error(traceback.format_exc())
            raise

    def _initialize_core_components(self) -> None:
        """初始化核心组件"""
        try:
            # 先初始化数据源路由器
            if self._service_container.is_registered(DataSourceRouter):
                self._data_router = self._service_container.resolve(DataSourceRouter)
            else:
                self._data_router = DataSourceRouter()

            # 使用路由器初始化TET数据管道
            self._data_pipeline = TETDataPipeline(self._data_router)

            # 初始化插件管理器
            if self._service_container.is_registered(PluginManager):
                self._plugin_manager = self._service_container.resolve(PluginManager)
                # 使用插件管理器初始化插件中心
                self._plugin_center = PluginCenter(self._plugin_manager)
            else:
                # 创建新的插件管理器
                self._plugin_manager = PluginManager(
                    plugin_dir="plugins",
                    main_window=None,
                    data_manager=None
                )
                self._plugin_center = PluginCenter(self._plugin_manager)

            # 初始化数据质量管理器
            self._quality_manager = DataQualityRiskManager()

            logger.info("Core components initialized")

        except Exception as e:
            logger.error(f"Failed to initialize core components: {e}")
            raise

    def _initialize_data_sources(self) -> None:
        """初始化数据源"""
        try:
            # 发现可用数据源
            if self._plugin_manager:
                plugins = self._plugin_manager.get_all_enhanced_plugins()
                for plugin_name, plugin_info in plugins.items():
                    if hasattr(plugin_info, 'plugin_type') and plugin_info.plugin_type == PluginType.DATA_SOURCE:
                        self._available_sources.add(plugin_name)
                        self._source_health[plugin_name] = True

            logger.info(f"Initialized {len(self._available_sources)} data sources")

        except Exception as e:
            logger.error(f"Failed to initialize data sources: {e}")

    def _start_background_tasks(self) -> None:
        """启动后台任务"""
        try:
            # 启动缓存清理任务
            threading.Thread(
                target=self._cache_cleanup_task,
                name="DataServiceCacheCleanup",
                daemon=True
            ).start()

            # 启动健康检查任务
            threading.Thread(
                target=self._health_check_task,
                name="DataServiceHealthCheck",
                daemon=True
            ).start()

            logger.info("Background tasks started")

        except Exception as e:
            logger.error(f"Failed to start background tasks: {e}")

    async def get_data(self, request: DataRequest) -> DataResponse:
        """
        获取数据（异步）

        Args:
            request: 数据请求

        Returns:
            数据响应
        """
        start_time = time.time()
        request_id = request.request_id or f"req_{int(time.time() * 1000)}"

        try:
            with self._lock:
                self._active_requests[request_id] = request
                self._metrics.total_requests += 1

            # 检查缓存
            if request.access_mode in [DataAccessMode.CACHED, DataAccessMode.HYBRID]:
                cached_data = self._get_cached_data(request)
                if cached_data is not None:
                    self._metrics.cache_hits += 1
                    latency_ms = (time.time() - start_time) * 1000

                    return DataResponse(
                        request_id=request_id,
                        success=True,
                        data=cached_data,
                        cached=True,
                        latency_ms=latency_ms,
                        quality_score=1.0
                    )
                else:
                    self._metrics.cache_misses += 1

            # 从数据源获取数据
            data_response = await self._fetch_from_sources(request)

            # 缓存数据
            if data_response.success and data_response.data is not None:
                self._cache_data(request, data_response.data)
                self._metrics.successful_requests += 1
            else:
                self._metrics.failed_requests += 1

            # 更新延迟
            latency_ms = (time.time() - start_time) * 1000
            data_response.latency_ms = latency_ms
            data_response.request_id = request_id

            return data_response

        except Exception as e:
            self._metrics.failed_requests += 1
            logger.error(f"Error processing data request {request_id}: {e}")

            return DataResponse(
                request_id=request_id,
                success=False,
                error_message=str(e),
                latency_ms=(time.time() - start_time) * 1000
            )

        finally:
            with self._lock:
                if request_id in self._active_requests:
                    del self._active_requests[request_id]

    def get_data_sync(self, request: DataRequest) -> DataResponse:
        """
        获取数据（同步）

        Args:
            request: 数据请求

        Returns:
            数据响应
        """
        try:
            # 创建新的事件循环（如果当前线程没有）
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # 如果循环正在运行，使用线程池执行
                    future = asyncio.run_coroutine_threadsafe(
                        self.get_data(request), loop
                    )
                    return future.result(timeout=request.timeout)
                else:
                    return loop.run_until_complete(self.get_data(request))
            except RuntimeError:
                # 没有事件循环，创建新的
                return asyncio.run(self.get_data(request))

        except Exception as e:
            logger.error(f"Error in sync data request: {e}")
            return DataResponse(
                request_id=request.request_id or "sync_req",
                success=False,
                error_message=str(e)
            )

    async def _fetch_from_sources(self, request: DataRequest) -> DataResponse:
        """从数据源获取数据"""
        try:
            # 使用TET数据管道
            if self._data_pipeline:
                query = StandardQuery(
                    asset_type=request.asset_type,
                    data_type=request.data_type,
                    symbol=request.symbol,
                    market=request.market,
                    start_date=request.start_date,
                    end_date=request.end_date,
                    parameters=request.parameters
                )

                result = await self._data_pipeline.execute_query(query)
                if result.success:
                    return DataResponse(
                        request_id=request.request_id or "",
                        success=True,
                        data=result.data,
                        data_source=result.data_source,
                        quality_score=result.quality_score or 0.8
                    )

            # 降级处理：使用数据路由器
            if self._data_router:
                # 这里需要适配数据路由器的接口
                # 暂时返回空数据
                pass

            # 最后的降级：直接从插件获取
            if self._plugin_manager:
                # 获取合适的数据源插件
                suitable_plugins = self._find_suitable_plugins(request)

                for plugin_info in suitable_plugins:
                    try:
                        plugin = self._plugin_manager.get_plugin(plugin_info.name)
                        if plugin and hasattr(plugin, 'get_data'):
                            data = await plugin.get_data(request)
                            if data is not None:
                                return DataResponse(
                                    request_id=request.request_id or "",
                                    success=True,
                                    data=data,
                                    data_source=plugin_info.name,
                                    quality_score=0.7
                                )
                    except Exception as e:
                        logger.warning(f"Plugin {plugin_info.name} failed: {e}")
                        continue

            # 所有数据源都失败
            return DataResponse(
                request_id=request.request_id or "",
                success=False,
                error_message="All data sources failed"
            )

        except Exception as e:
            logger.error(f"Error fetching from sources: {e}")
            return DataResponse(
                request_id=request.request_id or "",
                success=False,
                error_message=str(e)
            )

    def _find_suitable_plugins(self, request: DataRequest) -> List[PluginInfo]:
        """查找合适的插件"""
        suitable_plugins = []

        if not self._plugin_manager:
            return suitable_plugins

        try:
            available_plugins = self._plugin_manager.get_available_plugins()

            for plugin_info in available_plugins:
                if plugin_info.plugin_type == PluginType.DATA_SOURCE:
                    # 检查插件是否支持请求的数据类型
                    if self._plugin_supports_request(plugin_info, request):
                        suitable_plugins.append(plugin_info)

            # 按优先级排序
            suitable_plugins.sort(key=lambda p: getattr(p, 'priority', 50))

        except Exception as e:
            logger.error(f"Error finding suitable plugins: {e}")

        return suitable_plugins

    def _plugin_supports_request(self, plugin_info: PluginInfo, request: DataRequest) -> bool:
        """检查插件是否支持请求"""
        try:
            # 检查支持的资产类型
            if hasattr(plugin_info, 'supported_assets'):
                if request.asset_type not in plugin_info.supported_assets:
                    return False

            # 检查支持的数据类型
            if hasattr(plugin_info, 'supported_data_types'):
                if request.data_type not in plugin_info.supported_data_types:
                    return False

            # 检查市场支持
            if request.market and hasattr(plugin_info, 'supported_markets'):
                if request.market not in plugin_info.supported_markets:
                    return False

            return True

        except Exception as e:
            logger.error(f"Error checking plugin support: {e}")
            return False

    def _get_cached_data(self, request: DataRequest) -> Optional[Any]:
        """获取缓存数据"""
        try:
            cache_key = self._generate_cache_key(request)

            if cache_key in self._cache:
                # 检查缓存是否过期
                cache_time = self._cache_metadata.get(cache_key)
                if cache_time and datetime.now() - cache_time < self._cache_ttl:
                    return self._cache[cache_key]
                else:
                    # 清除过期缓存
                    del self._cache[cache_key]
                    del self._cache_metadata[cache_key]

            return None

        except Exception as e:
            logger.error(f"Error getting cached data: {e}")
            return None

    def _cache_data(self, request: DataRequest, data: Any) -> None:
        """缓存数据"""
        try:
            cache_key = self._generate_cache_key(request)
            self._cache[cache_key] = data
            self._cache_metadata[cache_key] = datetime.now()

        except Exception as e:
            logger.error(f"Error caching data: {e}")

    def _generate_cache_key(self, request: DataRequest) -> str:
        """生成缓存键"""
        key_parts = [
            str(request.asset_type.value),
            str(request.data_type.value),
            request.symbol or "ALL",
            request.market or "ALL"
        ]

        if request.start_date:
            key_parts.append(request.start_date.isoformat())
        if request.end_date:
            key_parts.append(request.end_date.isoformat())

        # 添加参数哈希
        if request.parameters:
            import hashlib
            params_str = str(sorted(request.parameters.items()))
            params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
            key_parts.append(params_hash)

        return ":".join(key_parts)

    def _cache_cleanup_task(self) -> None:
        """缓存清理任务"""
        while not self._disposed:
            try:
                current_time = datetime.now()
                expired_keys = []

                for cache_key, cache_time in self._cache_metadata.items():
                    if current_time - cache_time > self._cache_ttl:
                        expired_keys.append(cache_key)

                for key in expired_keys:
                    if key in self._cache:
                        del self._cache[key]
                    if key in self._cache_metadata:
                        del self._cache_metadata[key]

                if expired_keys:
                    logger.debug(f"Cleaned up {len(expired_keys)} expired cache entries")

                time.sleep(60)  # 每分钟清理一次

            except Exception as e:
                logger.error(f"Error in cache cleanup task: {e}")
                time.sleep(60)

    def _health_check_task(self) -> None:
        """健康检查任务"""
        while not self._disposed:
            try:
                # 检查数据源健康状态
                healthy_sources = 0
                total_sources = len(self._available_sources)

                for source_name in self._available_sources:
                    try:
                        # 这里可以实现具体的健康检查逻辑
                        self._source_health[source_name] = True
                        healthy_sources += 1
                    except Exception as e:
                        self._source_health[source_name] = False
                        logger.warning(f"Data source {source_name} health check failed: {e}")

                # 更新服务状态
                if total_sources > 0:
                    health_ratio = healthy_sources / total_sources
                    if health_ratio >= 0.8:
                        self._status = DataServiceStatus.READY
                    elif health_ratio >= 0.5:
                        self._status = DataServiceStatus.DEGRADED
                    else:
                        self._status = DataServiceStatus.OFFLINE

                # 更新指标
                self._metrics.data_sources_available = healthy_sources
                self._metrics.active_connections = len(self._active_requests)
                self._metrics.last_update = datetime.now()

                time.sleep(30)  # 每30秒检查一次

            except Exception as e:
                logger.error(f"Error in health check task: {e}")
                time.sleep(30)

    def get_service_status(self) -> Dict[str, Any]:
        """获取服务状态"""
        return {
            'status': self._status.value,
            'metrics': {
                'total_requests': self._metrics.total_requests,
                'successful_requests': self._metrics.successful_requests,
                'failed_requests': self._metrics.failed_requests,
                'success_rate': (
                    self._metrics.successful_requests / max(self._metrics.total_requests, 1)
                ) * 100,
                'cache_hits': self._metrics.cache_hits,
                'cache_misses': self._metrics.cache_misses,
                'cache_hit_rate': (
                    self._metrics.cache_hits / max(self._metrics.cache_hits + self._metrics.cache_misses, 1)
                ) * 100,
                'average_latency_ms': self._metrics.average_latency_ms,
                'active_connections': self._metrics.active_connections,
                'data_sources_available': self._metrics.data_sources_available,
                'cache_entries': len(self._cache),
                'last_update': self._metrics.last_update.isoformat()
            },
            'data_sources': {
                name: {'healthy': healthy}
                for name, healthy in self._source_health.items()
            }
        }

    def clear_cache(self, pattern: Optional[str] = None) -> int:
        """
        清除缓存

        Args:
            pattern: 缓存键模式，为None时清除所有缓存

        Returns:
            清除的缓存条目数量
        """
        try:
            if pattern is None:
                count = len(self._cache)
                self._cache.clear()
                self._cache_metadata.clear()
                logger.info(f"Cleared all {count} cache entries")
                return count
            else:
                import re
                regex = re.compile(pattern)
                keys_to_remove = [
                    key for key in self._cache.keys()
                    if regex.search(key)
                ]

                for key in keys_to_remove:
                    if key in self._cache:
                        del self._cache[key]
                    if key in self._cache_metadata:
                        del self._cache_metadata[key]

                logger.info(f"Cleared {len(keys_to_remove)} cache entries matching pattern: {pattern}")
                return len(keys_to_remove)

        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
            return 0

    def subscribe_to_updates(self, data_type: str, callback: Callable[[Any], None]) -> None:
        """
        订阅数据更新

        Args:
            data_type: 数据类型
            callback: 回调函数
        """
        if data_type not in self._data_subscribers:
            self._data_subscribers[data_type] = []

        self._data_subscribers[data_type].append(callback)
        logger.info(f"Added subscriber for {data_type} data updates")

    def unsubscribe_from_updates(self, data_type: str, callback: Callable[[Any], None]) -> None:
        """
        取消订阅数据更新

        Args:
            data_type: 数据类型
            callback: 回调函数
        """
        if data_type in self._data_subscribers:
            try:
                self._data_subscribers[data_type].remove(callback)
                logger.info(f"Removed subscriber for {data_type} data updates")
            except ValueError:
                logger.warning(f"Callback not found in {data_type} subscribers")

    def _notify_subscribers(self, data_type: str, data: Any) -> None:
        """通知订阅者"""
        try:
            subscribers = self._data_subscribers.get(data_type, [])
            for callback in subscribers:
                try:
                    callback(data)
                except Exception as e:
                    logger.error(f"Error in subscriber callback: {e}")

        except Exception as e:
            logger.error(f"Error notifying subscribers: {e}")

    def dispose(self) -> None:
        """释放服务资源"""
        logger.info("Disposing unified data service...")

        # 停止后台任务
        self._disposed = True

        # 关闭线程池
        if hasattr(self, '_thread_pool'):
            self._thread_pool.shutdown(wait=True)

        # 清理缓存
        self._cache.clear()
        self._cache_metadata.clear()

        # 清理订阅者
        self._data_subscribers.clear()

        super().dispose()
        logger.info("Unified data service disposed")


# 全局实例管理
_unified_data_service: Optional[UnifiedDataService] = None
_service_lock = threading.Lock()


def get_unified_data_service() -> UnifiedDataService:
    """获取全局统一数据服务实例"""
    global _unified_data_service
    if _unified_data_service is None:
        with _service_lock:
            if _unified_data_service is None:
                _unified_data_service = UnifiedDataService()
    return _unified_data_service


def set_unified_data_service(service: UnifiedDataService) -> None:
    """设置全局统一数据服务实例"""
    global _unified_data_service
    with _service_lock:
        _unified_data_service = service
