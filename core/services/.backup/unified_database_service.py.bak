"""
统一数据库服务

整合所有数据库管理器为单一服务，提供：
- 连接池管理
- 事务管理
- 查询优化
- 资源管理
- 性能监控
"""

import asyncio
import threading
import time
from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Type, Union, Callable
import sqlite3
import duckdb
from loguru import logger

from ..database.duckdb_manager import DuckDBConnectionManager
from ..database.duckdb_operations import DuckDBOperations
from ..database.sqlite_extensions import SQLiteExtensionManager
from ..database.duckdb_performance_optimizer import (
    DuckDBPerformanceOptimizer, WorkloadType, DuckDBConfig
)
from ..asset_database_manager import AssetSeparatedDatabaseManager
from ..enhanced_asset_database_manager import EnhancedAssetDatabaseManager
from ..database.factorweave_analytics_db import FactorWeaveAnalyticsDB
from optimization.database_schema import OptimizationDatabaseManager
from core.strategy.strategy_database import StrategyDatabaseManager
from ..services.base_service import BaseService
from ..events import EventBus, get_event_bus
from ..containers import ServiceContainer, get_service_container
from ..plugin_types import AssetType


class DatabaseType(Enum):
    """数据库类型"""
    DUCKDB = "duckdb"
    SQLITE = "sqlite"


class ConnectionPoolType(Enum):
    """连接池类型"""
    SHARED = "shared"  # 共享连接池
    DEDICATED = "dedicated"  # 专用连接池
    ASSET_SPECIFIC = "asset_specific"  # 资产特定连接池


@dataclass
class DatabaseConfig:
    """数据库配置"""
    db_type: DatabaseType
    db_path: str
    pool_size: int = 5
    pool_type: ConnectionPoolType = ConnectionPoolType.SHARED
    workload_type: WorkloadType = WorkloadType.MIXED
    timeout: float = 30.0
    enable_wal: bool = True
    auto_vacuum: bool = True
    journal_mode: str = "WAL"
    synchronous: str = "NORMAL"
    cache_size: int = -64000  # 64MB in pages


@dataclass
class ConnectionMetrics:
    """连接指标"""
    pool_name: str
    active_connections: int = 0
    idle_connections: int = 0
    total_connections: int = 0
    connection_requests: int = 0
    connection_timeouts: int = 0
    avg_response_time_ms: float = 0.0
    peak_connections: int = 0
    last_activity: Optional[datetime] = None


@dataclass
class TransactionMetrics:
    """事务指标"""
    transaction_id: str
    database_path: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: str = "ACTIVE"  # ACTIVE, COMMITTED, ROLLED_BACK
    operations_count: int = 0
    affected_rows: int = 0
    duration_ms: Optional[float] = None


@dataclass
class QueryMetrics:
    """查询指标"""
    query_id: str
    database_path: str
    sql: str
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_ms: Optional[float] = None
    rows_affected: int = 0
    cache_hit: bool = False
    error: Optional[str] = None


class UnifiedDatabaseService(BaseService):
    """
    统一数据库服务

    整合所有数据库管理器，提供统一接口：
    - DuckDB和SQLite连接管理
    - 连接池优化
    - 事务管理
    - 查询路由和优化
    - 性能监控和指标收集
    """

    def __init__(self,
                 service_container: Optional[ServiceContainer] = None,
                 event_bus: Optional[EventBus] = None):
        """
        初始化统一数据库服务

        Args:
            service_container: 服务容器
            event_bus: 事件总线
        """
        super().__init__(event_bus)

        self._service_container = service_container or get_service_container()

        # 核心组件
        self._duckdb_manager: Optional[DuckDBConnectionManager] = None
        self._duckdb_operations: Optional[DuckDBOperations] = None
        self._sqlite_manager: Optional[SQLiteExtensionManager] = None
        self._asset_db_manager: Optional[AssetSeparatedDatabaseManager] = None
        self._enhanced_asset_manager: Optional[EnhancedAssetDatabaseManager] = None
        self._analytics_db: Optional[FactorWeaveAnalyticsDB] = None
        self._optimization_db: Optional[OptimizationDatabaseManager] = None
        self._strategy_db: Optional[StrategyDatabaseManager] = None

        # 连接池管理
        self._connection_pools: Dict[str, Any] = {}
        self._pool_configs: Dict[str, DatabaseConfig] = {}
        self._pool_metrics: Dict[str, ConnectionMetrics] = {}

        # 性能优化器
        self._performance_optimizers: Dict[str, DuckDBPerformanceOptimizer] = {}

        # 事务管理
        self._active_transactions: Dict[str, TransactionMetrics] = {}
        self._transaction_counter = 0

        # 查询缓存和指标
        self._query_cache: Dict[str, Any] = {}
        self._query_metrics: Dict[str, QueryMetrics] = {}
        self._query_counter = 0

        # 线程锁
        self._pool_lock = threading.RLock()
        self._transaction_lock = threading.RLock()
        self._cache_lock = threading.RLock()

        # 后台任务
        self._monitoring_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None

        logger.info("Unified database service initialized")

    async def initialize(self) -> None:
        """初始化统一数据库服务"""
        try:
            logger.info("Initializing unified database service...")

            # 初始化核心数据库管理器
            await self._initialize_core_managers()

            # 配置连接池
            await self._setup_connection_pools()

            # 启动性能优化
            await self._setup_performance_optimization()

            # 启动后台监控
            await self._start_background_monitoring()

            self._initialized = True
            logger.info("✅ Unified database service initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize unified database service: {e}")
            raise

    async def _initialize_core_managers(self) -> None:
        """初始化核心数据库管理器"""
        try:
            # DuckDB管理器
            self._duckdb_manager = DuckDBConnectionManager()

            # 创建主数据库连接池
            await asyncio.to_thread(
                self._duckdb_manager.get_pool,
                'db/factorweave_analytics.duckdb',
                pool_size=10
            )

            # DuckDB操作管理器
            self._duckdb_operations = DuckDBOperations(self._duckdb_manager)

            # SQLite扩展管理器
            self._sqlite_manager = SQLiteExtensionManager(
                db_path='db/factorweave_system.sqlite'
            )

            # 资产数据库管理器
            self._asset_db_manager = AssetSeparatedDatabaseManager.get_instance()

            # 增强资产数据库管理器
            try:
                self._enhanced_asset_manager = EnhancedAssetDatabaseManager()
            except Exception as e:
                logger.warning(f"Enhanced asset manager initialization failed: {e}")
                self._enhanced_asset_manager = None

            # 分析数据库
            try:
                self._analytics_db = FactorWeaveAnalyticsDB()
            except Exception as e:
                logger.warning(f"Analytics DB initialization failed: {e}")
                self._analytics_db = None

            # 优化数据库管理器
            try:
                self._optimization_db = OptimizationDatabaseManager()
            except Exception as e:
                logger.warning(f"Optimization DB initialization failed: {e}")
                self._optimization_db = None

            # 策略数据库管理器
            try:
                self._strategy_db = StrategyDatabaseManager()
            except Exception as e:
                logger.warning(f"Strategy DB initialization failed: {e}")
                self._strategy_db = None

            logger.info("Core database managers initialized")

        except Exception as e:
            logger.error(f"Failed to initialize core managers: {e}")
            raise

    async def _setup_connection_pools(self) -> None:
        """设置连接池"""
        try:
            # 主要DuckDB连接池
            main_config = DatabaseConfig(
                db_type=DatabaseType.DUCKDB,
                db_path='db/factorweave_analytics.duckdb',
                pool_size=10,
                pool_type=ConnectionPoolType.SHARED,
                workload_type=WorkloadType.MIXED
            )
            await self._create_connection_pool('main_duckdb', main_config)

            # SQLite系统数据库连接池
            sqlite_config = DatabaseConfig(
                db_type=DatabaseType.SQLITE,
                db_path='db/factorweave_system.sqlite',
                pool_size=5,
                pool_type=ConnectionPoolType.SHARED
            )
            await self._create_connection_pool('system_sqlite', sqlite_config)

            # 资产特定连接池
            for asset_type in AssetType:
                asset_config = DatabaseConfig(
                    db_type=DatabaseType.DUCKDB,
                    db_path=f'db/assets/{asset_type.name.lower()}_data.duckdb',
                    pool_size=3,
                    pool_type=ConnectionPoolType.ASSET_SPECIFIC,
                    workload_type=WorkloadType.OLAP
                )
                await self._create_connection_pool(f'asset_{asset_type.name.lower()}', asset_config)

            logger.info(f"Connection pools setup complete: {len(self._connection_pools)} pools")

        except Exception as e:
            logger.error(f"Failed to setup connection pools: {e}")
            raise

    async def _create_connection_pool(self, pool_name: str, config: DatabaseConfig) -> None:
        """创建连接池"""
        try:
            with self._pool_lock:
                if config.db_type == DatabaseType.DUCKDB:
                    # 创建DuckDB连接池
                    optimizer = DuckDBPerformanceOptimizer(config.db_path)
                    optimizer.optimize_for_workload(config.workload_type)

                    self._connection_pools[pool_name] = optimizer
                    self._performance_optimizers[pool_name] = optimizer

                elif config.db_type == DatabaseType.SQLITE:
                    # SQLite连接池（简单实现）
                    self._connection_pools[pool_name] = {
                        'db_path': config.db_path,
                        'config': config
                    }

                self._pool_configs[pool_name] = config
                self._pool_metrics[pool_name] = ConnectionMetrics(
                    pool_name=pool_name,
                    total_connections=config.pool_size
                )

                logger.info(f"Connection pool '{pool_name}' created for {config.db_type.value}")

        except Exception as e:
            logger.error(f"Failed to create connection pool '{pool_name}': {e}")
            raise

    async def _setup_performance_optimization(self) -> None:
        """设置性能优化"""
        try:
            # 为每个DuckDB连接池应用性能优化
            for pool_name, optimizer in self._performance_optimizers.items():
                config = self._pool_configs[pool_name]
                await asyncio.to_thread(optimizer.optimize_for_workload, config.workload_type)

                # 获取性能建议
                recommendations = await asyncio.to_thread(optimizer.get_performance_recommendations)
                if recommendations:
                    logger.info(f"Performance recommendations for {pool_name}: {recommendations}")

            logger.info("Performance optimization setup complete")

        except Exception as e:
            logger.error(f"Failed to setup performance optimization: {e}")
            raise

    async def _start_background_monitoring(self) -> None:
        """启动后台监控"""
        try:
            # 连接池监控
            self._monitoring_task = asyncio.create_task(self._monitor_connection_pools())

            # 清理任务
            self._cleanup_task = asyncio.create_task(self._cleanup_resources())

            logger.info("Background monitoring started")

        except Exception as e:
            logger.error(f"Failed to start background monitoring: {e}")
            raise

    async def _monitor_connection_pools(self) -> None:
        """监控连接池"""
        while True:
            try:
                await asyncio.sleep(30)  # 每30秒监控一次

                with self._pool_lock:
                    for pool_name, metrics in self._pool_metrics.items():
                        # 更新连接池指标
                        metrics.last_activity = datetime.now()

                        # 记录峰值
                        if metrics.active_connections > metrics.peak_connections:
                            metrics.peak_connections = metrics.active_connections

                        # 检查连接池健康状态
                        if metrics.connection_timeouts > 10:
                            logger.warning(f"Connection pool '{pool_name}' has high timeout rate: {metrics.connection_timeouts}")

                        if metrics.active_connections > metrics.total_connections * 0.9:
                            logger.warning(f"Connection pool '{pool_name}' is near capacity: {metrics.active_connections}/{metrics.total_connections}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in connection pool monitoring: {e}")

    async def _cleanup_resources(self) -> None:
        """清理资源"""
        while True:
            try:
                await asyncio.sleep(300)  # 每5分钟清理一次

                # 清理过期的查询缓存
                cutoff_time = datetime.now() - timedelta(hours=1)
                with self._cache_lock:
                    expired_queries = [
                        query_id for query_id, metrics in self._query_metrics.items()
                        if metrics.end_time and metrics.end_time < cutoff_time
                    ]

                    for query_id in expired_queries:
                        if query_id in self._query_metrics:
                            del self._query_metrics[query_id]
                        if query_id in self._query_cache:
                            del self._query_cache[query_id]

                # 清理完成的事务记录
                cutoff_time = datetime.now() - timedelta(hours=6)
                with self._transaction_lock:
                    expired_transactions = [
                        tx_id for tx_id, metrics in self._active_transactions.items()
                        if metrics.end_time and metrics.end_time < cutoff_time
                    ]

                    for tx_id in expired_transactions:
                        del self._active_transactions[tx_id]

                logger.debug(f"Cleanup completed: removed {len(expired_queries)} query records, {len(expired_transactions)} transaction records")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in resource cleanup: {e}")

    @contextmanager
    def get_connection(self, pool_name: str = 'main_duckdb'):
        """
        获取数据库连接

        Args:
            pool_name: 连接池名称

        Yields:
            数据库连接对象
        """
        connection = None
        start_time = time.time()

        try:
            with self._pool_lock:
                if pool_name not in self._connection_pools:
                    raise ValueError(f"Connection pool '{pool_name}' not found")

                config = self._pool_configs[pool_name]
                metrics = self._pool_metrics[pool_name]

                # 更新指标
                metrics.connection_requests += 1
                metrics.active_connections += 1

                if config.db_type == DatabaseType.DUCKDB:
                    optimizer = self._connection_pools[pool_name]
                    connection = optimizer.get_connection()

                elif config.db_type == DatabaseType.SQLITE:
                    pool_info = self._connection_pools[pool_name]
                    connection = sqlite3.connect(
                        pool_info['db_path'],
                        timeout=config.timeout
                    )
                    connection.row_factory = sqlite3.Row

                response_time = (time.time() - start_time) * 1000
                metrics.avg_response_time_ms = (
                    (metrics.avg_response_time_ms * (metrics.connection_requests - 1) + response_time)
                    / metrics.connection_requests
                )

            yield connection

        except Exception as e:
            with self._pool_lock:
                self._pool_metrics[pool_name].connection_timeouts += 1
            logger.error(f"Failed to get connection from pool '{pool_name}': {e}")
            raise

        finally:
            if connection:
                try:
                    if config.db_type == DatabaseType.SQLITE:
                        connection.close()
                except Exception as e:
                    logger.error(f"Error closing connection: {e}")

            with self._pool_lock:
                if pool_name in self._pool_metrics:
                    self._pool_metrics[pool_name].active_connections -= 1

    @contextmanager
    def transaction(self, pool_name: str = 'main_duckdb', isolation_level: str = 'READ_COMMITTED'):
        """
        事务上下文管理器

        Args:
            pool_name: 连接池名称
            isolation_level: 事务隔离级别

        Yields:
            事务连接对象
        """
        transaction_id = f"tx_{self._transaction_counter}"
        self._transaction_counter += 1

        start_time = datetime.now()
        metrics = TransactionMetrics(
            transaction_id=transaction_id,
            database_path=self._pool_configs[pool_name].db_path,
            start_time=start_time
        )

        with self._transaction_lock:
            self._active_transactions[transaction_id] = metrics

        try:
            with self.get_connection(pool_name) as conn:
                # 开始事务
                if isinstance(conn, duckdb.DuckDBPyConnection):
                    conn.execute("BEGIN TRANSACTION")
                else:  # SQLite
                    conn.execute("BEGIN")

                logger.debug(f"Transaction {transaction_id} started")

                yield conn, transaction_id

                # 提交事务
                conn.execute("COMMIT")
                metrics.status = "COMMITTED"
                metrics.end_time = datetime.now()
                metrics.duration_ms = (metrics.end_time - start_time).total_seconds() * 1000

                logger.debug(f"Transaction {transaction_id} committed")

        except Exception as e:
            try:
                # 回滚事务
                if 'conn' in locals() and conn:
                    conn.execute("ROLLBACK")
                metrics.status = "ROLLED_BACK"
                metrics.end_time = datetime.now()
                metrics.duration_ms = (metrics.end_time - start_time).total_seconds() * 1000

                logger.error(f"Transaction {transaction_id} rolled back: {e}")
            except Exception as rollback_error:
                logger.error(f"Failed to rollback transaction {transaction_id}: {rollback_error}")

            raise

    async def execute_query(self,
                            sql: str,
                            params: Optional[List[Any]] = None,
                            pool_name: str = 'main_duckdb',
                            cache_ttl: Optional[int] = None) -> Any:
        """
        执行查询

        Args:
            sql: SQL语句
            params: 参数列表
            pool_name: 连接池名称
            cache_ttl: 缓存TTL（秒）

        Returns:
            查询结果
        """
        query_id = f"query_{self._query_counter}"
        self._query_counter += 1

        start_time = datetime.now()
        cache_hit = False

        # 检查缓存
        cache_key = f"{sql}:{params}" if params else sql
        if cache_ttl and cache_key in self._query_cache:
            cached_result, cached_time = self._query_cache[cache_key]
            if (datetime.now() - cached_time).total_seconds() < cache_ttl:
                cache_hit = True
                result = cached_result
                logger.debug(f"Cache hit for query {query_id}")
            else:
                # 缓存过期，删除
                del self._query_cache[cache_key]

        if not cache_hit:
            # 执行查询
            try:
                with self.get_connection(pool_name) as conn:
                    if params:
                        result = conn.execute(sql, params).fetchall()
                    else:
                        result = conn.execute(sql).fetchall()

                # 缓存结果
                if cache_ttl:
                    with self._cache_lock:
                        self._query_cache[cache_key] = (result, datetime.now())

            except Exception as e:
                logger.error(f"Query execution failed: {e}")
                raise

        # 记录查询指标
        end_time = datetime.now()
        duration_ms = (end_time - start_time).total_seconds() * 1000

        query_metrics = QueryMetrics(
            query_id=query_id,
            database_path=self._pool_configs[pool_name].db_path,
            sql=sql[:100] + "..." if len(sql) > 100 else sql,
            start_time=start_time,
            end_time=end_time,
            duration_ms=duration_ms,
            rows_affected=len(result) if hasattr(result, '__len__') else 0,
            cache_hit=cache_hit
        )

        self._query_metrics[query_id] = query_metrics

        return result

    def get_database_manager(self, manager_type: str) -> Any:
        """
        获取特定的数据库管理器

        Args:
            manager_type: 管理器类型

        Returns:
            数据库管理器实例
        """
        managers = {
            'duckdb': self._duckdb_manager,
            'duckdb_operations': self._duckdb_operations,
            'sqlite': self._sqlite_manager,
            'asset': self._asset_db_manager,
            'enhanced_asset': self._enhanced_asset_manager,
            'analytics': self._analytics_db,
            'optimization': self._optimization_db,
            'strategy': self._strategy_db
        }

        if manager_type not in managers:
            raise ValueError(f"Unknown manager type: {manager_type}")

        manager = managers[manager_type]
        if manager is None:
            raise ValueError(f"Manager type '{manager_type}' is not available (initialization failed)")

        return manager

    def get_connection_metrics(self) -> Dict[str, ConnectionMetrics]:
        """获取连接池指标"""
        with self._pool_lock:
            return self._pool_metrics.copy()

    def get_transaction_metrics(self) -> Dict[str, TransactionMetrics]:
        """获取事务指标"""
        with self._transaction_lock:
            return self._active_transactions.copy()

    def get_query_metrics(self) -> Dict[str, QueryMetrics]:
        """获取查询指标"""
        return self._query_metrics.copy()

    def get_performance_stats(self) -> Dict[str, Any]:
        """获取性能统计"""
        stats = {
            'connection_pools': len(self._connection_pools),
            'active_transactions': len(self._active_transactions),
            'query_cache_size': len(self._query_cache),
            'total_queries': len(self._query_metrics),
            'pool_metrics': self.get_connection_metrics(),
            'optimizers': list(self._performance_optimizers.keys())
        }

        # 计算平均查询时间
        if self._query_metrics:
            avg_query_time = sum(
                m.duration_ms for m in self._query_metrics.values()
                if m.duration_ms is not None
            ) / len(self._query_metrics)
            stats['avg_query_time_ms'] = avg_query_time

        return stats

    async def health_check(self) -> Dict[str, Any]:
        """健康检查"""
        health_status = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'connection_pools': {},
            'issues': []
        }

        try:
            # 检查连接池
            for pool_name, config in self._pool_configs.items():
                try:
                    with self.get_connection(pool_name) as conn:
                        # 简单查询测试
                        if config.db_type == DatabaseType.DUCKDB:
                            conn.execute("SELECT 1").fetchone()
                        else:  # SQLite
                            conn.execute("SELECT 1").fetchone()

                    health_status['connection_pools'][pool_name] = 'healthy'

                except Exception as e:
                    health_status['connection_pools'][pool_name] = f'unhealthy: {e}'
                    health_status['issues'].append(f"Connection pool '{pool_name}' failed health check: {e}")

            # 检查是否有问题
            if health_status['issues']:
                health_status['status'] = 'degraded'

        except Exception as e:
            health_status['status'] = 'unhealthy'
            health_status['issues'].append(f"Health check failed: {e}")

        return health_status

    async def shutdown(self) -> None:
        """关闭服务"""
        try:
            logger.info("Shutting down unified database service...")

            # 取消后台任务
            if self._monitoring_task:
                self._monitoring_task.cancel()
                try:
                    await self._monitoring_task
                except asyncio.CancelledError:
                    pass

            if self._cleanup_task:
                self._cleanup_task.cancel()
                try:
                    await self._cleanup_task
                except asyncio.CancelledError:
                    pass

            # 关闭性能优化器
            for optimizer in self._performance_optimizers.values():
                try:
                    optimizer.close()
                except Exception as e:
                    logger.error(f"Error closing optimizer: {e}")

            # 清理连接池
            self._connection_pools.clear()
            self._pool_configs.clear()
            self._pool_metrics.clear()

            logger.info("✅ Unified database service shutdown complete")

        except Exception as e:
            logger.error(f"Error during database service shutdown: {e}")


# 全局实例获取函数
_unified_database_service: Optional[UnifiedDatabaseService] = None


def get_unified_database_service() -> UnifiedDatabaseService:
    """获取统一数据库服务实例"""
    global _unified_database_service

    if _unified_database_service is None:
        _unified_database_service = UnifiedDatabaseService()

    return _unified_database_service
