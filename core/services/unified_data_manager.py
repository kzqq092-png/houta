from loguru import logger
"""
ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨

è´Ÿè´£åè°ƒå„æœåŠ¡çš„æ•°æ®åŠ è½½è¯·æ±‚ï¼Œé¿å…é‡å¤æ•°æ®åŠ è½½ï¼Œæä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£ã€‚
é›†æˆFactorWeave-Quantæ•°æ®ç®¡ç†åŠŸèƒ½ï¼ŒåŸºäºTETæ¡†æ¶å’Œæ’ä»¶æ¶æ„ã€‚
"""

import threading
import time
from typing import Dict, Any, Optional, List, Callable, Set
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, Future
from dataclasses import dataclass
from enum import Enum
import pandas as pd
import asyncio
from asyncio import Future as AsyncioFuture
import numpy as np
import sqlite3
import os
import traceback

from ..events import EventBus, DataUpdateEvent
from ..containers import ServiceContainer, get_service_container
from ..plugin_types import AssetType, DataType
from ..tet_data_pipeline import TETDataPipeline, StandardQuery, StandardData

# å¯¼å…¥UniPluginDataManager
try:
    from .uni_plugin_data_manager import UniPluginDataManager
except ImportError as e:
    logger.warning(f"UniPluginDataManagerå¯¼å…¥å¤±è´¥: {e}")
    UniPluginDataManager = None

# ç³»ç»ŸåŸºäºDuckDBä¼˜å…ˆæ¶æ„å’ŒTETæ¡†æ¶è¿è¡Œ

# ä¼ ç»Ÿæ•°æ®æºå·²è¿ç§»åˆ°TET+Pluginæ¶æ„ï¼Œä¸å†ç›´æ¥å¯¼å…¥
# æ•°æ®æºç°åœ¨é€šè¿‡UniPluginDataManagerç»Ÿä¸€ç®¡ç†

# å¯¼å…¥ç¼“å­˜å’Œå·¥å…·
try:
    # from utils.cache import Cache  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager
    # log_structuredå·²æ›¿æ¢ä¸ºç›´æ¥çš„loggerè°ƒç”¨
    from core.performance import measure_performance
except ImportError as e:
    logger.warning(f"å·¥å…·æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    # Cache = None  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager

# æ•°æ®åº“è·¯å¾„
DB_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data', 'factorweave_system.sqlite')


def get_unified_data_manager() -> Optional['UnifiedDataManager']:
    """
    è·å–ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨çš„å®ä¾‹

    Returns:
        ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœæœªæ³¨å†Œåˆ™è¿”å›None
    """
    try:
        container = get_service_container()
        if container:
            return container.resolve(UnifiedDataManager)
        return None
    except Exception as e:
        logger.error(f"è·å–ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å¤±è´¥: {e}")
        return None


class DataRequestStatus(Enum):
    """æ•°æ®è¯·æ±‚çŠ¶æ€"""
    PENDING = "pending"
    LOADING = "loading"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class DataRequest:
    """æ•°æ®è¯·æ±‚"""
    request_id: str
    symbol: str  # ç»Ÿä¸€ä½¿ç”¨symbolæ›¿ä»£stock_code
    asset_type: AssetType = AssetType.STOCK_A  # æ–°å¢èµ„äº§ç±»å‹æ”¯æŒï¼ˆAè‚¡ï¼‰
    data_type: str = 'kdata'  # 'kdata', 'indicators', 'analysis'
    period: str = 'D'
    time_range: int = 365
    parameters: Dict[str, Any] = None
    priority: int = 0  # 0=é«˜ä¼˜å…ˆçº§, 1=ä¸­ä¼˜å…ˆçº§, 2=ä½ä¼˜å…ˆçº§
    future: Optional[AsyncioFuture] = None  # ç”¨äºasync/await
    timestamp: float = 0
    status: DataRequestStatus = DataRequestStatus.PENDING

    # å‘åå…¼å®¹å±æ€§
    @property
    def stock_code(self) -> str:
        """
        æ¸…ç†ç¼“å­˜ - ä½¿ç”¨ç»Ÿä¸€çš„MultiLevelCacheManagerå‘åå…¼å®¹ï¼šè‚¡ç¥¨ä»£ç """
        return self.symbol

    @stock_code.setter
    def stock_code(self, value: str):
        """å‘åå…¼å®¹ï¼šè®¾ç½®è‚¡ç¥¨ä»£ç """
        self.symbol = value

    def __post_init__(self):
        if self.timestamp == 0:
            self.timestamp = time.time()
        if self.parameters is None:
            self.parameters = {}

    def __eq__(self, other):
        if not isinstance(other, DataRequest):
            return NotImplemented
        return (self.symbol == other.symbol and
                self.asset_type == other.asset_type and
                self.data_type == other.data_type and
                self.period == other.period and
                self.time_range == other.time_range and
                self.parameters == other.parameters)

    def __hash__(self):
        # The hash should be based on the immutable fields that define the request's identity
        # Note: self.parameters is mutable, so we convert it to a string representation of its items
        param_tuple = tuple(sorted((self.parameters or {}).items()))
        return hash((self.symbol,
                     self.asset_type,
                     self.data_type,
                     self.period,
                     self.time_range,
                     param_tuple))


class UnifiedDataManager:
    """
    ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. åè°ƒæ•°æ®åŠ è½½è¯·æ±‚
    2. é¿å…é‡å¤æ•°æ®åŠ è½½
    3. æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£
    4. ç®¡ç†æ•°æ®ç¼“å­˜
    5. ä¼˜åŒ–æ•°æ®åŠ è½½æ€§èƒ½
    6. æ”¯æŒTETæ•°æ®ç®¡é“ï¼ˆTransform-Extract-Transformï¼‰
    7. å¤šèµ„äº§ç±»å‹æ•°æ®å¤„ç†
    8. é›†æˆFactorWeave-Quantã€ä¸œæ–¹è´¢å¯Œã€æ–°æµªç­‰å¤šæ•°æ®æº
    9. è¡Œä¸šæ•°æ®ç®¡ç†
    10. SQLiteæ•°æ®åº“æ”¯æŒ
    """

    def __init__(self, service_container: ServiceContainer = None, event_bus: EventBus = None, max_workers: int = 3):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨

        Args:
            service_container: æœåŠ¡å®¹å™¨ (å¯é€‰)
            event_bus: äº‹ä»¶æ€»çº¿ (å¯é€‰)
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        # å…¼å®¹æ€§å¤„ç† - å…è®¸Noneå‚æ•°
        from ..containers import get_service_container
        self.service_container = service_container or get_service_container()
        self.event_bus = event_bus
        self.loop = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨å¼‚æ­¥æ–¹æ³•ä¸­è·å–

        # çº¿ç¨‹æ± 
        self._executor = ThreadPoolExecutor(
            max_workers=max_workers, thread_name_prefix="DataManager")

        # è¯·æ±‚ç®¡ç†
        self._pending_requests: Dict[str, DataRequest] = {}
        self._active_requests: Dict[str, DataRequest] = {}
        self._completed_requests: Dict[str, DataRequest] = {}
        self._request_lock = threading.Lock()

        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜TTL

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        # if Cache:  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager
        if False:
            self.cache_manager = Cache()
        else:
            self.cache_manager = None

        # æ•°æ®åº“è¿æ¥
        try:
            self.conn = sqlite3.connect(DB_PATH, check_same_thread=False)
            self._db_lock = threading.Lock()
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.conn = None
            self._db_lock = None

        # åˆå§‹åŒ–UniPluginDataManager (å»¶è¿Ÿæ¨¡å¼)

        self._uni_plugin_manager = None

        self._is_initialized = False

        # FactorWeave-Quantå·²ç§»é™¤ï¼Œç³»ç»ŸåŸºäºTETæ¡†æ¶å’Œæ’ä»¶æ¶æ„è¿è¡Œ
        self._invalid_stocks_cache = set()
        self._valid_stocks_cache = set()

        # å¤šæ•°æ®æºæ”¯æŒ - é»˜è®¤ä½¿ç”¨TETæ¡†æ¶
        self._current_source = 'tet_framework'
        self._data_sources = {}

        # æ’ä»¶åŒ–æ•°æ®æºç®¡ç†
        self._plugin_data_sources = {}
        self._registered_data_sources = {}  # å­˜å‚¨å·²æ³¨å†Œçš„æ•°æ®æºä¿¡æ¯
        self._data_source_priorities = {
            'stock': ['eastmoney', 'sina', 'tonghuashun'],
            'futures': [],
            'crypto': []
        }
        self._routing_strategy = 'priority'
        self._health_status = {}
        self._plugin_lock = threading.RLock()

        # è¡Œä¸šç®¡ç†å™¨åˆå§‹åŒ–
        try:
            from ..industry_manager import IndustryManager
            self.industry_manager = IndustryManager()
            self._load_industry_data()
        except Exception as e:
            logger.warning(f"è¡Œä¸šç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.industry_manager = None

        # å»é‡æœºåˆ¶
        self._request_dedup: Dict[str, Set[DataRequest]] = {}
        self._dedup_lock = threading.Lock()

        # è¯·æ±‚è·Ÿè¸ª
        self.request_tracker: Dict[str, Dict[str, Any]] = {}
        self.request_tracker_lock = threading.Lock()

        # TETæ•°æ®ç®¡é“æ”¯æŒ
        self.tet_enabled = True  # é»˜è®¤å¯ç”¨TETæ¨¡å¼
        self.tet_pipeline = None

        # æ•°æ®å¤„ç†ç­–ç•¥
        from ..tet_data_pipeline import HistoryDataStrategy, RealtimeDataStrategy
        self.history_data_strategy = HistoryDataStrategy()
        self.realtime_data_strategy = RealtimeDataStrategy()

        # åˆå§‹åŒ–TETç®¡é“
        try:
            from ..tet_data_pipeline import TETDataPipeline
            from ..data_source_router import DataSourceRouter

            # åˆ›å»ºæ•°æ®æºè·¯ç”±å™¨
            data_source_router = DataSourceRouter()

            # åˆå§‹åŒ–TETç®¡é“
            self.tet_pipeline = TETDataPipeline(data_source_router)
            logger.info("TETæ•°æ®ç®¡é“åˆå§‹åŒ–æˆåŠŸ")

            # æ³¨å†ŒFactorWeave-Quantæ•°æ®æºæ’ä»¶åˆ°è·¯ç”±å™¨å’ŒTETç®¡é“ - åˆ é™¤æ‰‹åŠ¨æ³¨å†Œï¼Œä½¿ç”¨è‡ªåŠ¨å‘ç°æœºåˆ¶
            # self._register_hikyuu_plugin_to_router(data_source_router)

            # æ’ä»¶å‘ç°çŠ¶æ€æ ‡è®°
            self._plugins_discovered = False

            # æ³¨å†Œä¼ ç»Ÿæ•°æ®æºåˆ°TETè·¯ç”±å™¨
            self._register_legacy_data_sources_to_router()

            # å»¶è¿Ÿæ’ä»¶å‘ç° - ä¸åœ¨åˆå§‹åŒ–æ—¶ç«‹å³æ‰§è¡Œ
            # å°†åœ¨æœåŠ¡å¼•å¯¼å®Œæˆåé€šè¿‡å¤–éƒ¨è°ƒç”¨æ‰§è¡Œ
            logger.info("TETæ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆï¼Œç­‰å¾…æ’ä»¶å‘ç°...")

        except ImportError as e:
            logger.error(f"TETæ•°æ®ç®¡é“æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            logger.info("ç¦ç”¨TETæ•°æ®ç®¡é“ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
            self.tet_enabled = False
            self.tet_pipeline = None
        except Exception as e:
            logger.warning(f"TETæ•°æ®ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("é™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼")
            self.tet_enabled = False
            self._plugins_discovered = False

        # æ¿å—æ•°æ®æœåŠ¡åˆå§‹åŒ–
        self._sector_data_service = None
        self._initialize_sector_service()

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'requests_total': 0,
            'requests_completed': 0,
            'requests_failed': 0,
            'requests_cancelled': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

        # DuckDBé›†æˆæ”¯æŒ - ç›´æ¥é›†æˆåˆ°ç°æœ‰ç®¡ç†å™¨
        self._init_duckdb_integration()

        logger.info("ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨æ„é€ å®Œæˆ")

        # ä»é…ç½®æœåŠ¡è¯»å–ç¼“å­˜å¯ç”¨çŠ¶æ€
        try:
            config_service = self.service_container.get('config_service')
            if config_service:
                self.cache_enabled = config_service.get('data.cache_enabled', True)
                logger.info(f"ç¼“å­˜å¯ç”¨çŠ¶æ€: {self.cache_enabled}")
            else:
                self.cache_enabled = True  # é…ç½®æœåŠ¡ä¸å¯ç”¨æ—¶é»˜è®¤å¯ç”¨ç¼“å­˜
                logger.warning("é…ç½®æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è®¾ç½®ï¼ˆå·²å¯ç”¨ï¼‰")
        except Exception as e:
            self.cache_enabled = True  # å‡ºé”™æ—¶é»˜è®¤å¯ç”¨ç¼“å­˜
            logger.warning(f"è¯»å–ç¼“å­˜é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")

    def initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼Œç”±æœåŠ¡å®¹å™¨æ§åˆ¶æ—¶æœº"""
        if self._is_initialized:
            logger.info("UnifiedDataManagerå·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return

        logger.info("å¼€å§‹åˆå§‹åŒ–UnifiedDataManager...")

        # ä»æœåŠ¡å®¹å™¨è·å–å·²æ³¨å†Œçš„å®ä¾‹ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„
        try:
            if UniPluginDataManager and hasattr(self, 'service_container') and self.service_container:
                if self.service_container.is_registered(UniPluginDataManager):
                    self._uni_plugin_manager = self.service_container.resolve(UniPluginDataManager)
                    logger.info("ä»æœåŠ¡å®¹å™¨è·å–UniPluginDataManageræˆåŠŸ")
                else:
                    logger.warning("UniPluginDataManageræœªåœ¨æœåŠ¡å®¹å™¨ä¸­æ³¨å†Œï¼Œå°†ä½¿ç”¨å»¶è¿Ÿåˆ›å»ºæ¨¡å¼")
            else:
                logger.warning("æœåŠ¡å®¹å™¨ä¸å¯ç”¨æˆ–UniPluginDataManageræœªå¯¼å…¥ï¼Œå°†ä½¿ç”¨å»¶è¿Ÿåˆ›å»ºæ¨¡å¼")
        except Exception as e:
            logger.error(f"[ERROR] ä»æœåŠ¡å®¹å™¨è·å–UniPluginDataManagerå¤±è´¥: {e}")

        # å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨ - åœ¨UniPluginDataManagerå¯ç”¨ååˆå§‹åŒ–
        self._init_enhanced_duckdb_downloader()

        self._is_initialized = True
        logger.info("UnifiedDataManageråˆå§‹åŒ–å®Œæˆ")

    def _init_duckdb_integration(self):
        """
        é›†æˆDuckDBåŠŸèƒ½åˆ°ç°æœ‰æ•°æ®ç®¡ç†å™¨

        åœ¨ç°æœ‰æ¶æ„åŸºç¡€ä¸Šå¢åŠ DuckDBæ”¯æŒï¼Œä¸ç ´åç°æœ‰åŠŸèƒ½
        """
        try:
            # å¯¼å…¥DuckDBæ ¸å¿ƒç»„ä»¶
            from ..database.duckdb_operations import get_duckdb_operations
            from ..database.duckdb_manager import get_connection_manager
            from ..database.table_manager import get_table_manager
            from ..integration.data_router import DataRouter
            from ..performance.cache_manager import MultiLevelCacheManager
            from ..asset_database_manager import AssetSeparatedDatabaseManager
            from ..asset_type_identifier import get_asset_type_identifier

            # åˆå§‹åŒ–DuckDBç»„ä»¶
            self.duckdb_operations = get_duckdb_operations()
            self.duckdb_manager = get_connection_manager()
            self.table_manager = get_table_manager()

            # åˆå§‹åŒ–èµ„äº§æ•°æ®åº“ç®¡ç†å™¨å’Œèµ„äº§ç±»å‹è¯†åˆ«å™¨
            self.asset_manager = AssetSeparatedDatabaseManager()
            self.asset_identifier = get_asset_type_identifier()

            # æ™ºèƒ½æ•°æ®è·¯ç”±å™¨
            self.data_router = DataRouter()

            # å¤šçº§ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¢å¼ºç°æœ‰ç¼“å­˜ï¼‰
            from ..performance.cache_manager import CacheLevel
            # ä½¿ç”¨æ­£ç¡®çš„æ„é€ å‡½æ•°å‚æ•°ï¼šmax_sizeå’Œttlï¼ˆç§’ï¼‰
            self.multi_cache = MultiLevelCacheManager(max_size=1000, ttl=1800)  # 30åˆ†é’Ÿ = 1800ç§’

            # DuckDBå¯ç”¨æ ‡å¿—
            self.duckdb_available = True

            logger.info("DuckDBåŠŸèƒ½é›†æˆæˆåŠŸï¼ˆåŒ…å«èµ„äº§æ•°æ®åº“ç®¡ç†å™¨ï¼‰")

        except ImportError as e:
            logger.warning(f" DuckDBæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼: {e}")
            self.duckdb_operations = None
            self.duckdb_manager = None
            self.table_manager = None
            self.asset_manager = None
            self.asset_identifier = None
            self.data_router = None
            self.multi_cache = None
            self.duckdb_available = False
        except Exception as e:
            logger.warning(f" DuckDBåŠŸèƒ½é›†æˆå¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼: {e}")
            self.duckdb_operations = None
            self.duckdb_manager = None
            self.table_manager = None
            self.asset_manager = None
            self.asset_identifier = None
            self.data_router = None
            self.multi_cache = None
            self.duckdb_available = False

    def _init_enhanced_duckdb_downloader(self):
        """
        åˆå§‹åŒ–å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨

        æä¾›å¼ºå¤§çš„æ•°æ®ä¸‹è½½å’Œå­˜å‚¨èƒ½åŠ›ï¼Œå®Œå…¨åŸºäºTETæ¡†æ¶å’Œæ’ä»¶æ¶æ„
        """
        try:
            from .enhanced_duckdb_data_downloader import get_enhanced_duckdb_downloader

            if self._uni_plugin_manager:
                self.enhanced_duckdb_downloader = get_enhanced_duckdb_downloader(self._uni_plugin_manager)
                logger.info("å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("UniPluginDataManagerä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–å¢å¼ºDuckDBä¸‹è½½å™¨")
                self.enhanced_duckdb_downloader = None

        except Exception as e:
            logger.warning(f" å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enhanced_duckdb_downloader = None

    def _create_uni_plugin_manager_if_needed(self):
        """åˆå§‹åŒ–UniPluginDataManager"""
        try:
            from core.plugin_manager import PluginManager
            from core.data_source_router import DataSourceRouter
            from core.tet_data_pipeline import TETDataPipeline
            from core.services.uni_plugin_data_manager import UniPluginDataManager

            logger.info("å¼€å§‹åˆå§‹åŒ–UniPluginDataManager...")

            # åˆ›å»ºå¿…è¦çš„ç»„ä»¶
            plugin_manager = PluginManager()
            data_source_router = DataSourceRouter()
            tet_pipeline = TETDataPipeline(data_source_router)

            # åˆ›å»ºUniPluginDataManager
            self._uni_plugin_manager = UniPluginDataManager(
                plugin_manager=plugin_manager,
                data_source_router=data_source_router,
                tet_pipeline=tet_pipeline
            )

            logger.info("UniPluginDataManageråˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"[ERROR] UniPluginDataManageråˆå§‹åŒ–å¤±è´¥: {e}")
            self._uni_plugin_manager = None

    def get_uni_plugin_manager(self):
        """è·å–UniPluginDataManagerå®ä¾‹"""
        return self._uni_plugin_manager

    def _register_legacy_data_source_to_router(self, source_id: str, legacy_source):
        """å°†ä¼ ç»Ÿæ•°æ®æºæ³¨å†Œåˆ°TETè·¯ç”±å™¨"""
        try:
            # æ£€æŸ¥TETç®¡é“æ˜¯å¦å¯ç”¨
            if not (hasattr(self, 'tet_pipeline') and self.tet_pipeline and hasattr(self.tet_pipeline, 'router')):
                logger.debug(f"TETç®¡é“ä¸å¯ç”¨ï¼Œè·³è¿‡æ³¨å†Œä¼ ç»Ÿæ•°æ®æº: {source_id}")
                return

            # åˆ›å»ºä¼ ç»Ÿæ•°æ®æºçš„é€‚é…å™¨
            from ..data_source_extensions import DataSourcePluginAdapter
            from .legacy_datasource_adapter import LegacyDataSourceAdapter

            # åŒ…è£…ä¼ ç»Ÿæ•°æ®æºä¸ºIDataSourcePluginæ¥å£
            plugin_adapter = LegacyDataSourceAdapter(legacy_source, source_id)

            # åˆ›å»ºæ•°æ®æºæ’ä»¶é€‚é…å™¨
            adapter = DataSourcePluginAdapter(plugin_adapter, source_id)

            # æ³¨å†Œåˆ°è·¯ç”±å™¨
            router = self.tet_pipeline.router
            success = router.register_data_source(source_id, adapter, priority=1, weight=1.0)

            if success:
                logger.info(f"ä¼ ç»Ÿæ•°æ®æº {source_id} å·²æ³¨å†Œåˆ°TETè·¯ç”±å™¨")

                # å…³é”®ä¿®å¤ï¼šåŒæ—¶æ³¨å†Œåˆ°TETç®¡é“çš„é€‚é…å™¨å­—å…¸
                if hasattr(self.tet_pipeline, '_adapters'):
                    self.tet_pipeline._adapters[source_id] = adapter
                    logger.info(f"ä¼ ç»Ÿæ•°æ®æº {source_id} å·²æ³¨å†Œåˆ°TETç®¡é“é€‚é…å™¨å­—å…¸")
                else:
                    logger.warning("TETç®¡é“ç¼ºå°‘_adapterså±æ€§")

                # å¦‚æœé€‚é…å™¨æœ‰å¯¹åº”çš„æ’ä»¶å®ä¾‹ï¼Œä¹Ÿæ³¨å†Œåˆ°_pluginså­—å…¸
                if hasattr(adapter, 'plugin') and hasattr(self.tet_pipeline, '_plugins'):
                    self.tet_pipeline._plugins[source_id] = adapter.plugin
                    logger.info(f"ä¼ ç»Ÿæ•°æ®æº {source_id} å·²æ³¨å†Œåˆ°TETç®¡é“æ’ä»¶å­—å…¸")
            else:
                logger.warning(f"ä¼ ç»Ÿæ•°æ®æº {source_id} æ³¨å†Œåˆ°TETè·¯ç”±å™¨å¤±è´¥")

        except Exception as e:
            logger.error(f"æ³¨å†Œä¼ ç»Ÿæ•°æ®æº {source_id} åˆ°TETè·¯ç”±å™¨å¼‚å¸¸: {e}")

    def _register_legacy_data_sources_to_router(self):
        """å°†æ‰€æœ‰ä¼ ç»Ÿæ•°æ®æºæ³¨å†Œåˆ°TETè·¯ç”±å™¨"""
        try:
            logger.info("å¼€å§‹æ³¨å†Œä¼ ç»Ÿæ•°æ®æºåˆ°TETè·¯ç”±å™¨")

            # æ³¨å†Œæ‰€æœ‰å·²åˆå§‹åŒ–çš„ä¼ ç»Ÿæ•°æ®æº
            for source_id, legacy_source in self._data_sources.items():
                if legacy_source is not None:
                    self._register_legacy_data_source_to_router(source_id, legacy_source)

            logger.info("ä¼ ç»Ÿæ•°æ®æºæ³¨å†Œåˆ°TETè·¯ç”±å™¨å®Œæˆ")
        except Exception as e:
            logger.error(f"æ³¨å†Œä¼ ç»Ÿæ•°æ®æºåˆ°TETè·¯ç”±å™¨å¼‚å¸¸: {e}")

    def _load_industry_data(self):
        """åŠ è½½è¡Œä¸šæ•°æ®"""
        if self.industry_manager:
            try:
                self.industry_manager.load_cache()
                self.industry_manager.update_industry_data()
                logger.info("è¡Œä¸šæ•°æ®åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"è¡Œä¸šæ•°æ®åŠ è½½å¤±è´¥: {e}")

    def get_available_sources(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®æºåˆ—è¡¨"""
        sources = []
        # FactorWeave-Quantå·²ç§»é™¤
        sources.extend(self._data_sources.keys())
        return sources

    def switch_data_source(self, source: str) -> bool:
        """åˆ‡æ¢æ•°æ®æº"""
        if source in self.get_available_sources():
            old_source = self._current_source
            self._current_source = source
            logger.info(f"æ•°æ®æºä» {old_source} åˆ‡æ¢åˆ° {source}")
            return True
        else:
            logger.error(f"æ•°æ®æº {source} ä¸å¯ç”¨")
            return False

    def get_stock_list(self, market: str = 'all') -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆDuckDBä¼˜å…ˆæ¶æ„ï¼‰- é‡æ„ä¸ºè°ƒç”¨é€šç”¨èµ„äº§åˆ—è¡¨æ–¹æ³•

        Args:
            market: å¸‚åœºç±»å‹ ('all', 'sh', 'sz', 'bj')

        Returns:
            è‚¡ç¥¨åˆ—è¡¨DataFrame
        """
        return self.get_asset_list(asset_type='stock', market=market)

    def _get_industry_info(self, stock_code: str) -> str:
        """è·å–è‚¡ç¥¨è¡Œä¸šä¿¡æ¯"""
        if self.industry_manager:
            try:
                industry_info = self.industry_manager.get_industry(stock_code)
                if industry_info:
                    return (industry_info.get('csrc_industry') or
                            industry_info.get('exchange_industry') or
                            industry_info.get('industry') or 'å…¶ä»–')
            except Exception as e:
                logger.warning(f"è·å–è‚¡ç¥¨ {stock_code} è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
        return 'å…¶ä»–'

    def get_kdata(self, stock_code: str, period: str = 'D', count: int = 365,
                  asset_type: AssetType = AssetType.STOCK_A) -> pd.DataFrame:
        """
        è·å–Kçº¿æ•°æ® - ç»Ÿä¸€æ¥å£ï¼ˆâœ… ä¼˜åŒ–ï¼šæ”¯æŒå¤šèµ„äº§ç±»å‹ + é›†æˆDuckDBæ™ºèƒ½è·¯ç”±ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆæˆ–å…¶ä»–èµ„äº§ä»£ç ï¼‰
            period: å‘¨æœŸ (D/W/M/1/5/15/30/60)
            count: æ•°æ®æ¡æ•°
            asset_type: èµ„äº§ç±»å‹ï¼ˆé»˜è®¤ä¸ºè‚¡ç¥¨ï¼Œæ”¯æŒCRYPTO/FUTURES/FOREX/INDEX/FUNDç­‰ï¼‰

        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            # âœ… ç¼“å­˜é”®åŒ…å«èµ„äº§ç±»å‹ï¼Œé¿å…è·¨èµ„äº§æ··æ·†
            cache_key = f"kdata_{asset_type.value}_{stock_code}_{period}_{count}"

            # 1. å¤šçº§ç¼“å­˜æ£€æŸ¥ï¼ˆå¢å¼ºç¼“å­˜ç­–ç•¥ï¼‰
            cached_data = self._get_cached_data(cache_key)
            if cached_data is not None and not cached_data.empty:
                logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {stock_code} ({asset_type.value})")
                return cached_data

            # 2. åˆå§‹åŒ–dfå˜é‡
            df = pd.DataFrame()

            # 3. âœ… ä¿®å¤ï¼šå§‹ç»ˆå°è¯•ä»DuckDBè·å–æ•°æ®ï¼ˆæ”¯æŒå¤šèµ„äº§ç±»å‹ï¼‰
            if self.duckdb_available:
                logger.debug(f"âœ… å°è¯•ä»DuckDBè·å–Kçº¿æ•°æ®: {stock_code}, period={period}, count={count}, asset_type={asset_type.value}")
                df = self._get_kdata_from_duckdb(stock_code, period, count, asset_type=asset_type)

                if not df.empty:
                    logger.info(f"âœ… ä»DuckDBè·å–æ•°æ®æˆåŠŸ: {stock_code} ({asset_type.value}), è®°å½•æ•°={len(df)}")
                    self._cache_data(cache_key, df)
                    return df
                else:
                    logger.warning(f"DuckDBä¸­æ²¡æœ‰æ•°æ®: {stock_code} ({asset_type.value})")
            else:
                logger.warning("DuckDBä¸å¯ç”¨ï¼Œæ— æ³•è·å–æ•°æ®")

            # 4. å¦‚æœDuckDBæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºDataFrame
            df = pd.DataFrame()

            # 4. æ•°æ®æ ‡å‡†åŒ–å’Œæ¸…æ´—
            if not df.empty:
                df = self._standardize_kdata_format(df, stock_code)

                # 5. æ™ºèƒ½å­˜å‚¨ï¼šå¤§æ•°æ®å­˜å‚¨åˆ°DuckDB
                if self.duckdb_available and len(df) > 1000:
                    self._store_to_duckdb(df, stock_code, period)

                # 6. ç¼“å­˜æ•°æ®
                self._cache_data(cache_key, df)

            return df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {stock_code} ({asset_type.value}) - {e}")
            return pd.DataFrame()

    def get_kdata_from_source(self, stock_code: str, period: str = 'D', count: int = 365,
                              data_source: str = None, asset_type: AssetType = None,
                              start_date=None, end_date=None) -> pd.DataFrame:
        """
        ä»æŒ‡å®šæ•°æ®æºè·å–Kçº¿æ•°æ®

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸ (D/W/M/1/5/15/30/60/daily/weekly/monthlyç­‰)
            count: æ•°æ®æ¡æ•°
            data_source: æ•°æ®æºåç§° (å¦‚: 'é€šè¾¾ä¿¡', 'akshare', 'eastmoney'ç­‰)
            asset_type: èµ„äº§ç±»å‹ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼Aè‚¡ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨è®¡ç®—ï¼Œæ ¼å¼: YYYY-MM-DDæˆ–datetimeå¯¹è±¡)
            end_date: ç»“æŸæ—¥æœŸ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨è®¡ç®—ï¼Œæ ¼å¼: YYYY-MM-DDæˆ–datetimeå¯¹è±¡)

        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            # æ ‡å‡†åŒ–å‘¨æœŸæ ¼å¼
            period_map = {
                'D': 'daily', 'daily': 'daily',
                'W': 'weekly', 'weekly': 'weekly',
                'M': 'monthly', 'monthly': 'monthly',
                '1': '1min', '5': '5min', '15': '15min',
                '30': '30min', '60': '60min'
            }
            frequency = period_map.get(period, period)

            cache_key = f"kdata_{stock_code}_{period}_{count}_{data_source}"

            # 1. æ£€æŸ¥ç¼“å­˜
            cached_data = self._get_cached_data(cache_key)
            if cached_data is not None and not cached_data.empty:
                logger.debug(f"ä»ç¼“å­˜è·å–Kçº¿æ•°æ®: {stock_code} (æ•°æ®æº: {data_source})")
                return cached_data

            # 2. ä½¿ç”¨UniPluginDataManagerè·å–æ•°æ®
            if self._uni_plugin_manager:
                try:
                    from ..plugin_types import AssetType
                    from datetime import datetime, timedelta

                    # âœ… ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸèŒƒå›´ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨è®¡ç®—
                    if start_date is None or end_date is None:
                        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆå½“æœªæä¾›æ—¥æœŸå‚æ•°æ—¶ï¼‰
                        end_date = datetime.now() if end_date is None else end_date
                        # æ ¹æ®å‘¨æœŸè®¡ç®—å¼€å§‹æ—¥æœŸ
                        if start_date is None:
                            if frequency == 'daily':
                                start_date = end_date - timedelta(days=count * 2)  # é¢„ç•™ç©ºé—´æ’é™¤éäº¤æ˜“æ—¥
                            elif frequency == 'weekly':
                                start_date = end_date - timedelta(weeks=count)
                            elif frequency == 'monthly':
                                start_date = end_date - timedelta(days=count * 31)
                            else:
                                start_date = end_date - timedelta(days=count)
                    else:
                        # âœ… ç¡®ä¿ end_date æ˜¯ datetime å¯¹è±¡
                        if isinstance(end_date, str):
                            end_date = datetime.strptime(end_date, '%Y-%m-%d')
                        elif end_date is None:
                            end_date = datetime.now()

                        # âœ… ç¡®ä¿ start_date æ˜¯ datetime å¯¹è±¡
                        if isinstance(start_date, str):
                            start_date = datetime.strptime(start_date, '%Y-%m-%d')

                    # âœ… éªŒè¯æ—¥æœŸèŒƒå›´çš„æœ‰æ•ˆæ€§
                    if start_date >= end_date:
                        logger.warning(f"æ—¥æœŸèŒƒå›´æ— æ•ˆ: start_date={start_date} >= end_date={end_date}ï¼Œè°ƒæ•´ä¸ºé»˜è®¤èŒƒå›´")
                        end_date = datetime.now()
                        start_date = end_date - timedelta(days=count * 2)

                    # è°ƒç”¨æ’ä»¶ç®¡ç†å™¨è·å–æ•°æ®ï¼Œä¼ é€’data_sourceå‚æ•°
                    # âœ… ä½¿ç”¨ä¼ å…¥çš„èµ„äº§ç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼Aè‚¡
                    final_asset_type = asset_type or AssetType.STOCK_A

                    # âœ… æ™ºèƒ½å¤„ç†countå‚æ•°ï¼šå¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼Œæ ¹æ®æ—¶é—´æ®µè®¡ç®—å®é™…éœ€è¦çš„æ•°æ®é‡
                    actual_count = count
                    if start_date and end_date:
                        # æ ¹æ®æ—¥æœŸèŒƒå›´å’Œé¢‘ç‡ä¼°ç®—éœ€è¦çš„æ•°æ®é‡ï¼ˆè€ƒè™‘äº¤æ˜“æ—¥å’Œä¸åŒé¢‘ç‡ï¼‰
                        try:
                            from datetime import datetime
                            days_diff = (end_date - start_date).days

                            # æ ¹æ®ä¸åŒçš„é¢‘ç‡ç±»å‹ï¼Œä½¿ç”¨ä¸åŒçš„ä¼°ç®—æ–¹æ³•
                            if frequency == 'daily':
                                # æ—¥çº¿ï¼šä¸€å¹´çº¦250ä¸ªäº¤æ˜“æ—¥ï¼Œä¼°ç®—å…¬å¼ï¼šå¤©æ•° * 0.7ï¼ˆè€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
                                estimated_count = int(days_diff * 0.7)
                            elif frequency == 'weekly':
                                # å‘¨çº¿ï¼šä¸€å¹´çº¦52å‘¨ï¼Œä¼°ç®—å…¬å¼ï¼šå¤©æ•° / 7 * 0.9ï¼ˆè€ƒè™‘èŠ‚å‡æ—¥ï¼‰
                                estimated_count = int(days_diff / 7 * 0.9)
                            elif frequency == 'monthly':
                                # æœˆçº¿ï¼šä¸€å¹´çº¦12ä¸ªæœˆï¼Œä¼°ç®—å…¬å¼ï¼šæœˆä»½æ•°
                                estimated_count = int(days_diff / 30)
                            elif frequency in ['1min', '5min', '15min', '30min', '60min']:
                                # åˆ†é’Ÿçº¿ï¼šæ ¹æ®é¢‘ç‡è®¡ç®—ï¼ˆ1åˆ†é’Ÿ=240æ¡/å¤©ï¼Œ5åˆ†é’Ÿ=48æ¡/å¤©ï¼Œ15åˆ†é’Ÿ=16æ¡/å¤©ï¼Œ30åˆ†é’Ÿ=8æ¡/å¤©ï¼Œ60åˆ†é’Ÿ=4æ¡/å¤©ï¼‰
                                minutes_per_day = {'1min': 240, '5min': 48, '15min': 16, '30min': 8, '60min': 4}
                                minutes_per_record = minutes_per_day.get(frequency, 240)
                                # ä¼°ç®—ï¼šå¤©æ•° * æ¯å¤©æ¡æ•° * 0.7ï¼ˆè€ƒè™‘éäº¤æ˜“æ—¶é—´ï¼‰
                                estimated_count = int(days_diff * minutes_per_record * 0.7)
                            else:
                                # å…¶ä»–é¢‘ç‡ï¼šä½¿ç”¨é»˜è®¤ä¼°ç®—æ–¹æ³•
                                estimated_count = int(days_diff * 0.7)

                            # âœ… ä¿®å¤ï¼šä¸å†å¼ºåˆ¶æœ€å°å€¼ä¸º800ï¼Œè€Œæ˜¯ä½¿ç”¨å®é™…è®¡ç®—å‡ºçš„æ•°é‡
                            # åªæœ‰è¶…è¿‡ä¸Šé™æ—¶æ‰é™åˆ¶ï¼Œä¸è¶…è¿‡800æ—¶å°±ä½¿ç”¨å®é™…è®¡ç®—çš„æ•°é‡
                            # ä¸Šé™è®¾ç½®ä¸º10000ï¼ˆè¶…è¿‡è¿™ä¸ªå€¼ä¼šåœ¨Tongdaxinæ’ä»¶ä¸­åˆ†ç‰‡ï¼‰
                            MAX_COUNT_LIMIT = 10000
                            if estimated_count > MAX_COUNT_LIMIT:
                                actual_count = MAX_COUNT_LIMIT
                                logger.warning(f"[æ•°æ®è·å–] ä¼°ç®—æ•°é‡{estimated_count}è¶…è¿‡ä¸Šé™{MAX_COUNT_LIMIT}ï¼Œè°ƒæ•´ä¸º{actual_count}ï¼ˆå°†åœ¨æ’ä»¶ä¸­åˆ†ç‰‡ï¼‰")
                            else:
                                # ä½¿ç”¨å®é™…è®¡ç®—å‡ºçš„æ•°é‡ï¼ˆå¯èƒ½æ˜¯1ã€10ã€100ç­‰ä»»ä½•å€¼ï¼Œä¸å†å¼ºåˆ¶800ï¼‰
                                actual_count = estimated_count

                            # ç¡®ä¿æœ€å°å€¼ä¸º1ï¼ˆé¿å…0æˆ–è´Ÿæ•°ï¼‰
                            if actual_count < 1:
                                actual_count = 1
                                logger.warning(f"[æ•°æ®è·å–] ä¼°ç®—æ•°é‡è¿‡å°ï¼Œè°ƒæ•´ä¸ºæœ€å°å€¼1")

                            logger.info(f"[æ•°æ®è·å–] å·²æŒ‡å®šæ—¶é—´èŒƒå›´ {start_date} ~ {end_date}ï¼Œ"
                                        f"æ—¥æœŸè·¨åº¦{days_diff}å¤©ï¼Œé¢‘ç‡={frequency}ï¼Œä¼°ç®—éœ€è¦{estimated_count}æ¡ï¼Œå®é™…è¯·æ±‚{actual_count}æ¡")
                        except Exception as e:
                            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼ å…¥çš„countå‚æ•°ï¼ˆè€Œä¸æ˜¯å¼ºåˆ¶800ï¼‰
                            actual_count = count if count > 0 else 365
                            logger.warning(f"[æ•°æ®è·å–] æ—¥æœŸèŒƒå›´è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ å…¥çš„count={actual_count}")
                    else:
                        logger.info(f"[æ•°æ®è·å–] æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨count={count}è·å–æœ€è¿‘æ•°æ®")

                    logger.info(f"[æ•°æ®è·å–] å¼€å§‹æŸ¥è¯¢ {stock_code}ï¼Œæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}ï¼Œé¢‘ç‡: {frequency}ï¼Œcount: {actual_count}ï¼Œæ•°æ®æº: {data_source}")

                    df = self._uni_plugin_manager.get_kline_data(
                        symbol=stock_code,
                        asset_type=final_asset_type,  # âœ… ä½¿ç”¨ä¼ å…¥çš„èµ„äº§ç±»å‹
                        start_date=start_date,
                        end_date=end_date,
                        frequency=frequency,
                        count=actual_count,  # âœ… ä½¿ç”¨æ™ºèƒ½è®¡ç®—åçš„count
                        data_source=data_source  # ä¼ é€’æŒ‡å®šçš„æ•°æ®æº
                    )

                    if not df.empty:
                        logger.info(f"[æ•°æ®è·å–] åŸå§‹æ•°æ®é‡: {len(df)} æ¡ï¼Œæ—¶é—´è·¨åº¦: {df['datetime'].min() if 'datetime' in df.columns else 'N/A'} ~ {df['datetime'].max() if 'datetime' in df.columns else 'N/A'}")

                        # âœ… æ”¹è¿›ï¼šæ•°æ®æˆªæ–­é€»è¾‘ - ä»…åœ¨æ˜æ˜¾è¶…é‡ä¸”æ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´æ—¶æ‰æˆªæ–­
                        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œåˆ™ä¸è¿›è¡Œæˆªæ–­ï¼ˆå°Šé‡ç”¨æˆ·æ„å›¾ï¼‰
                        should_truncate = False
                        if start_date is None or end_date is None:
                            # æœªæŒ‡å®šæ—¥æœŸèŒƒå›´æ—¶ï¼Œæ ¹æ®countåˆ¤æ–­æ˜¯å¦æˆªæ–­
                            if len(df) > count * 3:  # æé«˜é˜ˆå€¼åˆ°3å€ï¼Œæ›´å®½å®¹
                                should_truncate = True

                        # âœ… ä¿®å¤ï¼šå…ˆè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ï¼ˆåŒ…å«æ’åºï¼‰ï¼Œå†è¿›è¡Œæˆªæ–­
                        # ç¡®ä¿æ•°æ®åœ¨æˆªæ–­å‰å·²ç»æŒ‰æ—¶é—´å‡åºæ’åˆ—
                        df = self._standardize_kdata_format(df, stock_code)

                        if should_truncate and not df.empty:
                            original_len = len(df)
                            # âœ… ä¿®å¤ï¼šæ•°æ®å·²ç»æ ‡å‡†åŒ–å¹¶æ’åºï¼ˆå‡åºï¼‰ï¼Œä½¿ç”¨tailè·å–æœ€æ–°çš„countæ¡æ•°æ®
                            df = df.tail(count).reset_index(drop=True)
                            logger.warning(f"[æ•°æ®è·å–] æœªæŒ‡å®šæ—¥æœŸèŒƒå›´ä¸”æ•°æ®é‡ {original_len} è¶…è¿‡é™åˆ¶ {count * 3}ï¼Œæˆªæ–­ä¸º {len(df)} æ¡ï¼ˆæœ€æ–°æ•°æ®ï¼‰")
                        else:
                            logger.info(f"[æ•°æ®è·å–] ä¿ç•™å…¨éƒ¨ {len(df)} æ¡æ•°æ®ï¼ˆ{'å·²æŒ‡å®šæ—¥æœŸèŒƒå›´' if start_date and end_date else 'æ•°æ®é‡æœªè¶…é™'}ï¼‰")

                        # ç¼“å­˜æ•°æ®
                        self._cache_data(cache_key, df)

                        logger.info(f"[æ•°æ®è·å–] ä»æ•°æ®æº {data_source} è·å–Kçº¿æ•°æ®æˆåŠŸ: {stock_code}, æœ€ç»ˆæ•°æ®é‡: {len(df)}, æ—¶é—´è·¨åº¦: {df['datetime'].min() if 'datetime' in df.columns else 'N/A'} ~ {df['datetime'].max() if 'datetime' in df.columns else 'N/A'}")
                        return df
                    else:
                        logger.warning(f"ä»æ•°æ®æº {data_source} è·å–Kçº¿æ•°æ®ä¸ºç©º: {stock_code}ï¼Œæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")

                except Exception as e:
                    logger.error(f"ä½¿ç”¨UniPluginDataManagerä»æ•°æ®æº {data_source} è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")

            # 3. é™çº§åˆ°é»˜è®¤get_kdataæ–¹æ³•
            logger.warning(f"ä»æŒ‡å®šæ•°æ®æº {data_source} è·å–å¤±è´¥ï¼Œé™çº§åˆ°é»˜è®¤æ–¹æ³•")
            return self.get_kdata(stock_code, period, count)

        except Exception as e:
            logger.error(f"ä»æ•°æ®æº {data_source} è·å–Kçº¿æ•°æ®å¤±è´¥: {stock_code} - {e}")
            return pd.DataFrame()

    def _get_cached_data(self, cache_key: str) -> Optional[pd.DataFrame]:
        """å¢å¼ºç¼“å­˜è·å– - ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager"""
        try:
            # ä¼˜å…ˆä»å¤šçº§ç¼“å­˜è·å–
            if self.duckdb_available and self.multi_cache:
                cached_data = self.multi_cache.get(cache_key)
                if cached_data is not None:
                    return cached_data

            # å›é€€åˆ°ä¼ ç»Ÿç¼“å­˜
            if self.cache_manager:
                return self.cache_manager.get(cache_key)

            return None
        except Exception as e:
            logger.warning(f"ç¼“å­˜è·å–å¤±è´¥: {e}")
            return None

    def _cache_data(self, cache_key: str, data: pd.DataFrame):
        """å¢å¼ºç¼“å­˜å­˜å‚¨ - æ”¯æŒå¤šçº§ç¼“å­˜"""
        try:
            # å­˜å‚¨åˆ°å¤šçº§ç¼“å­˜
            if self.duckdb_available and self.multi_cache:
                self.multi_cache.set(cache_key, data, ttl=self._cache_ttl)

            # åŒæ—¶å­˜å‚¨åˆ°ä¼ ç»Ÿç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
            if self.cache_manager:
                self.cache_manager.set(cache_key, data)

        except Exception as e:
            logger.warning(f"ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")

    def get_asset_list(self, asset_type: str = 'stock', market: str = 'all') -> pd.DataFrame:
        """
        è·å–èµ„äº§åˆ—è¡¨ï¼ˆDuckDBä¼˜å…ˆæ¶æ„ï¼‰- æ”¯æŒæ‰€æœ‰èµ„äº§ç±»å‹

        Args:
            asset_type: èµ„äº§ç±»å‹ ('stock', 'crypto', 'fund', 'bond', 'index', 'sector')
            market: å¸‚åœºç±»å‹ ('all', 'sh', 'sz', 'bj', 'us', 'hk')

        Returns:
            èµ„äº§åˆ—è¡¨DataFrame
        """
        try:
            cache_key = f"asset_list_{asset_type}_{market}"

            # 1. ä¼˜å…ˆä»DuckDBæ•°æ®åº“è·å–èµ„äº§åˆ—è¡¨
            if self.duckdb_available and self.duckdb_operations:
                logger.debug(f"ğŸ—„ï¸ ä»DuckDBæ•°æ®åº“è·å–{asset_type}èµ„äº§åˆ—è¡¨")  # ä¼˜åŒ–ï¼šæ”¹ä¸ºdebugçº§åˆ«å‡å°‘æ—¥å¿—å™ªéŸ³
                try:
                    asset_list_df = self._get_asset_list_from_duckdb(asset_type, market)
                    if asset_list_df is not None and not asset_list_df.empty:
                        logger.debug(f"âœ… DuckDBæ•°æ®åº“è·å–{asset_type}èµ„äº§åˆ—è¡¨æˆåŠŸ: {len(asset_list_df)} ä¸ªèµ„äº§")  # ä¼˜åŒ–ï¼šæ”¹ä¸ºdebugçº§åˆ«
                        # ç¼“å­˜ç»“æœ
                        if self.cache_enabled:
                            self._cache_data(cache_key, asset_list_df)
                        return asset_list_df
                    else:
                        logger.info(f"ğŸ“¥ DuckDBä¸­æ²¡æœ‰{asset_type}èµ„äº§æ•°æ®")
                except Exception as e:
                    logger.warning(f"âš ï¸ DuckDB{asset_type}èµ„äº§åˆ—è¡¨è·å–å¤±è´¥: {e}")

            # 2. å¦‚æœDuckDBæ²¡æœ‰æ•°æ®ï¼Œè®°å½•è­¦å‘Šä½†ä¸å†ä½¿ç”¨æ’ä»¶ç³»ç»Ÿ
            logger.warning(f"âš ï¸ DuckDBä¸­æ²¡æœ‰{asset_type}èµ„äº§æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æ­£ç¡®åˆå§‹åŒ–")
            logger.info("ğŸ’¡ æç¤ºï¼šç³»ç»Ÿç°åœ¨å®Œå…¨ä¾èµ–DuckDBæ•°æ®åº“ï¼Œä¸å†ä½¿ç”¨æ•°æ®æºæ’ä»¶")
            logger.info("ğŸ’¡ å»ºè®®ï¼šè¯·è¿è¡Œæ•°æ®å¯¼å…¥è„šæœ¬æ¥åˆå§‹åŒ–DuckDBæ•°æ®åº“")

            # è¿”å›ç©ºDataFrameï¼Œä½†ä¿æŒæ­£ç¡®çš„åˆ—ç»“æ„
            import pandas as pd
            return pd.DataFrame(columns=['code', 'name', 'market', 'industry', 'sector', 'list_date', 'status', 'asset_type'])

        except Exception as e:
            logger.error(f"è·å–{asset_type}èµ„äº§åˆ—è¡¨å¤±è´¥: {e}")
            import pandas as pd
            return pd.DataFrame()

    def _get_asset_list_from_duckdb(self, asset_type: str, market: str = None) -> pd.DataFrame:
        """ä»DuckDBæ•°æ®åº“è·å–èµ„äº§åˆ—è¡¨ - æ”¯æŒå¤šç§èµ„äº§ç±»å‹"""
        try:
            import pandas as pd

            if not self.duckdb_operations:
                logger.warning("DuckDBæ“ä½œå™¨ä¸å¯ç”¨")
                return pd.DataFrame()

            # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºAssetTypeæšä¸¾
            from ..plugin_types import AssetType
            asset_type_enum_mapping = {
                'stock': AssetType.STOCK_A,  # é»˜è®¤ä½¿ç”¨STOCK_Aï¼ˆAè‚¡ï¼‰é¢å‘ä¸­å›½ç”¨æˆ·
                'crypto': AssetType.CRYPTO,
                'fund': AssetType.FUND,
                'bond': AssetType.BOND,
                'index': AssetType.INDEX,
                'sector': AssetType.SECTOR
            }
            asset_type_enum = asset_type_enum_mapping.get(asset_type, AssetType.STOCK_A)

            # âœ… æ–°æ¶æ„ï¼šæ‰€æœ‰èµ„äº§ç±»å‹ç»Ÿä¸€ä½¿ç”¨asset_metadataè¡¨
            table_name = 'asset_metadata'

            # èµ„äº§ç±»å‹æ˜ å°„ï¼ˆç”¨äºWHEREæ¡ä»¶ï¼‰
            asset_type_value_mapping = {
                'stock': 'stock_a',     # é»˜è®¤Aè‚¡
                'crypto': 'crypto',
                'fund': 'fund',
                'bond': 'bond',
                'index': 'index',
                'sector': 'sector'
            }
            asset_type_value = asset_type_value_mapping.get(asset_type, 'stock_a')

            # æ„å»ºæŸ¥è¯¢è¯­å¥ï¼ˆä½¿ç”¨æ–°çš„å­—æ®µåï¼‰
            # æ–°å­—æ®µæ˜ å°„ï¼šlist_dateâ†’listing_date, statusâ†’listing_status
            # åªé€‰æ‹©æœ‰å®é™…å€¼çš„æ ¸å¿ƒå­—æ®µï¼Œå‡å°‘ç©ºåˆ—æ˜¾ç¤º
            if market and market != 'all':
                query = f"""
                SELECT DISTINCT 
                    symbol as code,
                    name,
                    market,
                    CASE WHEN industry IS NOT NULL AND industry != '' THEN industry ELSE NULL END as industry,
                    CASE WHEN sector IS NOT NULL AND sector != '' THEN sector ELSE NULL END as sector,
                    listing_date as list_date,
                    listing_status as status
                FROM {table_name} 
                WHERE market = '{market.upper()}' 
                  AND listing_status = 'active'
                  AND asset_type = '{asset_type_value}'
                ORDER BY symbol
                """
            else:
                query = f"""
                SELECT DISTINCT 
                    symbol as code,
                    name,
                    market,
                    CASE WHEN industry IS NOT NULL AND industry != '' THEN industry ELSE NULL END as industry,
                    CASE WHEN sector IS NOT NULL AND sector != '' THEN sector ELSE NULL END as sector,
                    listing_date as list_date,
                    listing_status as status
                FROM {table_name} 
                WHERE listing_status = 'active'
                  AND asset_type = '{asset_type_value}'
                ORDER BY symbol
                """

            # æ‰§è¡ŒæŸ¥è¯¢ - ä½¿ç”¨query_dataæ–¹æ³•
            import sys
            import io

            # æ•è·æ‰€æœ‰è¾“å‡º
            old_stdout = sys.stdout
            old_stderr = sys.stderr
            captured_stdout = io.StringIO()
            captured_stderr = io.StringIO()

            try:
                sys.stdout = captured_stdout
                sys.stderr = captured_stderr

                result = self.duckdb_operations.query_data(
                    database_path=self.asset_manager.get_database_path(asset_type_enum),
                    table_name=table_name,
                    custom_sql=query
                )
            finally:
                sys.stdout = old_stdout
                sys.stderr = old_stderr

                # æ£€æŸ¥æ˜¯å¦æœ‰è¾“å‡º
                stdout_content = captured_stdout.getvalue()
                stderr_content = captured_stderr.getvalue()

                if stdout_content:
                    logger.warning(f"[CAPTURED STDOUT] query_data produced stdout output: {stdout_content!r}")
                if stderr_content:
                    logger.warning(f"[CAPTURED STDERR] query_data produced stderr output: {stderr_content!r}")

            # DEBUG: æ£€æŸ¥resultå¯¹è±¡
            logger.debug(f"[DEBUG] query_data returned: type={type(result)}, success={result.success if result else 'None'}")

            if result.success and not result.data.empty:
                df = result.data
                logger.debug(f"ä»DuckDBè·å–{asset_type}èµ„äº§åˆ—è¡¨æˆåŠŸ: {len(df)} ä¸ªèµ„äº§")  # ä¼˜åŒ–ï¼šæ”¹ä¸ºdebugçº§åˆ«å‡å°‘æ—¥å¿—å™ªéŸ³
                return df
            else:
                logger.info(f"DuckDBä¸­æ²¡æœ‰{asset_type}èµ„äº§åˆ—è¡¨æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä»DuckDBè·å–{asset_type}èµ„äº§åˆ—è¡¨å¤±è´¥: {e}")
            return pd.DataFrame()

    def _get_kdata_from_duckdb(self, stock_code: str, period: str, count: int, data_source: str = None, asset_type: AssetType = None) -> pd.DataFrame:
        """âœ… ä¼˜åŒ–ï¼šä»DuckDBè·å–Kçº¿æ•°æ®ï¼ˆä½¿ç”¨è§†å›¾è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è´¨é‡æ•°æ®ï¼‰"""
        try:
            if not self.duckdb_operations:
                logger.debug("DuckDB operationsä¸å¯ç”¨")
                return pd.DataFrame()

            # ä½¿ç”¨asset-separatedæ¶æ„çš„æ•°æ®åº“
            final_asset_type = asset_type or AssetType.STOCK_A
            database_path = self.asset_manager.get_database_path(final_asset_type)
            logger.debug(f"ğŸ“Š DuckDBè·¯å¾„: {database_path}, èµ„äº§ç±»å‹: {final_asset_type.value}")

            # âœ… å‘¨æœŸåˆ°é¢‘ç‡çš„æ˜ å°„ï¼ˆDuckDBè¡¨ä¸­çš„frequencyå­—æ®µï¼‰
            period_to_frequency_map = {
                'D': '1d', 'W': '1w', 'M': '1M',
                '1': '1min', '5': '5min', '15': '15min',
                '30': '30min', '60': '60min',
                'daily': '1d', 'weekly': '1w', 'monthly': '1M'
            }
            frequency = period_to_frequency_map.get(period, '1d')
            logger.debug(f"ğŸ“Š å‘¨æœŸæ˜ å°„: {period} -> {frequency}")

            # ğŸ”§ ä¿®å¤ï¼šå…ˆå°è¯•ç›´æ¥æŸ¥è¯¢åŸºç¡€è¡¨ï¼Œä¸ä¾èµ–è§†å›¾
            # åŸºç¡€è¡¨æŸ¥è¯¢ï¼ˆæ›´å¯é ï¼‰
            base_query = f"""
                SELECT 
                    symbol as code, 
                    timestamp as datetime, 
                    open, high, low, close, volume, amount,
                    data_source
                FROM historical_kline_data
                WHERE symbol = ? 
                  AND frequency = ?
                ORDER BY timestamp DESC 
                LIMIT ?
            """

            logger.info(f"ğŸ“Š [åŸºç¡€è¡¨æŸ¥è¯¢] database={database_path}, symbol={stock_code}, frequency={frequency}, limit={count}")

            try:
                # å…ˆå°è¯•åŸºç¡€è¡¨
                result = self.duckdb_operations.execute_query(
                    database_path=database_path,
                    query=base_query,
                    params=[stock_code, frequency, count]
                )

                if result.success and result.data is not None:
                    if isinstance(result.data, pd.DataFrame):
                        df = result.data
                    else:
                        df = pd.DataFrame(result.data)

                    if not df.empty:
                        logger.info(f"âœ… [åŸºç¡€è¡¨æŸ¥è¯¢æˆåŠŸ]: {stock_code}, frequency={frequency}, {len(df)} æ¡è®°å½•, æ•°æ®æº: {df['data_source'].unique().tolist() if 'data_source' in df.columns else 'æœªçŸ¥'}")
                        # âœ… ä¿®å¤ï¼šå¯¹ä»DuckDBè·å–çš„æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å’Œæ’åº
                        df = self._standardize_kdata_format(df, stock_code)
                        return df
                    else:
                        logger.warning(f"âš ï¸  [åŸºç¡€è¡¨æŸ¥è¯¢ç»“æœä¸ºç©º]: {stock_code}, frequency={frequency}")
                else:
                    logger.warning(f"âš ï¸  [åŸºç¡€è¡¨æŸ¥è¯¢å¤±è´¥æˆ–æ— æ•°æ®]: {stock_code}, success={result.success if result else None}")

            except Exception as base_error:
                logger.error(f"âŒ [åŸºç¡€è¡¨æŸ¥è¯¢å¼‚å¸¸]: {stock_code}, error={base_error}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")

            # å¦‚æœåŸºç¡€è¡¨ä¹Ÿæ²¡æ•°æ®ï¼Œå°è¯•è§†å›¾æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
            try:
                view_query = f"""
                    SELECT 
                        symbol as code, 
                        timestamp as datetime, 
                        open, high, low, close, volume, amount
                    FROM unified_best_quality_kline
                    WHERE symbol = ? 
                      AND frequency = ?
                    ORDER BY timestamp DESC 
                    LIMIT ?
                """

                logger.debug(f"ğŸ“Š [è§†å›¾æŸ¥è¯¢] å°è¯•ä½¿ç”¨è´¨é‡ä¼˜é€‰è§†å›¾...")

                result = self.duckdb_operations.execute_query(
                    database_path=database_path,
                    query=view_query,
                    params=[stock_code, frequency, count]
                )

                if result.success and result.data is not None:
                    df = result.data if isinstance(result.data, pd.DataFrame) else pd.DataFrame(result.data)
                    if not df.empty:
                        logger.info(f"âœ… [è§†å›¾æŸ¥è¯¢æˆåŠŸï¼ˆè´¨é‡ä¼˜é€‰ï¼‰]: {stock_code}, {len(df)} æ¡è®°å½•")
                        # âœ… ä¿®å¤ï¼šå¯¹ä»è§†å›¾è·å–çš„æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å’Œæ’åº
                        df = self._standardize_kdata_format(df, stock_code)
                        return df

            except Exception as view_error:
                logger.warning(f"âš ï¸  [è§†å›¾æŸ¥è¯¢å¤±è´¥]: {view_error}")

            logger.warning(f"âŒ [DuckDBæ— æ•°æ®]: {stock_code} (åŸºç¡€è¡¨å’Œè§†å›¾éƒ½æ— æ•°æ®)")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ [DuckDBæ•°æ®è·å–å¤±è´¥]: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return pd.DataFrame()

    def _store_to_duckdb(self, data: pd.DataFrame, stock_code: str, period: str):
        """å­˜å‚¨æ•°æ®åˆ°DuckDB"""
        try:
            if not self.duckdb_operations or data.empty:
                return

            # è¯†åˆ«èµ„äº§ç±»å‹
            asset_type = self.asset_identifier.identify_asset_type(stock_code)
            db_path = self.asset_manager.get_database_path(asset_type)

            table_name = f"kline_data_{period.lower()}"

            # ç¡®ä¿è¡¨å­˜åœ¨
            if self.table_manager:
                from ..database.table_manager import TableType
                actual_table_name = self.table_manager.ensure_table_exists(
                    db_path, TableType.KLINE_DATA, "unified_data_manager", period
                )
                if actual_table_name:
                    table_name = actual_table_name

            # æ’å…¥æ•°æ®ï¼ˆä½¿ç”¨upserté¿å…é‡å¤ï¼‰
            result = self.duckdb_operations.insert_dataframe(
                database_path=db_path,
                table_name=table_name,
                data=data,
                upsert=True
            )

            if result.success:
                logger.info(f" æ•°æ®å­˜å‚¨åˆ°DuckDBæˆåŠŸ: {stock_code}, {len(data)}æ¡")

        except Exception as e:
            logger.warning(f"DuckDBæ•°æ®å­˜å‚¨å¤±è´¥: {e}")

    # Kçº¿æ•°æ®è·å–ç»Ÿä¸€ä½¿ç”¨DuckDBä¼˜å…ˆæ¶æ„

    def get_historical_data(self, symbol: str, asset_type=None, period: str = "D", count: int = 365, **kwargs) -> Optional[pd.DataFrame]:
        """
        è·å–å†å²æ•°æ®ï¼ˆå…¼å®¹AssetServiceæ¥å£ï¼‰

        Args:
            symbol: èµ„äº§ä»£ç 
            asset_type: èµ„äº§ç±»å‹ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå¯é€‰ï¼‰
            period: å‘¨æœŸ
            count: æ•°æ®æ¡æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Optional[pd.DataFrame]: å†å²æ•°æ®
        """
        try:
            # å¯¹äºè‚¡ç¥¨æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨get_kdataæ–¹æ³•
            return self.get_kdata(symbol, period, count)
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥ {symbol}: {e}")
            return None

    # æ•°æ®è·å–ç»Ÿä¸€ä½¿ç”¨DuckDBä¼˜å…ˆæ¶æ„

    def _standardize_kdata_format(self, df: pd.DataFrame, stock_code: str) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Kçº¿æ•°æ®æ ¼å¼"""
        try:
            if df.empty:
                return df

            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.warning(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return pd.DataFrame()

            # âœ… ä¿®å¤ï¼šå¤„ç†datetimeåˆ—å’Œç´¢å¼•ï¼Œé¿å…datetimeæ—¢æ˜¯ç´¢å¼•åˆæ˜¯åˆ—
            if 'datetime' not in df.columns:
                # å¦‚æœæ²¡æœ‰datetimeåˆ—ï¼Œå°è¯•ä»ç´¢å¼•æˆ–dateåˆ—è·å–
                if isinstance(df.index, pd.DatetimeIndex):
                    # âœ… å…³é”®ä¿®å¤ï¼šå°†ç´¢å¼•è½¬ä¸ºåˆ—åï¼Œå¿…é¡»é‡ç½®ç´¢å¼•ä¸ºæ•°å­—ç´¢å¼•
                    df['datetime'] = df.index
                    df = df.reset_index(drop=True)
                    logger.debug("ä»DatetimeIndexåˆ›å»ºdatetimeåˆ—å¹¶é‡ç½®ç´¢å¼•")
                elif 'date' in df.columns:
                    df['datetime'] = pd.to_datetime(df['date'])
                else:
                    logger.warning("Kçº¿æ•°æ®ç¼ºå°‘datetimeå­—æ®µ")
                    return pd.DataFrame()
            else:
                # ç¡®ä¿datetimeåˆ—æ˜¯datetimeç±»å‹
                df['datetime'] = pd.to_datetime(df['datetime'])
                # âœ… ä¿®å¤ï¼šå¦‚æœdatetimeåŒæ—¶æ˜¯ç´¢å¼•åï¼Œé‡ç½®ç´¢å¼•é¿å…æ­§ä¹‰
                if df.index.name == 'datetime' or isinstance(df.index, pd.DatetimeIndex):
                    df = df.reset_index(drop=True)
                    logger.debug("æ£€æµ‹åˆ°datetimeåŒæ—¶æ˜¯åˆ—å’Œç´¢å¼•ï¼Œå·²é‡ç½®ç´¢å¼•")

            # æ•°æ®æ¸…æ´—
            df = df.replace([np.inf, -np.inf], np.nan)
            df = df.dropna(subset=['close'])  # è‡³å°‘è¦æœ‰æ”¶ç›˜ä»·

            # âœ… ä¿®å¤ï¼šç¡®ä¿code/symbolå­—æ®µå­˜åœ¨
            if 'code' not in df.columns and 'symbol' not in df.columns:
                df['code'] = stock_code
                logger.debug(f"æ·»åŠ codeå­—æ®µ: {stock_code}")
            elif 'symbol' in df.columns and 'code' not in df.columns:
                # å¦‚æœåªæœ‰symbolæ²¡æœ‰codeï¼Œä¿æŒsymbolä¸å˜
                logger.debug(f"æ•°æ®å·²åŒ…å«symbolå­—æ®µï¼Œè·³è¿‡codeå­—æ®µæ·»åŠ ")
            elif 'code' in df.columns and 'symbol' not in df.columns:
                # å¦‚æœåªæœ‰codeæ²¡æœ‰symbolï¼Œä¿æŒcodeä¸å˜
                logger.debug(f"æ•°æ®å·²åŒ…å«codeå­—æ®µï¼Œå°†åœ¨åç»­è½¬æ¢ä¸ºsymbol")

            # ç¡®ä¿amountå­—æ®µå­˜åœ¨
            if 'amount' not in df.columns:
                df['amount'] = 0.0

            # æ•°æ®ç±»å‹è½¬æ¢
            for col in ['open', 'high', 'low', 'close', 'volume', 'amount']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # âœ… ä¿®å¤ï¼šç»Ÿä¸€æŒ‰æ—¶é—´å‡åºæ’åºï¼Œç¡®ä¿Kçº¿å›¾æ˜¾ç¤ºé¡ºåºæ­£ç¡®
            # è¿™æ˜¯è§£å†³Kçº¿æ•°æ®å±•ç¤ºé¡ºåºé”™ä¹±é—®é¢˜çš„å…³é”®ä¿®å¤
            if 'datetime' in df.columns and not df.empty:
                try:
                    # ç¡®ä¿datetimeåˆ—æ˜¯datetimeç±»å‹ï¼ˆä¹‹å‰å·²ç»å¤„ç†è¿‡ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    # æŒ‰datetimeå‡åºæ’åºï¼ˆæ—¶é—´ä»æ—§åˆ°æ–°ï¼‰
                    df = df.sort_values(by='datetime', ascending=True).reset_index(drop=True)
                    logger.debug(f"âœ… Kçº¿æ•°æ®å·²æŒ‰æ—¶é—´å‡åºæ’åº: {stock_code}, è®°å½•æ•°={len(df)}, æ—¶é—´èŒƒå›´={df['datetime'].min()} ~ {df['datetime'].max()}")
                except Exception as sort_error:
                    logger.warning(f"âš ï¸ Kçº¿æ•°æ®æ’åºå¤±è´¥: {stock_code}, é”™è¯¯={sort_error}")
                    # å¦‚æœæ’åºå¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸ä¸­æ–­æµç¨‹

            return df

        except Exception as e:
            logger.error(f"æ ‡å‡†åŒ–Kçº¿æ•°æ®æ ¼å¼å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_stock_info(self, stock_code: str) -> Optional[Dict[str, Any]]:
        """è·å–è‚¡ç¥¨ä¿¡æ¯"""
        try:
            # FactorWeave-Quantå·²ç§»é™¤ï¼Œä½¿ç”¨TETæ¡†æ¶è·å–è‚¡ç¥¨ä¿¡æ¯

            # ä»è‚¡ç¥¨åˆ—è¡¨ä¸­æŸ¥æ‰¾
            stock_list = self.get_stock_list()
            if not stock_list.empty:
                matches = stock_list[stock_list['code'] == stock_code]
                if not matches.empty:
                    return matches.iloc[0].to_dict()

            return None

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {stock_code} - {e}")
            return None

    def search_stocks(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢è‚¡ç¥¨"""
        try:
            stock_list = self.get_stock_list()
            if stock_list.empty:
                return []

            keyword_lower = keyword.lower()
            matches = stock_list[
                (stock_list['code'].str.lower().str.contains(keyword_lower, na=False)) |
                (stock_list['name'].str.lower().str.contains(keyword_lower, na=False))
            ]

            return matches.to_dict('records')

        except Exception as e:
            logger.error(f"æœç´¢è‚¡ç¥¨å¤±è´¥: {keyword} - {e}")
            return []

    def get_fund_flow(self) -> Dict[str, Any]:
        """è·å–èµ„é‡‘æµæ•°æ® - é€šè¿‡TETæ¡†æ¶å’Œæ•°æ®æºæ’ä»¶è·å–çœŸå®æ•°æ®"""
        try:
            fund_flow_data = {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

            if self.tet_enabled and self.tet_pipeline:
                logger.info("ä½¿ç”¨TETæ•°æ®ç®¡é“è·å–èµ„é‡‘æµæ•°æ®")

                try:
                    # è·å–æ¿å—èµ„é‡‘æµæ•°æ®
                    sector_query = StandardQuery(
                        asset_type=AssetType.SECTOR,
                        data_type=DataType.SECTOR_FUND_FLOW,
                        symbol="",
                        extra_params={"period": "1d", "limit": 50}
                    )
                    sector_result = self.tet_pipeline.process(sector_query)

                    if sector_result and sector_result.success and sector_result.data is not None:
                        if isinstance(sector_result.data, pd.DataFrame):
                            fund_flow_data['sector_flow_rank'] = sector_result.data
                        else:
                            # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œè½¬æ¢ä¸ºDataFrame
                            fund_flow_data['sector_flow_rank'] = pd.DataFrame(sector_result.data)
                        logger.info(f" TETè·å–æ¿å—èµ„é‡‘æµæ•°æ®æˆåŠŸ: {len(fund_flow_data['sector_flow_rank'])} æ¡è®°å½•")
                    else:
                        logger.warning("TETæ¿å—èµ„é‡‘æµæ•°æ®ä¸ºç©ºæˆ–å¤±è´¥")

                except Exception as e:
                    logger.warning(f" TETè·å–æ¿å—èµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")

                try:
                    # è·å–ä¸ªè‚¡èµ„é‡‘æµæ•°æ®
                    individual_query = StandardQuery(
                        asset_type=AssetType.STOCK_A,
                        data_type=DataType.INDIVIDUAL_FUND_FLOW,
                        symbol="",
                        extra_params={"period": "1d", "limit": 100}
                    )
                    individual_result = self.tet_pipeline.process(individual_query)

                    if individual_result and individual_result.success and individual_result.data is not None:
                        if isinstance(individual_result.data, pd.DataFrame):
                            fund_flow_data['individual_flow'] = individual_result.data
                        else:
                            fund_flow_data['individual_flow'] = pd.DataFrame(individual_result.data)
                        logger.info(f" TETè·å–ä¸ªè‚¡èµ„é‡‘æµæ•°æ®æˆåŠŸ: {len(fund_flow_data['individual_flow'])} æ¡è®°å½•")
                    else:
                        logger.warning("TETä¸ªè‚¡èµ„é‡‘æµæ•°æ®ä¸ºç©ºæˆ–å¤±è´¥")

                except Exception as e:
                    logger.warning(f" TETè·å–ä¸ªè‚¡èµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")

                try:
                    # è·å–å¸‚åœºæ•´ä½“èµ„é‡‘æµæ•°æ®
                    market_query = StandardQuery(
                        asset_type=AssetType.INDEX,
                        data_type=DataType.MAIN_FUND_FLOW,
                        symbol="",
                        extra_params={"period": "1d"}
                    )
                    market_result = self.tet_pipeline.process(market_query)

                    if market_result and market_result.success and market_result.data is not None:
                        if isinstance(market_result.data, dict):
                            fund_flow_data['market_flow'] = market_result.data
                        elif isinstance(market_result.data, pd.DataFrame) and not market_result.data.empty:
                            # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸
                            fund_flow_data['market_flow'] = market_result.data.to_dict('records')[0] if len(market_result.data) > 0 else {}
                        else:
                            fund_flow_data['market_flow'] = {}
                        logger.info(f" TETè·å–å¸‚åœºèµ„é‡‘æµæ•°æ®æˆåŠŸ")
                    else:
                        logger.warning("TETå¸‚åœºèµ„é‡‘æµæ•°æ®ä¸ºç©ºæˆ–å¤±è´¥")

                except Exception as e:
                    logger.warning(f" TETè·å–å¸‚åœºèµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")

            else:
                logger.info("é™çº§åˆ°ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼è·å–èµ„é‡‘æµæ•°æ®")
                # ä½¿ç”¨ä¼ ç»Ÿæ•°æ®æºè·å–èµ„é‡‘æµæ•°æ®
                fund_flow_data = self._get_fund_flow_legacy()

            # å¦‚æœæ‰€æœ‰æ•°æ®éƒ½ä¸ºç©ºï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            if (fund_flow_data['sector_flow_rank'].empty and
                fund_flow_data['individual_flow'].empty and
                    not fund_flow_data['market_flow']):
                logger.info("ç”Ÿæˆæ¨¡æ‹Ÿèµ„é‡‘æµæ•°æ®ç”¨äºæµ‹è¯•")
                fund_flow_data = self._generate_mock_fund_flow_data()

            return fund_flow_data

        except Exception as e:
            logger.error(f"è·å–èµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")
            return {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

    def _generate_mock_fund_flow_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿèµ„é‡‘æµæ•°æ®ç”¨äºæµ‹è¯•"""
        import random
        from datetime import datetime, timedelta

        try:
            # ç”Ÿæˆæ¨¡æ‹Ÿæ¿å—èµ„é‡‘æµæ’è¡Œæ•°æ®
            sectors = ['é“¶è¡Œ', 'è¯åˆ¸', 'ä¿é™©', 'æˆ¿åœ°äº§', 'é’¢é“', 'ç…¤ç‚­', 'æœ‰è‰²é‡‘å±', 'çŸ³æ²¹çŸ³åŒ–',
                       'ç”µåŠ›', 'å…¬ç”¨äº‹ä¸š', 'äº¤é€šè¿è¾“', 'ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡', 'åŒ»è¯ç”Ÿç‰©']

            sector_data = []
            for i, sector in enumerate(sectors[:10]):  # å–å‰10ä¸ªæ¿å—
                sector_data.append({
                    'sector_name': sector,
                    'net_inflow': random.uniform(-50000, 100000),  # å‡€æµå…¥(ä¸‡å…ƒ)
                    'main_inflow': random.uniform(10000, 80000),   # ä¸»åŠ›æµå…¥
                    'main_outflow': random.uniform(10000, 60000),  # ä¸»åŠ›æµå‡º
                    'retail_inflow': random.uniform(5000, 30000),  # æ•£æˆ·æµå…¥
                    'retail_outflow': random.uniform(5000, 25000),  # æ•£æˆ·æµå‡º
                    'change_rate': random.uniform(-5.0, 8.0),      # æ¶¨è·Œå¹…%
                    'rank': i + 1
                })

            sector_df = pd.DataFrame(sector_data)

            # ç”Ÿæˆæ¨¡æ‹Ÿä¸ªè‚¡èµ„é‡‘æµæ•°æ®
            stocks = ['000001.SZ', '000002.SZ', '600000.SH', '600036.SH', '000858.SZ']
            individual_data = []
            for stock in stocks:
                individual_data.append({
                    'symbol': stock,
                    'name': f'è‚¡ç¥¨{stock[:6]}',
                    'net_inflow': random.uniform(-10000, 20000),
                    'main_inflow': random.uniform(2000, 15000),
                    'main_outflow': random.uniform(2000, 12000),
                    'price': random.uniform(10.0, 50.0),
                    'change_rate': random.uniform(-3.0, 5.0),
                    'volume': random.randint(100000, 1000000)
                })

            individual_df = pd.DataFrame(individual_data)

            # ç”Ÿæˆæ¨¡æ‹Ÿå¸‚åœºèµ„é‡‘æµæ•°æ®
            market_flow = {
                'total_net_inflow': random.uniform(-500000, 800000),
                'main_net_inflow': random.uniform(-300000, 500000),
                'retail_net_inflow': random.uniform(-200000, 300000),
                'north_fund_inflow': random.uniform(-50000, 100000),
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'market_status': 'open' if 9 <= datetime.now().hour <= 15 else 'closed'
            }

            logger.info(f"ç”Ÿæˆæ¨¡æ‹Ÿèµ„é‡‘æµæ•°æ®: æ¿å—{len(sector_df)}ä¸ª, ä¸ªè‚¡{len(individual_df)}ä¸ª")

            return {
                'sector_flow_rank': sector_df,
                'individual_flow': individual_df,
                'market_flow': market_flow
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨¡æ‹Ÿèµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")
            return {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

    def _get_fund_flow_legacy(self) -> Dict[str, Any]:
        """ä¼ ç»Ÿæ•°æ®æºè·å–èµ„é‡‘æµæ•°æ®"""
        try:
            # èµ„é‡‘æµæ•°æ®é€šè¿‡TETæ¡†æ¶è·å–
            fund_flow_data = {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }
            return fund_flow_data

        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ•°æ®æºè·å–èµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")
            return {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

            # è¿”å›ç©ºçš„èµ„é‡‘æµæ•°æ®ç»“æ„
            logger.info("èµ„é‡‘æµæ•°æ®éœ€è¦é€šè¿‡çœŸå®æ•°æ®æºè·å–")
            return {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨¡æ‹Ÿèµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")
            return {
                'sector_flow_rank': pd.DataFrame(),
                'individual_flow': pd.DataFrame(),
                'market_flow': {}
            }

    def test_connection(self) -> bool:
        """æµ‹è¯•æ•°æ®æºè¿æ¥"""
        try:
            # FactorWeave-Quantå·²ç§»é™¤ï¼Œä½¿ç”¨TETæ¡†æ¶æµ‹è¯•è¿æ¥
            if self._current_source in self._data_sources:
                # å°è¯•è·å–è‚¡ç¥¨åˆ—è¡¨æ¥æµ‹è¯•è¿æ¥
                test_list = self._data_sources[self._current_source].get_stock_list('sh')
                return not test_list.empty
            else:
                return True  # æ¨¡æ‹Ÿæ¨¡å¼æ€»æ˜¯å¯ç”¨

        except Exception as e:
            logger.error(f"æµ‹è¯•æ•°æ®æºè¿æ¥å¤±è´¥: {e}")
            return False

    def get_latest_price(self, stock_code: str) -> float:
        """è·å–æœ€æ–°ä»·æ ¼"""
        try:
            # è·å–æœ€è¿‘çš„Kçº¿æ•°æ®
            kdata = self.get_kdata(stock_code, 'D', 1)
            if not kdata.empty:
                return float(kdata['close'].iloc[-1])
            else:
                return 0.0

        except Exception as e:
            logger.error(f"è·å–æœ€æ–°ä»·æ ¼å¤±è´¥: {stock_code} - {e}")
            return 0.0

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, '_executor'):
                self._executor.shutdown(wait=True)

            # å…³é—­æ•°æ®åº“è¿æ¥
            if self.conn:
                self.conn.close()

            logger.info("ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")

    def get_asset_list_legacy_tet(self, asset_type: AssetType, market: str = None) -> List[Dict[str, Any]]:
        """
        è·å–èµ„äº§åˆ—è¡¨ï¼ˆå…¼å®¹æ¥å£ï¼‰- é‡å®šå‘åˆ°DuckDBä¼˜å…ˆæ–¹æ³•

        Args:
            asset_type: èµ„äº§ç±»å‹
            market: å¸‚åœºè¿‡æ»¤

        Returns:
            List[Dict]: æ ‡å‡†åŒ–çš„èµ„äº§åˆ—è¡¨
        """
        if self.tet_enabled and self.tet_pipeline:
            try:
                # æ‡’åŠ è½½æ£€æŸ¥ï¼šå¦‚æœæ’ä»¶è¿˜æ²¡å‘ç°ï¼Œé‡æ–°å°è¯•å‘ç°
                if not self._plugins_discovered:
                    logger.info("TETç®¡é“é¦–æ¬¡ä½¿ç”¨ï¼Œé‡æ–°å°è¯•æ’ä»¶å‘ç°...")
                    self._auto_discover_data_source_plugins()

                logger.info("ä½¿ç”¨TETæ•°æ®ç®¡é“è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ’ä»¶åŒ–æ¶æ„ï¼‰")
                query = StandardQuery(
                    symbol="",  # èµ„äº§åˆ—è¡¨æŸ¥è¯¢ä¸éœ€è¦å…·ä½“symbol
                    asset_type=asset_type,
                    data_type=DataType.ASSET_LIST,
                    market=market
                )

                result = self.tet_pipeline.process(query)

                # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºç©º
                if not result.data or len(result.data) == 0:
                    logger.warning("TETç®¡é“è¿”å›ç©ºæ•°æ®")
                    raise Exception("TETç®¡é“è¿”å›ç©ºæ•°æ®")

                return self._format_asset_list(result.data)

            except Exception as e:
                logger.warning(f"TETæ¨¡å¼è·å–èµ„äº§åˆ—è¡¨å¤±è´¥: {e}")
                logger.info("é™çº§åˆ°ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼")

        # é‡å®šå‘åˆ°æ–°çš„ç»Ÿä¸€èµ„äº§åˆ—è¡¨æ–¹æ³•ï¼ˆDuckDBä¼˜å…ˆï¼‰
        logger.info("ğŸ”„ é‡å®šå‘åˆ°DuckDBä¼˜å…ˆçš„èµ„äº§åˆ—è¡¨æ–¹æ³•")
        asset_type_str = asset_type.value.lower()
        df = self.get_asset_list(asset_type=asset_type_str, market=market)

        # è½¬æ¢DataFrameä¸ºList[Dict]æ ¼å¼ä»¥ä¿æŒæ¥å£å…¼å®¹æ€§
        if not df.empty:
            return df.to_dict('records')
        else:
            logger.warning(f"DuckDBä¸­æ²¡æœ‰{asset_type_str}èµ„äº§æ•°æ®")
            return []

    def get_current_source(self) -> str:
        """è·å–å½“å‰æ•°æ®æº"""
        return getattr(self, '_current_source', 'tet_framework')

    def get_historical_data(self, symbol: str, asset_type: AssetType = AssetType.STOCK_A,
                            period: str = "D", count: int = 365, **kwargs) -> Optional[pd.DataFrame]:
        """
        è·å–å†å²æ•°æ®ï¼ˆå…¼å®¹AssetServiceæ¥å£ï¼‰

        Args:
            symbol: èµ„äº§ä»£ç 
            asset_type: èµ„äº§ç±»å‹
            period: å‘¨æœŸ
            count: æ•°æ®æ¡æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Optional[pd.DataFrame]: å†å²æ•°æ®
        """
        try:
            if asset_type == AssetType.STOCK_A:
                # å¯¹äºè‚¡ç¥¨ï¼Œä½¿ç”¨get_kdataæ–¹æ³•
                return self.get_kdata(symbol, period, count)
            else:
                # å¯¹äºå…¶ä»–èµ„äº§ç±»å‹ï¼Œä½¿ç”¨get_asset_dataæ–¹æ³•
                return self.get_asset_data(symbol, asset_type, DataType.HISTORICAL_KLINE, period, **kwargs)
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥ {symbol}: {e}")
            return None

    def get_asset_data(self, symbol: str, asset_type: AssetType = AssetType.STOCK_A,
                       data_type: DataType = DataType.HISTORICAL_KLINE,
                       period: str = "D", **kwargs) -> Optional[pd.DataFrame]:
        """
        è·å–èµ„äº§æ•°æ®ï¼ˆTETæ¨¡å¼ï¼‰

        Args:
            symbol: äº¤æ˜“ä»£ç 
            asset_type: èµ„äº§ç±»å‹
            data_type: æ•°æ®ç±»å‹
            period: å‘¨æœŸ
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Optional[pd.DataFrame]: æ ‡å‡†åŒ–æ•°æ®
        """
        if self.tet_enabled and self.tet_pipeline:
            try:
                logger.info(f" ä½¿ç”¨TETæ¨¡å¼è·å–æ•°æ®: {symbol} ({asset_type.value})")

                query = StandardQuery(
                    symbol=symbol,
                    asset_type=asset_type,
                    data_type=data_type,
                    period=period,
                    **kwargs
                )

                result = self.tet_pipeline.process(query)

                # è®°å½•ä½¿ç”¨çš„æ•°æ®æº
                if result and hasattr(result, 'source_info') and result.source_info:
                    data_source = result.source_info.get('provider', 'Unknown')
                    logger.info(f" TETæ•°æ®è·å–æˆåŠŸ: {symbol} | æ•°æ®æº: {data_source} | è®°å½•æ•°: {len(result.data) if result.data is not None else 0}")
                else:
                    logger.info(f" TETæ•°æ®è·å–æˆåŠŸ: {symbol} | è®°å½•æ•°: {len(result.data) if result.data is not None else 0}")

                return result.data

            except Exception as e:
                logger.warning(f" TETæ¨¡å¼è·å–æ•°æ®å¤±è´¥: {symbol} - {e}")
                logger.info("é™çº§åˆ°ä¼ ç»Ÿæ•°æ®è·å–æ¨¡å¼")

        # é™çº§åˆ°ä¼ ç»Ÿæ–¹å¼
        if asset_type == AssetType.STOCK_A:
            logger.info(f" ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼è·å–è‚¡ç¥¨æ•°æ®: {symbol}")
            data = self._legacy_get_stock_data(symbol, period, **kwargs)
            if data is not None:
                logger.info(f" ä¼ ç»Ÿæ¨¡å¼æ•°æ®è·å–æˆåŠŸ: {symbol} | æ•°æ®æº: DataAccess | è®°å½•æ•°: {len(data)}")
            else:
                logger.warning(f" ä¼ ç»Ÿæ¨¡å¼æ•°æ®è·å–å¤±è´¥: {symbol}")
            return data
        else:
            logger.warning(f" ä¼ ç»Ÿæ¨¡å¼ä¸æ”¯æŒèµ„äº§ç±»å‹: {asset_type.value} | å»ºè®®å¯ç”¨TETæ¨¡å¼")
            return None

    def _format_asset_list(self, asset_data: pd.DataFrame) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–èµ„äº§åˆ—è¡¨ä¸ºæ ‡å‡†æ ¼å¼"""
        if asset_data.empty:
            return []

        result = []
        for _, row in asset_data.iterrows():
            result.append({
                'symbol': row.get('symbol', ''),
                'name': row.get('name', ''),
                'asset_type': row.get('asset_type', ''),
                'market': row.get('market', ''),
                'status': row.get('status', 'active')
            })

        return result

    def register_data_source_plugin(self, plugin_id: str, adapter, priority: int = 0, weight: float = 1.0) -> bool:
        """
        æ³¨å†Œæ•°æ®æºæ’ä»¶åˆ°è·¯ç”±å™¨å’ŒTETç®¡é“

        Args:
            plugin_id: æ’ä»¶ID
            adapter: æ’ä»¶é€‚é…å™¨
            priority: ä¼˜å…ˆçº§
            weight: æƒé‡

        Returns:
            bool: æ³¨å†Œæ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥TETç®¡é“æ˜¯å¦å¯ç”¨
            if not (hasattr(self, 'tet_pipeline') and self.tet_pipeline):
                logger.warning("TETæ•°æ®ç®¡é“ä¸å¯ç”¨ï¼Œæ— æ³•æ³¨å†Œæ’ä»¶")
                return False

            # æ³¨å†Œåˆ°TETç®¡é“çš„è·¯ç”±å™¨
            if hasattr(self.tet_pipeline, 'router'):
                router = self.tet_pipeline.router
                router_success = router.register_data_source(plugin_id, adapter, priority, weight)
                if router_success:
                    logger.info(f" æ’ä»¶ {plugin_id} å·²æ³¨å†Œåˆ°TETæ•°æ®ç®¡é“è·¯ç”±å™¨")
                else:
                    logger.error(f" æ’ä»¶ {plugin_id} æ³¨å†Œåˆ°TETæ•°æ®ç®¡é“è·¯ç”±å™¨å¤±è´¥")
                    return False
            else:
                logger.error("TETæ•°æ®ç®¡é“ç¼ºå°‘è·¯ç”±å™¨")
                return False

            # å…³é”®ä¿®å¤ï¼šåŒæ—¶æ³¨å†Œåˆ°TETç®¡é“çš„é€‚é…å™¨å­—å…¸
            if hasattr(self.tet_pipeline, '_adapters'):
                self.tet_pipeline._adapters[plugin_id] = adapter
                logger.info(f" æ’ä»¶ {plugin_id} å·²æ³¨å†Œåˆ°TETç®¡é“é€‚é…å™¨å­—å…¸")
            else:
                logger.warning("TETç®¡é“ç¼ºå°‘_adapterså±æ€§")

            # å¦‚æœé€‚é…å™¨æœ‰å¯¹åº”çš„æ’ä»¶å®ä¾‹ï¼Œä¹Ÿæ³¨å†Œåˆ°_pluginså­—å…¸
            if hasattr(adapter, 'plugin') and hasattr(self.tet_pipeline, '_plugins'):
                self.tet_pipeline._plugins[plugin_id] = adapter.plugin
                logger.info(f" æ’ä»¶ {plugin_id} å·²æ³¨å†Œåˆ°TETç®¡é“æ’ä»¶å­—å…¸")

            # è®°å½•å·²æ³¨å†Œçš„æ•°æ®æºä¿¡æ¯
            plugin_info = {
                'plugin_id': plugin_id,
                'adapter': adapter,
                'priority': priority,
                'weight': weight,
                'display_name': getattr(adapter, 'display_name', plugin_id),
                'supported_assets': getattr(adapter, 'supported_assets', []),
                'status': 'active'
            }
            self._registered_data_sources[plugin_id] = plugin_info
            logger.info(f" æ•°æ®æº {plugin_id} ä¿¡æ¯å·²è®°å½•")

            return True

        except Exception as e:
            logger.error(f" æ³¨å†Œæ•°æ®æºæ’ä»¶å¤±è´¥ {plugin_id}: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False

    def get_registered_data_sources(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·²æ³¨å†Œçš„æ•°æ®æº

        Returns:
            Dict[str, Dict[str, Any]]: å·²æ³¨å†Œçš„æ•°æ®æºä¿¡æ¯
        """
        return self._registered_data_sources.copy()

    def get_available_data_source_names(self) -> List[str]:
        """
        è·å–å¯ç”¨æ•°æ®æºåç§°åˆ—è¡¨

        Returns:
            List[str]: æ•°æ®æºåç§°åˆ—è¡¨
        """
        # åŸºç¡€æ•°æ®æº
        base_sources = ['ä¸œæ–¹è´¢å¯Œ', 'æ–°æµªè´¢ç»', 'åŒèŠ±é¡º']

        # æ·»åŠ å·²æ³¨å†Œçš„æ’ä»¶æ•°æ®æº
        plugin_sources = []
        for plugin_id, info in self._registered_data_sources.items():
            display_name = info.get('display_name', plugin_id)
            if display_name not in base_sources:
                plugin_sources.append(display_name)

        # åˆå¹¶å¹¶å»é‡
        all_sources = base_sources + plugin_sources
        return list(dict.fromkeys(all_sources))  # ä¿æŒé¡ºåºçš„å»é‡

    def get_data_source_info(self, plugin_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ•°æ®æºçš„è¯¦ç»†ä¿¡æ¯

        Args:
            plugin_id: æ•°æ®æºæ’ä»¶ID

        Returns:
            Optional[Dict[str, Any]]: æ•°æ®æºä¿¡æ¯æˆ–None
        """
        return self._registered_data_sources.get(plugin_id)

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®ç®¡ç†å™¨çš„ç»Ÿè®¡ä¿¡æ¯

        ç”¨äºæ•°æ®è´¨é‡ç›‘æ§å’Œç³»ç»ŸçŠ¶æ€è¯„ä¼°

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
                - requests: è¯·æ±‚ç»Ÿè®¡
                - cache: ç¼“å­˜ç»Ÿè®¡
                - data_sources: æ•°æ®æºç»Ÿè®¡
                - data_quality: æ•°æ®è´¨é‡ç»Ÿè®¡
                - system: ç³»ç»ŸçŠ¶æ€ç»Ÿè®¡
        """
        try:
            # 1. è¯·æ±‚ç»Ÿè®¡
            request_stats = self._stats.copy()

            # è®¡ç®—æˆåŠŸç‡
            total_requests = request_stats.get('requests_total', 0)
            if total_requests > 0:
                success_rate = (request_stats.get('requests_completed', 0) / total_requests) * 100
                request_stats['success_rate'] = round(success_rate, 2)
            else:
                request_stats['success_rate'] = 0.0

            # 2. ç¼“å­˜ç»Ÿè®¡
            cache_total = request_stats.get('cache_hits', 0) + request_stats.get('cache_misses', 0)
            if cache_total > 0:
                cache_hit_rate = (request_stats.get('cache_hits', 0) / cache_total) * 100
            else:
                cache_hit_rate = 0.0

            cache_stats = {
                'hits': request_stats.get('cache_hits', 0),
                'misses': request_stats.get('cache_misses', 0),
                'hit_rate': round(cache_hit_rate, 2),
                'total_queries': cache_total
            }

            # 3. æ•°æ®æºç»Ÿè®¡
            data_source_stats = {
                'total_registered': len(self._registered_data_sources),
                'available_sources': len(self.get_available_data_source_names()),
                'registered_plugins': list(self._registered_data_sources.keys())
            }

            # 4. æ•°æ®è´¨é‡ç»Ÿè®¡ï¼ˆåŸºäºè¯·æ±‚ç»Ÿè®¡ä¼°ç®—ï¼‰
            # ä¸ºUIæ•°æ®è´¨é‡ç›‘æ§æä¾›æ‰€éœ€çš„å­—æ®µ
            completed = request_stats.get('requests_completed', 0)
            failed = request_stats.get('requests_failed', 0)

            quality_stats = {
                # UIæœŸæœ›çš„å­—æ®µ
                'expected_records': total_requests,  # é¢„æœŸè®°å½•æ•°
                'actual_records': completed,  # å®é™…è®°å½•æ•°
                'total_count': completed,  # æ€»æ•°ï¼ˆå®é™…å®Œæˆçš„ï¼‰
                'error_count': failed,  # é”™è¯¯æ•°
                'failed_records': failed,  # å¤±è´¥è®°å½•æ•°
                'cancelled_records': request_stats.get('requests_cancelled', 0),  # å–æ¶ˆè®°å½•æ•°
                'inconsistent_records': 0,  # ä¸ä¸€è‡´è®°å½•æ•°ï¼ˆæš‚æ— ï¼‰
                'invalid_records': failed,  # æ— æ•ˆè®°å½•æ•°ï¼ˆä¸å¤±è´¥æ•°ç›¸åŒï¼‰
                'duplicate_records': 0,  # é‡å¤è®°å½•æ•°ï¼ˆæš‚æ— ï¼‰
                'quality_score': request_stats.get('success_rate', 0) / 100,  # è´¨é‡åˆ†æ•°ï¼ˆ0-1ï¼‰
                'last_update_time': datetime.now()  # æœ€åæ›´æ–°æ—¶é—´
            }

            # 5. ç³»ç»ŸçŠ¶æ€ç»Ÿè®¡
            system_stats = {
                'initialized': self._is_initialized,
                'tet_enabled': self.tet_enabled,
                'plugins_discovered': self._plugins_discovered,
                'active_requests': len(self._active_requests),
                'pending_requests': len(self._pending_requests),
                'completed_requests': len(self._completed_requests)
            }

            # 6. DuckDBç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            duckdb_stats = {}
            if hasattr(self, 'duckdb_manager') and self.duckdb_manager:
                try:
                    # è·å–DuckDBè¿æ¥æ± ç»Ÿè®¡
                    duckdb_stats = {
                        'enabled': True,
                        'database_path': str(getattr(self.duckdb_manager, 'db_path', 'unknown'))
                    }
                except:
                    duckdb_stats = {'enabled': False}
            else:
                duckdb_stats = {'enabled': False}

            # ç»„è£…å®Œæ•´ç»Ÿè®¡ä¿¡æ¯
            statistics = {
                'requests': request_stats,
                'cache': cache_stats,
                'data_sources': data_source_stats,
                'data_quality': quality_stats,
                'system': system_stats,
                'duckdb': duckdb_stats,
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_requests': total_requests,
                    'success_rate': request_stats.get('success_rate', 0),
                    'cache_hit_rate': round(cache_hit_rate, 2),
                    'data_quality_score': quality_stats['quality_score'],
                    'active_data_sources': data_source_stats['total_registered']
                }
            }

            return statistics

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())

            # è¿”å›é»˜è®¤ç»Ÿè®¡ä¿¡æ¯
            return {
                'requests': self._stats.copy(),
                'cache': {'hits': 0, 'misses': 0, 'hit_rate': 0.0},
                'data_sources': {'total_registered': 0, 'available_sources': 0},
                'data_quality': {'expected_records': 0, 'actual_records': 0, 'quality_score': 0.0},
                'system': {'initialized': self._is_initialized},
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }

    def _legacy_get_stock_data(self, symbol: str, period: str = "D", **kwargs) -> Optional[pd.DataFrame]:
        """ä¼ ç»Ÿæ–¹å¼è·å–è‚¡ç¥¨æ•°æ®"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„è‚¡ç¥¨æ•°æ®è·å–é€»è¾‘
            from ..data.data_access import DataAccess
            data_access = DataAccess()
            return data_access.get_kdata(symbol, period)
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ–¹å¼è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            return None

    async def get_stock_data(self, code: str, freq: str, start_date=None, end_date=None, request_id=None):
        """ç»Ÿä¸€çš„æ•°æ®è¯·æ±‚æ–¹æ³•ï¼ŒåŒºåˆ†å†å²å’Œå®æ—¶æ•°æ®"""
        if request_id:
            self._register_request(request_id)

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å®æ—¶æ•°æ®
            if self._needs_realtime_data(end_date):
                return await self.realtime_data_strategy.get_data(code, freq, start_date, end_date)
            else:
                return await self.history_data_strategy.get_data(code, freq, start_date, end_date)
        except Exception as e:
            logger.error(f"Error fetching data for {code}: {e}")
            return None
        finally:
            if request_id:
                self._unregister_request(request_id)

    def _needs_realtime_data(self, end_date=None):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å®æ—¶æ•°æ®"""
        if end_date is None:
            # æ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œéœ€è¦å®æ—¶æ•°æ®
            return True

        # å¦‚æœç»“æŸæ—¥æœŸæ˜¯ä»Šå¤©æˆ–æœªæ¥ï¼Œéœ€è¦å®æ—¶æ•°æ®
        today = datetime.now().date()
        if isinstance(end_date, str):
            try:
                end_date = datetime.strptime(end_date, "%Y-%m-%d").date()
            except:
                return True

        if isinstance(end_date, datetime):
            end_date = end_date.date()

        return end_date >= today

    async def request_data(self, stock_code: str, data_type: str = 'kdata',
                           period: str = 'D', time_range: str = "æœ€è¿‘1å¹´",
                           asset_type: AssetType = AssetType.STOCK_A, **kwargs) -> Any:
        """è¯·æ±‚æ•°æ®ï¼ˆâœ… ä¼˜åŒ–ï¼šæ”¯æŒå¤šèµ„äº§ç±»å‹ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆæˆ–å…¶ä»–èµ„äº§ä»£ç ï¼‰
            data_type: æ•°æ®ç±»å‹ï¼Œå¦‚'kdata', 'financial', 'news'ç­‰
            period: å‘¨æœŸï¼Œå¦‚'D'(æ—¥çº¿)ã€'W'(å‘¨çº¿)ã€'M'(æœˆçº¿)ã€'60'(60åˆ†é’Ÿ)ç­‰
            time_range: æ—¶é—´èŒƒå›´ï¼Œå¦‚"æœ€è¿‘7å¤©"ã€"æœ€è¿‘30å¤©"ã€"æœ€è¿‘1å¹´"ç­‰
            asset_type: èµ„äº§ç±»å‹ï¼ˆé»˜è®¤ä¸ºè‚¡ç¥¨ï¼Œæ”¯æŒCRYPTO/FUTURES/FOREX/INDEX/FUNDç­‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            è¯·æ±‚çš„æ•°æ®
        """
        try:
            # å¤„ç†å‘¨æœŸæ˜ å°„
            period_map = {
                'åˆ†æ—¶': 'min',
                'æ—¥çº¿': 'D',
                'å‘¨çº¿': 'W',
                'æœˆçº¿': 'M',
                '5åˆ†é’Ÿ': '5',
                '15åˆ†é’Ÿ': '15',
                '30åˆ†é’Ÿ': '30',
                '60åˆ†é’Ÿ': '60'
            }

            # å¦‚æœperiodæ˜¯ä¸­æ–‡æè¿°ï¼Œè½¬æ¢ä¸ºå¯¹åº”ä»£ç 
            actual_period = period_map.get(period, period)

            # å¤„ç†æ—¶é—´èŒƒå›´æ˜ å°„ï¼ˆè½¬æ¢ä¸ºå¤©æ•°ï¼‰
            time_range_map = {
                "æœ€è¿‘7å¤©": 7,
                "æœ€è¿‘30å¤©": 30,
                "æœ€è¿‘90å¤©": 90,
                "æœ€è¿‘180å¤©": 180,
                "æœ€è¿‘1å¹´": 365,
                "æœ€è¿‘2å¹´": 365 * 2,
                "æœ€è¿‘3å¹´": 365 * 3,
                "æœ€è¿‘5å¹´": 365 * 5,
                "å…¨éƒ¨": -1  # è¡¨ç¤ºæ‰€æœ‰å¯ç”¨æ•°æ®
            }

            # è·å–å¤©æ•°ï¼Œé»˜è®¤ä¸º365å¤©ï¼ˆçº¦1å¹´ï¼‰
            count = time_range_map.get(time_range, 365)

            logger.info(f"âœ… è¯·æ±‚æ•°æ®ï¼šä»£ç ={stock_code}, ç±»å‹={data_type}, å‘¨æœŸ={actual_period}, æ—¶é—´èŒƒå›´={count}å¤©, èµ„äº§ç±»å‹={asset_type.value}")

            if data_type == 'kdata':
                # âœ… è·å–Kçº¿æ•°æ®ï¼ˆä¼ é€’èµ„äº§ç±»å‹ï¼‰
                return await self._get_kdata(stock_code, period=actual_period, count=count, asset_type=asset_type)
            elif data_type == 'financial':
                # è·å–è´¢åŠ¡æ•°æ®
                return await self._get_financial_data(stock_code)
            elif data_type == 'news':
                # è·å–æ–°é—»æ•°æ®
                return await self._get_news(stock_code)
            elif data_type == 'all':
                # âœ… è·å–æ‰€æœ‰æ•°æ®ï¼ˆä¼ é€’èµ„äº§ç±»å‹ï¼‰
                kdata = await self._get_kdata(stock_code, period=actual_period, count=count, asset_type=asset_type)
                financial = await self._get_financial_data(stock_code)
                news = await self._get_news(stock_code)
                return {
                    'kdata': kdata,
                    'financial': financial,
                    'news': news
                }
            else:
                logger.error(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {data_type}")
                return None
        except Exception as e:
            logger.error(f"è¯·æ±‚æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None

    async def _get_kdata(self, stock_code: str, period: str = 'D', count: int = 365,
                         asset_type: AssetType = AssetType.STOCK_A) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ®ï¼ˆâœ… ä¼˜åŒ–ï¼šæ”¯æŒå¤šèµ„äº§ç±»å‹ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆæˆ–å…¶ä»–èµ„äº§ä»£ç ï¼‰
            period: å‘¨æœŸï¼Œå¦‚'D'ã€'W'ã€'M'
            count: è·å–çš„å¤©æ•°
            asset_type: èµ„äº§ç±»å‹ï¼ˆé»˜è®¤ä¸ºè‚¡ç¥¨ï¼‰

        Returns:
            Kçº¿DataFrame
        """
        try:
            logger.info(f"âœ… è·å–Kçº¿æ•°æ®: {stock_code}, å‘¨æœŸ={period}, æ•°é‡={count}, èµ„äº§ç±»å‹={asset_type.value}")

            # å°è¯•ä»æœåŠ¡å®¹å™¨è§£æChartService
            from core.services.chart_service import ChartService
            chart_service = self.service_container.resolve(ChartService)

            if chart_service:
                # âœ… ChartServiceæ”¯æŒasset_typeå‚æ•°ï¼Œä¼ é€’è¿‡å»
                return chart_service.get_kdata(stock_code, period, count, asset_type=asset_type)

            # å¦‚æœæ²¡æœ‰ChartServiceï¼Œä½¿ç”¨é»˜è®¤æ•°æ®æº
            # æ³¨æ„ï¼šcore.data_managerå·²è¿ç§»ï¼Œä½¿ç”¨å½“å‰å®ä¾‹
            data_manager = self

            if data_manager:
                # âœ… ä¼ é€’asset_typeå‚æ•°
                return data_manager.get_kdata(stock_code, period, count, asset_type=asset_type)

            logger.error("æ— æ³•è·å–Kçº¿æ•°æ®ï¼šæœªæ‰¾åˆ°æ•°æ®æœåŠ¡")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()

    async def _get_financial_data(self, stock_code: str) -> Dict[str, Any]:
        """è·å–è´¢åŠ¡æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼šé›†æˆDuckDBå­˜å‚¨ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            è´¢åŠ¡æ•°æ®å­—å…¸
        """
        try:
            logger.info(f"è·å–è´¢åŠ¡æ•°æ®: {stock_code}")

            cache_key = f"financial_{stock_code}"

            # 1. å°è¯•ä»DuckDBè·å–è´¢åŠ¡æ•°æ®
            if self.duckdb_available and self.duckdb_operations:
                financial_data = await self._get_financial_from_duckdb(stock_code)
                if financial_data:
                    return financial_data

            # 2. é€šè¿‡TETç®¡é“è·å–è´¢åŠ¡æ•°æ®
            if self.tet_enabled and self.tet_pipeline:
                try:
                    from ..tet_data_pipeline import StandardQuery
                    from ..plugin_types import AssetType, DataType

                    query = StandardQuery(
                        symbol=stock_code,
                        asset_type=AssetType.STOCK_A,
                        data_type=DataType.FINANCIAL_STATEMENT,
                        provider=self._current_source
                    )

                    result = self.tet_pipeline.process(query)
                    if result and result.data:
                        # å­˜å‚¨åˆ°DuckDB
                        if self.duckdb_available:
                            await self._store_financial_to_duckdb(stock_code, result.data)
                        return result.data

                except Exception as e:
                    logger.warning(f"TETç®¡é“è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")

            # 3. å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            return {}

        except Exception as e:
            logger.error(f"è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {}

    async def _get_financial_from_duckdb(self, stock_code: str, asset_type: AssetType = None) -> Optional[Dict[str, Any]]:
        """ä»DuckDBè·å–è´¢åŠ¡æ•°æ®"""
        try:
            query = """
                SELECT * FROM financial_statements 
                WHERE symbol = ? 
                ORDER BY report_date DESC 
                LIMIT 1
            """

            final_asset_type = asset_type or AssetType.STOCK_A
            result = self.duckdb_operations.execute_query(
                database_path=self.asset_manager.get_database_path(final_asset_type),
                query=query,
                params=[stock_code]
            )

            if result.success and result.data:
                return result.data[0] if result.data else None

            return None

        except Exception as e:
            logger.error(f"DuckDBè´¢åŠ¡æ•°æ®è·å–å¤±è´¥: {e}")
            return None

    async def _store_financial_to_duckdb(self, stock_code: str, data: Dict[str, Any]):
        """å­˜å‚¨è´¢åŠ¡æ•°æ®åˆ°DuckDB"""
        try:
            if not data:
                return

            # è¯†åˆ«èµ„äº§ç±»å‹
            asset_type = self.asset_identifier.identify_asset_type(stock_code)
            db_path = self.asset_manager.get_database_path(asset_type)

            # ç¡®ä¿è´¢åŠ¡æ•°æ®è¡¨å­˜åœ¨
            if self.table_manager:
                from ..database.table_manager import TableType
                if not self.table_manager.ensure_table_exists(
                    db_path, TableType.FINANCIAL_STATEMENT, "unified_data_manager"
                ):
                    logger.error("åˆ›å»ºè´¢åŠ¡æ•°æ®è¡¨å¤±è´¥")
                    return

            # è½¬æ¢ä¸ºDataFrameå¹¶å­˜å‚¨
            df = pd.DataFrame([data])
            result = self.duckdb_operations.insert_dataframe(
                database_path=db_path,
                table_name="financial_statements",
                data=df,
                upsert=True
            )

            if result.success:
                logger.info(f" è´¢åŠ¡æ•°æ®å­˜å‚¨åˆ°DuckDBæˆåŠŸ: {stock_code}")

        except Exception as e:
            logger.warning(f"DuckDBè´¢åŠ¡æ•°æ®å­˜å‚¨å¤±è´¥: {e}")

    def get_macro_economic_data(self, indicator: str, period: str = 'M', count: int = 100) -> pd.DataFrame:
        """
        è·å–å®è§‚ç»æµæ•°æ®ï¼ˆæ–°å¢æ–¹æ³•ï¼šé›†æˆDuckDBå­˜å‚¨ï¼‰

        Args:
            indicator: ç»æµæŒ‡æ ‡åç§° (GDP, CPI, PMIç­‰)
            period: æ•°æ®å‘¨æœŸ (M/Q/Y)
            count: æ•°æ®æ¡æ•°

        Returns:
            å®è§‚ç»æµæ•°æ®DataFrame
        """
        try:
            cache_key = f"macro_{indicator}_{period}_{count}"

            # 1. å¤šçº§ç¼“å­˜æ£€æŸ¥
            cached_data = self._get_cached_data(cache_key)
            if cached_data is not None and not cached_data.empty:
                return cached_data

            # 2. ä»DuckDBè·å–
            if self.duckdb_available and self.duckdb_operations:
                df = self._get_macro_from_duckdb(indicator, period, count)
                if not df.empty:
                    self._cache_data(cache_key, df)
                    return df

            # 3. é€šè¿‡TETç®¡é“è·å–
            if self.tet_enabled and self.tet_pipeline:
                try:
                    from ..tet_data_pipeline import StandardQuery
                    from ..plugin_types import AssetType, DataType

                    query = StandardQuery(
                        symbol=indicator,
                        asset_type=AssetType.MACRO,
                        data_type=DataType.MACRO_ECONOMIC,
                        period=period,
                        provider=self._current_source,
                        extra_params={'count': count}
                    )

                    result = self.tet_pipeline.process(query)
                    if result and result.data is not None:
                        if isinstance(result.data, pd.DataFrame) and not result.data.empty:
                            # å­˜å‚¨åˆ°DuckDB
                            self._store_macro_to_duckdb(result.data, indicator, period)
                            self._cache_data(cache_key, result.data)
                            return result.data

                except Exception as e:
                    logger.warning(f"TETç®¡é“è·å–å®è§‚æ•°æ®å¤±è´¥: {e}")

            # 4. è¿”å›ç©ºDataFrame
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–å®è§‚ç»æµæ•°æ®å¤±è´¥: {indicator} - {e}")
            return pd.DataFrame()

    def _get_macro_from_duckdb(self, indicator: str, period: str, count: int, asset_type: AssetType = None) -> pd.DataFrame:
        """ä»DuckDBè·å–å®è§‚ç»æµæ•°æ®"""
        try:
            query = """
                SELECT * FROM macro_economic_data 
                WHERE indicator = ? AND frequency = ?
                ORDER BY release_date DESC 
                LIMIT ?
            """

            final_asset_type = asset_type or AssetType.STOCK_A
            result = self.duckdb_operations.execute_query(
                database_path=self.asset_manager.get_database_path(final_asset_type),
                query=query,
                params=[indicator, period, count]
            )

            if result.success and result.data:
                df = pd.DataFrame(result.data)
                logger.info(f" ä»DuckDBè·å–å®è§‚æ•°æ®æˆåŠŸ: {indicator}, {len(df)}æ¡")
                return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"DuckDBå®è§‚æ•°æ®è·å–å¤±è´¥: {e}")
            return pd.DataFrame()

    def _store_macro_to_duckdb(self, data: pd.DataFrame, indicator: str, period: str):
        """å­˜å‚¨å®è§‚ç»æµæ•°æ®åˆ°DuckDB"""
        try:
            if not self.duckdb_operations or data.empty:
                return

            # å®è§‚æ•°æ®ä½¿ç”¨MACROèµ„äº§ç±»å‹
            from ..plugin_types import AssetType
            asset_type = AssetType.MACRO
            db_path = self.asset_manager.get_database_path(asset_type)

            # ç¡®ä¿å®è§‚æ•°æ®è¡¨å­˜åœ¨
            if self.table_manager:
                from ..database.table_manager import TableType
                if not self.table_manager.ensure_table_exists(
                    db_path, TableType.MACRO_ECONOMIC, "unified_data_manager"
                ):
                    logger.error("åˆ›å»ºå®è§‚æ•°æ®è¡¨å¤±è´¥")
                    return

            # æ’å…¥æ•°æ®
            result = self.duckdb_operations.insert_dataframe(
                database_path=db_path,
                table_name="macro_economic_data",
                data=data,
                upsert=True
            )

            if result.success:
                logger.info(f" å®è§‚æ•°æ®å­˜å‚¨åˆ°DuckDBæˆåŠŸ: {indicator}, {len(data)}æ¡")

        except Exception as e:
            logger.warning(f"DuckDBå®è§‚æ•°æ®å­˜å‚¨å¤±è´¥: {e}")

    # ==================== å¢å¼ºæ•°æ®ä¸‹è½½åŠŸèƒ½æ¥å£ ====================

    async def download_historical_data_batch(self,
                                             symbols: List[str],
                                             period: str = 'D',
                                             days_back: int = 365) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡ä¸‹è½½å†å²æ•°æ® - é€šè¿‡å¢å¼ºDuckDBä¸‹è½½å™¨è·å–æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            period: æ•°æ®å‘¨æœŸ
            days_back: å›æº¯å¤©æ•°

        Returns:
            Dict[symbol, DataFrame]: ä¸‹è½½çš„å†å²æ•°æ®
        """
        if not hasattr(self, 'enhanced_duckdb_downloader') or not self.enhanced_duckdb_downloader:
            logger.error("å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨ä¸å¯ç”¨")
            return {}

        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        return await self.enhanced_duckdb_downloader.download_historical_kline_data(
            symbols=symbols,
            period=period,
            start_date=start_date,
            end_date=end_date,
            force_update=False
        )

    async def update_stock_universe(self, market: str = 'all') -> pd.DataFrame:
        """
        æ›´æ–°è‚¡ç¥¨æ±  - é€šè¿‡å¢å¼ºDuckDBä¸‹è½½å™¨è·å–è‚¡ç¥¨åˆ—è¡¨

        Args:
            market: å¸‚åœºä»£ç 

        Returns:
            DataFrame: æ›´æ–°åçš„è‚¡ç¥¨åˆ—è¡¨
        """
        if not hasattr(self, 'enhanced_duckdb_downloader') or not self.enhanced_duckdb_downloader:
            logger.error("å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨ä¸å¯ç”¨")
            return pd.DataFrame()

        return await self.enhanced_duckdb_downloader.download_stock_list(market=market)

    async def incremental_data_update(self, max_symbols: int = 100) -> Dict[str, Any]:
        """
        å¢é‡æ•°æ®æ›´æ–° - é€šè¿‡å¢å¼ºDuckDBä¸‹è½½å™¨è¿›è¡Œæ•°æ®æ›´æ–°

        Args:
            max_symbols: æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡

        Returns:
            Dict: æ›´æ–°ç»“æœç»Ÿè®¡
        """
        if not hasattr(self, 'enhanced_duckdb_downloader') or not self.enhanced_duckdb_downloader:
            logger.error("å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨ä¸å¯ç”¨")
            return {}

        return await self.enhanced_duckdb_downloader.incremental_update_all_data(max_symbols=max_symbols)

    def get_data_storage_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®å­˜å‚¨ç»Ÿè®¡ - é€šè¿‡å¢å¼ºDuckDBä¸‹è½½å™¨è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict: æ•°æ®å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        """
        if not hasattr(self, 'enhanced_duckdb_downloader') or not self.enhanced_duckdb_downloader:
            logger.error("å¢å¼ºDuckDBæ•°æ®ä¸‹è½½å™¨ä¸å¯ç”¨")
            return {}

        import asyncio
        return asyncio.run(self.enhanced_duckdb_downloader.get_data_statistics())

    async def _get_news(self, stock_code: str) -> Dict[str, Any]:
        """è·å–æ–°é—»æ•°æ®

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            æ–°é—»æ•°æ®å­—å…¸
        """
        try:
            logger.info(f"è·å–æ–°é—»æ•°æ®: {stock_code}")

            # è·å–æ–°é—»æ•°æ®å¯èƒ½éœ€è¦ç‰¹å®šçš„æœåŠ¡
            # è¿™é‡Œä»…ä½œä¸ºç¤ºä¾‹å®ç°ï¼Œè¿”å›ç©ºå­—å…¸
            return {}

        except Exception as e:
            logger.error(f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {}

    def cancel_request(self, request_id: str) -> bool:
        """
        å–æ¶ˆè¯·æ±‚

        Args:
            request_id: è¯·æ±‚ID

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        with self.request_tracker_lock:
            if request_id in self.request_tracker:
                task = self.request_tracker[request_id].get('task')
                if task and not task.done():
                    task.cancel()
                    logger.info(f"Request {request_id} cancelled")

                # æ¸…ç†èµ„æº
                self._cleanup_resources(request_id)

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._stats['requests_cancelled'] += 1

                return True

        with self._request_lock:
            # æ£€æŸ¥å¾…å¤„ç†è¯·æ±‚
            if request_id in self._pending_requests:
                request = self._pending_requests[request_id]
                request.status = DataRequestStatus.CANCELLED
                del self._pending_requests[request_id]
                logger.debug(f"Cancelled pending request {request_id}")
                return True

            # æ£€æŸ¥æ´»åŠ¨è¯·æ±‚
            if request_id in self._active_requests:
                request = self._active_requests[request_id]
                if request.future and not request.future.done():
                    request.future.cancel()
                request.status = DataRequestStatus.CANCELLED
                del self._active_requests[request_id]
                logger.debug(f"Cancelled active request {request_id}")
                return True

        return False

    def _register_request(self, request_id: str):
        """æ³¨å†Œè¯·æ±‚åˆ°è·Ÿè¸ªå™¨"""
        with self.request_tracker_lock:
            try:
                task = asyncio.current_task() if asyncio.iscoroutinefunction(
                    self.get_stock_data) else None
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯
                task = None
            self.request_tracker[request_id] = {
                'timestamp': time.time(),
                'task': task
            }

    def _unregister_request(self, request_id: str):
        """ä»è·Ÿè¸ªå™¨ä¸­æ³¨é”€è¯·æ±‚"""
        with self.request_tracker_lock:
            if request_id in self.request_tracker:
                del self.request_tracker[request_id]

    def _cleanup_resources(self, request_id: str):
        """æ¸…ç†è¯·æ±‚ç›¸å…³èµ„æº"""
        # ä»å„ç§é›†åˆä¸­ç§»é™¤è¯·æ±‚
        with self._request_lock:
            if request_id in self._pending_requests:
                del self._pending_requests[request_id]

            if request_id in self._active_requests:
                del self._active_requests[request_id]

            if request_id in self._completed_requests:
                del self._completed_requests[request_id]

        # ä»å»é‡æœºåˆ¶ä¸­ç§»é™¤
        with self._dedup_lock:
            for key, requests in list(self._request_dedup.items()):
                if request_id in requests:
                    requests.remove(request_id)
                    if not requests:
                        del self._request_dedup[key]
                    break

        # ä»è·Ÿè¸ªå™¨ä¸­ç§»é™¤
        self._unregister_request(request_id)

        logger.debug(f"Resources cleaned up for request {request_id}")

    def preload_data(self, code: str, freq: str = 'D', priority: str = 'low'):
        """é¢„åŠ è½½æ•°æ®"""
        # è½¬æ¢ä¼˜å…ˆçº§å­—ç¬¦ä¸²åˆ°æ•°å€¼
        priority_map = {'high': 0, 'normal': 1, 'low': 2}
        priority_value = priority_map.get(priority.lower(), 2)

        # ä½¿ç”¨ä½ä¼˜å…ˆçº§è¯·æ±‚é¢„åŠ è½½æ•°æ®
        self.request_data(
            stock_code=code,
            data_type='kdata',
            period=freq,
            priority=priority_value,
            callback=None  # æ— éœ€å›è°ƒ
        )

        logger.debug(f"Preloading data for {code} with priority {priority}")

        return True

    def get_request_status(self, request_id: str) -> Optional[DataRequestStatus]:
        """
        è·å–è¯·æ±‚çŠ¶æ€

        Args:
            request_id: è¯·æ±‚ID

        Returns:
            è¯·æ±‚çŠ¶æ€
        """
        with self._request_lock:
            if request_id in self._pending_requests:
                return self._pending_requests[request_id].status
            elif request_id in self._active_requests:
                return self._active_requests[request_id].status
            elif request_id in self._completed_requests:
                return self._completed_requests[request_id].status

        return None

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._request_lock:
            return {
                **self._stats,
                'pending_requests': len(self._pending_requests),
                'active_requests': len(self._active_requests),
                'completed_requests': len(self._completed_requests),
                'cache_size': self.multi_cache.get_statistics()['total_items'] if self.multi_cache else 0
            }

    def clear_cache(self, stock_code: str = None, data_type: str = None) -> None:
        """
        æ¸…ç†ç¼“å­˜

        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¯é€‰ï¼Œæ¸…ç†ç‰¹å®šè‚¡ç¥¨çš„ç¼“å­˜ï¼‰
            data_type: æ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼Œæ¸…ç†ç‰¹å®šç±»å‹çš„ç¼“å­˜ï¼‰
        """
        with self._cache_lock:
            if stock_code is None and data_type is None:
                # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                self._data_cache.clear()
                self._cache_timestamps.clear()
                logger.info("All cache cleared")
            else:
                # æ¸…ç†ç‰¹å®šç¼“å­˜
                keys_to_remove = []
                for key in self._data_cache.keys():
                    if stock_code and stock_code not in key:
                        continue
                    if data_type and data_type not in key:
                        continue
                    keys_to_remove.append(key)

                for key in keys_to_remove:
                    del self._data_cache[key]
                    if key in self._cache_timestamps:
                        del self._cache_timestamps[key]

                logger.info(f"Cleared {len(keys_to_remove)} cache entries")

    def _submit_request(self, request: DataRequest) -> None:
        """æäº¤è¯·æ±‚åˆ°çº¿ç¨‹æ± """
        with self._request_lock:
            self._pending_requests[request.request_id] = request

        # æäº¤åˆ°çº¿ç¨‹æ± 
        future = self._executor.submit(self._process_request, request)
        request.future = future

        logger.debug(
            f"Submitted request {request.request_id} for {request.stock_code}")

    def _process_request(self, request: DataRequest) -> None:
        """
        å¤„ç†æ•°æ®è¯·æ±‚
        """
        try:
            data = None
            if request.data_type == 'kdata':
                kline_data = self._load_kdata(request)
                # ä¿®æ”¹ï¼šå°†Kçº¿æ•°æ®åŒ…è£…åœ¨å­—å…¸ä¸­ï¼Œä¿æŒæ•°æ®ç»“æ„ä¸€è‡´æ€§
                data = {
                    'kline_data': kline_data,
                    'stock_code': request.stock_code,
                    'period': request.period
                }
            elif request.data_type == 'indicators':
                data = self._load_indicators(request)
            elif request.data_type == 'analysis':
                data = self._load_analysis(request)
            elif request.data_type == 'chart':
                kline_data = self._load_kdata(request)
                indicators_data = self._load_indicators(request)
                data = {
                    'kline_data': kline_data,
                    'indicators_data': indicators_data
                }
            else:
                raise ValueError(f"Unsupported data type: {request.data_type}")

            self._complete_request(request, data)

        except Exception as e:
            logger.error(
                f"Failed to process request {request.request_id}: {e}")
            self._complete_request(request, None, str(e))

    def _complete_request(self, request: DataRequest, data: Any, error: str = None) -> None:
        """
        å®Œæˆè¯·æ±‚å¹¶é€šè¿‡Futureè¿”å›ç»“æœ
        """
        request_key = self._get_request_key(
            request.stock_code, request.data_type, request.period, request.time_range, request.parameters)

        with self._dedup_lock:
            request_group = self._request_dedup.pop(request_key, set())

        for req in request_group:
            if req.future and not req.future.done():
                if error:
                    exception = Exception(error)
                    self.loop.call_soon_threadsafe(
                        req.future.set_exception, exception)
                else:
                    self.loop.call_soon_threadsafe(req.future.set_result, data)

            with self._request_lock:
                self._completed_requests[req.request_id] = req
                req.status = DataRequestStatus.COMPLETED if not error else DataRequestStatus.FAILED

        if not error:
            self._stats['requests_completed'] += len(request_group)
        else:
            self._stats['requests_failed'] += len(request_group)

    def _load_kdata(self, request: DataRequest) -> pd.DataFrame:
        """åŠ è½½Kçº¿æ•°æ®"""
        try:
            from .stock_service import StockService
            stock_service = self.service_container.resolve(StockService)
            return stock_service.get_stock_data(
                request.stock_code, request.period, request.time_range
            )
        except Exception as e:
            logger.error(f"Failed to load kdata: {e}")
            raise

    def _load_indicators(self, request: DataRequest) -> Dict[str, Any]:
        """åŠ è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        try:
            from .analysis_service import AnalysisService
            analysis_service = self.service_container.resolve(AnalysisService)

            indicators = request.parameters.get('indicators', ['MA', 'MACD'])
            return analysis_service.calculate_technical_indicators(
                request.stock_code, indicators, request.period, request.time_range
            )
        except Exception as e:
            logger.error(f"Failed to load indicators: {e}")
            raise

    def _load_analysis(self, request: DataRequest) -> Dict[str, Any]:
        """åŠ è½½åˆ†ææ•°æ®"""
        try:
            from .analysis_service import AnalysisService
            analysis_service = self.service_container.resolve(AnalysisService)

            analysis_type = request.parameters.get(
                'analysis_type', 'comprehensive')
            return analysis_service.analyze_stock(request.stock_code, analysis_type)
        except Exception as e:
            logger.error(f"Failed to load analysis: {e}")
            raise

    def _get_cache_key(self, stock_code: str, data_type: str, period: str,
                       time_range: int, parameters: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        param_hash = hash(str(sorted(parameters.items()))
                          if parameters else "")
        return f"{data_type}_{stock_code}_{period}_{time_range}_{param_hash}"

    def _get_request_key(self, stock_code: str, data_type: str, period: str,
                         time_range: int, parameters: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯·æ±‚é”®"""
        return self._get_cache_key(stock_code, data_type, period, time_range, parameters)

    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–æ•°æ® - ä½¿ç”¨ç»Ÿä¸€çš„MultiLevelCacheManager"""
        with self._cache_lock:
            # if cache_key in self._data_cache:  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager
            if self.multi_cache and self.multi_cache.get(cache_key) is not None:
                timestamp = self._cache_timestamps.get(cache_key, 0)
                if time.time() - timestamp < self._cache_ttl:
                    return self.multi_cache.get(cache_key)
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œæ¸…ç†
                    del self._data_cache[cache_key]
                    if cache_key in self._cache_timestamps:
                        del self._cache_timestamps[cache_key]

        return None

    def _put_to_cache(self, cache_key: str, data: Any) -> None:
        """å°†æ•°æ®æ”¾å…¥ç¼“å­˜ - ä½¿ç”¨ç»Ÿä¸€çš„MultiLevelCacheManager"""
        with self._cache_lock:
            # self._data_cache[cache_key] = data  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager
            if self.multi_cache:
                self.multi_cache.set(cache_key, data, ttl=self._cache_ttl)
            # self._cache_timestamps[cache_key] = time.time()  # å·²ç»Ÿä¸€ä½¿ç”¨MultiLevelCacheManager

    def dispose(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("Disposing unified data manager")

        # å–æ¶ˆæ‰€æœ‰å¾…å¤„ç†è¯·æ±‚
        with self._request_lock:
            for request in list(self._pending_requests.values()):
                self.cancel_request(request.request_id)

            for request in list(self._active_requests.values()):
                self.cancel_request(request.request_id)

        # å…³é—­çº¿ç¨‹æ± 
        self._executor.shutdown(wait=True)

        # æ¸…ç†ç¼“å­˜
        self.clear_cache()

        logger.info("Unified data manager disposed")

    def _auto_discover_data_source_plugins(self) -> None:
        """è‡ªåŠ¨å‘ç°å’Œæ³¨å†Œæ•°æ®æºæ’ä»¶"""
        try:
            # ä»æœåŠ¡å®¹å™¨è·å–æ’ä»¶ç®¡ç†å™¨
            plugin_manager = None
            if self.service_container:
                try:
                    from ..plugin_manager import PluginManager
                    plugin_manager = self.service_container.resolve(PluginManager)
                except:
                    logger.warning("æ— æ³•è·å–æ’ä»¶ç®¡ç†å™¨ï¼Œè·³è¿‡æ’ä»¶è‡ªåŠ¨å‘ç°")
                    return

            if not plugin_manager:
                logger.warning("æ’ä»¶ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ’ä»¶è‡ªåŠ¨å‘ç°")
                return

            # è·å–æ‰€æœ‰å·²åŠ è½½çš„æ’ä»¶
            all_plugins = plugin_manager.get_all_plugins()
            registered_count = 0

            for plugin_name, plugin_instance in all_plugins.items():
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®æºæ’ä»¶
                    if self._is_data_source_plugin(plugin_instance):
                        # æ³¨å†Œåˆ°TETæ•°æ®ç®¡é“
                        success = self.register_data_source_plugin(
                            plugin_name,
                            plugin_instance,
                            priority=getattr(plugin_instance, 'priority', 50),
                            weight=getattr(plugin_instance, 'weight', 1.0)
                        )

                        if success:
                            registered_count += 1
                            logger.info(f" è‡ªåŠ¨æ³¨å†Œæ•°æ®æºæ’ä»¶: {plugin_name}")
                        else:
                            logger.warning(f" æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥: {plugin_name}")

                except Exception as e:
                    logger.warning(f" æ£€æŸ¥æ’ä»¶å¤±è´¥ {plugin_name}: {e}")

            if registered_count > 0:
                logger.info(f" è‡ªåŠ¨å‘ç°å¹¶æ³¨å†Œäº† {registered_count} ä¸ªæ•°æ®æºæ’ä»¶")
                self._plugins_discovered = True
            else:
                logger.info("æœªå‘ç°æ–°çš„æ•°æ®æºæ’ä»¶")

        except Exception as e:
            logger.error(f" è‡ªåŠ¨å‘ç°æ•°æ®æºæ’ä»¶å¤±è´¥: {e}")

    def _is_data_source_plugin(self, plugin_instance) -> bool:
        """æ£€æŸ¥æ’ä»¶æ˜¯å¦æ˜¯æ•°æ®æºæ’ä»¶"""
        try:
            from ..data_source_extensions import IDataSourcePlugin
            return isinstance(plugin_instance, IDataSourcePlugin)
        except Exception:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ–¹æ³•
            required_methods = ['get_asset_list', 'get_kdata', 'health_check']
            return all(hasattr(plugin_instance, method) for method in required_methods)

    def discover_and_register_data_source_plugins(self) -> None:
        """
        å‘ç°å¹¶æ³¨å†Œæ•°æ®æºæ’ä»¶ï¼ˆå…¬å…±æ–¹æ³•ï¼‰
        åœ¨æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–å®Œæˆåè°ƒç”¨
        """
        if self._plugins_discovered:
            logger.info("æ’ä»¶å·²å‘ç°ï¼Œè·³è¿‡é‡å¤å‘ç°")
            return

        logger.info("ğŸ” å¼€å§‹å‘ç°å’Œæ³¨å†Œæ•°æ®æºæ’ä»¶...")

        try:
            # ä½¿ç”¨æ’ä»¶ç®¡ç†å™¨åŠ¨æ€åŠ è½½æ’ä»¶ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
            registered_count = self._register_plugins_from_plugin_manager()

            if registered_count > 0:
                self._plugins_discovered = True
                logger.info(f"âœ… æ’ä»¶å‘ç°å’Œæ³¨å†Œå®Œæˆ: å…±æ³¨å†Œ {registered_count} ä¸ªæ’ä»¶")
            else:
                logger.warning("âš ï¸ æœªæ³¨å†Œä»»ä½•æ’ä»¶ï¼Œè¯·æ£€æŸ¥æ’ä»¶ç®¡ç†å™¨çŠ¶æ€")

        except Exception as e:
            logger.error(f"âŒ æ’ä»¶å‘ç°å’Œæ³¨å†Œå¤±è´¥: {e}")
            logger.error(traceback.format_exc())

    def _register_plugins_from_plugin_manager(self) -> int:
        """
        ä»æ’ä»¶ç®¡ç†å™¨åŠ¨æ€æ³¨å†Œæ•°æ®æºæ’ä»¶

        Returns:
            æˆåŠŸæ³¨å†Œçš„æ’ä»¶æ•°é‡
        """
        # è·å–æ’ä»¶ç®¡ç†å™¨
        plugin_manager = None

        # æ–¹æ³•1: ä»service_containerè·å–
        if hasattr(self, 'service_container') and self.service_container:
            try:
                from core.plugin_manager import PluginManager
                if self.service_container.is_registered(PluginManager):
                    plugin_manager = self.service_container.resolve(PluginManager)
                    logger.debug("ä»æœåŠ¡å®¹å™¨è·å–PluginManageræˆåŠŸ")
            except Exception as e:
                logger.debug(f"ä»æœåŠ¡å®¹å™¨è·å–PluginManagerå¤±è´¥: {e}")

        # æ–¹æ³•2: ä»å…¨å±€å®ä¾‹è·å–
        if not plugin_manager:
            try:
                from core.plugin_manager import PluginManager
                # é€šè¿‡ServiceContainerè·å–PluginManagerå®ä¾‹
                from core.containers import get_service_container
                container = get_service_container()
                plugin_manager = container.resolve(PluginManager) if container else None
            except:
                pass

        if not plugin_manager:
            logger.warning("âš ï¸ æ’ä»¶ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ³¨å†Œæ’ä»¶")
            return 0

        registered_count = 0

        try:
            from core.plugin_types import PluginType

            # 1. è·å–æ‰€æœ‰æ’ä»¶å®ä¾‹
            all_plugins = plugin_manager.plugin_instances

            if not all_plugins:
                logger.warning("âš ï¸ æ’ä»¶ç®¡ç†å™¨ä¸­æ²¡æœ‰åŠ è½½ä»»ä½•æ’ä»¶")
                return 0

            logger.info(f"ğŸ“¦ æ’ä»¶ç®¡ç†å™¨ä¸­æœ‰ {len(all_plugins)} ä¸ªæ’ä»¶")

            # 2. ç­›é€‰æ•°æ®æºæ’ä»¶
            data_source_plugins = []
            for plugin_id, plugin_instance in all_plugins.items():
                # è·å–æ’ä»¶å…ƒæ•°æ®
                metadata = plugin_manager.plugin_metadata.get(plugin_id, {})
                plugin_type = metadata.get('plugin_type') or metadata.get('type')

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®æºæ’ä»¶
                is_data_source = False
                if plugin_type:
                    if isinstance(plugin_type, str):
                        is_data_source = 'data_source' in plugin_type.lower()
                    elif hasattr(plugin_type, 'value'):
                        is_data_source = 'data_source' in str(plugin_type.value).lower()
                    else:
                        is_data_source = 'data_source' in str(plugin_type).lower()

                # ä¹Ÿæ£€æŸ¥plugin_idå‰ç¼€
                if not is_data_source:
                    is_data_source = plugin_id.startswith('data_sources.')

                if is_data_source:
                    data_source_plugins.append((plugin_id, plugin_instance, metadata))

            logger.info(f"ğŸ” å‘ç° {len(data_source_plugins)} ä¸ªæ•°æ®æºæ’ä»¶")

            # 3. æ³¨å†Œæ¯ä¸ªæ•°æ®æºæ’ä»¶
            for plugin_id, plugin_instance, metadata in data_source_plugins:
                try:
                    # æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨
                    is_enabled = metadata.get('enabled', True)
                    if not is_enabled:
                        logger.debug(f"â­ï¸ è·³è¿‡ç¦ç”¨çš„æ’ä»¶: {plugin_id}")
                        continue

                    # éªŒè¯æ’ä»¶æœ‰å¿…è¦çš„æ–¹æ³•
                    if not self._is_data_source_plugin(plugin_instance):
                        logger.warning(f"âš ï¸ æ’ä»¶ç¼ºå°‘å¿…è¦æ–¹æ³•ï¼Œè·³è¿‡: {plugin_id}")
                        continue

                    # è·å–ä¼˜å…ˆçº§å’Œæƒé‡
                    priority = 0
                    weight = 1.0

                    if hasattr(plugin_instance, 'priority'):
                        priority = plugin_instance.priority
                    elif 'priority' in metadata:
                        priority = metadata['priority']

                    if hasattr(plugin_instance, 'weight'):
                        weight = plugin_instance.weight
                    elif 'weight' in metadata:
                        weight = metadata['weight']

                    # æ³¨å†Œæ’ä»¶
                    success = self.register_data_source_plugin(
                        plugin_id=plugin_id,
                        adapter=plugin_instance,
                        priority=priority,
                        weight=weight
                    )

                    if success:
                        registered_count += 1
                        plugin_name = metadata.get('name', plugin_id)
                        logger.info(f"  âœ… æˆåŠŸæ³¨å†Œ: {plugin_name} ({plugin_id})")
                    else:
                        logger.warning(f"  âš ï¸ æ³¨å†Œå¤±è´¥: {plugin_id}")

                except Exception as e:
                    logger.error(f"  âŒ æ³¨å†Œæ’ä»¶å¼‚å¸¸ {plugin_id}: {e}")
                    continue

            logger.info(f"ğŸ“Š æ’ä»¶æ³¨å†Œç»Ÿè®¡: æˆåŠŸ {registered_count}/{len(data_source_plugins)}")
            return registered_count

        except Exception as e:
            logger.error(f"âŒ ä»æ’ä»¶ç®¡ç†å™¨æ³¨å†Œæ’ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return registered_count

    # ==================================================================================
    # ğŸ—‘ï¸ å·²åºŸå¼ƒï¼š_manual_register_core_plugins - ç¡¬ç¼–ç æ’ä»¶æ³¨å†Œæ–¹æ³•
    # æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ _register_plugins_from_plugin_manager() åŠ¨æ€åŠ è½½æ’ä»¶
    # ä¿ç•™æ­¤ä»£ç ç”¨äºå‚è€ƒï¼Œå¾…å®Œå…¨éªŒè¯ååˆ é™¤
    # ==================================================================================
    def _manual_register_core_plugins_DEPRECATED(self) -> None:
        """
        ã€å·²åºŸå¼ƒã€‘æ‰‹åŠ¨æ³¨å†Œæ ¸å¿ƒæ•°æ®æºæ’ä»¶

        âš ï¸ æ­¤æ–¹æ³•å·²è¢« _register_plugins_from_plugin_manager() æ›¿ä»£
        åŸå› ï¼šç¡¬ç¼–ç å¯¼å…¥18ä¸ªexamplesæ’ä»¶ï¼Œéš¾ä»¥ç»´æŠ¤

        è¯·å‹¿ä½¿ç”¨æ­¤æ–¹æ³•ï¼
        """
        logger.warning("âš ï¸ è°ƒç”¨äº†å·²åºŸå¼ƒçš„ _manual_register_core_plugins æ–¹æ³•")
        logger.warning("âš ï¸ è¯·ä½¿ç”¨ _register_plugins_from_plugin_manager æ›¿ä»£")
        return  # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ

        # ä»¥ä¸‹ä»£ç å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå‚è€ƒ
        """
        registered_count = 0

        # æ’ä»¶æ³¨å†Œå¼€å§‹

        # 2. æ³¨å†ŒAkShareæ’ä»¶ï¼ˆæ”¯æŒsector_fund_flowï¼‰
        try:
            # æ³¨æ„ï¼šakshare_stock_pluginå·²è¿ç§»åˆ°TET+Pluginæ¶æ„
            # é€šè¿‡æ’ä»¶ä¸­å¿ƒè‡ªåŠ¨å‘ç°å’Œæ³¨å†Œ
            logger.info("AkShareæ’ä»¶é€šè¿‡TET+Pluginæ¶æ„è‡ªåŠ¨ç®¡ç†")

            # AkShareæ’ä»¶ç°åœ¨é€šè¿‡TET+Pluginæ¶æ„ç®¡ç†
            # ä¸å†éœ€è¦æ‰‹åŠ¨æ³¨å†Œå’Œæ‰©å±•
            logger.info("AkShareæ’ä»¶å°†é€šè¿‡æ’ä»¶ä¸­å¿ƒè‡ªåŠ¨å‘ç°å’Œæ³¨å†Œ")
            # å‡è®¾æˆåŠŸï¼Œå› ä¸ºé€šè¿‡æ’ä»¶ä¸­å¿ƒç®¡ç†
            registered_count += 1

        except Exception as e:
            logger.warning(f" AkShareæ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 3. æ³¨å†ŒWindæ’ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            from plugins.examples.wind_data_plugin import WindDataPlugin
            wind_plugin = WindDataPlugin()

            success = self.register_data_source_plugin(
                "wind_data_source",
                wind_plugin,
                priority=5,  # è¾ƒé«˜ä¼˜å…ˆçº§ï¼Œä¸“ä¸šæ•°æ®æº
                weight=1.8
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒWindæ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("Windæ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" Windæ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 4. æ³¨å†Œä¸œæ–¹è´¢å¯Œæ’ä»¶
        try:
            from plugins.data_sources.eastmoney_plugin import EastMoneyStockPlugin
            eastmoney_plugin = EastMoneyStockPlugin()

            success = self.register_data_source_plugin(
                "eastmoney_stock",
                eastmoney_plugin,
                priority=20,
                weight=1.0
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œä¸œæ–¹è´¢å¯Œæ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("ä¸œæ–¹è´¢å¯Œæ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" ä¸œæ–¹è´¢å¯Œæ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 5. æ³¨å†Œé€šè¾¾ä¿¡æ’ä»¶
        try:
            from plugins.examples.tongdaxin_stock_plugin import TongdaxinStockPlugin
            tongdaxin_plugin = TongdaxinStockPlugin()

            success = self.register_data_source_plugin(
                "tongdaxin_stock",
                tongdaxin_plugin,
                priority=15,
                weight=1.3
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œé€šè¾¾ä¿¡æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("é€šè¾¾ä¿¡æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" é€šè¾¾ä¿¡æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 6. æ³¨å†ŒYahoo Financeæ’ä»¶
        try:
            from plugins.examples.yahoo_finance_datasource import YahooFinanceDataSourcePlugin
            yahoo_plugin = YahooFinanceDataSourcePlugin()

            success = self.register_data_source_plugin(
                "yahoo_finance",
                yahoo_plugin,
                priority=25,
                weight=1.2
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒYahoo Financeæ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("Yahoo Financeæ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" Yahoo Financeæ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 7. æ³¨å†ŒæœŸè´§æ•°æ®æ’ä»¶
        try:
            from plugins.examples.futures_data_plugin import FuturesDataPlugin
            futures_plugin = FuturesDataPlugin()

            success = self.register_data_source_plugin(
                "futures_data_source",
                futures_plugin,
                priority=30,
                weight=1.2
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒæœŸè´§æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("æœŸè´§æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" æœŸè´§æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 8. æ³¨å†ŒCTPæœŸè´§æ’ä»¶
        try:
            from plugins.examples.ctp_futures_plugin import CTPFuturesPlugin
            ctp_plugin = CTPFuturesPlugin()

            success = self.register_data_source_plugin(
                "ctp_futures",
                ctp_plugin,
                priority=12,  # è¾ƒé«˜ä¼˜å…ˆçº§çš„æœŸè´§æ•°æ®æº
                weight=1.6
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒCTPæœŸè´§æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("CTPæœŸè´§æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" CTPæœŸè´§æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 9. æ³¨å†Œæ–‡åè´¢ç»æ’ä»¶
        try:
            from plugins.examples.wenhua_data_plugin import WenhuaDataPlugin
            wenhua_plugin = WenhuaDataPlugin()

            success = self.register_data_source_plugin(
                "wenhua_data",
                wenhua_plugin,
                priority=18,
                weight=1.4
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œæ–‡åè´¢ç»æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("æ–‡åè´¢ç»æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" æ–‡åè´¢ç»æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 10. æ³¨å†Œå¤–æ±‡æ•°æ®æ’ä»¶
        try:
            from plugins.examples.forex_data_plugin import ForexDataPlugin
            forex_plugin = ForexDataPlugin()

            success = self.register_data_source_plugin(
                "forex_data_source",
                forex_plugin,
                priority=35,
                weight=1.0
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œå¤–æ±‡æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("å¤–æ±‡æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" å¤–æ±‡æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 11. æ³¨å†Œå€ºåˆ¸æ•°æ®æ’ä»¶
        try:
            from plugins.examples.bond_data_plugin import BondDataPlugin
            bond_plugin = BondDataPlugin()

            success = self.register_data_source_plugin(
                "bond_data_source",
                bond_plugin,
                priority=40,
                weight=1.0
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œå€ºåˆ¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("å€ºåˆ¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" å€ºåˆ¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 12. æ³¨å†ŒåŠ å¯†è´§å¸æ•°æ®æ’ä»¶
        try:
            from plugins.examples.crypto_data_plugin import CryptoDataPlugin
            crypto_plugin = CryptoDataPlugin()

            success = self.register_data_source_plugin(
                "crypto_data_source",
                crypto_plugin,
                priority=45,
                weight=1.1
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒåŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("åŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" åŠ å¯†è´§å¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 13. æ³¨å†Œå¸å®‰åŠ å¯†è´§å¸æ’ä»¶
        try:
            from plugins.examples.binance_crypto_plugin import BinanceCryptoPlugin
            binance_plugin = BinanceCryptoPlugin()

            success = self.register_data_source_plugin(
                "binance_crypto",
                binance_plugin,
                priority=22,  # è¾ƒé«˜ä¼˜å…ˆçº§çš„åŠ å¯†è´§å¸æ•°æ®æº
                weight=1.4
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œå¸å®‰åŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("å¸å®‰åŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" å¸å®‰åŠ å¯†è´§å¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 14. æ³¨å†Œç«å¸åŠ å¯†è´§å¸æ’ä»¶
        try:
            from plugins.examples.huobi_crypto_plugin import HuobiCryptoPlugin
            huobi_plugin = HuobiCryptoPlugin()

            success = self.register_data_source_plugin(
                "huobi_crypto",
                huobi_plugin,
                priority=24,
                weight=1.3
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œç«å¸åŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("ç«å¸åŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" ç«å¸åŠ å¯†è´§å¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 15. æ³¨å†ŒOKXåŠ å¯†è´§å¸æ’ä»¶
        try:
            from plugins.examples.okx_crypto_plugin import OKXCryptoPlugin
            okx_plugin = OKXCryptoPlugin()

            success = self.register_data_source_plugin(
                "okx_crypto",
                okx_plugin,
                priority=26,
                weight=1.3
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒOKXåŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("OKXåŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" OKXåŠ å¯†è´§å¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 16. æ³¨å†ŒCoinbaseåŠ å¯†è´§å¸æ’ä»¶
        try:
            from plugins.examples.coinbase_crypto_plugin import CoinbaseCryptoPlugin
            coinbase_plugin = CoinbaseCryptoPlugin()

            success = self.register_data_source_plugin(
                "coinbase_crypto",
                coinbase_plugin,
                priority=28,
                weight=1.2
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†ŒCoinbaseåŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("CoinbaseåŠ å¯†è´§å¸æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" CoinbaseåŠ å¯†è´§å¸æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 17. æ³¨å†Œæˆ‘çš„é’¢é“ç½‘æ•°æ®æ’ä»¶
        try:
            from plugins.examples.mysteel_data_plugin import MySteelDataPlugin
            mysteel_plugin = MySteelDataPlugin()

            success = self.register_data_source_plugin(
                "mysteel_data",
                mysteel_plugin,
                priority=50,
                weight=0.8
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œæˆ‘çš„é’¢é“ç½‘æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("æˆ‘çš„é’¢é“ç½‘æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" æˆ‘çš„é’¢é“ç½‘æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        # 18. æ³¨å†Œè‡ªå®šä¹‰æ•°æ®æ’ä»¶
        try:
            from plugins.examples.custom_data_plugin import CustomDataPlugin
            custom_plugin = CustomDataPlugin()

            success = self.register_data_source_plugin(
                "custom_data_source",
                custom_plugin,
                priority=99,  # æœ€ä½ä¼˜å…ˆçº§
                weight=0.5
            )

            if success:
                registered_count += 1
                logger.info("æ‰‹åŠ¨æ³¨å†Œè‡ªå®šä¹‰æ•°æ®æºæ’ä»¶æˆåŠŸ")
            else:
                logger.warning("è‡ªå®šä¹‰æ•°æ®æºæ’ä»¶æ³¨å†Œå¤±è´¥")

        except Exception as e:
            logger.warning(f" è‡ªå®šä¹‰æ’ä»¶æ³¨å†Œå¤±è´¥: {e}")

        if registered_count > 0:
            logger.info(f" æ‰‹åŠ¨æ³¨å†Œäº† {registered_count} ä¸ªæ ¸å¿ƒæ•°æ®æºæ’ä»¶")
            self._plugins_discovered = True
        else:
            logger.warning("æœªèƒ½æ³¨å†Œä»»ä½•æ•°æ®æºæ’ä»¶ï¼Œåˆ›å»ºåŸºæœ¬å›é€€æ•°æ®æº")
            # åˆ›å»ºåŸºæœ¬å›é€€æ•°æ®æºï¼Œé¿å…TETç®¡é“å®Œå…¨æ— æ³•å·¥ä½œ
            self._create_fallback_data_source()
            self._plugins_discovered = True
        """  # åºŸå¼ƒä»£ç ç»“æŸ

    def _create_fallback_data_source_DEPRECATED(self) -> None:
        """åˆ›å»ºåŸºæœ¬å›é€€æ•°æ®æºï¼Œç¡®ä¿TETç®¡é“æœ‰å¯ç”¨çš„æ•°æ®æº"""
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å›é€€æ•°æ®æºç±»
            class FallbackDataSource:
                def __init__(self):
                    # ä¼ ç»Ÿæ•°æ®æºfallback
                    self.name = "fallback_source"
                    self.priority = 999  # æœ€ä½ä¼˜å…ˆçº§
                    self.weight = 0.1

                def get_stock_list(self, market='all'):
                    return pd.DataFrame()

                def get_kdata(self, symbol, period, start_date, end_date):
                    return pd.DataFrame()

            fallback_source = FallbackDataSource()

            # å°è¯•æ³¨å†Œåˆ°TETç®¡é“
            if hasattr(self, 'tet_pipeline') and self.tet_pipeline and hasattr(self.tet_pipeline, 'router'):
                success = self.tet_pipeline.router.register_data_source(
                    "fallback_source",
                    fallback_source,
                    priority=999,  # æœ€ä½ä¼˜å…ˆçº§
                    weight=0.1
                )

                if success:
                    logger.info("åˆ›å»ºå›é€€æ•°æ®æºæˆåŠŸ")
                else:
                    logger.warning("åˆ›å»ºå›é€€æ•°æ®æºå¤±è´¥")
            else:
                logger.warning("TETç®¡é“ä¸å¯ç”¨ï¼Œæ— æ³•æ³¨å†Œå›é€€æ•°æ®æº")

        except Exception as e:
            logger.error(f" åˆ›å»ºå›é€€æ•°æ®æºå¼‚å¸¸: {e}")

    def _extend_akshare_plugin_for_sector_flow(self, akshare_plugin) -> None:
        """æ‰©å±•AkShareæ’ä»¶ä»¥æ”¯æŒSECTOR_FUND_FLOWæ•°æ®ç±»å‹"""
        try:
            # æ·»åŠ SECTOR_FUND_FLOWåˆ°æ”¯æŒçš„æ•°æ®ç±»å‹
            if hasattr(akshare_plugin, 'plugin_info'):
                plugin_info = akshare_plugin.plugin_info
                if hasattr(plugin_info, 'supported_data_types'):
                    from ..plugin_types import DataType
                    if DataType.SECTOR_FUND_FLOW not in plugin_info.supported_data_types:
                        plugin_info.supported_data_types.append(DataType.SECTOR_FUND_FLOW)
                        logger.info("AkShareæ’ä»¶å·²æ‰©å±•æ”¯æŒSECTOR_FUND_FLOW")

            # æ·»åŠ è·å–æ¿å—èµ„é‡‘æµçš„æ–¹æ³•
            def get_sector_fund_flow_data(symbol: str, **kwargs):
                try:
                    import akshare as ak
                    # æ ¹æ®symbolç±»å‹é€‰æ‹©åˆé€‚çš„akshareå‡½æ•°
                    if symbol == "sector":
                        return ak.stock_sector_fund_flow_rank(indicator="ä»Šæ—¥")
                    else:
                        return ak.stock_sector_fund_flow_summary(symbol=symbol, indicator="ä»Šæ—¥")
                except Exception as e:
                    logger.error(f"è·å–æ¿å—èµ„é‡‘æµæ•°æ®å¤±è´¥: {e}")
                    return None

            # åŠ¨æ€æ·»åŠ æ–¹æ³•åˆ°æ’ä»¶å®ä¾‹
            akshare_plugin.get_sector_fund_flow_data = get_sector_fund_flow_data
            logger.info("AkShareæ’ä»¶å·²æ·»åŠ æ¿å—èµ„é‡‘æµæ•°æ®è·å–æ–¹æ³•")

        except Exception as e:
            logger.error(f"æ‰©å±•AkShareæ’ä»¶å¤±è´¥: {e}")

    @property
    def data_source_router(self):
        """
        å…¼å®¹æ€§å±æ€§ï¼šæä¾›å¯¹æ•°æ®æºè·¯ç”±å™¨çš„è®¿é—®

        Returns:
            æ•°æ®æºè·¯ç”±å™¨å®ä¾‹ï¼Œå¦‚æœTETç®¡é“å¯ç”¨çš„è¯
        """
        if hasattr(self, 'tet_pipeline') and self.tet_pipeline:
            return self.tet_pipeline.router
        return None

    def set_asset_routing_priorities(self, asset_type: AssetType, priorities: List[str]) -> bool:
        """
        è®¾ç½®èµ„äº§ç±»å‹çš„æ•°æ®æºè·¯ç”±ä¼˜å…ˆçº§

        Args:
            asset_type: èµ„äº§ç±»å‹
            priorities: æ•°æ®æºä¼˜å…ˆçº§åˆ—è¡¨

        Returns:
            bool: è®¾ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            router = self.data_source_router
            if router is None:
                logger.error("æ•°æ®æºè·¯ç”±å™¨ä¸å¯ç”¨ï¼Œæ— æ³•è®¾ç½®ä¼˜å…ˆçº§")
                return False

            # è°ƒç”¨è·¯ç”±å™¨çš„set_asset_prioritiesæ–¹æ³•
            router.set_asset_priorities(asset_type, priorities)
            logger.info(f" æˆåŠŸè®¾ç½®{asset_type.value}çš„è·¯ç”±ä¼˜å…ˆçº§: {priorities}")
            return True

        except Exception as e:
            logger.error(f" è®¾ç½®èµ„äº§è·¯ç”±ä¼˜å…ˆçº§å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def get_asset_routing_priorities(self, asset_type: AssetType) -> List[str]:
        """
        è·å–èµ„äº§ç±»å‹çš„æ•°æ®æºè·¯ç”±ä¼˜å…ˆçº§

        Args:
            asset_type: èµ„äº§ç±»å‹

        Returns:
            List[str]: æ•°æ®æºä¼˜å…ˆçº§åˆ—è¡¨
        """
        try:
            router = self.data_source_router
            if router is None:
                logger.warning("æ•°æ®æºè·¯ç”±å™¨ä¸å¯ç”¨ï¼Œè¿”å›ç©ºä¼˜å…ˆçº§åˆ—è¡¨")
                return []

            return router.asset_priorities.get(asset_type, [])

        except Exception as e:
            logger.error(f" è·å–èµ„äº§è·¯ç”±ä¼˜å…ˆçº§å¤±è´¥: {e}")
            return []

    def _initialize_sector_service(self):
        """
        åˆå§‹åŒ–æ¿å—æ•°æ®æœåŠ¡
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from .sector_data_service import get_sector_data_service

            # è·å–ç¼“å­˜ç®¡ç†å™¨
            cache_manager = getattr(self, 'cache_manager', None)

            # åˆå§‹åŒ–æ¿å—æ•°æ®æœåŠ¡
            self._sector_data_service = get_sector_data_service(
                cache_manager=cache_manager,
                tet_pipeline=self.tet_pipeline
            )

            logger.info("æ¿å—æ•°æ®æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"æ¿å—æ•°æ®æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self._sector_data_service = None

    def get_sector_fund_flow_service(self):
        """
        è·å–æ¿å—èµ„é‡‘æµæœåŠ¡å®ä¾‹

        Returns:
            SectorDataService: æ¿å—æ•°æ®æœåŠ¡å®ä¾‹ï¼Œå¦‚æœåˆå§‹åŒ–å¤±è´¥åˆ™è¿”å›None
        """
        return self._sector_data_service

    def get_sector_fund_flow_ranking(self, date_range: str = "today", sort_by: str = 'main_net_inflow'):
        """
        è·å–æ¿å—èµ„é‡‘æµæ’è¡Œæ¦œï¼ˆç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å…¥å£ï¼‰

        Args:
            date_range: æ—¶é—´èŒƒå›´ï¼Œå¦‚ "today", "3d", "5d", "1m"
            sort_by: æ’åºå­—æ®µï¼Œé»˜è®¤æŒ‰ä¸»åŠ›å‡€æµå…¥æ’åº

        Returns:
            pd.DataFrame: æ¿å—æ’è¡Œæ¦œæ•°æ®
        """
        try:
            if self._sector_data_service is None:
                logger.warning("æ¿å—æ•°æ®æœåŠ¡ä¸å¯ç”¨")
                return pd.DataFrame()

            return self._sector_data_service.get_sector_fund_flow_ranking(date_range, sort_by)

        except Exception as e:
            logger.error(f"è·å–æ¿å—èµ„é‡‘æµæ’è¡Œæ¦œå¤±è´¥: {e}")
            return pd.DataFrame()

    def get_sector_historical_trend(self, sector_id: str, period: int = 30):
        """
        è·å–å•æ¿å—å†å²è¶‹åŠ¿æ•°æ®ï¼ˆç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å…¥å£ï¼‰

        Args:
            sector_id: æ¿å—IDï¼Œå¦‚ "BK0001"
            period: æŸ¥è¯¢å¤©æ•°ï¼Œé»˜è®¤30å¤©

        Returns:
            pd.DataFrame: æ¿å—å†å²è¶‹åŠ¿æ•°æ®
        """
        try:
            if self._sector_data_service is None:
                logger.warning("æ¿å—æ•°æ®æœåŠ¡ä¸å¯ç”¨")
                return pd.DataFrame()

            return self._sector_data_service.get_sector_historical_trend(sector_id, period)

        except Exception as e:
            logger.error(f"è·å–æ¿å—å†å²è¶‹åŠ¿å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_sector_intraday_flow(self, sector_id: str, date: str):
        """
        è·å–æ¿å—åˆ†æ—¶èµ„é‡‘æµæ•°æ®ï¼ˆç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å…¥å£ï¼‰

        Args:
            sector_id: æ¿å—IDï¼Œå¦‚ "BK0001"
            date: æŸ¥è¯¢æ—¥æœŸï¼Œæ ¼å¼ "YYYY-MM-DD"

        Returns:
            pd.DataFrame: æ¿å—åˆ†æ—¶èµ„é‡‘æµæ•°æ®
        """
        try:
            if self._sector_data_service is None:
                logger.warning("æ¿å—æ•°æ®æœåŠ¡ä¸å¯ç”¨")
                return pd.DataFrame()

            return self._sector_data_service.get_sector_intraday_flow(sector_id, date)

        except Exception as e:
            logger.error(f"è·å–æ¿å—åˆ†æ—¶èµ„é‡‘æµå¤±è´¥: {e}")
            return pd.DataFrame()

    def import_sector_historical_data(self, source: str, start_date: str, end_date: str):
        """
        å¯¼å…¥æ¿å—å†å²æ•°æ®ï¼ˆç»Ÿä¸€æ•°æ®ç®¡ç†å™¨å…¥å£ï¼‰

        Args:
            source: æ•°æ®æºåç§°ï¼Œå¦‚ "akshare", "eastmoney"
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ "YYYY-MM-DD"
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ "YYYY-MM-DD"

        Returns:
            Dict[str, Any]: å¯¼å…¥ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if self._sector_data_service is None:
                logger.warning("æ¿å—æ•°æ®æœåŠ¡ä¸å¯ç”¨")
                return {"success": False, "error": "æ¿å—æ•°æ®æœåŠ¡ä¸å¯ç”¨"}

            return self._sector_data_service.import_sector_historical_data(source, start_date, end_date)

        except Exception as e:
            logger.error(f"å¯¼å…¥æ¿å—å†å²æ•°æ®å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

# æ•°æ®ç­–ç•¥ç±»
