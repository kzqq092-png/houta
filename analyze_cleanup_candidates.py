#!/usr/bin/env python3
"""
HIkyuu-UI æ— æ•ˆä»£ç åˆ†æå·¥å…· - ä»…åˆ†æï¼Œä¸åˆ é™¤
ç”Ÿæˆè¯¦ç»†çš„æ¸…ç†å€™é€‰æ–‡ä»¶æŠ¥å‘Š
"""

import os
from pathlib import Path
from typing import List, Dict
import json
from datetime import datetime


class CleanupAnalyzer:
    """æ¸…ç†åˆ†æå™¨ - ä»…åˆ†æï¼Œä¸åˆ é™¤"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)

    def analyze_backup_files(self) -> Dict:
        """åˆ†æå¤‡ä»½æ–‡ä»¶"""
        backup_patterns = [
            "**/*.backup",
            "**/*.backup2",
            "**/*.backup3",
            "**/config/app_config.json.backup.*"
        ]

        backup_files = []
        total_size = 0

        for pattern in backup_patterns:
            files = list(self.project_root.glob(pattern))
            for file_path in files:
                if file_path.exists() and file_path.is_file():
                    size = file_path.stat().st_size
                    backup_files.append({
                        'path': str(file_path.relative_to(self.project_root)),
                        'size_bytes': size,
                        'size_mb': size / (1024 * 1024),
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                    })
                    total_size += size

        return {
            'files': backup_files,
            'count': len(backup_files),
            'total_size_mb': total_size / (1024 * 1024)
        }

    def analyze_temp_files(self) -> Dict:
        """åˆ†æä¸´æ—¶æ–‡ä»¶"""
        temp_patterns = [
            "**/architecture_diagnosis_report_*.md",
            "**/startup_log.txt",
            "**/startup_error.txt",
            "**/*_report_*.txt",
            "**/*_report_*.md",
            "**/comprehensive_*.py",
            "**/debug_*.py",
            "**/fix_*.py",
            "**/check_*.py",
            "**/verify_*.py",
            "**/diagnose_*.py",
            "**/enhance_*.py",
            "**/optimize_*.py",
            "**/immediate_*.py"
        ]

        temp_files = []
        total_size = 0

        for pattern in temp_patterns:
            files = list(self.project_root.glob(pattern))
            for file_path in files:
                if file_path.exists() and file_path.is_file() and not self._is_important_file(file_path):
                    size = file_path.stat().st_size
                    temp_files.append({
                        'path': str(file_path.relative_to(self.project_root)),
                        'size_bytes': size,
                        'size_mb': size / (1024 * 1024),
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                        'category': self._categorize_temp_file(file_path)
                    })
                    total_size += size

        return {
            'files': temp_files,
            'count': len(temp_files),
            'total_size_mb': total_size / (1024 * 1024)
        }

    def analyze_duplicate_files(self) -> Dict:
        """åˆ†æé‡å¤æ–‡ä»¶"""
        duplicate_patterns = [
            "**/backups_professional/",
            "**/*_clean.py",
            "**/*_backup.py"
        ]

        duplicate_files = []
        total_size = 0

        for pattern in duplicate_patterns:
            files = list(self.project_root.glob(pattern))
            for file_path in files:
                if file_path.exists():
                    if file_path.is_file():
                        size = file_path.stat().st_size
                        duplicate_files.append({
                            'path': str(file_path.relative_to(self.project_root)),
                            'type': 'file',
                            'size_bytes': size,
                            'size_mb': size / (1024 * 1024),
                            'modified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                        })
                        total_size += size
                    elif file_path.is_dir():
                        dir_size = sum(f.stat().st_size for f in file_path.rglob('*') if f.is_file())
                        duplicate_files.append({
                            'path': str(file_path.relative_to(self.project_root)),
                            'type': 'directory',
                            'size_bytes': dir_size,
                            'size_mb': dir_size / (1024 * 1024),
                            'modified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                        })
                        total_size += dir_size

        return {
            'files': duplicate_files,
            'count': len(duplicate_files),
            'total_size_mb': total_size / (1024 * 1024)
        }

    def _is_important_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯é‡è¦æ–‡ä»¶"""
        important_patterns = [
            "tests/final/",
            "tests/integration/",
            "tests/performance/",
            "tests/compatibility/",
            "main.py",
            "__init__.py"
        ]

        file_str = str(file_path)
        return any(pattern in file_str for pattern in important_patterns)

    def _categorize_temp_file(self, file_path: Path) -> str:
        """åˆ†ç±»ä¸´æ—¶æ–‡ä»¶"""
        name = file_path.name.lower()
        if 'report' in name:
            return 'report'
        elif 'debug' in name:
            return 'debug'
        elif 'fix' in name:
            return 'fix'
        elif 'check' in name or 'verify' in name:
            return 'verification'
        elif 'diagnose' in name:
            return 'diagnosis'
        elif 'log' in name or 'error' in name:
            return 'log'
        else:
            return 'other'

    def generate_analysis_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ” åˆ†æé¡¹ç›®æ–‡ä»¶...")

        backup_analysis = self.analyze_backup_files()
        temp_analysis = self.analyze_temp_files()
        duplicate_analysis = self.analyze_duplicate_files()

        total_files = backup_analysis['count'] + temp_analysis['count'] + duplicate_analysis['count']
        total_size = backup_analysis['total_size_mb'] + temp_analysis['total_size_mb'] + duplicate_analysis['total_size_mb']

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = f"""# HIkyuu-UI æ— æ•ˆä»£ç åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**é¡¹ç›®è·¯å¾„**: {self.project_root}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°**: {total_files} ä¸ª
- **æ€»å¤§å°**: {total_size:.2f} MB
- **å¤‡ä»½æ–‡ä»¶**: {backup_analysis['count']} ä¸ª ({backup_analysis['total_size_mb']:.2f} MB)
- **ä¸´æ—¶æ–‡ä»¶**: {temp_analysis['count']} ä¸ª ({temp_analysis['total_size_mb']:.2f} MB)
- **é‡å¤æ–‡ä»¶**: {duplicate_analysis['count']} ä¸ª ({duplicate_analysis['total_size_mb']:.2f} MB)

## ğŸ—‚ï¸ å¤‡ä»½æ–‡ä»¶è¯¦æƒ… ({backup_analysis['count']} ä¸ª)

è¿™äº›æ˜¯é‡æ„è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¤‡ä»½æ–‡ä»¶ï¼Œç°åœ¨å¯ä»¥å®‰å…¨åˆ é™¤ï¼š

"""

        # æ·»åŠ å¤‡ä»½æ–‡ä»¶åˆ—è¡¨
        for file_info in sorted(backup_analysis['files'], key=lambda x: x['size_mb'], reverse=True):
            report_content += f"- `{file_info['path']}` ({file_info['size_mb']:.2f} MB, {file_info['modified']})\n"

        report_content += f"""

## ğŸ“„ ä¸´æ—¶æ–‡ä»¶è¯¦æƒ… ({temp_analysis['count']} ä¸ª)

æŒ‰ç±»åˆ«åˆ†ç»„çš„ä¸´æ—¶æ–‡ä»¶ï¼š

"""

        # æŒ‰ç±»åˆ«åˆ†ç»„ä¸´æ—¶æ–‡ä»¶
        temp_by_category = {}
        for file_info in temp_analysis['files']:
            category = file_info['category']
            if category not in temp_by_category:
                temp_by_category[category] = []
            temp_by_category[category].append(file_info)

        for category, files in temp_by_category.items():
            report_content += f"### {category.title()} æ–‡ä»¶ ({len(files)} ä¸ª)\n\n"
            for file_info in sorted(files, key=lambda x: x['size_mb'], reverse=True):
                report_content += f"- `{file_info['path']}` ({file_info['size_mb']:.2f} MB, {file_info['modified']})\n"
            report_content += "\n"

        report_content += f"""## ğŸ”„ é‡å¤æ–‡ä»¶è¯¦æƒ… ({duplicate_analysis['count']} ä¸ª)

è¿™äº›æ˜¯é‡å¤æˆ–è¿‡æ—¶çš„æ–‡ä»¶/ç›®å½•ï¼š

"""

        # æ·»åŠ é‡å¤æ–‡ä»¶åˆ—è¡¨
        for file_info in sorted(duplicate_analysis['files'], key=lambda x: x['size_mb'], reverse=True):
            file_type = "ğŸ“" if file_info['type'] == 'directory' else "ğŸ“„"
            report_content += f"- {file_type} `{file_info['path']}` ({file_info['size_mb']:.2f} MB, {file_info['modified']})\n"

        report_content += f"""

## ğŸ¯ æ¸…ç†å»ºè®®

### ğŸŸ¢ å®‰å…¨æ¸…ç† (æ¨è)
å¯ä»¥å®‰å…¨åˆ é™¤ä»¥ä¸‹æ–‡ä»¶ï¼Œä¸ä¼šå½±å“ç³»ç»ŸåŠŸèƒ½ï¼š
- æ‰€æœ‰ `.backup` æ–‡ä»¶ ({backup_analysis['count']} ä¸ª)
- è¯Šæ–­æŠ¥å‘Šæ–‡ä»¶ (report ç±»åˆ«)
- æ—¥å¿—æ–‡ä»¶ (log ç±»åˆ«)
- `backups_professional/` ç›®å½•

**é¢„è®¡èŠ‚çœç©ºé—´**: {backup_analysis['total_size_mb'] + sum(f['size_mb'] for f in temp_analysis['files'] if f['category'] in ['report', 'log']):.2f} MB

### ğŸŸ¡ è°¨æ…æ¸…ç†
éœ€è¦ä»”ç»†æ£€æŸ¥çš„æ–‡ä»¶ï¼š
- è°ƒè¯•è„šæœ¬ (debug ç±»åˆ«) - å¯èƒ½è¿˜æœ‰ç”¨
- ä¿®å¤è„šæœ¬ (fix ç±»åˆ«) - å¯èƒ½åŒ…å«é‡è¦é€»è¾‘
- éªŒè¯è„šæœ¬ (verification ç±»åˆ«) - å¯èƒ½ç”¨äºæµ‹è¯•

### ğŸ”´ ä¿ç•™æ–‡ä»¶
å»ºè®®ä¿ç•™çš„é‡è¦æ–‡ä»¶ï¼š
- `tests/` ç›®å½•ä¸‹çš„æµ‹è¯•æ–‡ä»¶
- `main.py` å’Œ `__init__.py` æ–‡ä»¶
- æ ¸å¿ƒåŠŸèƒ½ç›¸å…³çš„è„šæœ¬

## ğŸ“‹ æ¸…ç†å‘½ä»¤

å¦‚æœæ‚¨ç¡®è®¤è¦åˆ é™¤è¿™äº›æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# åˆ é™¤å¤‡ä»½æ–‡ä»¶
find . -name "*.backup*" -type f -delete

# åˆ é™¤ç‰¹å®šä¸´æ—¶æ–‡ä»¶
rm -f architecture_diagnosis_report_*.md
rm -f startup_log.txt startup_error.txt
rm -f *_report_*.txt *_report_*.md

# åˆ é™¤é‡å¤ç›®å½•
rm -rf backups_professional/
```

**âš ï¸ è­¦å‘Š**: åˆ é™¤å‰è¯·ç¡®ä¿æ‚¨æœ‰å®Œæ•´çš„é¡¹ç›®å¤‡ä»½ï¼

## ğŸ“ˆ æ¸…ç†æ•ˆæœé¢„ä¼°

æ¸…ç†å®Œæˆåï¼š
- **æ–‡ä»¶æ•°é‡å‡å°‘**: {total_files} ä¸ª
- **å­˜å‚¨ç©ºé—´é‡Šæ”¾**: {total_size:.2f} MB
- **é¡¹ç›®ç»“æ„ä¼˜åŒ–**: ç§»é™¤å†—ä½™æ–‡ä»¶ï¼Œæé«˜å¯ç»´æŠ¤æ€§
- **æ„å»ºæ€§èƒ½æå‡**: å‡å°‘æ–‡ä»¶æ‰«æå¼€é”€

---

*æ­¤æŠ¥å‘Šç”± HIkyuu-UI æ— æ•ˆä»£ç åˆ†æå·¥å…·ç”Ÿæˆ*
"""

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"cleanup_analysis_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        # åŒæ—¶ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_data = {
            'timestamp': timestamp,
            'summary': {
                'total_files': total_files,
                'total_size_mb': total_size,
                'backup_files': backup_analysis['count'],
                'temp_files': temp_analysis['count'],
                'duplicate_files': duplicate_analysis['count']
            },
            'backup_files': backup_analysis,
            'temp_files': temp_analysis,
            'duplicate_files': duplicate_analysis
        }

        json_file = f"cleanup_analysis_data_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š åˆ†æå®Œæˆ!")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"ğŸ“‹ æ•°æ®æ–‡ä»¶: {json_file}")

        return report_file


def main():
    """ä¸»å‡½æ•°"""
    project_root = os.getcwd()

    print("HIkyuu-UI æ— æ•ˆä»£ç åˆ†æå·¥å…·")
    print("=" * 50)
    print(f"é¡¹ç›®è·¯å¾„: {project_root}")
    print("æ³¨æ„: æ­¤å·¥å…·ä»…åˆ†ææ–‡ä»¶ï¼Œä¸ä¼šåˆ é™¤ä»»ä½•å†…å®¹")
    print("=" * 50)

    analyzer = CleanupAnalyzer(project_root)
    report_file = analyzer.generate_analysis_report()

    print(f"\nâœ… åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print("ğŸ“– è¯·æŸ¥çœ‹æŠ¥å‘Šäº†è§£è¯¦ç»†çš„æ¸…ç†å»ºè®®")


if __name__ == "__main__":
    main()
