#!/usr/bin/env python3
"""
ç³»ç»Ÿæ€§èƒ½è¯„ä¼°æ¨¡å—

æä¾›å…¨é¢çš„ç³»ç»Ÿæ€§èƒ½è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- äº¤æ˜“ç­–ç•¥æ€§èƒ½è¯„ä¼°
- å½¢æ€è¯†åˆ«æ€§èƒ½è¯„ä¼°
- ç³»ç»Ÿè¿è¡Œæ€§èƒ½è¯„ä¼°
- é£é™©è°ƒæ•´åæ”¶ç›Šè¯„ä¼°
- ç®—æ³•æ€§èƒ½è¯„ä¼°
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import sqlite3
import json
import time
import tracemalloc
from dataclasses import dataclass, asdict
from enum import Enum

# å°è¯•å¯¼å…¥psutil
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


class PerformanceMetricType(Enum):
    """æ€§èƒ½æŒ‡æ ‡ç±»å‹"""
    RETURN = "return"           # æ”¶ç›ŠæŒ‡æ ‡
    RISK = "risk"              # é£é™©æŒ‡æ ‡
    EFFICIENCY = "efficiency"   # æ•ˆç‡æŒ‡æ ‡
    STABILITY = "stability"     # ç¨³å®šæ€§æŒ‡æ ‡
    ACCURACY = "accuracy"       # å‡†ç¡®æ€§æŒ‡æ ‡
    BUSINESS = "business"       # ä¸šåŠ¡æŒ‡æ ‡


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    name: str
    value: float
    type: PerformanceMetricType
    description: str
    benchmark: Optional[float] = None
    score: Optional[float] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


@dataclass
class AlgorithmMetrics:
    """ç®—æ³•æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    # å‡†ç¡®æ€§æŒ‡æ ‡
    true_positives: int = 0
    false_positives: int = 0
    true_negatives: int = 0
    false_negatives: int = 0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    accuracy: float = 0.0

    # æ€§èƒ½æŒ‡æ ‡
    execution_time: float = 0.0
    memory_usage: float = 0.0
    cpu_usage: float = 0.0

    # ä¸šåŠ¡æŒ‡æ ‡
    signal_quality: float = 0.0
    confidence_avg: float = 0.0
    confidence_std: float = 0.0
    patterns_found: int = 0

    # ç¨³å®šæ€§æŒ‡æ ‡
    robustness_score: float = 0.0
    parameter_sensitivity: float = 0.0

    # ç»¼åˆè¯„åˆ†
    overall_score: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class PerformanceEvaluator:
    """ç³»ç»Ÿæ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self, db_path: str = 'db/hikyuu_system.db', debug_mode: bool = False):
        """
        åˆå§‹åŒ–æ€§èƒ½è¯„ä¼°å™¨

        Args:
            db_path: æ•°æ®åº“è·¯å¾„
            debug_mode: è°ƒè¯•æ¨¡å¼
        """
        self.db_path = db_path
        self.debug_mode = debug_mode
        self.metrics_cache = {}

    def evaluate_strategy_performance(self,
                                      strategy_name: str,
                                      returns: pd.Series,
                                      benchmark_returns: Optional[pd.Series] = None,
                                      risk_free_rate: float = 0.03) -> Dict[str, PerformanceMetric]:
        """
        è¯„ä¼°ç­–ç•¥æ€§èƒ½

        Args:
            strategy_name: ç­–ç•¥åç§°
            returns: ç­–ç•¥æ”¶ç›Šç‡åºåˆ—
            benchmark_returns: åŸºå‡†æ”¶ç›Šç‡åºåˆ—
            risk_free_rate: æ— é£é™©åˆ©ç‡

        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        metrics = {}

        # åŸºç¡€æ”¶ç›ŠæŒ‡æ ‡
        total_return = (1 + returns).prod() - 1
        annual_return = (1 + total_return) ** (252 / len(returns)) - 1

        metrics['total_return'] = PerformanceMetric(
            name="æ€»æ”¶ç›Šç‡",
            value=total_return,
            type=PerformanceMetricType.RETURN,
            description="ç­–ç•¥æ€»æ”¶ç›Šç‡"
        )

        metrics['annual_return'] = PerformanceMetric(
            name="å¹´åŒ–æ”¶ç›Šç‡",
            value=annual_return,
            type=PerformanceMetricType.RETURN,
            description="ç­–ç•¥å¹´åŒ–æ”¶ç›Šç‡"
        )

        # é£é™©æŒ‡æ ‡
        volatility = returns.std() * np.sqrt(252)
        max_drawdown = self._calculate_max_drawdown(returns)

        metrics['volatility'] = PerformanceMetric(
            name="å¹´åŒ–æ³¢åŠ¨ç‡",
            value=volatility,
            type=PerformanceMetricType.RISK,
            description="ç­–ç•¥å¹´åŒ–æ³¢åŠ¨ç‡"
        )

        metrics['max_drawdown'] = PerformanceMetric(
            name="æœ€å¤§å›æ’¤",
            value=max_drawdown,
            type=PerformanceMetricType.RISK,
            description="ç­–ç•¥æœ€å¤§å›æ’¤"
        )

        # é£é™©è°ƒæ•´æ”¶ç›ŠæŒ‡æ ‡
        if volatility > 0:
            sharpe_ratio = (annual_return - risk_free_rate) / volatility
            metrics['sharpe_ratio'] = PerformanceMetric(
                name="å¤æ™®æ¯”ç‡",
                value=sharpe_ratio,
                type=PerformanceMetricType.EFFICIENCY,
                description="é£é™©è°ƒæ•´åæ”¶ç›ŠæŒ‡æ ‡"
            )

        if max_drawdown < 0:
            calmar_ratio = annual_return / abs(max_drawdown)
            metrics['calmar_ratio'] = PerformanceMetric(
                name="å¡ç›æ¯”ç‡",
                value=calmar_ratio,
                type=PerformanceMetricType.EFFICIENCY,
                description="å¹´åŒ–æ”¶ç›Šä¸æœ€å¤§å›æ’¤æ¯”ç‡"
            )

        # åŸºå‡†æ¯”è¾ƒæŒ‡æ ‡
        if benchmark_returns is not None:
            excess_returns = returns - benchmark_returns
            tracking_error = excess_returns.std() * np.sqrt(252)

            if tracking_error > 0:
                information_ratio = excess_returns.mean() * 252 / tracking_error
                metrics['information_ratio'] = PerformanceMetric(
                    name="ä¿¡æ¯æ¯”ç‡",
                    value=information_ratio,
                    type=PerformanceMetricType.EFFICIENCY,
                    description="ç›¸å¯¹åŸºå‡†çš„é£é™©è°ƒæ•´æ”¶ç›Š"
                )

        # ç¨³å®šæ€§æŒ‡æ ‡
        win_rate = (returns > 0).mean()
        metrics['win_rate'] = PerformanceMetric(
            name="èƒœç‡",
            value=win_rate,
            type=PerformanceMetricType.STABILITY,
            description="ç›ˆåˆ©äº¤æ˜“å æ¯”"
        )

        return metrics

    def evaluate_pattern_performance(self,
                                     pattern_name: str,
                                     start_date: Optional[datetime] = None,
                                     end_date: Optional[datetime] = None) -> Dict[str, PerformanceMetric]:
        """
        è¯„ä¼°å½¢æ€è¯†åˆ«æ€§èƒ½

        Args:
            pattern_name: å½¢æ€åç§°
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        metrics = {}

        try:
            conn = sqlite3.connect(self.db_path)

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = "WHERE pattern_type = ?"
            params = [pattern_name]

            if start_date:
                where_clause += " AND trigger_date >= ?"
                params.append(start_date.strftime('%Y-%m-%d'))

            if end_date:
                where_clause += " AND trigger_date <= ?"
                params.append(end_date.strftime('%Y-%m-%d'))

            # æŸ¥è¯¢å½¢æ€å†å²æ•°æ®
            query = f"""
                SELECT 
                    COUNT(*) as total_signals,
                    AVG(confidence) as avg_confidence,
                    AVG(return_rate) as avg_return,
                    COUNT(CASE WHEN is_successful = 1 THEN 1 END) as successful_signals,
                    AVG(CASE WHEN is_successful = 1 THEN return_rate END) as avg_successful_return,
                    AVG(CASE WHEN is_successful = 0 THEN return_rate END) as avg_failed_return
                FROM pattern_history 
                {where_clause}
            """

            cursor = conn.cursor()
            cursor.execute(query, params)
            result = cursor.fetchone()

            if result and result[0] > 0:
                total_signals, avg_confidence, avg_return, successful_signals, avg_successful_return, avg_failed_return = result

                # ä¿¡å·è´¨é‡æŒ‡æ ‡
                metrics['total_signals'] = PerformanceMetric(
                    name="ä¿¡å·æ€»æ•°",
                    value=total_signals,
                    type=PerformanceMetricType.EFFICIENCY,
                    description="å½¢æ€è¯†åˆ«ä¿¡å·æ€»æ•°"
                )

                metrics['avg_confidence'] = PerformanceMetric(
                    name="å¹³å‡ç½®ä¿¡åº¦",
                    value=avg_confidence or 0,
                    type=PerformanceMetricType.EFFICIENCY,
                    description="å½¢æ€è¯†åˆ«å¹³å‡ç½®ä¿¡åº¦"
                )

                # æˆåŠŸç‡æŒ‡æ ‡
                success_rate = successful_signals / total_signals if total_signals > 0 else 0
                metrics['success_rate'] = PerformanceMetric(
                    name="æˆåŠŸç‡",
                    value=success_rate,
                    type=PerformanceMetricType.STABILITY,
                    description="å½¢æ€è¯†åˆ«æˆåŠŸç‡"
                )

                # æ”¶ç›ŠæŒ‡æ ‡
                metrics['avg_return'] = PerformanceMetric(
                    name="å¹³å‡æ”¶ç›Šç‡",
                    value=avg_return or 0,
                    type=PerformanceMetricType.RETURN,
                    description="å½¢æ€ä¿¡å·å¹³å‡æ”¶ç›Šç‡"
                )

                if avg_successful_return is not None:
                    metrics['avg_successful_return'] = PerformanceMetric(
                        name="æˆåŠŸä¿¡å·å¹³å‡æ”¶ç›Š",
                        value=avg_successful_return,
                        type=PerformanceMetricType.RETURN,
                        description="æˆåŠŸå½¢æ€ä¿¡å·å¹³å‡æ”¶ç›Šç‡"
                    )

                if avg_failed_return is not None:
                    metrics['avg_failed_return'] = PerformanceMetric(
                        name="å¤±è´¥ä¿¡å·å¹³å‡æ”¶ç›Š",
                        value=avg_failed_return,
                        type=PerformanceMetricType.RETURN,
                        description="å¤±è´¥å½¢æ€ä¿¡å·å¹³å‡æ”¶ç›Šç‡"
                    )

            conn.close()

        except Exception as e:
            print(f"è¯„ä¼°å½¢æ€æ€§èƒ½å¤±è´¥: {e}")

        return metrics

    def evaluate_system_performance(self) -> Dict[str, PerformanceMetric]:
        """
        è¯„ä¼°ç³»ç»Ÿè¿è¡Œæ€§èƒ½

        Returns:
            ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        metrics = {}

        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            metrics['cpu_usage'] = PerformanceMetric(
                name="CPUä½¿ç”¨ç‡",
                value=cpu_percent,
                type=PerformanceMetricType.EFFICIENCY,
                description="ç³»ç»ŸCPUä½¿ç”¨ç‡",
                benchmark=80.0  # 80%ä¸ºåŸºå‡†
            )

            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            metrics['memory_usage'] = PerformanceMetric(
                name="å†…å­˜ä½¿ç”¨ç‡",
                value=memory.percent,
                type=PerformanceMetricType.EFFICIENCY,
                description="ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡",
                benchmark=80.0  # 80%ä¸ºåŸºå‡†
            )

            # ç£ç›˜ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')
            metrics['disk_usage'] = PerformanceMetric(
                name="ç£ç›˜ä½¿ç”¨ç‡",
                value=disk.percent,
                type=PerformanceMetricType.EFFICIENCY,
                description="ç³»ç»Ÿç£ç›˜ä½¿ç”¨ç‡",
                benchmark=90.0  # 90%ä¸ºåŸºå‡†
            )

        except ImportError:
            print("psutilæœªå®‰è£…ï¼Œæ— æ³•è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        except Exception as e:
            print(f"è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")

        return metrics

    def evaluate_algorithm_performance(self,
                                       pattern_name: str,
                                       test_datasets: List[pd.DataFrame],
                                       ground_truth: Optional[List[List[Dict]]] = None) -> AlgorithmMetrics:
        """
        è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼ˆæ•´åˆä¼˜åŒ–æ¨¡å—åŠŸèƒ½ï¼‰

        Args:
            pattern_name: å½¢æ€åç§°
            test_datasets: æµ‹è¯•æ•°æ®é›†åˆ—è¡¨
            ground_truth: çœŸå®æ ‡ç­¾

        Returns:
            ç®—æ³•æ€§èƒ½æŒ‡æ ‡
        """
        if self.debug_mode:
            print(f"ğŸ” å¼€å§‹è¯„ä¼°ç®—æ³•: {pattern_name}")

        try:
            # å°è¯•å¯¼å…¥æ¨¡å¼ç®¡ç†å™¨
            from analysis.pattern_manager import PatternManager
            from analysis.pattern_base import PatternAlgorithmFactory

            manager = PatternManager()
            config = manager.get_pattern_by_name(pattern_name)
            if not config:
                raise ValueError(f"æœªæ‰¾åˆ°å½¢æ€é…ç½®: {pattern_name}")

            recognizer = PatternAlgorithmFactory.create(config)
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œè¿”å›é»˜è®¤æŒ‡æ ‡
            return AlgorithmMetrics()

        metrics = AlgorithmMetrics()
        all_results = []
        execution_times = []
        memory_usages = []
        cpu_usages = []

        # æ€§èƒ½æµ‹è¯•
        for i, dataset in enumerate(test_datasets):
            if self.debug_mode:
                print(f"  æµ‹è¯•æ•°æ®é›† {i+1}/{len(test_datasets)}")

            # å†…å­˜ç›‘æ§
            tracemalloc.start()

            # CPUç›‘æ§
            if PSUTIL_AVAILABLE:
                cpu_before = psutil.cpu_percent()

            # æ‰§è¡Œæ—¶é—´ç›‘æ§
            start_time = time.time()

            try:
                # æ‰§è¡Œå½¢æ€è¯†åˆ«
                results = recognizer.recognize(dataset)
                all_results.extend(results)
            except Exception as e:
                if self.debug_mode:
                    print(f"    è¯†åˆ«å¤±è´¥: {e}")
                continue

            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            execution_time = time.time() - start_time
            execution_times.append(execution_time)

            # å†…å­˜ä½¿ç”¨
            current, peak = tracemalloc.get_traced_memory()
            memory_usages.append(peak / 1024 / 1024)  # MB
            tracemalloc.stop()

            # CPUä½¿ç”¨
            if PSUTIL_AVAILABLE:
                cpu_after = psutil.cpu_percent()
                cpu_usages.append(cpu_after - cpu_before)

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if execution_times:
            metrics.execution_time = np.mean(execution_times)
        if memory_usages:
            metrics.memory_usage = np.mean(memory_usages)
        if cpu_usages:
            metrics.cpu_usage = np.mean(cpu_usages)

        # è®¡ç®—ä¸šåŠ¡æŒ‡æ ‡
        if all_results:
            metrics.patterns_found = len(all_results)
            confidences = [r.confidence for r in all_results if hasattr(r, 'confidence')]
            if confidences:
                metrics.confidence_avg = np.mean(confidences)
                metrics.confidence_std = np.std(confidences)
                metrics.signal_quality = self._calculate_signal_quality(all_results)

        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        if execution_times:
            metrics.robustness_score = 1.0 - min(1.0, np.std(execution_times) / np.mean(execution_times))

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        metrics.overall_score = self._calculate_algorithm_overall_score(metrics)

        if self.debug_mode:
            print(f"  âœ“ è¯„ä¼°å®Œæˆï¼Œç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")

        return metrics

    def _calculate_signal_quality(self, results) -> float:
        """è®¡ç®—ä¿¡å·è´¨é‡"""
        if not results:
            return 0.0

        # åŸºäºç½®ä¿¡åº¦åˆ†å¸ƒå’Œä¿¡å·ä¸€è‡´æ€§è®¡ç®—è´¨é‡
        confidences = [r.confidence for r in results if hasattr(r, 'confidence')]
        if not confidences:
            return 0.0

        # é«˜ç½®ä¿¡åº¦ç»“æœçš„æ¯”ä¾‹
        high_confidence_ratio = sum(1 for c in confidences if c > 0.7) / len(confidences)

        # ç½®ä¿¡åº¦çš„ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå¥½ï¼‰
        confidence_stability = 1.0 - min(1.0, np.std(confidences))

        # ä¿¡å·å¼ºåº¦ï¼ˆå¹³å‡ç½®ä¿¡åº¦ï¼‰
        signal_strength = np.mean(confidences)

        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality = (high_confidence_ratio * 0.4 +
                   confidence_stability * 0.3 +
                   signal_strength * 0.3)

        return quality

    def _calculate_algorithm_overall_score(self, metrics: AlgorithmMetrics) -> float:
        """è®¡ç®—ç®—æ³•ç»¼åˆè¯„åˆ†"""
        scores = []
        weights = []

        # ä¸šåŠ¡æŒ‡æ ‡æƒé‡æœ€é«˜
        if metrics.signal_quality > 0:
            scores.append(metrics.signal_quality)
            weights.append(0.3)

        if metrics.confidence_avg > 0:
            scores.append(metrics.confidence_avg)
            weights.append(0.2)

        # æ€§èƒ½æŒ‡æ ‡
        if metrics.execution_time > 0:
            # æ‰§è¡Œæ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¯„åˆ†
            time_score = max(0, min(1.0, 1.0 - metrics.execution_time / 10.0))
            scores.append(time_score)
            weights.append(0.15)

        # ç¨³å®šæ€§æŒ‡æ ‡
        if metrics.robustness_score > 0:
            scores.append(metrics.robustness_score)
            weights.append(0.15)

        if metrics.parameter_sensitivity > 0:
            scores.append(metrics.parameter_sensitivity)
            weights.append(0.1)

        # å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if metrics.f1_score > 0:
            scores.append(metrics.f1_score)
            weights.append(0.1)

        if not scores:
            return 0.5  # é»˜è®¤è¯„åˆ†

        # åŠ æƒå¹³å‡
        weighted_score = sum(s * w for s, w in zip(scores, weights)) / sum(weights)
        return max(0.0, min(1.0, weighted_score))

    def create_test_datasets(self, pattern_name: str, count: int = 3) -> List[pd.DataFrame]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        datasets = []

        for i in range(count):
            # ç”Ÿæˆæ¨¡æ‹ŸKçº¿æ•°æ®
            dates = pd.date_range(start='2023-01-01', periods=100, freq='D')

            # ç”Ÿæˆä»·æ ¼æ•°æ®
            np.random.seed(42 + i)  # ç¡®ä¿å¯é‡å¤æ€§
            returns = np.random.normal(0.001, 0.02, 100)
            prices = 100 * np.cumprod(1 + returns)

            # ç”ŸæˆOHLCVæ•°æ®
            opens = prices * (1 + np.random.normal(0, 0.005, 100))
            closes = prices
            highs = np.maximum(opens, closes) * (1 + np.random.uniform(0, 0.02, 100))
            lows = np.minimum(opens, closes) * (1 - np.random.uniform(0, 0.02, 100))
            volumes = np.random.lognormal(15, 1, 100)

            dataset = pd.DataFrame({
                'datetime': dates,
                'open': opens,
                'high': highs,
                'low': lows,
                'close': closes,
                'volume': volumes
            })

            datasets.append(dataset)

        return datasets

    def generate_performance_report(self,
                                    strategy_name: Optional[str] = None,
                                    pattern_name: Optional[str] = None,
                                    include_system: bool = True) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

        Args:
            strategy_name: ç­–ç•¥åç§°
            pattern_name: å½¢æ€åç§°
            include_system: æ˜¯å¦åŒ…å«ç³»ç»Ÿæ€§èƒ½

        Returns:
            å®Œæ•´æ€§èƒ½æŠ¥å‘Š
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'metrics': {},
            'summary': {},
            'recommendations': []
        }

        # ç­–ç•¥æ€§èƒ½è¯„ä¼°
        if strategy_name:
            # è¿™é‡Œéœ€è¦ä»æ•°æ®åº“è·å–ç­–ç•¥æ”¶ç›Šæ•°æ®
            # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            returns = pd.Series(np.random.normal(0.001, 0.02, 252))
            strategy_metrics = self.evaluate_strategy_performance(strategy_name, returns)
            report['metrics']['strategy'] = {k: self._metric_to_dict(v) for k, v in strategy_metrics.items()}

        # å½¢æ€æ€§èƒ½è¯„ä¼°
        if pattern_name:
            pattern_metrics = self.evaluate_pattern_performance(pattern_name)
            report['metrics']['pattern'] = {k: self._metric_to_dict(v) for k, v in pattern_metrics.items()}

        # ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
        if include_system:
            system_metrics = self.evaluate_system_performance()
            report['metrics']['system'] = {k: self._metric_to_dict(v) for k, v in system_metrics.items()}

        # ç”Ÿæˆæ€»ç»“å’Œå»ºè®®
        report['summary'] = self._generate_summary(report['metrics'])
        report['recommendations'] = self._generate_recommendations(report['metrics'])

        return report

    def _calculate_max_drawdown(self, returns: pd.Series) -> float:
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        cumulative = (1 + returns).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        return drawdown.min()

    def _metric_to_dict(self, metric: PerformanceMetric) -> Dict[str, Any]:
        """å°†æ€§èƒ½æŒ‡æ ‡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': metric.name,
            'value': metric.value,
            'type': metric.type.value,
            'description': metric.description,
            'benchmark': metric.benchmark,
            'score': metric.score
        }

    def _generate_summary(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
        summary = {
            'overall_score': 0.0,
            'strengths': [],
            'weaknesses': [],
            'key_metrics': {}
        }

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scores = []

        # ç­–ç•¥è¯„åˆ†
        if 'strategy' in metrics:
            strategy_score = self._calculate_strategy_score(metrics['strategy'])
            scores.append(strategy_score)
            summary['key_metrics']['strategy_score'] = strategy_score

        # å½¢æ€è¯„åˆ†
        if 'pattern' in metrics:
            pattern_score = self._calculate_pattern_score(metrics['pattern'])
            scores.append(pattern_score)
            summary['key_metrics']['pattern_score'] = pattern_score

        # ç³»ç»Ÿè¯„åˆ†
        if 'system' in metrics:
            system_score = self._calculate_system_score(metrics['system'])
            scores.append(system_score)
            summary['key_metrics']['system_score'] = system_score

        if scores:
            summary['overall_score'] = np.mean(scores)

        return summary

    def _generate_recommendations(self, metrics: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºæŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if 'strategy' in metrics:
            strategy_metrics = metrics['strategy']

            if 'sharpe_ratio' in strategy_metrics and strategy_metrics['sharpe_ratio']['value'] < 1.0:
                recommendations.append("å¤æ™®æ¯”ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–é£é™©ç®¡ç†ç­–ç•¥")

            if 'max_drawdown' in strategy_metrics and abs(strategy_metrics['max_drawdown']['value']) > 0.2:
                recommendations.append("æœ€å¤§å›æ’¤è¿‡å¤§ï¼Œå»ºè®®åŠ å¼ºæ­¢æŸæ§åˆ¶")

        if 'system' in metrics:
            system_metrics = metrics['system']

            if 'cpu_usage' in system_metrics and system_metrics['cpu_usage']['value'] > 80:
                recommendations.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æ€§èƒ½")

            if 'memory_usage' in system_metrics and system_metrics['memory_usage']['value'] > 80:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")

        return recommendations

    def _calculate_strategy_score(self, strategy_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ç­–ç•¥è¯„åˆ†"""
        score = 50.0  # åŸºç¡€åˆ†

        # å¤æ™®æ¯”ç‡è¯„åˆ†
        if 'sharpe_ratio' in strategy_metrics:
            sharpe = strategy_metrics['sharpe_ratio']['value']
            if sharpe > 2.0:
                score += 20
            elif sharpe > 1.0:
                score += 10
            elif sharpe > 0.5:
                score += 5

        # æœ€å¤§å›æ’¤è¯„åˆ†
        if 'max_drawdown' in strategy_metrics:
            drawdown = abs(strategy_metrics['max_drawdown']['value'])
            if drawdown < 0.05:
                score += 15
            elif drawdown < 0.1:
                score += 10
            elif drawdown < 0.2:
                score += 5

        # èƒœç‡è¯„åˆ†
        if 'win_rate' in strategy_metrics:
            win_rate = strategy_metrics['win_rate']['value']
            if win_rate > 0.6:
                score += 15
            elif win_rate > 0.5:
                score += 10
            elif win_rate > 0.4:
                score += 5

        return min(score, 100.0)

    def _calculate_pattern_score(self, pattern_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—å½¢æ€è¯„åˆ†"""
        score = 50.0  # åŸºç¡€åˆ†

        # æˆåŠŸç‡è¯„åˆ†
        if 'success_rate' in pattern_metrics:
            success_rate = pattern_metrics['success_rate']['value']
            if success_rate > 0.8:
                score += 25
            elif success_rate > 0.6:
                score += 15
            elif success_rate > 0.5:
                score += 10

        # å¹³å‡ç½®ä¿¡åº¦è¯„åˆ†
        if 'avg_confidence' in pattern_metrics:
            confidence = pattern_metrics['avg_confidence']['value']
            if confidence > 0.8:
                score += 15
            elif confidence > 0.6:
                score += 10
            elif confidence > 0.5:
                score += 5

        # å¹³å‡æ”¶ç›Šè¯„åˆ†
        if 'avg_return' in pattern_metrics:
            avg_return = pattern_metrics['avg_return']['value']
            if avg_return > 0.05:
                score += 10
            elif avg_return > 0.02:
                score += 5
            elif avg_return > 0:
                score += 2

        return min(score, 100.0)

    def _calculate_system_score(self, system_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ç³»ç»Ÿè¯„åˆ†"""
        score = 100.0  # æ»¡åˆ†å¼€å§‹

        # CPUä½¿ç”¨ç‡æ‰£åˆ†
        if 'cpu_usage' in system_metrics:
            cpu_usage = system_metrics['cpu_usage']['value']
            if cpu_usage > 90:
                score -= 30
            elif cpu_usage > 80:
                score -= 20
            elif cpu_usage > 70:
                score -= 10

        # å†…å­˜ä½¿ç”¨ç‡æ‰£åˆ†
        if 'memory_usage' in system_metrics:
            memory_usage = system_metrics['memory_usage']['value']
            if memory_usage > 90:
                score -= 25
            elif memory_usage > 80:
                score -= 15
            elif memory_usage > 70:
                score -= 8

        # ç£ç›˜ä½¿ç”¨ç‡æ‰£åˆ†
        if 'disk_usage' in system_metrics:
            disk_usage = system_metrics['disk_usage']['value']
            if disk_usage > 95:
                score -= 20
            elif disk_usage > 90:
                score -= 10
            elif disk_usage > 85:
                score -= 5

        return max(score, 0.0)


def create_performance_evaluator(db_path: str = 'db/hikyuu_system.db', debug_mode: bool = False) -> PerformanceEvaluator:
    """
    åˆ›å»ºæ€§èƒ½è¯„ä¼°å™¨å®ä¾‹

    Args:
        db_path: æ•°æ®åº“è·¯å¾„
        debug_mode: è°ƒè¯•æ¨¡å¼

    Returns:
        æ€§èƒ½è¯„ä¼°å™¨å®ä¾‹
    """
    return PerformanceEvaluator(db_path, debug_mode)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    evaluator = create_performance_evaluator()

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = evaluator.generate_performance_report(
        pattern_name="hammer",
        include_system=True
    )

    print("æ€§èƒ½è¯„ä¼°æŠ¥å‘Š:")
    print("=" * 50)
    print(f"æ€»ä½“è¯„åˆ†: {report['summary']['overall_score']:.1f}")
    print(f"ç”Ÿæˆæ—¶é—´: {report['timestamp']}")

    if report['recommendations']:
        print("\næ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"{i}. {rec}")
