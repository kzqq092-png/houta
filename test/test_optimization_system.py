#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å½¢æ€è¯†åˆ«ç®—æ³•ä¼˜åŒ–ç³»ç»Ÿå®Œæ•´æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ç»„ä»¶çš„åŠŸèƒ½å’Œé›†æˆæ•ˆæœ
"""

from analysis.pattern_manager import PatternManager
from optimization.main_controller import OptimizationController
from optimization.ui_integration import UIIntegration
from optimization.auto_tuner import AutoTuner
from optimization.algorithm_optimizer import AlgorithmOptimizer, OptimizationConfig
from optimization.version_manager import VersionManager
from optimization.performance_evaluator import PerformanceEvaluator
from optimization.database_schema import OptimizationDatabaseManager
import sys
import os
import time
import traceback
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ‰€æœ‰ä¼˜åŒ–ç³»ç»Ÿç»„ä»¶


class OptimizationSystemTester:
    """ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•å™¨"""

    def __init__(self):
        self.test_results = {}
        self.start_time = datetime.now()

        print("ğŸ§ª HiKyuu å½¢æ€è¯†åˆ«ç®—æ³•ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•")
        print("=" * 60)
        print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {self.start_time}")
        print()

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        tests = [
            ("æ•°æ®åº“æ¶æ„æµ‹è¯•", self.test_database_schema),
            ("æ€§èƒ½è¯„ä¼°å™¨æµ‹è¯•", self.test_performance_evaluator),
            ("ç‰ˆæœ¬ç®¡ç†å™¨æµ‹è¯•", self.test_version_manager),
            ("ç®—æ³•ä¼˜åŒ–å™¨æµ‹è¯•", self.test_algorithm_optimizer),
            ("è‡ªåŠ¨è°ƒä¼˜å™¨æµ‹è¯•", self.test_auto_tuner),
            ("UIé›†æˆæµ‹è¯•", self.test_ui_integration),
            ("ä¸»æ§åˆ¶å™¨æµ‹è¯•", self.test_main_controller),
            ("ç³»ç»Ÿé›†æˆæµ‹è¯•", self.test_system_integration)
        ]

        for test_name, test_func in tests:
            print(f"ğŸ” {test_name}")
            print("-" * 40)

            try:
                start_time = time.time()
                result = test_func()
                end_time = time.time()

                self.test_results[test_name] = {
                    "status": "PASS" if result else "FAIL",
                    "duration": end_time - start_time,
                    "details": result if isinstance(result, dict) else {}
                }

                status_icon = "âœ…" if result else "âŒ"
                print(f"{status_icon} {test_name}: {'é€šè¿‡' if result else 'å¤±è´¥'} "
                      f"({end_time - start_time:.3f}ç§’)")

            except Exception as e:
                self.test_results[test_name] = {
                    "status": "ERROR",
                    "duration": 0,
                    "error": str(e)
                }
                print(f"ğŸ’¥ {test_name}: é”™è¯¯ - {e}")
                if "--debug" in sys.argv:
                    traceback.print_exc()

            print()

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()

    def test_database_schema(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“æ¶æ„"""
        try:
            # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
            db_manager = OptimizationDatabaseManager()

            # æµ‹è¯•è¡¨åˆ›å»º
            db_manager.init_tables()
            print("  âœ“ æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")

            # æµ‹è¯•ç‰ˆæœ¬ä¿å­˜
            version_id = db_manager.save_algorithm_version(
                pattern_id=1,
                pattern_name="test_pattern",
                algorithm_code="# Test algorithm",
                parameters={"test_param": 1.0},
                description="æµ‹è¯•ç‰ˆæœ¬"
            )
            print(f"  âœ“ ç‰ˆæœ¬ä¿å­˜æˆåŠŸï¼ŒID: {version_id}")

            # æµ‹è¯•æ€§èƒ½æŒ‡æ ‡ä¿å­˜
            metrics = {
                "overall_score": 0.85,
                "signal_quality": 0.8,
                "confidence_avg": 0.75,
                "execution_time": 0.01
            }
            metric_id = db_manager.save_performance_metrics(version_id, "test_pattern", metrics)
            print(f"  âœ“ æ€§èƒ½æŒ‡æ ‡ä¿å­˜æˆåŠŸï¼ŒID: {metric_id}")

            # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯è·å–
            stats = db_manager.get_optimization_statistics()
            print(f"  âœ“ ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ: {len(stats)} é¡¹")

            return True

        except Exception as e:
            print(f"  âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_performance_evaluator(self) -> bool:
        """æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨"""
        try:
            evaluator = PerformanceEvaluator(debug_mode=True)

            # æµ‹è¯•æ•°æ®é›†åˆ›å»º
            datasets = evaluator.create_test_datasets("hammer", count=2)
            print(f"  âœ“ åˆ›å»ºæµ‹è¯•æ•°æ®é›†: {len(datasets)} ä¸ª")

            # æµ‹è¯•æ€§èƒ½è¯„ä¼°
            metrics = evaluator.evaluate_algorithm("hammer", datasets)
            print(f"  âœ“ æ€§èƒ½è¯„ä¼°å®Œæˆï¼Œç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")

            # éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§
            required_metrics = [
                'overall_score', 'signal_quality', 'confidence_avg',
                'execution_time', 'patterns_found', 'robustness_score'
            ]

            for metric in required_metrics:
                if not hasattr(metrics, metric):
                    raise ValueError(f"ç¼ºå°‘æŒ‡æ ‡: {metric}")

            print("  âœ“ æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡å®Œæ•´")
            return True

        except Exception as e:
            print(f"  âŒ æ€§èƒ½è¯„ä¼°å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_version_manager(self) -> bool:
        """æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨"""
        try:
            version_manager = VersionManager()

            # æµ‹è¯•ç‰ˆæœ¬ä¿å­˜
            version_id = version_manager.save_version(
                pattern_id=1,
                pattern_name="test_pattern_vm",
                algorithm_code="# Test version management",
                parameters={"vm_param": 2.0},
                description="ç‰ˆæœ¬ç®¡ç†æµ‹è¯•"
            )
            print(f"  âœ“ ç‰ˆæœ¬ä¿å­˜æˆåŠŸï¼ŒID: {version_id}")

            # æµ‹è¯•ç‰ˆæœ¬è·å–
            versions = version_manager.get_versions("test_pattern_vm")
            print(f"  âœ“ è·å–ç‰ˆæœ¬åˆ—è¡¨: {len(versions)} ä¸ªç‰ˆæœ¬")

            # æµ‹è¯•ç‰ˆæœ¬æ¿€æ´»
            if versions:
                success = version_manager.activate_version(versions[0].id)
                print(f"  âœ“ ç‰ˆæœ¬æ¿€æ´»: {'æˆåŠŸ' if success else 'å¤±è´¥'}")

            # æµ‹è¯•æœ€ä½³ç‰ˆæœ¬è·å–
            best_version = version_manager.get_best_version("test_pattern_vm")
            print(f"  âœ“ æœ€ä½³ç‰ˆæœ¬è·å–: {'æˆåŠŸ' if best_version else 'æ— ç‰ˆæœ¬'}")

            return True

        except Exception as e:
            print(f"  âŒ ç‰ˆæœ¬ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_algorithm_optimizer(self) -> bool:
        """æµ‹è¯•ç®—æ³•ä¼˜åŒ–å™¨"""
        try:
            optimizer = AlgorithmOptimizer(debug_mode=True)

            # åˆ›å»ºç®€å•çš„ä¼˜åŒ–é…ç½®
            config = OptimizationConfig(
                method="random",  # ä½¿ç”¨å¿«é€Ÿçš„éšæœºä¼˜åŒ–
                max_iterations=3,
                population_size=5,
                timeout_minutes=2
            )

            # æµ‹è¯•å•ä¸ªå½¢æ€ä¼˜åŒ–
            result = optimizer.optimize_algorithm("hammer", config)

            print(f"  âœ“ ä¼˜åŒ–å®Œæˆ")
            print(f"    åŸºå‡†è¯„åˆ†: {result.get('baseline_score', 0):.3f}")
            print(f"    æœ€ä½³è¯„åˆ†: {result.get('best_score', 0):.3f}")
            print(f"    æ€§èƒ½æå‡: {result.get('improvement_percentage', 0):.3f}%")
            print(f"    è¿­ä»£æ¬¡æ•°: {result.get('iterations', 0)}")

            # éªŒè¯ç»“æœå®Œæ•´æ€§
            required_keys = ['baseline_score', 'best_score', 'improvement_percentage', 'iterations']
            for key in required_keys:
                if key not in result:
                    raise ValueError(f"ç¼ºå°‘ç»“æœå­—æ®µ: {key}")

            return True

        except Exception as e:
            print(f"  âŒ ç®—æ³•ä¼˜åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_auto_tuner(self) -> bool:
        """æµ‹è¯•è‡ªåŠ¨è°ƒä¼˜å™¨"""
        try:
            auto_tuner = AutoTuner(max_workers=2, debug_mode=True)

            # æµ‹è¯•çŠ¶æ€è·å–
            status = auto_tuner.get_optimization_status()
            print(f"  âœ“ è·å–ä¼˜åŒ–çŠ¶æ€: {status['active_optimizations']} ä¸ªæ´»è·ƒä»»åŠ¡")

            # æµ‹è¯•ä¸€é”®ä¼˜åŒ–ï¼ˆä»…æµ‹è¯•å‡ ä¸ªå½¢æ€ï¼‰
            test_patterns = ["hammer", "doji"]
            result = auto_tuner.one_click_optimize(
                pattern_names=test_patterns,
                optimization_method="random",
                max_iterations=3
            )

            summary = result.get("summary", {})
            print(f"  âœ“ ä¸€é”®ä¼˜åŒ–å®Œæˆ")
            print(f"    æ€»ä»»åŠ¡æ•°: {summary.get('total_tasks', 0)}")
            print(f"    æˆåŠŸä»»åŠ¡æ•°: {summary.get('successful_tasks', 0)}")
            print(f"    å¹³å‡æ”¹è¿›: {summary.get('average_improvement', 0):.3f}%")

            return True

        except Exception as e:
            print(f"  âŒ è‡ªåŠ¨è°ƒä¼˜å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_ui_integration(self) -> bool:
        """æµ‹è¯•UIé›†æˆ"""
        try:
            ui_integration = UIIntegration(debug_mode=True)

            # æµ‹è¯•çŠ¶æ€è·å–
            status = ui_integration.get_optimization_status()
            print(f"  âœ“ UIçŠ¶æ€è·å–: {status['active_optimizations']} ä¸ªæ´»è·ƒä¼˜åŒ–")

            # æµ‹è¯•å³é”®èœå•åˆ›å»ºï¼ˆæ— GUIæ¨¡å¼ï¼‰
            menu = ui_integration.create_pattern_context_menu("hammer")
            print(f"  âœ“ å³é”®èœå•åˆ›å»º: {'æˆåŠŸ' if menu is not None else 'æ— GUIæ¨¡å¼'}")

            # æµ‹è¯•å¿«é€Ÿä¼˜åŒ–
            print("  â³ æµ‹è¯•å¿«é€Ÿä¼˜åŒ–...")
            ui_integration.quick_optimize("hammer")
            print("  âœ“ å¿«é€Ÿä¼˜åŒ–å¯åŠ¨æˆåŠŸ")

            # ç­‰å¾…ä¼˜åŒ–å®Œæˆ
            time.sleep(2)

            return True

        except Exception as e:
            print(f"  âŒ UIé›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_main_controller(self) -> bool:
        """æµ‹è¯•ä¸»æ§åˆ¶å™¨"""
        try:
            controller = OptimizationController(debug_mode=True)

            # æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–
            controller.initialize_system()
            print("  âœ“ ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

            # æµ‹è¯•å½¢æ€åˆ—è¡¨
            patterns = controller.pattern_manager.get_all_patterns()
            print(f"  âœ“ å½¢æ€åˆ—è¡¨è·å–: {len(patterns)} ä¸ªå½¢æ€")

            # æµ‹è¯•æ€§èƒ½è¯„ä¼°
            if patterns:
                test_pattern = patterns[0].english_name
                controller.evaluate_pattern(test_pattern, dataset_count=2)
                print(f"  âœ“ æ€§èƒ½è¯„ä¼°å®Œæˆ: {test_pattern}")

            return True

        except Exception as e:
            print(f"  âŒ ä¸»æ§åˆ¶å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_system_integration(self) -> bool:
        """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
        try:
            # æµ‹è¯•ç»„ä»¶é—´åä½œ
            pattern_manager = PatternManager()
            db_manager = OptimizationDatabaseManager()

            # è·å–å½¢æ€åˆ—è¡¨
            patterns = pattern_manager.get_all_patterns()
            print(f"  âœ“ è·å–å½¢æ€åˆ—è¡¨: {len(patterns)} ä¸ª")

            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            stats = db_manager.get_optimization_statistics()
            print(f"  âœ“ æ•°æ®åº“è¿æ¥æ­£å¸¸ï¼Œç»Ÿè®¡é¡¹: {len(stats)}")

            # æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹
            if patterns:
                test_pattern = patterns[0].english_name

                # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¯„ä¼°
                evaluator = PerformanceEvaluator(debug_mode=True)
                datasets = evaluator.create_test_datasets(test_pattern, count=1)
                metrics = evaluator.evaluate_algorithm(test_pattern, datasets)

                # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
                metric_id = db_manager.save_performance_metrics(
                    version_id=1,
                    pattern_name=test_pattern,
                    metrics={
                        "overall_score": metrics.overall_score,
                        "signal_quality": metrics.signal_quality,
                        "confidence_avg": metrics.confidence_avg,
                        "execution_time": metrics.execution_time,
                        "patterns_found": metrics.patterns_found
                    }
                )

                print(f"  âœ“ ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•å®Œæˆï¼ŒæŒ‡æ ‡ID: {metric_id}")

            return True

        except Exception as e:
            print(f"  âŒ ç³»ç»Ÿé›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False

    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        end_time = datetime.now()
        duration = end_time - self.start_time

        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {self.start_time}")
        print(f"æµ‹è¯•ç»“æŸæ—¶é—´: {end_time}")
        print(f"æ€»æµ‹è¯•æ—¶é—´: {duration}")
        print()

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r["status"] == "PASS")
        failed_tests = sum(1 for r in self.test_results.values() if r["status"] == "FAIL")
        error_tests = sum(1 for r in self.test_results.values() if r["status"] == "ERROR")

        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests} âœ…")
        print(f"å¤±è´¥æµ‹è¯•: {failed_tests} âŒ")
        print(f"é”™è¯¯æµ‹è¯•: {error_tests} ğŸ’¥")
        print(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        print()

        # è¯¦ç»†ç»“æœ
        print("è¯¦ç»†ç»“æœ:")
        print("-" * 40)
        for test_name, result in self.test_results.items():
            status_icon = {"PASS": "âœ…", "FAIL": "âŒ", "ERROR": "ğŸ’¥"}[result["status"]]
            duration = result.get("duration", 0)
            print(f"{status_icon} {test_name:<25} {result['status']:<6} ({duration:.3f}s)")

            if "error" in result:
                print(f"    é”™è¯¯: {result['error']}")

        print()

        # ç³»ç»Ÿè¯„ä¼°
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¼˜åŒ–ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
        elif passed_tests >= total_tests * 0.8:
            print("âš ï¸  å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œä½†éœ€è¦ä¿®å¤éƒ¨åˆ†é—®é¢˜ã€‚")
        else:
            print("âŒ å¤šä¸ªæµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"HiKyuu å½¢æ€è¯†åˆ«ç®—æ³•ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {end_time}\n")
                f.write(f"æµ‹è¯•æ—¶é•¿: {duration}\n\n")

                for test_name, result in self.test_results.items():
                    f.write(f"{test_name}: {result['status']}\n")
                    if "error" in result:
                        f.write(f"  é”™è¯¯: {result['error']}\n")
                    f.write(f"  è€—æ—¶: {result.get('duration', 0):.3f}ç§’\n\n")

            print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å½¢æ€è¯†åˆ«ç®—æ³•ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•")
    print()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if "--help" in sys.argv or "-h" in sys.argv:
        print("ç”¨æ³•: python test_optimization_system.py [--debug]")
        print("  --debug: å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return

    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
    tester = OptimizationSystemTester()
    tester.run_all_tests()


if __name__ == "__main__":
    main()
