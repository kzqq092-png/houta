"""
HIkyuuå›æµ‹ç³»ç»Ÿå…¨åŠŸèƒ½ç»¼åˆæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰å›æµ‹åŠŸèƒ½çš„æ­£ç¡®æ€§ã€æ€§èƒ½å’Œç¨³å®šæ€§
å¯¹æ ‡ä¸“ä¸šé‡åŒ–è½¯ä»¶æµ‹è¯•æ ‡å‡†
"""

import sys
import os
import time
import threading
import multiprocessing
import psutil
import traceback
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import unittest
from unittest.mock import Mock, patch
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
try:
    from backtest.unified_backtest_engine import UnifiedBacktestEngine, BacktestLevel
    from backtest.real_time_backtest_monitor import RealTimeBacktestMonitor, MonitoringLevel
    from backtest.ultra_performance_optimizer import UltraPerformanceOptimizer, PerformanceLevel
    from backtest.backtest_validator import ProfessionalBacktestValidator
    from backtest.professional_ui_system import ProfessionalUISystem, create_professional_ui
    BACKTEST_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: å›æµ‹æ¨¡å—å¯¼å…¥å¤±è´¥ - {e}")
    BACKTEST_MODULES_AVAILABLE = False

try:
    from gui.widgets.backtest_widget import ProfessionalBacktestWidget, create_backtest_widget
    from gui.backtest_ui_launcher import BacktestUILauncher, launch_streamlit_only, launch_pyqt5_only
    GUI_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: GUIæ¨¡å—å¯¼å…¥å¤±è´¥ - {e}")
    GUI_MODULES_AVAILABLE = False

try:
    from core.logger import LogManager
    from utils.config_manager import ConfigManager
    CORE_MODULES_AVAILABLE = True
except ImportError:
    # å¦‚æœæ ¸å¿ƒæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
    try:
        # å°è¯•å¯¼å…¥åŸºç¡€æ—¥å¿—ç®¡ç†å™¨
        from core.base_logger import BaseLogManager as LogManager
    except ImportError:
        class LogManager:
            def log(self, message, level):
                print(f"[{level}] {message}")

            def info(self, message):
                print(f"[INFO] {message}")

            def warning(self, message):
                print(f"[WARNING] {message}")

            def error(self, message):
                print(f"[ERROR] {message}")

    # ç®€åŒ–ç‰ˆé…ç½®ç®¡ç†å™¨
    class ConfigManager:
        def __init__(self):
            self.config = {
                'backtest': {
                    'initial_capital': 100000,
                    'commission_pct': 0.001,
                    'slippage_pct': 0.001
                },
                'risk': {
                    'max_position_size': 0.95,
                    'stop_loss_pct': 0.05
                }
            }

        def get(self, key, default=None):
            keys = key.split('.')
            value = self.config
            for k in keys:
                if isinstance(value, dict) and k in value:
                    value = value[k]
                else:
                    return default
            return value

    CORE_MODULES_AVAILABLE = False

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.start_memory = None
        self.end_memory = None
        self.start_cpu = None
        self.end_cpu = None
        self.process = psutil.Process()

    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.start_cpu = self.process.cpu_percent()

    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.end_time = time.time()
        self.end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.end_cpu = self.process.cpu_percent()

    def get_metrics(self) -> Dict:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            'execution_time': self.end_time - self.start_time if self.end_time else 0,
            'memory_usage': self.end_memory - self.start_memory if self.end_memory else 0,
            'peak_memory': self.end_memory if self.end_memory else 0,
            'cpu_usage': self.end_cpu if self.end_cpu else 0
        }


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_kline_data(days: int = 252, start_price: float = 100.0) -> pd.DataFrame:
        """ç”ŸæˆKçº¿æ•°æ®"""
        dates = pd.date_range(start='2023-01-01', periods=days, freq='D')

        # ç”Ÿæˆä»·æ ¼æ•°æ®
        np.random.seed(42)
        returns = np.random.normal(0.0005, 0.02, days)
        prices = start_price * np.cumprod(1 + returns)

        # ç”ŸæˆOHLCVæ•°æ®
        high_factor = np.random.uniform(1.0, 1.05, days)
        low_factor = np.random.uniform(0.95, 1.0, days)
        volume = np.random.uniform(1000000, 10000000, days)

        kline_data = pd.DataFrame({
            'open': prices * np.random.uniform(0.98, 1.02, days),
            'high': prices * high_factor,
            'low': prices * low_factor,
            'close': prices,
            'volume': volume,
            'amount': volume * prices
        }, index=dates)

        return kline_data

    @staticmethod
    def generate_signal_data(kline_data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·æ•°æ®"""
        signals = np.random.choice([-1, 0, 1], len(kline_data), p=[0.1, 0.8, 0.1])
        signal_data = kline_data.copy()
        signal_data['signal'] = signals
        return signal_data


class BacktestEngineTest(unittest.TestCase):
    """å›æµ‹å¼•æ“æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.performance_monitor = PerformanceMonitor()
        self.test_data = TestDataGenerator.generate_kline_data(252)

    def test_engine_initialization(self):
        """æµ‹è¯•å¼•æ“åˆå§‹åŒ–"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å›æµ‹å¼•æ“åˆå§‹åŒ– ===")

        for level in [BacktestLevel.RETAIL,
                      BacktestLevel.INSTITUTIONAL,
                      BacktestLevel.HEDGE_FUND,
                      BacktestLevel.INVESTMENT_BANK]:

            with self.subTest(level=level):
                self.performance_monitor.start()

                engine = UnifiedBacktestEngine(
                    backtest_level=level,
                    log_manager=self.log_manager
                )

                self.performance_monitor.stop()
                metrics = self.performance_monitor.get_metrics()

                self.assertIsNotNone(engine)
                self.assertEqual(engine.backtest_level, level)

                print(f"  âœ… {level.value} çº§åˆ«åˆå§‹åŒ–æˆåŠŸ")
                print(f"     æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
                print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")

    def test_backtest_execution(self):
        """æµ‹è¯•å›æµ‹æ‰§è¡Œ"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å›æµ‹æ‰§è¡Œ ===")

        engine = UnifiedBacktestEngine(
            backtest_level=BacktestLevel.INVESTMENT_BANK,
            log_manager=self.log_manager
        )

        # å‡†å¤‡å›æµ‹å‚æ•°
        backtest_params = {
            'initial_capital': 1000000,
            'position_size': 0.95,
            'commission_pct': 0.0003,
            'slippage_pct': 0.0001
        }

        signal_data = TestDataGenerator.generate_signal_data(self.test_data)

        self.performance_monitor.start()

        # æ‰§è¡Œå›æµ‹
        result = engine.run_backtest(signal_data, **backtest_params)

        self.performance_monitor.stop()
        metrics = self.performance_monitor.get_metrics()

        # éªŒè¯ç»“æœ
        self.assertIsNotNone(result)
        self.assertIn('backtest_result', result)
        self.assertIn('risk_metrics', result)
        self.assertIn('performance_metrics', result)

        print(f"  âœ… å›æµ‹æ‰§è¡ŒæˆåŠŸ")
        print(f"     æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
        print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")
        print(f"     æ•°æ®ç‚¹æ•°: {len(signal_data)}")

        # éªŒè¯æ€§èƒ½è¦æ±‚
        self.assertLess(metrics['execution_time'], 10.0, "æ‰§è¡Œæ—¶é—´åº”å°äº10ç§’")
        self.assertLess(metrics['memory_usage'], 500.0, "å†…å­˜ä½¿ç”¨åº”å°äº500MB")

    def test_risk_metrics_calculation(self):
        """æµ‹è¯•é£é™©æŒ‡æ ‡è®¡ç®—"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•é£é™©æŒ‡æ ‡è®¡ç®— ===")

        engine = UnifiedBacktestEngine(
            backtest_level=BacktestLevel.INVESTMENT_BANK,
            log_manager=self.log_manager
        )

        signal_data = TestDataGenerator.generate_signal_data(self.test_data)

        result = engine.run_backtest(signal_data, initial_capital=1000000)
        risk_metrics = result['risk_metrics']

        # éªŒè¯å¿…è¦çš„é£é™©æŒ‡æ ‡
        required_metrics = [
            'total_return', 'annualized_return', 'volatility', 'sharpe_ratio',
            'max_drawdown', 'win_rate', 'profit_factor', 'var_95'
        ]

        for metric in required_metrics:
            with self.subTest(metric=metric):
                self.assertTrue(hasattr(risk_metrics, metric), f"ç¼ºå°‘é£é™©æŒ‡æ ‡: {metric}")
                value = getattr(risk_metrics, metric)
                self.assertIsInstance(value, (int, float), f"{metric} åº”ä¸ºæ•°å€¼ç±»å‹")
                print(f"  âœ… {metric}: {value:.4f}")


class MonitorTest(unittest.TestCase):
    """ç›‘æ§ç³»ç»Ÿæµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.performance_monitor = PerformanceMonitor()

    def test_monitor_initialization(self):
        """æµ‹è¯•ç›‘æ§å™¨åˆå§‹åŒ–"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•ç›‘æ§å™¨åˆå§‹åŒ– ===")

        for level in [MonitoringLevel.BASIC, MonitoringLevel.STANDARD,
                      MonitoringLevel.ADVANCED, MonitoringLevel.REAL_TIME]:

            with self.subTest(level=level):
                monitor = RealTimeBacktestMonitor(
                    monitoring_level=level,
                    log_manager=self.log_manager
                )

                self.assertIsNotNone(monitor)
                self.assertEqual(monitor.monitoring_level, level)
                print(f"  âœ… {level.value} çº§åˆ«ç›‘æ§å™¨åˆå§‹åŒ–æˆåŠŸ")

    def test_real_time_monitoring(self):
        """æµ‹è¯•å®æ—¶ç›‘æ§"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å®æ—¶ç›‘æ§ ===")

        monitor = RealTimeBacktestMonitor(
            monitoring_level=MonitoringLevel.REAL_TIME,
            log_manager=self.log_manager
        )

        # å¯åŠ¨ç›‘æ§
        monitor.start_monitoring()

        # æ¨¡æ‹Ÿç›‘æ§æ•°æ®
        for i in range(10):
            mock_data = {
                'timestamp': datetime.now(),
                'current_return': np.random.normal(0, 0.02),
                'cumulative_return': np.random.uniform(-0.1, 0.3),
                'current_drawdown': np.random.uniform(0, 0.1),
                'sharpe_ratio': np.random.uniform(-0.5, 2.5)
            }
            monitor.update_metrics(mock_data)
            time.sleep(0.1)

        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()

        # éªŒè¯ç›‘æ§æ•°æ®
        monitoring_data = monitor.get_monitoring_data()
        self.assertGreater(len(monitoring_data), 0)

        print(f"  âœ… å®æ—¶ç›‘æ§æµ‹è¯•å®Œæˆï¼Œæ”¶é›†äº† {len(monitoring_data)} ä¸ªæ•°æ®ç‚¹")


class OptimizerTest(unittest.TestCase):
    """ä¼˜åŒ–å™¨æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.performance_monitor = PerformanceMonitor()

    def test_optimizer_initialization(self):
        """æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ–"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ– ===")

        for level in [PerformanceLevel.STANDARD, PerformanceLevel.HIGH,
                      PerformanceLevel.ULTRA, PerformanceLevel.EXTREME]:

            with self.subTest(level=level):
                optimizer = UltraPerformanceOptimizer(
                    performance_level=level,
                    log_manager=self.log_manager
                )

                self.assertIsNotNone(optimizer)
                self.assertEqual(optimizer.performance_level, level)
                print(f"  âœ… {level.value} çº§åˆ«ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")

    def test_performance_optimization(self):
        """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•æ€§èƒ½ä¼˜åŒ– ===")

        optimizer = UltraPerformanceOptimizer(
            performance_level=PerformanceLevel.ULTRA,
            log_manager=self.log_manager
        )

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = np.random.randn(10000, 100)

        self.performance_monitor.start()

        # æ‰§è¡Œä¼˜åŒ–è®¡ç®—
        optimized_result = optimizer.optimize_calculation(test_data)

        self.performance_monitor.stop()
        metrics = self.performance_monitor.get_metrics()

        self.assertIsNotNone(optimized_result)

        print(f"  âœ… æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
        print(f"     æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
        print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")
        print(f"     æ•°æ®è§„æ¨¡: {test_data.shape}")


class ValidatorTest(unittest.TestCase):
    """éªŒè¯å™¨æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.test_data = TestDataGenerator.generate_kline_data(252)

    def test_validator_initialization(self):
        """æµ‹è¯•éªŒè¯å™¨åˆå§‹åŒ–"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•éªŒè¯å™¨åˆå§‹åŒ– ===")

        validator = ProfessionalBacktestValidator(self.log_manager)

        self.assertIsNotNone(validator)
        print("  âœ… éªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸ")

    def test_data_validation(self):
        """æµ‹è¯•æ•°æ®éªŒè¯"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•æ•°æ®éªŒè¯ ===")

        validator = ProfessionalBacktestValidator(self.log_manager)

        # éªŒè¯æ­£å¸¸æ•°æ®
        validation_result = validator.validate_kline_data(self.test_data)

        self.assertIsNotNone(validation_result)
        self.assertIn('is_valid', validation_result)
        self.assertIn('quality_score', validation_result)
        self.assertIn('issues', validation_result)

        print(f"  âœ… æ•°æ®éªŒè¯å®Œæˆ")
        print(f"     éªŒè¯ç»“æœ: {'é€šè¿‡' if validation_result['is_valid'] else 'å¤±è´¥'}")
        print(f"     è´¨é‡è¯„åˆ†: {validation_result['quality_score']:.2f}")
        print(f"     é—®é¢˜æ•°é‡: {len(validation_result['issues'])}")


class UISystemTest(unittest.TestCase):
    """UIç³»ç»Ÿæµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()

    def test_streamlit_ui_creation(self):
        """æµ‹è¯•Streamlit UIåˆ›å»º"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•Streamlit UIåˆ›å»º ===")

        try:
            ui_system = create_professional_ui("dark")
            self.assertIsNotNone(ui_system)
            print("  âœ… Streamlit UIç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ Streamlit UIåˆ›å»ºå¤±è´¥: {e}")

    def test_pyqt5_widget_creation(self):
        """æµ‹è¯•PyQt5ç»„ä»¶åˆ›å»º"""
        if not GUI_MODULES_AVAILABLE:
            self.skipTest("GUIæ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•PyQt5ç»„ä»¶åˆ›å»º ===")

        try:
            # æ¨¡æ‹ŸQApplicationç¯å¢ƒ
            from PyQt5.QtWidgets import QApplication

            if not QApplication.instance():
                app = QApplication(sys.argv)

            widget = create_backtest_widget()
            self.assertIsNotNone(widget)
            print("  âœ… PyQt5å›æµ‹ç»„ä»¶åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            print(f"  âš ï¸ PyQt5ç»„ä»¶åˆ›å»ºå¤±è´¥: {e}")


class IntegrationTest(unittest.TestCase):
    """é›†æˆæµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.performance_monitor = PerformanceMonitor()

    def test_full_backtest_workflow(self):
        """æµ‹è¯•å®Œæ•´å›æµ‹å·¥ä½œæµ"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å®Œæ•´å›æµ‹å·¥ä½œæµ ===")

        self.performance_monitor.start()

        # 1. æ•°æ®å‡†å¤‡
        test_data = TestDataGenerator.generate_kline_data(252)
        signal_data = TestDataGenerator.generate_signal_data(test_data)
        print("  âœ… æ­¥éª¤1: æ•°æ®å‡†å¤‡å®Œæˆ")

        # 2. æ•°æ®éªŒè¯
        validator = ProfessionalBacktestValidator(self.log_manager)
        validation_result = validator.validate_kline_data(test_data)
        self.assertTrue(validation_result['is_valid'])
        print("  âœ… æ­¥éª¤2: æ•°æ®éªŒè¯é€šè¿‡")

        # 3. å›æµ‹æ‰§è¡Œ
        engine = UnifiedBacktestEngine(
            backtest_level=BacktestLevel.INVESTMENT_BANK,
            log_manager=self.log_manager
        )
        backtest_result = engine.run_backtest(signal_data, initial_capital=1000000)
        self.assertIsNotNone(backtest_result)
        print("  âœ… æ­¥éª¤3: å›æµ‹æ‰§è¡Œå®Œæˆ")

        # 4. å®æ—¶ç›‘æ§
        monitor = RealTimeBacktestMonitor(
            monitoring_level=MonitoringLevel.REAL_TIME,
            log_manager=self.log_manager
        )
        monitor.start_monitoring()

        # æ¨¡æ‹Ÿç›‘æ§æ•°æ®
        for i in range(5):
            mock_data = {
                'timestamp': datetime.now(),
                'current_return': np.random.normal(0, 0.02),
                'cumulative_return': np.random.uniform(-0.1, 0.3)
            }
            monitor.update_metrics(mock_data)
            time.sleep(0.1)

        monitor.stop_monitoring()
        monitoring_data = monitor.get_monitoring_data()
        self.assertGreater(len(monitoring_data), 0)
        print("  âœ… æ­¥éª¤4: å®æ—¶ç›‘æ§å®Œæˆ")

        # 5. æ€§èƒ½ä¼˜åŒ–
        optimizer = UltraPerformanceOptimizer(
            performance_level=PerformanceLevel.ULTRA,
            log_manager=self.log_manager
        )
        optimization_result = optimizer.optimize_calculation(test_data.values)
        self.assertIsNotNone(optimization_result)
        print("  âœ… æ­¥éª¤5: æ€§èƒ½ä¼˜åŒ–å®Œæˆ")

        self.performance_monitor.stop()
        metrics = self.performance_monitor.get_metrics()

        print(f"\n  ğŸ“Š å®Œæ•´å·¥ä½œæµæ€§èƒ½æŒ‡æ ‡:")
        print(f"     æ€»æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
        print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")
        print(f"     å³°å€¼å†…å­˜: {metrics['peak_memory']:.2f}MB")

        # éªŒè¯æ€§èƒ½è¦æ±‚
        self.assertLess(metrics['execution_time'], 30.0, "å®Œæ•´å·¥ä½œæµåº”åœ¨30ç§’å†…å®Œæˆ")
        self.assertLess(metrics['peak_memory'], 1000.0, "å³°å€¼å†…å­˜ä½¿ç”¨åº”å°äº1GB")


class StressTest(unittest.TestCase):
    """å‹åŠ›æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.log_manager = LogManager()
        self.performance_monitor = PerformanceMonitor()

    def test_large_dataset_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½ ===")

        # ç”Ÿæˆå¤§æ•°æ®é›†ï¼ˆ5å¹´æ—¥çº¿æ•°æ®ï¼‰
        large_dataset = TestDataGenerator.generate_kline_data(1260)  # 5å¹´
        signal_data = TestDataGenerator.generate_signal_data(large_dataset)

        print(f"  æ•°æ®è§„æ¨¡: {len(large_dataset)} ä¸ªäº¤æ˜“æ—¥")

        self.performance_monitor.start()

        engine = UnifiedBacktestEngine(
            backtest_level=BacktestLevel.INVESTMENT_BANK,
            log_manager=self.log_manager
        )

        result = engine.run_backtest(signal_data, initial_capital=1000000)

        self.performance_monitor.stop()
        metrics = self.performance_monitor.get_metrics()

        self.assertIsNotNone(result)

        print(f"  âœ… å¤§æ•°æ®é›†å›æµ‹å®Œæˆ")
        print(f"     æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
        print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")
        print(f"     å¤„ç†é€Ÿåº¦: {len(large_dataset)/metrics['execution_time']:.0f} æ¡/ç§’")

        # æ€§èƒ½è¦æ±‚
        self.assertLess(metrics['execution_time'], 60.0, "å¤§æ•°æ®é›†å›æµ‹åº”åœ¨60ç§’å†…å®Œæˆ")
        self.assertGreater(len(large_dataset)/metrics['execution_time'], 20, "å¤„ç†é€Ÿåº¦åº”å¤§äº20æ¡/ç§’")

    def test_concurrent_backtests(self):
        """æµ‹è¯•å¹¶å‘å›æµ‹"""
        if not BACKTEST_MODULES_AVAILABLE:
            self.skipTest("å›æµ‹æ¨¡å—ä¸å¯ç”¨")

        print("\n=== æµ‹è¯•å¹¶å‘å›æµ‹ ===")

        def run_single_backtest(test_id):
            """è¿è¡Œå•ä¸ªå›æµ‹"""
            try:
                test_data = TestDataGenerator.generate_kline_data(252)
                signal_data = TestDataGenerator.generate_signal_data(test_data)

                engine = UnifiedBacktestEngine(
                    backtest_level=BacktestLevel.INSTITUTIONAL,
                    log_manager=self.log_manager
                )

                result = engine.run_backtest(signal_data, initial_capital=1000000)
                return test_id, True, result

            except Exception as e:
                return test_id, False, str(e)

        self.performance_monitor.start()

        # å¯åŠ¨å¤šä¸ªå¹¶å‘å›æµ‹
        num_concurrent = 4
        with multiprocessing.Pool(num_concurrent) as pool:
            results = pool.map(run_single_backtest, range(num_concurrent))

        self.performance_monitor.stop()
        metrics = self.performance_monitor.get_metrics()

        # éªŒè¯ç»“æœ
        successful_tests = sum(1 for _, success, _ in results if success)

        print(f"  âœ… å¹¶å‘å›æµ‹å®Œæˆ")
        print(f"     å¹¶å‘æ•°é‡: {num_concurrent}")
        print(f"     æˆåŠŸæ•°é‡: {successful_tests}")
        print(f"     æ€»æ‰§è¡Œæ—¶é—´: {metrics['execution_time']:.3f}ç§’")
        print(f"     å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.2f}MB")

        self.assertEqual(successful_tests, num_concurrent, "æ‰€æœ‰å¹¶å‘å›æµ‹éƒ½åº”æˆåŠŸ")


class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.end_time = None

    def start_testing(self):
        """å¼€å§‹æµ‹è¯•"""
        self.start_time = datetime.now()
        print("=" * 80)
        print("ğŸš€ HIkyuuå›æµ‹ç³»ç»Ÿå…¨åŠŸèƒ½ç»¼åˆæµ‹è¯•å¼€å§‹")
        print(f"â° å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

    def end_testing(self):
        """ç»“æŸæµ‹è¯•"""
        self.end_time = datetime.now()
        duration = self.end_time - self.start_time

        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯•å®Œæˆ - ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        self.generate_report(duration)

    def add_test_result(self, test_name: str, success: bool, details: Dict = None):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            'test_name': test_name,
            'success': success,
            'details': details or {},
            'timestamp': datetime.now()
        })

    def generate_report(self, duration: timedelta):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results if result['success'])
        failed_tests = total_tests - successful_tests
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        print(f"\nğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæ•°é‡: {successful_tests}")
        print(f"   å¤±è´¥æ•°é‡: {failed_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   æ€»è€—æ—¶: {duration.total_seconds():.2f}ç§’")

        print(f"\nğŸ” è¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
            print(f"   {status} {result['test_name']}")

            if result['details']:
                for key, value in result['details'].items():
                    print(f"      {key}: {value}")

        # ç³»ç»Ÿä¿¡æ¯
        print(f"\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
        print(f"   Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
        print(f"   æ“ä½œç³»ç»Ÿ: {os.name}")
        print(f"   CPUæ ¸å¿ƒæ•°: {multiprocessing.cpu_count()}")
        print(f"   å†…å­˜æ€»é‡: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")

        # æ¨¡å—å¯ç”¨æ€§
        print(f"\nğŸ“¦ æ¨¡å—å¯ç”¨æ€§:")
        print(f"   å›æµ‹æ¨¡å—: {'âœ… å¯ç”¨' if BACKTEST_MODULES_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        print(f"   GUIæ¨¡å—: {'âœ… å¯ç”¨' if GUI_MODULES_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        print(f"   æ ¸å¿ƒæ¨¡å—: {'âœ… å¯ç”¨' if CORE_MODULES_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")

        # æ€§èƒ½è¯„çº§
        performance_grade = self.calculate_performance_grade(duration.total_seconds(), success_rate)
        print(f"\nğŸ† æ€§èƒ½è¯„çº§: {performance_grade}")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self.save_report_to_file(duration, success_rate, performance_grade)

    def calculate_performance_grade(self, duration: float, success_rate: float) -> str:
        """è®¡ç®—æ€§èƒ½è¯„çº§"""
        if success_rate >= 95 and duration <= 60:
            return "A+ (ä¼˜ç§€)"
        elif success_rate >= 90 and duration <= 120:
            return "A (è‰¯å¥½)"
        elif success_rate >= 80 and duration <= 180:
            return "B (ä¸€èˆ¬)"
        elif success_rate >= 70:
            return "C (éœ€è¦æ”¹è¿›)"
        else:
            return "D (ä¸åˆæ ¼)"

    def save_report_to_file(self, duration: timedelta, success_rate: float, grade: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            report_data = {
                'test_summary': {
                    'start_time': self.start_time.isoformat(),
                    'end_time': self.end_time.isoformat(),
                    'duration_seconds': duration.total_seconds(),
                    'total_tests': len(self.test_results),
                    'successful_tests': sum(1 for r in self.test_results if r['success']),
                    'success_rate': success_rate,
                    'performance_grade': grade
                },
                'test_results': self.test_results,
                'system_info': {
                    'python_version': sys.version.split()[0],
                    'os_name': os.name,
                    'cpu_count': multiprocessing.cpu_count(),
                    'memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024
                },
                'module_availability': {
                    'backtest_modules': BACKTEST_MODULES_AVAILABLE,
                    'gui_modules': GUI_MODULES_AVAILABLE,
                    'core_modules': CORE_MODULES_AVAILABLE
                }
            }

            report_file = f"backtest_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)

            print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        except Exception as e:
            print(f"\nâš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def run_comprehensive_tests():
    """è¿è¡Œå…¨é¢æµ‹è¯•"""
    report_generator = TestReportGenerator()
    report_generator.start_testing()

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()

    # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
    test_classes = [
        BacktestEngineTest,
        MonitorTest,
        OptimizerTest,
        ValidatorTest,
        UISystemTest,
        IntegrationTest,
        StressTest
    ]

    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)

    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(test_suite)

    # è®°å½•æµ‹è¯•ç»“æœ
    for test, error in result.failures + result.errors:
        test_name = f"{test.__class__.__name__}.{test._testMethodName}"
        report_generator.add_test_result(test_name, False, {'error': str(error)})

    # è®°å½•æˆåŠŸçš„æµ‹è¯•
    successful_count = result.testsRun - len(result.failures) - len(result.errors)
    for i in range(successful_count):
        report_generator.add_test_result(f"Test_{i+1}", True)

    report_generator.end_testing()

    return result


def main():
    """ä¸»å‡½æ•°"""
    print("HIkyuuå›æµ‹ç³»ç»Ÿå…¨åŠŸèƒ½ç»¼åˆæµ‹è¯•")
    print("=" * 50)

    # æ£€æŸ¥ä¾èµ–
    missing_deps = []

    try:
        import pandas
        import numpy
    except ImportError as e:
        missing_deps.append(str(e))

    if missing_deps:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–:")
        for dep in missing_deps:
            print(f"   {dep}")
        print("\nè¯·å®‰è£…ç¼ºå°‘çš„ä¾èµ–åé‡æ–°è¿è¡Œæµ‹è¯•")
        return

    # è¿è¡Œæµ‹è¯•
    try:
        result = run_comprehensive_tests()

        # è¿”å›é€€å‡ºç 
        if result.wasSuccessful():
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            sys.exit(0)
        else:
            print(f"\nâš ï¸ æœ‰ {len(result.failures + result.errors)} ä¸ªæµ‹è¯•å¤±è´¥")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
