#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤§è§„æ¨¡æ•°æ®å¤„ç†å‹åŠ›æµ‹è¯•

æ‰§è¡Œå¤§è§„æ¨¡æ•°æ®å¤„ç†çš„å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½éªŒè¯ï¼ŒéªŒè¯ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚
æµ‹è¯•åœºæ™¯åŒ…æ‹¬ï¼š
1. å¤§æ•°æ®é‡å¯¼å…¥æµ‹è¯•
2. é«˜å¹¶å‘ä»»åŠ¡æ‰§è¡Œæµ‹è¯•
3. é•¿æ—¶é—´è¿è¡Œå‹åŠ›æµ‹è¯•
4. å†…å­˜å’ŒCPUå‹åŠ›æµ‹è¯•
5. æ•°æ®åº“æ€§èƒ½å‹åŠ›æµ‹è¯•
"""

import pytest
import unittest
import tempfile
import os
import time
import threading
import multiprocessing
import psutil
import gc
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from unittest.mock import patch, Mock
import sqlite3
import json
from pathlib import Path

# å¯¼å…¥å¾…æµ‹è¯•çš„ç»„ä»¶
from core.importdata.intelligent_config_manager import (
    IntelligentConfigManager, ImportTaskConfig, DataFrequency, ImportMode
)
from core.ai.data_anomaly_detector import (
    DataAnomalyDetector, AnomalyDetectionConfig
)
from core.ui_integration.smart_data_integration import (
    SmartDataIntegration, UIIntegrationConfig
)


class LargeScalePerformanceTest(unittest.TestCase):
    """å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•åŸºç±»"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.test_start_time = time.time()
        cls.process = psutil.Process()
        cls.initial_memory = cls.process.memory_info().rss / 1024 / 1024  # MB
        cls.initial_cpu_percent = cls.process.cpu_percent()
        
        print(f"\n{'='*80}")
        print(f"å¼€å§‹å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•")
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {cls.initial_memory:.1f} MB")
        print(f"æµ‹è¯•è¿›ç¨‹PID: {os.getpid()}")
        print(f"CPUæ ¸å¿ƒæ•°: {multiprocessing.cpu_count()}")
        print(f"{'='*80}")

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        final_memory = cls.process.memory_info().rss / 1024 / 1024  # MB
        final_cpu_percent = cls.process.cpu_percent()
        test_duration = time.time() - cls.test_start_time
        
        print(f"\n{'='*80}")
        print(f"å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"æµ‹è¯•æ€»è€—æ—¶: {test_duration:.2f} ç§’")
        print(f"æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.1f} MB")
        print(f"å†…å­˜å¢é•¿: {final_memory - cls.initial_memory:.1f} MB")
        print(f"å¹³å‡CPUä½¿ç”¨ç‡: {final_cpu_percent:.1f}%")
        print(f"{'='*80}")

    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„å‡†å¤‡"""
        self.temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
        self.temp_db.close()
        self.db_path = self.temp_db.name
        
        # è®°å½•æµ‹è¯•å¼€å§‹æ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µ
        self.test_start_memory = self.process.memory_info().rss / 1024 / 1024
        self.test_start_time = time.time()

    def tearDown(self):
        """æ¯ä¸ªæµ‹è¯•åçš„æ¸…ç†"""
        try:
            os.unlink(self.db_path)
        except:
            pass
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è®°å½•æµ‹è¯•ç»“æŸæ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µ
        test_end_memory = self.process.memory_info().rss / 1024 / 1024
        test_duration = time.time() - self.test_start_time
        memory_increase = test_end_memory - self.test_start_memory
        
        print(f"  æµ‹è¯•è€—æ—¶: {test_duration:.2f}s, å†…å­˜å¢é•¿: {memory_increase:.1f}MB")


class TestLargeDatasetProcessing(LargeScalePerformanceTest):
    """å¤§æ•°æ®é›†å¤„ç†æµ‹è¯•"""

    def test_massive_task_creation(self):
        """æµ‹è¯•å¤§é‡ä»»åŠ¡åˆ›å»ºæ€§èƒ½"""
        print("\n--- æµ‹è¯•å¤§é‡ä»»åŠ¡åˆ›å»ºæ€§èƒ½ ---")
        
        manager = IntelligentConfigManager(self.db_path)
        
        # åˆ›å»º1000ä¸ªä»»åŠ¡
        num_tasks = 1000
        tasks_created = 0
        failed_tasks = 0
        
        start_time = time.time()
        
        for i in range(num_tasks):
            try:
                config = ImportTaskConfig(
                    task_id=f"massive_test_{i:04d}",
                    name=f"å¤§è§„æ¨¡æµ‹è¯•ä»»åŠ¡{i}",
                    data_source="tongdaxin" if i % 2 == 0 else "akshare",
                    asset_type="stock",
                    data_type="kline",
                    symbols=[f"{j:06d}" for j in range(i % 10, (i % 10) + 5)],
                    frequency=DataFrequency.DAILY,
                    mode=ImportMode.BATCH,
                    max_workers=2 + (i % 4),
                    batch_size=500 + (i * 10)
                )
                
                success = manager.add_import_task(config)
                if success:
                    tasks_created += 1
                else:
                    failed_tasks += 1
                    
            except Exception as e:
                failed_tasks += 1
                if failed_tasks <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                    print(f"ä»»åŠ¡åˆ›å»ºå¤±è´¥ {i}: {e}")
        
        creation_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        all_tasks = manager.get_all_import_tasks()
        
        print(f"ä»»åŠ¡åˆ›å»ºå®Œæˆ:")
        print(f"  ç›®æ ‡ä»»åŠ¡æ•°: {num_tasks}")
        print(f"  æˆåŠŸåˆ›å»º: {tasks_created}")
        print(f"  åˆ›å»ºå¤±è´¥: {failed_tasks}")
        print(f"  å®é™…å­˜å‚¨: {len(all_tasks)}")
        print(f"  æ€»è€—æ—¶: {creation_time:.2f}ç§’")
        print(f"  å¹³å‡åˆ›å»ºé€Ÿåº¦: {tasks_created/creation_time:.1f}ä»»åŠ¡/ç§’")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(tasks_created, num_tasks * 0.95)  # è‡³å°‘95%æˆåŠŸ
        self.assertLess(creation_time, 60)  # 1åˆ†é’Ÿå†…å®Œæˆ
        self.assertGreater(tasks_created/creation_time, 10)  # è‡³å°‘10ä»»åŠ¡/ç§’

    def test_massive_performance_data_recording(self):
        """æµ‹è¯•å¤§é‡æ€§èƒ½æ•°æ®è®°å½•"""
        print("\n--- æµ‹è¯•å¤§é‡æ€§èƒ½æ•°æ®è®°å½• ---")
        
        manager = IntelligentConfigManager(self.db_path)
        
        # åˆ›å»ºåŸºç¡€ä»»åŠ¡
        base_config = ImportTaskConfig(
            task_id="perf_test_base",
            name="æ€§èƒ½æµ‹è¯•åŸºç¡€ä»»åŠ¡",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH
        )
        manager.add_import_task(base_config)
        
        # è®°å½•å¤§é‡æ€§èƒ½æ•°æ®
        num_records = 10000
        records_created = 0
        
        start_time = time.time()
        
        for i in range(num_records):
            try:
                manager.record_performance_feedback(
                    config=base_config,
                    execution_time=np.random.uniform(30, 300),
                    success_rate=np.random.uniform(0.7, 1.0),
                    error_rate=np.random.uniform(0.0, 0.3),
                    throughput=np.random.uniform(100, 2000)
                )
                records_created += 1
                
            except Exception as e:
                if records_created <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                    print(f"æ€§èƒ½è®°å½•å¤±è´¥ {i}: {e}")
        
        recording_time = time.time() - start_time
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_intelligent_statistics()
        
        print(f"æ€§èƒ½æ•°æ®è®°å½•å®Œæˆ:")
        print(f"  ç›®æ ‡è®°å½•æ•°: {num_records}")
        print(f"  æˆåŠŸè®°å½•: {records_created}")
        print(f"  ç»Ÿè®¡è®°å½•æ•°: {stats['performance_history_count']}")
        print(f"  æ€»è€—æ—¶: {recording_time:.2f}ç§’")
        print(f"  å¹³å‡è®°å½•é€Ÿåº¦: {records_created/recording_time:.1f}è®°å½•/ç§’")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(records_created, num_records * 0.95)  # è‡³å°‘95%æˆåŠŸ
        self.assertLess(recording_time, 120)  # 2åˆ†é’Ÿå†…å®Œæˆ
        self.assertGreater(records_created/recording_time, 50)  # è‡³å°‘50è®°å½•/ç§’

    def test_large_dataset_anomaly_detection(self):
        """æµ‹è¯•å¤§æ•°æ®é›†å¼‚å¸¸æ£€æµ‹æ€§èƒ½"""
        print("\n--- æµ‹è¯•å¤§æ•°æ®é›†å¼‚å¸¸æ£€æµ‹æ€§èƒ½ ---")
        
        config = AnomalyDetectionConfig(
            auto_repair_enabled=True,
            enable_outlier_detection=True,
            enable_missing_data_detection=True,
            enable_duplicate_detection=True
        )
        detector = DataAnomalyDetector(config, self.db_path)
        
        # åˆ›å»ºå¤§å‹æ•°æ®é›† (100,000 è¡Œ)
        print("ç”Ÿæˆå¤§å‹æµ‹è¯•æ•°æ®é›†...")
        dataset_size = 100000
        
        np.random.seed(42)
        
        # ç”ŸæˆåŸºç¡€æ•°æ®
        base_data = pd.DataFrame({
            'timestamp': pd.date_range('2020-01-01', periods=dataset_size, freq='min'),
            'symbol': np.random.choice(['000001', '000002', '000300', '000858'], dataset_size),
            'price': np.random.normal(100, 15, dataset_size),
            'volume': np.random.randint(1000, 100000, dataset_size),
            'high': np.random.normal(105, 15, dataset_size),
            'low': np.random.normal(95, 15, dataset_size),
            'amount': np.random.uniform(100000, 10000000, dataset_size)
        })
        
        # æ³¨å…¥å„ç§å¼‚å¸¸
        print("æ³¨å…¥å¼‚å¸¸æ•°æ®...")
        
        # 1. ç¼ºå¤±æ•°æ® (5%)
        missing_indices = np.random.choice(dataset_size, int(dataset_size * 0.05), replace=False)
        base_data.loc[missing_indices, 'price'] = np.nan
        
        # 2. å¼‚å¸¸å€¼ (3%)
        outlier_indices = np.random.choice(dataset_size, int(dataset_size * 0.03), replace=False)
        base_data.loc[outlier_indices, 'price'] = np.random.choice([500, 1000, -50, 0], len(outlier_indices))
        
        # 3. é‡å¤æ•°æ® (2%)
        duplicate_indices = np.random.choice(dataset_size-1, int(dataset_size * 0.02), replace=False)
        for idx in duplicate_indices:
            if idx < dataset_size - 1:
                base_data.loc[idx + 1] = base_data.loc[idx]
        
        # 4. é›¶äº¤æ˜“é‡ (1%)
        zero_volume_indices = np.random.choice(dataset_size, int(dataset_size * 0.01), replace=False)
        base_data.loc[zero_volume_indices, 'volume'] = 0
        
        print(f"æ•°æ®é›†ç”Ÿæˆå®Œæˆ: {len(base_data)} è¡Œ")
        
        # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        print("å¼€å§‹å¼‚å¸¸æ£€æµ‹...")
        detection_start = time.time()
        
        anomalies = detector.detect_anomalies(
            data=base_data,
            data_source="large_scale_test",
            symbol="LARGE_DATASET",
            data_type="kline"
        )
        
        detection_time = time.time() - detection_start
        
        print(f"å¼‚å¸¸æ£€æµ‹å®Œæˆ:")
        print(f"  æ•°æ®é›†å¤§å°: {len(base_data):,} è¡Œ")
        print(f"  æ£€æµ‹åˆ°å¼‚å¸¸: {len(anomalies)} ä¸ª")
        print(f"  æ£€æµ‹è€—æ—¶: {detection_time:.2f}ç§’")
        print(f"  æ£€æµ‹é€Ÿåº¦: {len(base_data)/detection_time:.0f} è¡Œ/ç§’")
        
        # åˆ†æå¼‚å¸¸ç±»å‹åˆ†å¸ƒ
        anomaly_types = {}
        for anomaly in anomalies:
            anomaly_type = anomaly.anomaly_type.value
            anomaly_types[anomaly_type] = anomaly_types.get(anomaly_type, 0) + 1
        
        print(f"  å¼‚å¸¸ç±»å‹åˆ†å¸ƒ: {anomaly_types}")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(len(anomalies), 0)  # åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸
        self.assertLess(detection_time, 300)  # 5åˆ†é’Ÿå†…å®Œæˆ
        self.assertGreater(len(base_data)/detection_time, 100)  # è‡³å°‘100è¡Œ/ç§’

    def test_concurrent_anomaly_detection(self):
        """æµ‹è¯•å¹¶å‘å¼‚å¸¸æ£€æµ‹æ€§èƒ½"""
        print("\n--- æµ‹è¯•å¹¶å‘å¼‚å¸¸æ£€æµ‹æ€§èƒ½ ---")
        
        config = AnomalyDetectionConfig(auto_repair_enabled=False)  # å…³é—­è‡ªåŠ¨ä¿®å¤ä»¥æé«˜é€Ÿåº¦
        detector = DataAnomalyDetector(config, self.db_path)
        
        # åˆ›å»ºå¤šä¸ªä¸­ç­‰å¤§å°çš„æ•°æ®é›†
        def create_test_dataset(dataset_id, size=10000):
            """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
            np.random.seed(42 + dataset_id)
            
            data = pd.DataFrame({
                'timestamp': pd.date_range('2024-01-01', periods=size, freq='min'),
                'symbol': f'TEST{dataset_id:03d}',
                'price': np.concatenate([
                    np.random.normal(100, 10, int(size * 0.9)),  # 90% æ­£å¸¸æ•°æ®
                    np.random.normal(300, 50, int(size * 0.05)),  # 5% å¼‚å¸¸å€¼
                    [np.nan] * int(size * 0.05)  # 5% ç¼ºå¤±æ•°æ®
                ]),
                'volume': np.random.randint(1000, 100000, size)
            })
            
            return data, dataset_id
        
        def detect_anomalies_worker(args):
            """å¼‚å¸¸æ£€æµ‹å·¥ä½œå‡½æ•°"""
            data, dataset_id = args
            start_time = time.time()
            
            try:
                anomalies = detector.detect_anomalies(
                    data=data,
                    data_source="concurrent_test",
                    symbol=f"DATASET_{dataset_id}",
                    data_type="kline"
                )
                
                detection_time = time.time() - start_time
                
                return {
                    'dataset_id': dataset_id,
                    'success': True,
                    'anomaly_count': len(anomalies),
                    'detection_time': detection_time,
                    'data_size': len(data),
                    'error': None
                }
                
            except Exception as e:
                return {
                    'dataset_id': dataset_id,
                    'success': False,
                    'anomaly_count': 0,
                    'detection_time': time.time() - start_time,
                    'data_size': len(data),
                    'error': str(e)
                }
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        num_datasets = 20
        datasets = []
        
        print(f"åˆ›å»º {num_datasets} ä¸ªæµ‹è¯•æ•°æ®é›†...")
        for i in range(num_datasets):
            dataset = create_test_dataset(i, size=5000)  # æ¯ä¸ªæ•°æ®é›†5000è¡Œ
            datasets.append(dataset)
        
        # å¹¶å‘æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        print("å¼€å§‹å¹¶å‘å¼‚å¸¸æ£€æµ‹...")
        concurrent_start = time.time()
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(detect_anomalies_worker, datasets))
        
        concurrent_time = time.time() - concurrent_start
        
        # åˆ†æç»“æœ
        successful_detections = [r for r in results if r['success']]
        failed_detections = [r for r in results if not r['success']]
        
        total_anomalies = sum(r['anomaly_count'] for r in successful_detections)
        total_data_size = sum(r['data_size'] for r in successful_detections)
        avg_detection_time = np.mean([r['detection_time'] for r in successful_detections]) if successful_detections else 0
        
        print(f"å¹¶å‘å¼‚å¸¸æ£€æµ‹å®Œæˆ:")
        print(f"  æ•°æ®é›†æ•°é‡: {num_datasets}")
        print(f"  æˆåŠŸæ£€æµ‹: {len(successful_detections)}")
        print(f"  æ£€æµ‹å¤±è´¥: {len(failed_detections)}")
        print(f"  æ€»æ•°æ®é‡: {total_data_size:,} è¡Œ")
        print(f"  æ€»å¼‚å¸¸æ•°: {total_anomalies}")
        print(f"  å¹¶å‘æ€»è€—æ—¶: {concurrent_time:.2f}ç§’")
        print(f"  å¹³å‡å•ä¸ªæ£€æµ‹æ—¶é—´: {avg_detection_time:.2f}ç§’")
        print(f"  å¹¶å‘å¤„ç†é€Ÿåº¦: {total_data_size/concurrent_time:.0f} è¡Œ/ç§’")
        
        if failed_detections:
            print(f"  å¤±è´¥è¯¦æƒ…:")
            for failure in failed_detections[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥
                print(f"    æ•°æ®é›†{failure['dataset_id']}: {failure['error']}")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(len(successful_detections), num_datasets * 0.8)  # è‡³å°‘80%æˆåŠŸ
        self.assertLess(concurrent_time, 120)  # 2åˆ†é’Ÿå†…å®Œæˆ
        self.assertGreater(total_data_size/concurrent_time, 500)  # è‡³å°‘500è¡Œ/ç§’


class TestHighConcurrencyOperations(LargeScalePerformanceTest):
    """é«˜å¹¶å‘æ“ä½œæµ‹è¯•"""

    def test_concurrent_task_management(self):
        """æµ‹è¯•å¹¶å‘ä»»åŠ¡ç®¡ç†æ€§èƒ½"""
        print("\n--- æµ‹è¯•å¹¶å‘ä»»åŠ¡ç®¡ç†æ€§èƒ½ ---")
        
        manager = IntelligentConfigManager(self.db_path)
        
        def task_management_worker(worker_id, num_tasks_per_worker=50):
            """ä»»åŠ¡ç®¡ç†å·¥ä½œå‡½æ•°"""
            results = {
                'worker_id': worker_id,
                'tasks_created': 0,
                'tasks_updated': 0,
                'tasks_queried': 0,
                'errors': []
            }
            
            created_task_ids = []
            
            try:
                # åˆ›å»ºä»»åŠ¡
                for i in range(num_tasks_per_worker):
                    task_id = f"worker_{worker_id}_task_{i:03d}"
                    config = ImportTaskConfig(
                        task_id=task_id,
                        name=f"å¹¶å‘æµ‹è¯•ä»»åŠ¡ Worker{worker_id}-{i}",
                        data_source="tongdaxin" if i % 2 == 0 else "akshare",
                        asset_type="stock",
                        data_type="kline",
                        symbols=[f"{(worker_id*100+i):06d}"],
                        frequency=DataFrequency.DAILY,
                        mode=ImportMode.BATCH,
                        max_workers=2 + (i % 4),
                        batch_size=500 + (i * 10)
                    )
                    
                    if manager.add_import_task(config):
                        results['tasks_created'] += 1
                        created_task_ids.append(task_id)
                
                # æ›´æ–°ä»»åŠ¡
                for task_id in created_task_ids[:num_tasks_per_worker//2]:
                    try:
                        config = manager.get_import_task(task_id)
                        if config:
                            config.max_workers = 6
                            config.batch_size = 1500
                            if manager.update_import_task(config):
                                results['tasks_updated'] += 1
                    except Exception as e:
                        results['errors'].append(f"æ›´æ–°ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
                
                # æŸ¥è¯¢ä»»åŠ¡
                for task_id in created_task_ids:
                    try:
                        config = manager.get_import_task(task_id)
                        if config:
                            results['tasks_queried'] += 1
                    except Exception as e:
                        results['errors'].append(f"æŸ¥è¯¢ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
                
            except Exception as e:
                results['errors'].append(f"Worker {worker_id} æ‰§è¡Œå¼‚å¸¸: {e}")
            
            return results
        
        # å¯åŠ¨å¤šä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹
        num_workers = 10
        tasks_per_worker = 30
        
        print(f"å¯åŠ¨ {num_workers} ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹ï¼Œæ¯ä¸ªåˆ›å»º {tasks_per_worker} ä¸ªä»»åŠ¡...")
        
        concurrent_start = time.time()
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(task_management_worker, i, tasks_per_worker) 
                      for i in range(num_workers)]
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"Workeræ‰§è¡Œå¼‚å¸¸: {e}")
        
        concurrent_time = time.time() - concurrent_start
        
        # ç»Ÿè®¡ç»“æœ
        total_created = sum(r['tasks_created'] for r in results)
        total_updated = sum(r['tasks_updated'] for r in results)
        total_queried = sum(r['tasks_queried'] for r in results)
        total_errors = sum(len(r['errors']) for r in results)
        
        # éªŒè¯æ•°æ®åº“ä¸­çš„å®é™…ä»»åŠ¡æ•°
        all_tasks = manager.get_all_import_tasks()
        
        print(f"å¹¶å‘ä»»åŠ¡ç®¡ç†å®Œæˆ:")
        print(f"  å·¥ä½œçº¿ç¨‹æ•°: {num_workers}")
        print(f"  ç›®æ ‡ä»»åŠ¡æ€»æ•°: {num_workers * tasks_per_worker}")
        print(f"  å®é™…åˆ›å»ºä»»åŠ¡: {total_created}")
        print(f"  æ›´æ–°ä»»åŠ¡æ•°: {total_updated}")
        print(f"  æŸ¥è¯¢ä»»åŠ¡æ•°: {total_queried}")
        print(f"  æ•°æ®åº“ä»»åŠ¡æ•°: {len(all_tasks)}")
        print(f"  æ€»é”™è¯¯æ•°: {total_errors}")
        print(f"  å¹¶å‘æ€»è€—æ—¶: {concurrent_time:.2f}ç§’")
        print(f"  ä»»åŠ¡åˆ›å»ºé€Ÿåº¦: {total_created/concurrent_time:.1f} ä»»åŠ¡/ç§’")
        
        # æ˜¾ç¤ºéƒ¨åˆ†é”™è¯¯ä¿¡æ¯
        if total_errors > 0:
            print("  é”™è¯¯ç¤ºä¾‹:")
            error_count = 0
            for result in results:
                for error in result['errors'][:2]:  # æ¯ä¸ªworkeræœ€å¤šæ˜¾ç¤º2ä¸ªé”™è¯¯
                    print(f"    {error}")
                    error_count += 1
                    if error_count >= 5:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé”™è¯¯
                        break
                if error_count >= 5:
                    break
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(total_created, num_workers * tasks_per_worker * 0.8)  # è‡³å°‘80%æˆåŠŸ
        self.assertLess(concurrent_time, 60)  # 1åˆ†é’Ÿå†…å®Œæˆ
        self.assertGreater(total_created/concurrent_time, 20)  # è‡³å°‘20ä»»åŠ¡/ç§’

    def test_concurrent_data_integration(self):
        """æµ‹è¯•å¹¶å‘æ•°æ®é›†æˆæ€§èƒ½"""
        print("\n--- æµ‹è¯•å¹¶å‘æ•°æ®é›†æˆæ€§èƒ½ ---")
        
        config = UIIntegrationConfig(
            enable_caching=True,
            cache_expiry_seconds=300,
            enable_predictive_loading=True,
            enable_adaptive_caching=True
        )
        
        with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
            integration = SmartDataIntegration(config)
        
        def data_integration_worker(worker_id, num_requests=100):
            """æ•°æ®é›†æˆå·¥ä½œå‡½æ•°"""
            results = {
                'worker_id': worker_id,
                'successful_requests': 0,
                'failed_requests': 0,
                'cache_hits': 0,
                'total_response_time': 0,
                'errors': []
            }
            
            try:
                for i in range(num_requests):
                    request_start = time.time()
                    
                    try:
                        # æ¨¡æ‹Ÿæ•°æ®è¯·æ±‚
                        widget_type = f"stock_quote_{i % 5}"
                        symbol = f"{(worker_id*1000+i):06d}"
                        data_type = "realtime" if i % 2 == 0 else "daily"
                        
                        # æ£€æŸ¥ç¼“å­˜
                        cache_key = f"{widget_type}_{symbol}_{data_type}"
                        cached_data = integration._get_from_intelligent_cache(cache_key)
                        
                        if cached_data:
                            results['cache_hits'] += 1
                            results['successful_requests'] += 1
                        else:
                            # æ¨¡æ‹Ÿæ•°æ®è·å–
                            mock_data = {
                                'symbol': symbol,
                                'price': 10.0 + (i % 100) * 0.1,
                                'volume': 1000 + i * 10,
                                'timestamp': datetime.now().isoformat()
                            }
                            
                            # å­˜å…¥ç¼“å­˜
                            integration._put_to_intelligent_cache(
                                cache_key, mock_data, "high", 300
                            )
                            
                            results['successful_requests'] += 1
                        
                        # è®°å½•ä½¿ç”¨æ¨¡å¼
                        integration._record_usage_pattern(widget_type, symbol, data_type)
                        
                    except Exception as e:
                        results['failed_requests'] += 1
                        if len(results['errors']) < 5:  # åªè®°å½•å‰5ä¸ªé”™è¯¯
                            results['errors'].append(f"è¯·æ±‚å¤±è´¥ {i}: {e}")
                    
                    request_time = time.time() - request_start
                    results['total_response_time'] += request_time
                
            except Exception as e:
                results['errors'].append(f"Worker {worker_id} æ‰§è¡Œå¼‚å¸¸: {e}")
            
            return results
        
        # å¯åŠ¨å¹¶å‘æ•°æ®é›†æˆæµ‹è¯•
        num_workers = 15
        requests_per_worker = 50
        
        print(f"å¯åŠ¨ {num_workers} ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹ï¼Œæ¯ä¸ªæ‰§è¡Œ {requests_per_worker} ä¸ªæ•°æ®è¯·æ±‚...")
        
        concurrent_start = time.time()
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(data_integration_worker, i, requests_per_worker) 
                      for i in range(num_workers)]
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"Workeræ‰§è¡Œå¼‚å¸¸: {e}")
        
        concurrent_time = time.time() - concurrent_start
        
        # ç»Ÿè®¡ç»“æœ
        total_successful = sum(r['successful_requests'] for r in results)
        total_failed = sum(r['failed_requests'] for r in results)
        total_cache_hits = sum(r['cache_hits'] for r in results)
        total_response_time = sum(r['total_response_time'] for r in results)
        total_errors = sum(len(r['errors']) for r in results)
        
        avg_response_time = total_response_time / total_successful if total_successful > 0 else 0
        cache_hit_rate = total_cache_hits / total_successful if total_successful > 0 else 0
        
        # è·å–é›†æˆç»Ÿè®¡ä¿¡æ¯
        integration_stats = integration.get_statistics()
        
        print(f"å¹¶å‘æ•°æ®é›†æˆå®Œæˆ:")
        print(f"  å·¥ä½œçº¿ç¨‹æ•°: {num_workers}")
        print(f"  ç›®æ ‡è¯·æ±‚æ€»æ•°: {num_workers * requests_per_worker}")
        print(f"  æˆåŠŸè¯·æ±‚æ•°: {total_successful}")
        print(f"  å¤±è´¥è¯·æ±‚æ•°: {total_failed}")
        print(f"  ç¼“å­˜å‘½ä¸­æ•°: {total_cache_hits}")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2%}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time*1000:.2f}ms")
        print(f"  å¹¶å‘æ€»è€—æ—¶: {concurrent_time:.2f}ç§’")
        print(f"  è¯·æ±‚å¤„ç†é€Ÿåº¦: {total_successful/concurrent_time:.1f} è¯·æ±‚/ç§’")
        print(f"  æ€»é”™è¯¯æ•°: {total_errors}")
        
        # æ¸…ç†èµ„æº
        try:
            integration.close()
        except:
            pass
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(total_successful, num_workers * requests_per_worker * 0.9)  # è‡³å°‘90%æˆåŠŸ
        self.assertLess(concurrent_time, 30)  # 30ç§’å†…å®Œæˆ
        self.assertGreater(total_successful/concurrent_time, 50)  # è‡³å°‘50è¯·æ±‚/ç§’
        self.assertLess(avg_response_time, 0.1)  # å¹³å‡å“åº”æ—¶é—´å°äº100ms


class TestMemoryAndResourceStress(LargeScalePerformanceTest):
    """å†…å­˜å’Œèµ„æºå‹åŠ›æµ‹è¯•"""

    def test_memory_usage_under_extreme_load(self):
        """æµ‹è¯•æç«¯è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨"""
        print("\n--- æµ‹è¯•æç«¯è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨ ---")
        
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        peak_memory = initial_memory
        
        # åˆ›å»ºå¤šä¸ªç»„ä»¶å®ä¾‹
        managers = []
        detectors = []
        integrations = []
        
        try:
            # åˆ›å»ºå¤šä¸ªé…ç½®ç®¡ç†å™¨å®ä¾‹
            print("åˆ›å»ºå¤šä¸ªé…ç½®ç®¡ç†å™¨å®ä¾‹...")
            for i in range(10):
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                manager = IntelligentConfigManager(temp_db.name)
                managers.append((manager, temp_db.name))
                
                # ä¸ºæ¯ä¸ªç®¡ç†å™¨åˆ›å»ºä»»åŠ¡
                for j in range(100):
                    config = ImportTaskConfig(
                        task_id=f"memory_test_{i}_{j}",
                        name=f"å†…å­˜æµ‹è¯•ä»»åŠ¡{i}-{j}",
                        data_source="tongdaxin",
                        asset_type="stock",
                        data_type="kline",
                        symbols=[f"{(i*100+j):06d}"],
                        frequency=DataFrequency.DAILY,
                        mode=ImportMode.BATCH
                    )
                    manager.add_import_task(config)
                
                current_memory = self.process.memory_info().rss / 1024 / 1024
                peak_memory = max(peak_memory, current_memory)
            
            print(f"é…ç½®ç®¡ç†å™¨åˆ›å»ºå®Œæˆï¼Œå½“å‰å†…å­˜: {current_memory:.1f}MB")
            
            # åˆ›å»ºå¤šä¸ªå¼‚å¸¸æ£€æµ‹å™¨å®ä¾‹
            print("åˆ›å»ºå¤šä¸ªå¼‚å¸¸æ£€æµ‹å™¨å®ä¾‹...")
            for i in range(5):
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                config = AnomalyDetectionConfig()
                detector = DataAnomalyDetector(config, temp_db.name)
                detectors.append((detector, temp_db.name))
                
                # ä¸ºæ¯ä¸ªæ£€æµ‹å™¨å¤„ç†æ•°æ®
                test_data = pd.DataFrame({
                    'price': np.random.normal(100, 10, 10000),
                    'volume': np.random.randint(1000, 100000, 10000),
                    'timestamp': pd.date_range('2024-01-01', periods=10000, freq='min')
                })
                
                # æ³¨å…¥å¼‚å¸¸
                test_data.loc[np.random.choice(10000, 500, replace=False), 'price'] = np.nan
                
                anomalies = detector.detect_anomalies(
                    data=test_data,
                    data_source=f"memory_test_{i}",
                    symbol=f"MEM_TEST_{i}",
                    data_type="kline"
                )
                
                current_memory = self.process.memory_info().rss / 1024 / 1024
                peak_memory = max(peak_memory, current_memory)
            
            print(f"å¼‚å¸¸æ£€æµ‹å™¨åˆ›å»ºå®Œæˆï¼Œå½“å‰å†…å­˜: {current_memory:.1f}MB")
            
            # åˆ›å»ºå¤šä¸ªæ•°æ®é›†æˆå®ä¾‹
            print("åˆ›å»ºå¤šä¸ªæ•°æ®é›†æˆå®ä¾‹...")
            for i in range(5):
                config = UIIntegrationConfig(enable_caching=True)
                
                with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
                    integration = SmartDataIntegration(config)
                    integrations.append(integration)
                
                # ä¸ºæ¯ä¸ªé›†æˆå®ä¾‹æ·»åŠ ç¼“å­˜æ•°æ®
                for j in range(1000):
                    cache_key = f"memory_test_{i}_{j}"
                    test_data = {
                        'symbol': f"TEST{j:06d}",
                        'data': [{'price': 10.0 + k * 0.1, 'volume': 1000 + k * 10} 
                                for k in range(100)]
                    }
                    integration._put_to_intelligent_cache(cache_key, test_data, "high", 3600)
                
                current_memory = self.process.memory_info().rss / 1024 / 1024
                peak_memory = max(peak_memory, current_memory)
            
            print(f"æ•°æ®é›†æˆå®ä¾‹åˆ›å»ºå®Œæˆï¼Œå½“å‰å†…å­˜: {current_memory:.1f}MB")
            
            # æ‰§è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ
            print("æ‰§è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ...")
            
            # å¤§é‡æ•°æ®å¤„ç†
            large_datasets = []
            for i in range(5):
                large_data = pd.DataFrame({
                    'timestamp': pd.date_range('2020-01-01', periods=50000, freq='min'),
                    'symbol': f'LARGE_{i}',
                    'price': np.random.normal(100, 15, 50000),
                    'volume': np.random.randint(1000, 100000, 50000)
                })
                large_datasets.append(large_data)
                
                current_memory = self.process.memory_info().rss / 1024 / 1024
                peak_memory = max(peak_memory, current_memory)
            
            print(f"å¤§æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå½“å‰å†…å­˜: {current_memory:.1f}MB")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            print("æ‰§è¡Œåƒåœ¾å›æ”¶...")
            gc.collect()
            
            after_gc_memory = self.process.memory_info().rss / 1024 / 1024
            
            print(f"å†…å­˜ä½¿ç”¨æƒ…å†µ:")
            print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
            print(f"  å³°å€¼å†…å­˜: {peak_memory:.1f}MB")
            print(f"  GCåå†…å­˜: {after_gc_memory:.1f}MB")
            print(f"  å†…å­˜å¢é•¿: {peak_memory - initial_memory:.1f}MB")
            print(f"  GCå›æ”¶: {peak_memory - after_gc_memory:.1f}MB")
            
            # å†…å­˜ä½¿ç”¨æ–­è¨€
            memory_increase = peak_memory - initial_memory
            self.assertLess(memory_increase, 2000)  # å†…å­˜å¢é•¿ä¸è¶…è¿‡2GB
            
            # GCæ•ˆæœæ–­è¨€
            gc_recovered = peak_memory - after_gc_memory
            self.assertGreater(gc_recovered, memory_increase * 0.3)  # GCè‡³å°‘å›æ”¶30%
            
        finally:
            # æ¸…ç†èµ„æº
            print("æ¸…ç†æµ‹è¯•èµ„æº...")
            
            for manager, db_path in managers:
                try:
                    os.unlink(db_path)
                except:
                    pass
            
            for detector, db_path in detectors:
                try:
                    os.unlink(db_path)
                except:
                    pass
            
            for integration in integrations:
                try:
                    integration.close()
                except:
                    pass
            
            # æœ€ç»ˆåƒåœ¾å›æ”¶
            gc.collect()
            
            final_memory = self.process.memory_info().rss / 1024 / 1024
            print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")

    def test_cpu_intensive_operations(self):
        """æµ‹è¯•CPUå¯†é›†å‹æ“ä½œ"""
        print("\n--- æµ‹è¯•CPUå¯†é›†å‹æ“ä½œ ---")
        
        initial_cpu = self.process.cpu_percent()
        
        def cpu_intensive_task(task_id, duration=10):
            """CPUå¯†é›†å‹ä»»åŠ¡"""
            start_time = time.time()
            operation_count = 0
            
            # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
            temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
            temp_db.close()
            
            try:
                manager = IntelligentConfigManager(temp_db.name)
                detector_config = AnomalyDetectionConfig()
                detector = DataAnomalyDetector(detector_config, temp_db.name)
                
                while time.time() - start_time < duration:
                    # 1. åˆ›å»ºå’Œç®¡ç†ä»»åŠ¡
                    for i in range(10):
                        config = ImportTaskConfig(
                            task_id=f"cpu_test_{task_id}_{operation_count}_{i}",
                            name=f"CPUæµ‹è¯•ä»»åŠ¡{task_id}-{operation_count}-{i}",
                            data_source="tongdaxin",
                            asset_type="stock",
                            data_type="kline",
                            symbols=[f"{(task_id*10000+operation_count*10+i):06d}"],
                            frequency=DataFrequency.DAILY,
                            mode=ImportMode.BATCH
                        )
                        manager.add_import_task(config)
                    
                    # 2. ç”Ÿæˆå’Œå¤„ç†æ•°æ®
                    test_data = pd.DataFrame({
                        'price': np.random.normal(100, 10, 1000),
                        'volume': np.random.randint(1000, 100000, 1000),
                        'timestamp': pd.date_range('2024-01-01', periods=1000, freq='min')
                    })
                    
                    # æ³¨å…¥å¼‚å¸¸
                    test_data.loc[np.random.choice(1000, 50, replace=False), 'price'] = np.nan
                    test_data.loc[np.random.choice(1000, 30, replace=False), 'price'] = np.random.normal(500, 50, 30)
                    
                    # 3. å¼‚å¸¸æ£€æµ‹
                    anomalies = detector.detect_anomalies(
                        data=test_data,
                        data_source=f"cpu_test_{task_id}",
                        symbol=f"CPU_TEST_{operation_count}",
                        data_type="kline"
                    )
                    
                    # 4. æ•°å­¦è®¡ç®—
                    matrix_a = np.random.rand(100, 100)
                    matrix_b = np.random.rand(100, 100)
                    result = np.dot(matrix_a, matrix_b)
                    
                    operation_count += 1
                
                return {
                    'task_id': task_id,
                    'operations_completed': operation_count,
                    'duration': time.time() - start_time,
                    'operations_per_second': operation_count / (time.time() - start_time)
                }
                
            finally:
                try:
                    os.unlink(temp_db.name)
                except:
                    pass
        
        # å¯åŠ¨å¤šä¸ªCPUå¯†é›†å‹ä»»åŠ¡
        num_cpu_tasks = multiprocessing.cpu_count()
        task_duration = 15  # 15ç§’
        
        print(f"å¯åŠ¨ {num_cpu_tasks} ä¸ªCPUå¯†é›†å‹ä»»åŠ¡ï¼Œæ¯ä¸ªè¿è¡Œ {task_duration} ç§’...")
        
        cpu_test_start = time.time()
        
        with ProcessPoolExecutor(max_workers=num_cpu_tasks) as executor:
            futures = [executor.submit(cpu_intensive_task, i, task_duration) 
                      for i in range(num_cpu_tasks)]
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"CPUä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        cpu_test_time = time.time() - cpu_test_start
        
        # è·å–CPUä½¿ç”¨ç‡
        final_cpu = self.process.cpu_percent()
        
        # ç»Ÿè®¡ç»“æœ
        total_operations = sum(r['operations_completed'] for r in results)
        avg_ops_per_second = np.mean([r['operations_per_second'] for r in results])
        
        print(f"CPUå¯†é›†å‹æµ‹è¯•å®Œæˆ:")
        print(f"  å¹¶å‘ä»»åŠ¡æ•°: {num_cpu_tasks}")
        print(f"  æ€»æµ‹è¯•æ—¶é—´: {cpu_test_time:.2f}ç§’")
        print(f"  æ€»æ“ä½œæ¬¡æ•°: {total_operations}")
        print(f"  å¹³å‡æ“ä½œé€Ÿåº¦: {avg_ops_per_second:.1f} æ“ä½œ/ç§’")
        print(f"  åˆå§‹CPUä½¿ç”¨ç‡: {initial_cpu:.1f}%")
        print(f"  æœ€ç»ˆCPUä½¿ç”¨ç‡: {final_cpu:.1f}%")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(total_operations, num_cpu_tasks * 10)  # æ¯ä¸ªä»»åŠ¡è‡³å°‘å®Œæˆ10ä¸ªæ“ä½œ
        self.assertLess(cpu_test_time, task_duration + 5)  # æµ‹è¯•æ—¶é—´ä¸åº”è¯¥è¶…å‡ºå¤ªå¤š


class TestDatabasePerformanceStress(LargeScalePerformanceTest):
    """æ•°æ®åº“æ€§èƒ½å‹åŠ›æµ‹è¯•"""

    def test_massive_database_operations(self):
        """æµ‹è¯•å¤§è§„æ¨¡æ•°æ®åº“æ“ä½œ"""
        print("\n--- æµ‹è¯•å¤§è§„æ¨¡æ•°æ®åº“æ“ä½œ ---")
        
        manager = IntelligentConfigManager(self.db_path)
        
        # 1. å¤§é‡æ’å…¥æ“ä½œ
        print("æ‰§è¡Œå¤§é‡æ’å…¥æ“ä½œ...")
        insert_start = time.time()
        
        num_tasks = 5000
        batch_size = 100
        
        for batch_start in range(0, num_tasks, batch_size):
            batch_end = min(batch_start + batch_size, num_tasks)
            
            # æ‰¹é‡åˆ›å»ºä»»åŠ¡
            for i in range(batch_start, batch_end):
                config = ImportTaskConfig(
                    task_id=f"db_stress_{i:05d}",
                    name=f"æ•°æ®åº“å‹åŠ›æµ‹è¯•ä»»åŠ¡{i}",
                    data_source="tongdaxin" if i % 3 == 0 else ("akshare" if i % 3 == 1 else "wind"),
                    asset_type="stock" if i % 2 == 0 else "index",
                    data_type="kline",
                    symbols=[f"{(i % 1000):06d}", f"{((i+1) % 1000):06d}"],
                    frequency=DataFrequency.DAILY if i % 2 == 0 else DataFrequency.MINUTE,
                    mode=ImportMode.BATCH if i % 3 == 0 else ImportMode.SCHEDULED,
                    max_workers=2 + (i % 6),
                    batch_size=500 + (i % 1000)
                )
                
                manager.add_import_task(config)
            
            # æ¯æ‰¹æ¬¡åæ£€æŸ¥è¿›åº¦
            if (batch_end) % 1000 == 0:
                current_time = time.time() - insert_start
                print(f"  å·²æ’å…¥ {batch_end} ä¸ªä»»åŠ¡ï¼Œè€—æ—¶ {current_time:.1f}ç§’")
        
        insert_time = time.time() - insert_start
        
        # éªŒè¯æ’å…¥ç»“æœ
        all_tasks = manager.get_all_import_tasks()
        
        print(f"æ’å…¥æ“ä½œå®Œæˆ:")
        print(f"  ç›®æ ‡æ’å…¥æ•°: {num_tasks}")
        print(f"  å®é™…æ’å…¥æ•°: {len(all_tasks)}")
        print(f"  æ’å…¥è€—æ—¶: {insert_time:.2f}ç§’")
        print(f"  æ’å…¥é€Ÿåº¦: {len(all_tasks)/insert_time:.1f} ä»»åŠ¡/ç§’")
        
        # 2. å¤§é‡æŸ¥è¯¢æ“ä½œ
        print("æ‰§è¡Œå¤§é‡æŸ¥è¯¢æ“ä½œ...")
        query_start = time.time()
        
        num_queries = 10000
        successful_queries = 0
        
        task_ids = list(all_tasks.keys())
        
        for i in range(num_queries):
            # éšæœºé€‰æ‹©ä»»åŠ¡IDè¿›è¡ŒæŸ¥è¯¢
            task_id = np.random.choice(task_ids)
            
            try:
                config = manager.get_import_task(task_id)
                if config:
                    successful_queries += 1
            except Exception as e:
                if successful_queries <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                    print(f"æŸ¥è¯¢å¤±è´¥ {task_id}: {e}")
        
        query_time = time.time() - query_start
        
        print(f"æŸ¥è¯¢æ“ä½œå®Œæˆ:")
        print(f"  æŸ¥è¯¢æ¬¡æ•°: {num_queries}")
        print(f"  æˆåŠŸæŸ¥è¯¢: {successful_queries}")
        print(f"  æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ç§’")
        print(f"  æŸ¥è¯¢é€Ÿåº¦: {successful_queries/query_time:.1f} æŸ¥è¯¢/ç§’")
        
        # 3. å¤§é‡æ›´æ–°æ“ä½œ
        print("æ‰§è¡Œå¤§é‡æ›´æ–°æ“ä½œ...")
        update_start = time.time()
        
        num_updates = 1000
        successful_updates = 0
        
        update_task_ids = np.random.choice(task_ids, num_updates, replace=False)
        
        for task_id in update_task_ids:
            try:
                config = manager.get_import_task(task_id)
                if config:
                    # ä¿®æ”¹é…ç½®
                    config.max_workers = np.random.randint(2, 10)
                    config.batch_size = np.random.randint(500, 2000)
                    
                    if manager.update_import_task(config):
                        successful_updates += 1
            except Exception as e:
                if successful_updates <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                    print(f"æ›´æ–°å¤±è´¥ {task_id}: {e}")
        
        update_time = time.time() - update_start
        
        print(f"æ›´æ–°æ“ä½œå®Œæˆ:")
        print(f"  æ›´æ–°æ¬¡æ•°: {num_updates}")
        print(f"  æˆåŠŸæ›´æ–°: {successful_updates}")
        print(f"  æ›´æ–°è€—æ—¶: {update_time:.2f}ç§’")
        print(f"  æ›´æ–°é€Ÿåº¦: {successful_updates/update_time:.1f} æ›´æ–°/ç§’")
        
        # 4. å¤§é‡æ€§èƒ½æ•°æ®è®°å½•
        print("æ‰§è¡Œå¤§é‡æ€§èƒ½æ•°æ®è®°å½•...")
        perf_start = time.time()
        
        num_perf_records = 50000
        successful_records = 0
        
        # é€‰æ‹©ä¸€äº›ä»»åŠ¡è¿›è¡Œæ€§èƒ½è®°å½•
        perf_task_ids = np.random.choice(task_ids, min(100, len(task_ids)), replace=False)
        
        for i in range(num_perf_records):
            task_id = np.random.choice(perf_task_ids)
            
            try:
                config = manager.get_import_task(task_id)
                if config:
                    manager.record_performance_feedback(
                        config=config,
                        execution_time=np.random.uniform(30, 300),
                        success_rate=np.random.uniform(0.7, 1.0),
                        error_rate=np.random.uniform(0.0, 0.3),
                        throughput=np.random.uniform(100, 2000)
                    )
                    successful_records += 1
            except Exception as e:
                if successful_records <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                    print(f"æ€§èƒ½è®°å½•å¤±è´¥: {e}")
        
        perf_time = time.time() - perf_start
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_intelligent_statistics()
        
        print(f"æ€§èƒ½è®°å½•å®Œæˆ:")
        print(f"  è®°å½•æ¬¡æ•°: {num_perf_records}")
        print(f"  æˆåŠŸè®°å½•: {successful_records}")
        print(f"  è®°å½•è€—æ—¶: {perf_time:.2f}ç§’")
        print(f"  è®°å½•é€Ÿåº¦: {successful_records/perf_time:.1f} è®°å½•/ç§’")
        print(f"  ç»Ÿè®¡è®°å½•æ•°: {stats['performance_history_count']}")
        
        # 5. æ•°æ®åº“æ–‡ä»¶å¤§å°æ£€æŸ¥
        db_size = os.path.getsize(self.db_path) / 1024 / 1024  # MB
        print(f"æ•°æ®åº“æ–‡ä»¶å¤§å°: {db_size:.1f}MB")
        
        # æ€»ä½“æ€§èƒ½æ–­è¨€
        total_time = insert_time + query_time + update_time + perf_time
        
        print(f"\næ•°æ®åº“å‹åŠ›æµ‹è¯•æ€»ç»“:")
        print(f"  æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
        print(f"  æ•°æ®åº“å¤§å°: {db_size:.1f}MB")
        
        # æ€§èƒ½æ–­è¨€
        self.assertGreater(len(all_tasks), num_tasks * 0.95)  # è‡³å°‘95%æ’å…¥æˆåŠŸ
        self.assertGreater(successful_queries, num_queries * 0.95)  # è‡³å°‘95%æŸ¥è¯¢æˆåŠŸ
        self.assertGreater(successful_updates, num_updates * 0.9)  # è‡³å°‘90%æ›´æ–°æˆåŠŸ
        self.assertGreater(successful_records, num_perf_records * 0.9)  # è‡³å°‘90%è®°å½•æˆåŠŸ
        self.assertLess(total_time, 300)  # æ€»æ—¶é—´ä¸è¶…è¿‡5åˆ†é’Ÿ
        self.assertLess(db_size, 500)  # æ•°æ®åº“å¤§å°ä¸è¶…è¿‡500MB


def run_large_scale_performance_tests():
    """è¿è¡Œå¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œå¤§è§„æ¨¡æ•°æ®å¤„ç†å‹åŠ›æµ‹è¯•...")
    print("=" * 100)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestLargeDatasetProcessing,
        TestHighConcurrencyOperations,
        TestMemoryAndResourceStress,
        TestDatabasePerformanceStress
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(test_suite)
    
    print("=" * 100)
    print(f"å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print(f"æˆåŠŸ: {'æ˜¯' if result.wasSuccessful() else 'å¦'}")
    print(f"å¤±è´¥æ•°: {len(result.failures)}")
    print(f"é”™è¯¯æ•°: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("ğŸ‰ æ‰€æœ‰å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°è‰¯å¥½ï¼Œæ»¡è¶³ç”Ÿäº§ç¯å¢ƒè¦æ±‚ã€‚")
    else:
        print("âŒ å­˜åœ¨æ€§èƒ½æµ‹è¯•å¤±è´¥æˆ–é”™è¯¯")
        
        if result.failures:
            print("\nå¤±è´¥çš„æµ‹è¯•:")
            for test, traceback in result.failures:
                print(f"  - {test}")
        
        if result.errors:
            print("\né”™è¯¯çš„æµ‹è¯•:")
            for test, traceback in result.errors:
                print(f"  - {test}")
    
    return result.wasSuccessful(), len(result.failures), len(result.errors)


if __name__ == "__main__":
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    os.environ['TESTING'] = '1'
    
    # è¿è¡Œå¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•
    success, failures, errors = run_large_scale_performance_tests()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    exit_code = 0 if success else 1
    exit(exit_code)
