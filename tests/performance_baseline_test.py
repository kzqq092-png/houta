#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HIkyuu-UI ç³»ç»Ÿæ€§èƒ½åŸºçº¿æµ‹è¯•

æµ‹è¯•ç³»ç»Ÿå„ä¸ªå…³é”®ç»„ä»¶çš„æ€§èƒ½è¡¨ç°ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿ã€‚
"""

import sys
import os
import time
import psutil
import threading
import statistics
from typing import Dict, List, Any, Optional
from datetime import datetime
import pandas as pd
import numpy as np
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥æµ‹è¯•ç›®æ ‡ç»„ä»¶
try:
    from core.performance.unified_monitor import UnifiedPerformanceMonitor, get_performance_monitor
    from core.performance.cache_manager import MultiLevelCacheManager, CacheLevel
    from core.importdata.import_execution_engine import DataImportExecutionEngine
    from core.services.unified_data_manager import UnifiedDataManager
    from core.services.ai_prediction_service import AIPredictionService
    from core.risk_monitoring.enhanced_risk_monitor import EnhancedRiskMonitor
    COMPONENTS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"éƒ¨åˆ†ç»„ä»¶ä¸å¯ç”¨: {e}")
    COMPONENTS_AVAILABLE = False


class PerformanceBaseline:
    """æ€§èƒ½åŸºçº¿æµ‹è¯•å™¨"""

    def __init__(self):
        self.results = {}
        self.start_time = None
        self.system_baseline = {}

    def get_system_baseline(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸåŸºçº¿æ€§èƒ½æŒ‡æ ‡"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')

            return {
                'cpu_usage': cpu_percent,
                'memory_usage': memory.percent,
                'memory_available_gb': memory.available / (1024**3),
                'disk_usage': disk.percent,
                'disk_free_gb': disk.free / (1024**3)
            }
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸåŸºçº¿å¤±è´¥: {e}")
            return {}

    def test_cache_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜ç³»ç»Ÿæ€§èƒ½"""
        logger.info("ğŸ”„ å¼€å§‹ç¼“å­˜æ€§èƒ½æµ‹è¯•...")

        try:
            # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
            cache_config = {
                'levels': [CacheLevel.MEMORY, CacheLevel.DISK],
                'default_ttl_minutes': 30,
                'memory': {'max_size_mb': 100},
                'disk': {'cache_dir': 'test_cache', 'max_size_mb': 200}
            }
            cache_manager = MultiLevelCacheManager(cache_config)

            # æµ‹è¯•æ•°æ®
            test_data = {f"key_{i}": f"value_{i}" * 100 for i in range(1000)}

            # å†™å…¥æ€§èƒ½æµ‹è¯•
            write_times = []
            for key, value in test_data.items():
                start = time.perf_counter()
                cache_manager.set(key, value)
                write_times.append(time.perf_counter() - start)

            # è¯»å–æ€§èƒ½æµ‹è¯•
            read_times = []
            for key in test_data.keys():
                start = time.perf_counter()
                cache_manager.get(key)
                read_times.append(time.perf_counter() - start)

            # ç»Ÿè®¡ä¿¡æ¯
            stats = cache_manager.get_statistics()

            return {
                'write_avg_ms': statistics.mean(write_times) * 1000,
                'write_p95_ms': statistics.quantiles(write_times, n=20)[18] * 1000,
                'read_avg_ms': statistics.mean(read_times) * 1000,
                'read_p95_ms': statistics.quantiles(read_times, n=20)[18] * 1000,
                'cache_stats': stats,
                'test_items': len(test_data)
            }

        except Exception as e:
            logger.error(f"ç¼“å­˜æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}

    def test_data_manager_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®ç®¡ç†å™¨æ€§èƒ½"""
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®ç®¡ç†å™¨æ€§èƒ½æµ‹è¯•...")

        try:
            # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
            data_manager = UnifiedDataManager()

            # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨è·å–æ€§èƒ½
            start = time.perf_counter()
            stock_list = data_manager.get_stock_list()
            stock_list_time = time.perf_counter() - start

            # æµ‹è¯•Kçº¿æ•°æ®è·å–æ€§èƒ½ï¼ˆå¦‚æœæœ‰è‚¡ç¥¨æ•°æ®ï¼‰
            kdata_times = []
            if not stock_list.empty and len(stock_list) > 0:
                test_stocks = stock_list.head(5)  # æµ‹è¯•å‰5åªè‚¡ç¥¨
                for _, stock in test_stocks.iterrows():
                    try:
                        start = time.perf_counter()
                        kdata = data_manager.get_kdata(stock['code'], 'D', 100)
                        kdata_times.append(time.perf_counter() - start)
                    except Exception:
                        continue

            return {
                'stock_list_time_ms': stock_list_time * 1000,
                'stock_count': len(stock_list),
                'kdata_avg_time_ms': statistics.mean(kdata_times) * 1000 if kdata_times else 0,
                'kdata_tests': len(kdata_times)
            }

        except Exception as e:
            logger.error(f"æ•°æ®ç®¡ç†å™¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}

    def test_ai_prediction_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•AIé¢„æµ‹æœåŠ¡æ€§èƒ½"""
        logger.info("ğŸ§  å¼€å§‹AIé¢„æµ‹æœåŠ¡æ€§èƒ½æµ‹è¯•...")

        try:
            ai_service = AIPredictionService()

            # æµ‹è¯•æ‰§è¡Œæ—¶é—´é¢„æµ‹
            test_data = {
                'data_size': 1000,
                'complexity': 'medium',
                'system_load': 0.5
            }

            prediction_times = []
            for _ in range(10):  # æµ‹è¯•10æ¬¡
                start = time.perf_counter()
                result = ai_service.predict_execution_time(test_data)
                prediction_times.append(time.perf_counter() - start)

            # æµ‹è¯•å‚æ•°ä¼˜åŒ–
            optimization_times = []
            for _ in range(5):  # æµ‹è¯•5æ¬¡
                start = time.perf_counter()
                result = ai_service.optimize_parameters(test_data)
                optimization_times.append(time.perf_counter() - start)

            return {
                'prediction_avg_ms': statistics.mean(prediction_times) * 1000,
                'prediction_p95_ms': statistics.quantiles(prediction_times, n=20)[18] * 1000,
                'optimization_avg_ms': statistics.mean(optimization_times) * 1000,
                'optimization_p95_ms': statistics.quantiles(optimization_times, n=20)[18] * 1000,
                'prediction_tests': len(prediction_times),
                'optimization_tests': len(optimization_times)
            }

        except Exception as e:
            logger.error(f"AIé¢„æµ‹æœåŠ¡æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}

    def test_performance_monitor_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½"""
        logger.info("ğŸ“ˆ å¼€å§‹æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•...")

        try:
            monitor = get_performance_monitor()

            # æµ‹è¯•æŒ‡æ ‡è®°å½•æ€§èƒ½
            record_times = []
            for i in range(1000):
                start = time.perf_counter()
                # å¯¼å…¥å¿…éœ€çš„æšä¸¾
                from core.performance.unified_monitor import PerformanceCategory, MetricType
                monitor.record_metric(
                    f"test_metric_{i % 10}",
                    i * 0.1,
                    PerformanceCategory.SYSTEM,
                    MetricType.GAUGE
                )
                record_times.append(time.perf_counter() - start)

            # æµ‹è¯•ç»Ÿè®¡è·å–æ€§èƒ½
            stats_times = []
            for _ in range(100):
                start = time.perf_counter()
                stats = monitor.get_statistics()
                stats_times.append(time.perf_counter() - start)

            return {
                'record_avg_ms': statistics.mean(record_times) * 1000,
                'record_p95_ms': statistics.quantiles(record_times, n=20)[18] * 1000,
                'stats_avg_ms': statistics.mean(stats_times) * 1000,
                'stats_p95_ms': statistics.quantiles(stats_times, n=20)[18] * 1000,
                'record_tests': len(record_times),
                'stats_tests': len(stats_times)
            }

        except Exception as e:
            logger.error(f"æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}

    def test_risk_monitor_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•é£é™©ç›‘æ§å™¨æ€§èƒ½"""
        logger.info("ğŸ›¡ï¸ å¼€å§‹é£é™©ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•...")

        try:
            risk_monitor = EnhancedRiskMonitor()

            # æµ‹è¯•é£é™©è¯„ä¼°æ€§èƒ½
            test_data = {
                'portfolio_value': 1000000,
                'positions': [
                    {'symbol': 'TEST001', 'quantity': 1000, 'price': 10.5},
                    {'symbol': 'TEST002', 'quantity': 2000, 'price': 5.2},
                ]
            }

            assessment_times = []
            for _ in range(100):
                start = time.perf_counter()
                result = risk_monitor.assess_portfolio_risk(test_data)
                assessment_times.append(time.perf_counter() - start)

            # æµ‹è¯•é£é™©è§„åˆ™æ£€æŸ¥æ€§èƒ½
            rule_check_times = []
            for _ in range(100):
                start = time.perf_counter()
                result = risk_monitor.check_risk_rules(test_data)
                rule_check_times.append(time.perf_counter() - start)

            return {
                'assessment_avg_ms': statistics.mean(assessment_times) * 1000,
                'assessment_p95_ms': statistics.quantiles(assessment_times, n=20)[18] * 1000,
                'rule_check_avg_ms': statistics.mean(rule_check_times) * 1000,
                'rule_check_p95_ms': statistics.quantiles(rule_check_times, n=20)[18] * 1000,
                'assessment_tests': len(assessment_times),
                'rule_check_tests': len(rule_check_times)
            }

        except Exception as e:
            logger.error(f"é£é™©ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}

    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½åŸºçº¿æµ‹è¯•...")

        if not COMPONENTS_AVAILABLE:
            logger.error("ç»„ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
            return {'error': 'ç»„ä»¶ä¸å¯ç”¨'}

        self.start_time = datetime.now()

        # è·å–ç³»ç»ŸåŸºçº¿
        self.system_baseline = self.get_system_baseline()
        logger.info(f"ç³»ç»ŸåŸºçº¿: {self.system_baseline}")

        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_results = {}

        # 1. ç¼“å­˜æ€§èƒ½æµ‹è¯•
        test_results['cache'] = self.test_cache_performance()

        # 2. æ•°æ®ç®¡ç†å™¨æ€§èƒ½æµ‹è¯•
        test_results['data_manager'] = self.test_data_manager_performance()

        # 3. AIé¢„æµ‹æœåŠ¡æ€§èƒ½æµ‹è¯•
        test_results['ai_prediction'] = self.test_ai_prediction_performance()

        # 4. æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•
        test_results['performance_monitor'] = self.test_performance_monitor_performance()

        # 5. é£é™©ç›‘æ§å™¨æ€§èƒ½æµ‹è¯•
        test_results['risk_monitor'] = self.test_risk_monitor_performance()

        # è·å–æµ‹è¯•åçš„ç³»ç»ŸçŠ¶æ€
        system_after = self.get_system_baseline()

        # æ±‡æ€»ç»“æœ
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()

        summary = {
            'test_start': self.start_time.isoformat(),
            'test_end': end_time.isoformat(),
            'total_duration_seconds': total_duration,
            'system_baseline': self.system_baseline,
            'system_after': system_after,
            'test_results': test_results,
            'performance_summary': self._generate_performance_summary(test_results)
        }

        return summary

    def _generate_performance_summary(self, test_results: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
        summary = {}

        for component, results in test_results.items():
            if 'error' in results:
                summary[component] = f"âŒ æµ‹è¯•å¤±è´¥: {results['error']}"
            else:
                # æ ¹æ®ä¸åŒç»„ä»¶ç”Ÿæˆä¸åŒçš„æ€»ç»“
                if component == 'cache':
                    avg_write = results.get('write_avg_ms', 0)
                    avg_read = results.get('read_avg_ms', 0)
                    if avg_write < 1 and avg_read < 1:
                        summary[component] = "âœ… ä¼˜ç§€ (å†™å…¥<1ms, è¯»å–<1ms)"
                    elif avg_write < 5 and avg_read < 5:
                        summary[component] = "âœ… è‰¯å¥½ (å†™å…¥<5ms, è¯»å–<5ms)"
                    else:
                        summary[component] = f"âš ï¸ éœ€ä¼˜åŒ– (å†™å…¥{avg_write:.2f}ms, è¯»å–{avg_read:.2f}ms)"

                elif component == 'data_manager':
                    stock_time = results.get('stock_list_time_ms', 0)
                    kdata_time = results.get('kdata_avg_time_ms', 0)
                    if stock_time < 100 and kdata_time < 200:
                        summary[component] = "âœ… ä¼˜ç§€ (è‚¡ç¥¨åˆ—è¡¨<100ms, Kçº¿<200ms)"
                    elif stock_time < 500 and kdata_time < 1000:
                        summary[component] = "âœ… è‰¯å¥½ (å“åº”æ—¶é—´åˆç†)"
                    else:
                        summary[component] = f"âš ï¸ éœ€ä¼˜åŒ– (è‚¡ç¥¨åˆ—è¡¨{stock_time:.0f}ms, Kçº¿{kdata_time:.0f}ms)"

                elif component == 'ai_prediction':
                    pred_time = results.get('prediction_avg_ms', 0)
                    opt_time = results.get('optimization_avg_ms', 0)
                    if pred_time < 50 and opt_time < 500:
                        summary[component] = "âœ… ä¼˜ç§€ (é¢„æµ‹<50ms, ä¼˜åŒ–<500ms)"
                    elif pred_time < 200 and opt_time < 2000:
                        summary[component] = "âœ… è‰¯å¥½ (å“åº”æ—¶é—´åˆç†)"
                    else:
                        summary[component] = f"âš ï¸ éœ€ä¼˜åŒ– (é¢„æµ‹{pred_time:.0f}ms, ä¼˜åŒ–{opt_time:.0f}ms)"

                else:
                    summary[component] = "âœ… æµ‹è¯•å®Œæˆ"

        return summary


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("HIkyuu-UI ç³»ç»Ÿæ€§èƒ½åŸºçº¿æµ‹è¯•")
    logger.info("=" * 60)

    baseline_tester = PerformanceBaseline()
    results = baseline_tester.run_comprehensive_test()

    # è¾“å‡ºç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 60)

    if 'error' in results:
        logger.error(f"æµ‹è¯•å¤±è´¥: {results['error']}")
        return

    # è¾“å‡ºæ€§èƒ½æ€»ç»“
    logger.info("\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
    for component, summary in results['performance_summary'].items():
        logger.info(f"  {component}: {summary}")

    # è¾“å‡ºç³»ç»Ÿèµ„æºä½¿ç”¨
    logger.info(f"\nğŸ’» ç³»ç»Ÿèµ„æº:")
    baseline = results['system_baseline']
    after = results['system_after']
    logger.info(f"  CPUä½¿ç”¨ç‡: {baseline.get('cpu_usage', 0):.1f}% â†’ {after.get('cpu_usage', 0):.1f}%")
    logger.info(f"  å†…å­˜ä½¿ç”¨ç‡: {baseline.get('memory_usage', 0):.1f}% â†’ {after.get('memory_usage', 0):.1f}%")

    # è¾“å‡ºæµ‹è¯•æ—¶é•¿
    logger.info(f"\nâ±ï¸ æµ‹è¯•è€—æ—¶: {results['total_duration_seconds']:.2f}ç§’")

    logger.info("\n" + "=" * 60)
    logger.info("æ€§èƒ½åŸºçº¿æµ‹è¯•å®Œæˆ")
    logger.info("=" * 60)

    return results


if __name__ == "__main__":
    main()
