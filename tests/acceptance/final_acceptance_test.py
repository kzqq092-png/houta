#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœ€ç»ˆç³»ç»Ÿé›†æˆå’Œç”¨æˆ·éªŒæ”¶æµ‹è¯•

æ‰§è¡Œæœ€ç»ˆçš„ç³»ç»Ÿé›†æˆå’Œç”¨æˆ·éªŒæ”¶æµ‹è¯•ï¼ŒéªŒè¯æ‰€æœ‰åŠŸèƒ½éœ€æ±‚çš„æ­£ç¡®å®ç°ã€‚
æµ‹è¯•è¦†ç›–ï¼š
1. æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½çš„ç«¯åˆ°ç«¯æµ‹è¯•
2. ç”¨æˆ·å·¥ä½œæµç¨‹éªŒè¯
3. æ€§èƒ½è¦æ±‚éªŒè¯
4. æ•°æ®è´¨é‡å’Œå‡†ç¡®æ€§éªŒè¯
5. ç³»ç»Ÿç¨³å®šæ€§å’Œå¯é æ€§éªŒè¯
6. ç”¨æˆ·ç•Œé¢å’Œäº¤äº’éªŒè¯
7. é›†æˆåŠŸèƒ½éªŒè¯
"""

import pytest
import unittest
import tempfile
import os
import time
import threading
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from unittest.mock import patch, Mock, MagicMock
import sqlite3
import json
from pathlib import Path
import sys

# å¯¼å…¥æ‰€æœ‰å¾…æµ‹è¯•çš„ç»„ä»¶
from core.importdata.intelligent_config_manager import (
    IntelligentConfigManager, ImportTaskConfig, DataFrequency, ImportMode
)
from core.ai.data_anomaly_detector import (
    DataAnomalyDetector, AnomalyDetectionConfig, AnomalyType, AnomalySeverity
)
from core.ui_integration.smart_data_integration import (
    SmartDataIntegration, UIIntegrationConfig, IntegrationMode
)
from core.ai.config_recommendation_engine import ConfigRecommendationEngine
from core.ai.config_impact_analyzer import ConfigImpactAnalyzer


class AcceptanceTestResult:
    """éªŒæ”¶æµ‹è¯•ç»“æœç±»"""
    
    def __init__(self):
        self.test_results = []
        self.requirement_coverage = {}
        self.performance_metrics = {}
        self.issues_found = []
        self.start_time = time.time()
        
    def add_test_result(self, test_name, passed, details=None, requirement_id=None):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        result = {
            'test_name': test_name,
            'passed': passed,
            'details': details or {},
            'requirement_id': requirement_id,
            'timestamp': time.time()
        }
        self.test_results.append(result)
        
        if requirement_id:
            if requirement_id not in self.requirement_coverage:
                self.requirement_coverage[requirement_id] = []
            self.requirement_coverage[requirement_id].append(result)
    
    def add_performance_metric(self, metric_name, value, target=None, unit=""):
        """æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics[metric_name] = {
            'value': value,
            'target': target,
            'unit': unit,
            'meets_target': target is None or value <= target
        }
    
    def add_issue(self, severity, description, requirement_id=None):
        """æ·»åŠ å‘ç°çš„é—®é¢˜"""
        issue = {
            'severity': severity,
            'description': description,
            'requirement_id': requirement_id,
            'timestamp': time.time()
        }
        self.issues_found.append(issue)
    
    def get_summary(self):
        """è·å–æµ‹è¯•æ€»ç»“"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['passed'])
        
        total_requirements = len(self.requirement_coverage)
        covered_requirements = sum(1 for req_tests in self.requirement_coverage.values() 
                                 if any(t['passed'] for t in req_tests))
        
        performance_targets_met = sum(1 for m in self.performance_metrics.values() 
                                    if m['meets_target'])
        total_performance_metrics = len(self.performance_metrics)
        
        critical_issues = sum(1 for i in self.issues_found if i['severity'] == 'critical')
        major_issues = sum(1 for i in self.issues_found if i['severity'] == 'major')
        minor_issues = sum(1 for i in self.issues_found if i['severity'] == 'minor')
        
        return {
            'test_success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'requirement_coverage_rate': covered_requirements / total_requirements if total_requirements > 0 else 0,
            'performance_success_rate': performance_targets_met / total_performance_metrics if total_performance_metrics > 0 else 0,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'total_requirements': total_requirements,
            'covered_requirements': covered_requirements,
            'uncovered_requirements': total_requirements - covered_requirements,
            'critical_issues': critical_issues,
            'major_issues': major_issues,
            'minor_issues': minor_issues,
            'total_issues': len(self.issues_found),
            'test_duration': time.time() - self.start_time
        }


class FinalAcceptanceTest(unittest.TestCase):
    """æœ€ç»ˆéªŒæ”¶æµ‹è¯•åŸºç±»"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.acceptance_result = AcceptanceTestResult()
        
        print(f"\n{'='*100}")
        print(f"å¼€å§‹æœ€ç»ˆç³»ç»Ÿé›†æˆå’Œç”¨æˆ·éªŒæ”¶æµ‹è¯•")
        print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*100}")

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†å’Œç»“æœæŠ¥å‘Š"""
        summary = cls.acceptance_result.get_summary()
        
        print(f"\n{'='*100}")
        print(f"æœ€ç»ˆç³»ç»Ÿé›†æˆå’Œç”¨æˆ·éªŒæ”¶æµ‹è¯•å®Œæˆ")
        print(f"æµ‹è¯•ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•æ€»è€—æ—¶: {summary['test_duration']:.2f} ç§’")
        print(f"{'='*100}")
        
        # æµ‹è¯•ç»“æœç»Ÿè®¡
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"  é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"  å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"  æµ‹è¯•æˆåŠŸç‡: {summary['test_success_rate']:.2%}")
        
        # éœ€æ±‚è¦†ç›–ç»Ÿè®¡
        print(f"\nğŸ“‹ éœ€æ±‚è¦†ç›–ç»Ÿè®¡:")
        print(f"  æ€»éœ€æ±‚æ•°: {summary['total_requirements']}")
        print(f"  å·²è¦†ç›–éœ€æ±‚: {summary['covered_requirements']}")
        print(f"  æœªè¦†ç›–éœ€æ±‚: {summary['uncovered_requirements']}")
        print(f"  éœ€æ±‚è¦†ç›–ç‡: {summary['requirement_coverage_rate']:.2%}")
        
        # æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡:")
        print(f"  æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ç‡: {summary['performance_success_rate']:.2%}")
        for metric_name, metric_data in cls.acceptance_result.performance_metrics.items():
            status = "âœ…" if metric_data['meets_target'] else "âŒ"
            target_info = f" (ç›®æ ‡: {metric_data['target']}{metric_data['unit']})" if metric_data['target'] else ""
            print(f"  {status} {metric_name}: {metric_data['value']}{metric_data['unit']}{target_info}")
        
        # é—®é¢˜ç»Ÿè®¡
        print(f"\nğŸ› é—®é¢˜ç»Ÿè®¡:")
        print(f"  ä¸¥é‡é—®é¢˜: {summary['critical_issues']}")
        print(f"  é‡è¦é—®é¢˜: {summary['major_issues']}")
        print(f"  è½»å¾®é—®é¢˜: {summary['minor_issues']}")
        print(f"  é—®é¢˜æ€»æ•°: {summary['total_issues']}")
        
        if cls.acceptance_result.issues_found:
            print(f"\nå‘ç°çš„é—®é¢˜è¯¦æƒ…:")
            for i, issue in enumerate(cls.acceptance_result.issues_found, 1):
                severity_icon = {"critical": "ğŸ”´", "major": "ğŸŸ¡", "minor": "ğŸŸ¢"}.get(issue['severity'], "âšª")
                print(f"  {i}. {severity_icon} [{issue['severity'].upper()}] {issue['description']}")
                if issue['requirement_id']:
                    print(f"     å…³è”éœ€æ±‚: {issue['requirement_id']}")
        
        # æœ€ç»ˆéªŒæ”¶ç»“è®º
        print(f"\nğŸ¯ æœ€ç»ˆéªŒæ”¶ç»“è®º:")
        
        # éªŒæ”¶æ ‡å‡†
        min_test_success_rate = 0.95  # 95%æµ‹è¯•é€šè¿‡ç‡
        min_requirement_coverage = 0.90  # 90%éœ€æ±‚è¦†ç›–ç‡
        min_performance_success_rate = 0.85  # 85%æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ç‡
        max_critical_issues = 0  # 0ä¸ªä¸¥é‡é—®é¢˜
        max_major_issues = 2  # æœ€å¤š2ä¸ªé‡è¦é—®é¢˜
        
        acceptance_criteria = [
            (summary['test_success_rate'] >= min_test_success_rate, 
             f"æµ‹è¯•æˆåŠŸç‡ {summary['test_success_rate']:.2%} >= {min_test_success_rate:.2%}"),
            (summary['requirement_coverage_rate'] >= min_requirement_coverage, 
             f"éœ€æ±‚è¦†ç›–ç‡ {summary['requirement_coverage_rate']:.2%} >= {min_requirement_coverage:.2%}"),
            (summary['performance_success_rate'] >= min_performance_success_rate, 
             f"æ€§èƒ½è¾¾æ ‡ç‡ {summary['performance_success_rate']:.2%} >= {min_performance_success_rate:.2%}"),
            (summary['critical_issues'] <= max_critical_issues, 
             f"ä¸¥é‡é—®é¢˜æ•° {summary['critical_issues']} <= {max_critical_issues}"),
            (summary['major_issues'] <= max_major_issues, 
             f"é‡è¦é—®é¢˜æ•° {summary['major_issues']} <= {max_major_issues}")
        ]
        
        all_criteria_met = all(criteria[0] for criteria in acceptance_criteria)
        
        for criteria_met, description in acceptance_criteria:
            status = "âœ…" if criteria_met else "âŒ"
            print(f"  {status} {description}")
        
        if all_criteria_met:
            print(f"\nğŸ‰ ç³»ç»ŸéªŒæ”¶æµ‹è¯•é€šè¿‡ï¼")
            print(f"ç³»ç»Ÿæ»¡è¶³æ‰€æœ‰éªŒæ”¶æ ‡å‡†ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ã€‚")
        else:
            print(f"\nâŒ ç³»ç»ŸéªŒæ”¶æµ‹è¯•æœªé€šè¿‡ï¼")
            print(f"ç³»ç»Ÿå­˜åœ¨æœªæ»¡è¶³çš„éªŒæ”¶æ ‡å‡†ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚")
        
        print(f"{'='*100}")

    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„å‡†å¤‡"""
        self.temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
        self.temp_db.close()
        self.db_path = self.temp_db.name

    def tearDown(self):
        """æ¯ä¸ªæµ‹è¯•åçš„æ¸…ç†"""
        try:
            os.unlink(self.db_path)
        except:
            pass


class TestCoreSystemFunctionality(FinalAcceptanceTest):
    """æ ¸å¿ƒç³»ç»ŸåŠŸèƒ½éªŒæ”¶æµ‹è¯•"""

    def test_intelligent_config_manager_functionality(self):
        """æµ‹è¯•æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŠŸèƒ½ - éœ€æ±‚1.1, 1.2"""
        print("\n--- æµ‹è¯•æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŠŸèƒ½ ---")
        
        try:
            # åˆ›å»ºé…ç½®ç®¡ç†å™¨
            manager = IntelligentConfigManager(self.db_path)
            
            # æµ‹è¯•1: åŸºæœ¬é…ç½®ç®¡ç†
            config = ImportTaskConfig(
                task_id="acceptance_test_1",
                name="éªŒæ”¶æµ‹è¯•ä»»åŠ¡1",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001", "000002"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=4,
                batch_size=1000
            )
            
            success = manager.add_import_task(config)
            self.assertTrue(success, "æ·»åŠ å¯¼å…¥ä»»åŠ¡å¤±è´¥")
            
            retrieved_config = manager.get_import_task("acceptance_test_1")
            self.assertIsNotNone(retrieved_config, "è·å–å¯¼å…¥ä»»åŠ¡å¤±è´¥")
            self.assertEqual(retrieved_config.name, "éªŒæ”¶æµ‹è¯•ä»»åŠ¡1", "ä»»åŠ¡é…ç½®ä¸åŒ¹é…")
            
            # æµ‹è¯•2: æ€§èƒ½åé¦ˆè®°å½•
            manager.record_performance_feedback(
                config=config,
                execution_time=85.5,
                success_rate=0.95,
                error_rate=0.05,
                throughput=1200
            )
            
            # æµ‹è¯•3: æ™ºèƒ½ç»Ÿè®¡
            stats = manager.get_intelligent_statistics()
            self.assertIsInstance(stats, dict, "ç»Ÿè®¡ä¿¡æ¯æ ¼å¼é”™è¯¯")
            self.assertIn('total_tasks', stats, "ç»Ÿè®¡ä¿¡æ¯ç¼ºå°‘ä»»åŠ¡æ€»æ•°")
            self.assertGreater(stats['total_tasks'], 0, "ä»»åŠ¡æ€»æ•°åº”å¤§äº0")
            
            # æµ‹è¯•4: å†²çªæ£€æµ‹
            conflicts = manager.detect_conflicts()
            self.assertIsInstance(conflicts, list, "å†²çªæ£€æµ‹ç»“æœæ ¼å¼é”™è¯¯")
            
            # æµ‹è¯•5: è‡ªåŠ¨é…ç½®ä¼˜åŒ–
            manager.enable_auto_optimization("performance", {"target_throughput": 1500})
            optimized_config = manager.get_optimized_config("acceptance_test_1")
            self.assertIsNotNone(optimized_config, "è‡ªåŠ¨é…ç½®ä¼˜åŒ–å¤±è´¥")
            
            self.acceptance_result.add_test_result(
                "æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½", True, 
                {"tasks_created": 1, "stats_available": True, "conflicts_detected": len(conflicts)},
                "1.1"
            )
            
            print("  âœ… æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½", False, 
                {"error": str(e)}, "1.1"
            )
            self.acceptance_result.add_issue("critical", f"æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŠŸèƒ½å¼‚å¸¸: {e}", "1.1")
            print(f"  âŒ æ™ºèƒ½é…ç½®ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise

    def test_data_anomaly_detection_functionality(self):
        """æµ‹è¯•æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½ - éœ€æ±‚2.1, 2.2"""
        print("\n--- æµ‹è¯•æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½ ---")
        
        try:
            # åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨
            config = AnomalyDetectionConfig(
                auto_repair_enabled=True,
                detection_sensitivity=0.8,
                max_repair_attempts=3
            )
            detector = DataAnomalyDetector(config, self.db_path)
            
            # åˆ›å»ºåŒ…å«å¼‚å¸¸çš„æµ‹è¯•æ•°æ®
            test_data = pd.DataFrame({
                'timestamp': pd.date_range('2024-01-01', periods=100, freq='min'),
                'symbol': 'TEST001',
                'price': np.concatenate([
                    np.random.normal(100, 5, 80),  # æ­£å¸¸æ•°æ®
                    [np.nan, np.nan],  # ç¼ºå¤±å€¼
                    [500, 600],  # å¼‚å¸¸å€¼
                    np.random.normal(100, 5, 16)  # æ­£å¸¸æ•°æ®
                ]),
                'volume': np.random.randint(1000, 10000, 100),
                'high': np.random.uniform(95, 105, 100),
                'low': np.random.uniform(95, 105, 100),
                'close': np.random.uniform(95, 105, 100)
            })
            
            # æµ‹è¯•1: å¼‚å¸¸æ£€æµ‹
            anomalies = detector.detect_anomalies(
                data=test_data,
                data_source="acceptance_test",
                symbol="TEST001",
                data_type="kline"
            )
            
            self.assertIsInstance(anomalies, list, "å¼‚å¸¸æ£€æµ‹ç»“æœæ ¼å¼é”™è¯¯")
            self.assertGreater(len(anomalies), 0, "åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸")
            
            # éªŒè¯æ£€æµ‹åˆ°çš„å¼‚å¸¸ç±»å‹
            anomaly_types = [a.anomaly_type for a in anomalies]
            self.assertIn(AnomalyType.MISSING_DATA, anomaly_types, "åº”è¯¥æ£€æµ‹åˆ°ç¼ºå¤±æ•°æ®å¼‚å¸¸")
            self.assertIn(AnomalyType.OUTLIER, anomaly_types, "åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸å€¼")
            
            # æµ‹è¯•2: è‡ªåŠ¨ä¿®å¤
            repair_count = 0
            for anomaly in anomalies[:3]:  # ä¿®å¤å‰3ä¸ªå¼‚å¸¸
                success = detector.auto_repair_anomaly(anomaly.anomaly_id)
                if success:
                    repair_count += 1
            
            self.assertGreater(repair_count, 0, "åº”è¯¥æˆåŠŸä¿®å¤è‡³å°‘ä¸€ä¸ªå¼‚å¸¸")
            
            # æµ‹è¯•3: å¼‚å¸¸ç»Ÿè®¡
            stats = detector.get_anomaly_statistics()
            self.assertIsInstance(stats, dict, "å¼‚å¸¸ç»Ÿè®¡æ ¼å¼é”™è¯¯")
            self.assertIn('total_anomalies', stats, "ç»Ÿè®¡ä¿¡æ¯ç¼ºå°‘å¼‚å¸¸æ€»æ•°")
            
            # æµ‹è¯•4: æ•°æ®è´¨é‡è¯„ä¼°
            quality_score = detector.assess_data_quality(test_data, "TEST001")
            self.assertIsInstance(quality_score, (int, float), "æ•°æ®è´¨é‡è¯„åˆ†æ ¼å¼é”™è¯¯")
            self.assertGreaterEqual(quality_score, 0, "è´¨é‡è¯„åˆ†åº”å¤§äºç­‰äº0")
            self.assertLessEqual(quality_score, 1, "è´¨é‡è¯„åˆ†åº”å°äºç­‰äº1")
            
            self.acceptance_result.add_test_result(
                "æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½", True,
                {
                    "anomalies_detected": len(anomalies),
                    "repairs_successful": repair_count,
                    "quality_score": quality_score
                },
                "2.1"
            )
            
            print(f"  âœ… æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½æµ‹è¯•é€šè¿‡ (æ£€æµ‹åˆ°{len(anomalies)}ä¸ªå¼‚å¸¸ï¼Œä¿®å¤{repair_count}ä¸ª)")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½", False,
                {"error": str(e)}, "2.1"
            )
            self.acceptance_result.add_issue("critical", f"æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½å¼‚å¸¸: {e}", "2.1")
            print(f"  âŒ æ•°æ®å¼‚å¸¸æ£€æµ‹åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise

    def test_smart_data_integration_functionality(self):
        """æµ‹è¯•æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½ - éœ€æ±‚3.1, 3.2"""
        print("\n--- æµ‹è¯•æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½ ---")
        
        try:
            # åˆ›å»ºæ•°æ®é›†æˆç»„ä»¶
            config = UIIntegrationConfig(
                enable_caching=True,
                cache_expiry_seconds=300,
                enable_predictive_loading=True,
                enable_adaptive_caching=True,
                max_cache_size=1000
            )
            
            with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
                integration = SmartDataIntegration(config)
            
            # æµ‹è¯•1: ç¼“å­˜åŠŸèƒ½
            test_data = {
                'symbol': 'TEST001',
                'data': [{'price': 100.0, 'volume': 1000}],
                'timestamp': time.time()
            }
            
            cache_key = "test_cache_key"
            integration._put_to_intelligent_cache(cache_key, test_data, "high", 300)
            
            cached_data = integration._get_from_intelligent_cache(cache_key)
            self.assertIsNotNone(cached_data, "ç¼“å­˜æ•°æ®è·å–å¤±è´¥")
            self.assertEqual(cached_data['symbol'], 'TEST001', "ç¼“å­˜æ•°æ®ä¸åŒ¹é…")
            
            # æµ‹è¯•2: ä½¿ç”¨æ¨¡å¼è®°å½•
            for i in range(10):
                integration._record_usage_pattern(
                    f"widget_{i % 3}",
                    f"symbol_{i:03d}",
                    "realtime" if i % 2 == 0 else "daily"
                )
            
            # æµ‹è¯•3: æ€§èƒ½ä¼˜åŒ–
            optimization_result = integration.optimize_performance()
            self.assertIsInstance(optimization_result, dict, "æ€§èƒ½ä¼˜åŒ–ç»“æœæ ¼å¼é”™è¯¯")
            
            # æµ‹è¯•4: ç»Ÿè®¡ä¿¡æ¯
            stats = integration.get_statistics()
            self.assertIsInstance(stats, dict, "ç»Ÿè®¡ä¿¡æ¯æ ¼å¼é”™è¯¯")
            self.assertIn('cache_stats', stats, "ç»Ÿè®¡ä¿¡æ¯ç¼ºå°‘ç¼“å­˜ç»Ÿè®¡")
            self.assertIn('performance_stats', stats, "ç»Ÿè®¡ä¿¡æ¯ç¼ºå°‘æ€§èƒ½ç»Ÿè®¡")
            
            # æµ‹è¯•5: æ•°æ®æºé€‰æ‹©
            mock_widget = Mock()
            mock_widget.widget_type = "test_widget"
            
            with patch.object(integration, '_select_optimal_data_source') as mock_select:
                mock_select.return_value = "tongdaxin"
                result = integration.check_data_for_widget(mock_widget, "TEST001", "realtime")
                mock_select.assert_called()
            
            integration.close()
            
            self.acceptance_result.add_test_result(
                "æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½", True,
                {
                    "cache_operations": True,
                    "usage_patterns_recorded": 10,
                    "optimization_successful": True
                },
                "3.1"
            )
            
            print("  âœ… æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½", False,
                {"error": str(e)}, "3.1"
            )
            self.acceptance_result.add_issue("critical", f"æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½å¼‚å¸¸: {e}", "3.1")
            print(f"  âŒ æ™ºèƒ½æ•°æ®é›†æˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise


class TestAIEnhancedFeatures(FinalAcceptanceTest):
    """AIå¢å¼ºåŠŸèƒ½éªŒæ”¶æµ‹è¯•"""

    def test_config_recommendation_engine(self):
        """æµ‹è¯•é…ç½®æ¨èå¼•æ“ - éœ€æ±‚4.1"""
        print("\n--- æµ‹è¯•é…ç½®æ¨èå¼•æ“ ---")
        
        try:
            # åˆ›å»ºæ¨èå¼•æ“
            engine = ConfigRecommendationEngine(self.db_path)
            
            # åˆ›å»ºå†å²é…ç½®æ•°æ®
            manager = IntelligentConfigManager(self.db_path)
            
            # æ·»åŠ å¤šä¸ªå†å²é…ç½®å’Œæ€§èƒ½æ•°æ®
            for i in range(5):
                config = ImportTaskConfig(
                    task_id=f"recommendation_test_{i}",
                    name=f"æ¨èæµ‹è¯•ä»»åŠ¡{i}",
                    data_source="tongdaxin",
                    asset_type="stock",
                    data_type="kline",
                    symbols=[f"{i:06d}"],
                    frequency=DataFrequency.DAILY,
                    mode=ImportMode.BATCH,
                    max_workers=2 + i,
                    batch_size=500 + i * 100
                )
                
                manager.add_import_task(config)
                
                # è®°å½•æ€§èƒ½æ•°æ®
                manager.record_performance_feedback(
                    config=config,
                    execution_time=60 + i * 10,
                    success_rate=0.9 + i * 0.02,
                    error_rate=0.1 - i * 0.02,
                    throughput=1000 + i * 200
                )
            
            # æµ‹è¯•1: è·å–é…ç½®æ¨è
            recommendations = engine.get_recommendations(
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols_count=100
            )
            
            self.assertIsInstance(recommendations, list, "æ¨èç»“æœæ ¼å¼é”™è¯¯")
            self.assertGreater(len(recommendations), 0, "åº”è¯¥æœ‰é…ç½®æ¨è")
            
            # éªŒè¯æ¨èå†…å®¹
            for rec in recommendations:
                self.assertIn('parameter', rec, "æ¨èç¼ºå°‘å‚æ•°å")
                self.assertIn('recommended_value', rec, "æ¨èç¼ºå°‘æ¨èå€¼")
                self.assertIn('confidence', rec, "æ¨èç¼ºå°‘ç½®ä¿¡åº¦")
                self.assertIn('reason', rec, "æ¨èç¼ºå°‘åŸå› ")
                
                # ç½®ä¿¡åº¦åº”è¯¥åœ¨0-1ä¹‹é—´
                self.assertGreaterEqual(rec['confidence'], 0, "ç½®ä¿¡åº¦åº”å¤§äºç­‰äº0")
                self.assertLessEqual(rec['confidence'], 1, "ç½®ä¿¡åº¦åº”å°äºç­‰äº1")
            
            # æµ‹è¯•2: åŸºäºç¯å¢ƒçš„æ¨è
            env_recommendations = engine.get_environment_based_recommendations(
                current_load=0.7,
                available_memory=8192,
                network_latency=50
            )
            
            self.assertIsInstance(env_recommendations, list, "ç¯å¢ƒæ¨èç»“æœæ ¼å¼é”™è¯¯")
            
            # æµ‹è¯•3: æ¨èç»Ÿè®¡
            stats = engine.get_recommendation_statistics()
            self.assertIsInstance(stats, dict, "æ¨èç»Ÿè®¡æ ¼å¼é”™è¯¯")
            
            self.acceptance_result.add_test_result(
                "é…ç½®æ¨èå¼•æ“", True,
                {
                    "recommendations_count": len(recommendations),
                    "env_recommendations_count": len(env_recommendations),
                    "avg_confidence": np.mean([r['confidence'] for r in recommendations]) if recommendations else 0
                },
                "4.1"
            )
            
            print(f"  âœ… é…ç½®æ¨èå¼•æ“æµ‹è¯•é€šè¿‡ (ç”Ÿæˆ{len(recommendations)}ä¸ªæ¨è)")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "é…ç½®æ¨èå¼•æ“", False,
                {"error": str(e)}, "4.1"
            )
            self.acceptance_result.add_issue("major", f"é…ç½®æ¨èå¼•æ“å¼‚å¸¸: {e}", "4.1")
            print(f"  âŒ é…ç½®æ¨èå¼•æ“æµ‹è¯•å¤±è´¥: {e}")

    def test_config_impact_analyzer(self):
        """æµ‹è¯•é…ç½®å½±å“åˆ†æå™¨ - éœ€æ±‚4.2"""
        print("\n--- æµ‹è¯•é…ç½®å½±å“åˆ†æå™¨ ---")
        
        try:
            # åˆ›å»ºå½±å“åˆ†æå™¨
            analyzer = ConfigImpactAnalyzer(self.db_path)
            
            # åˆ›å»ºåŸºç¡€é…ç½®
            current_config = ImportTaskConfig(
                task_id="impact_test",
                name="å½±å“åˆ†ææµ‹è¯•",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=4,
                batch_size=1000
            )
            
            # åˆ›å»ºå˜æ›´é…ç½®
            new_config = ImportTaskConfig(
                task_id="impact_test",
                name="å½±å“åˆ†ææµ‹è¯•",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=8,  # å˜æ›´ï¼šå¢åŠ å·¥ä½œçº¿ç¨‹
                batch_size=2000  # å˜æ›´ï¼šå¢åŠ æ‰¹æ¬¡å¤§å°
            )
            
            # æµ‹è¯•1: é…ç½®å˜æ›´å½±å“åˆ†æ
            impact_analysis = analyzer.analyze_config_change(current_config, new_config)
            
            self.assertIsInstance(impact_analysis, dict, "å½±å“åˆ†æç»“æœæ ¼å¼é”™è¯¯")
            self.assertIn('performance_impact', impact_analysis, "å½±å“åˆ†æç¼ºå°‘æ€§èƒ½å½±å“")
            self.assertIn('risk_assessment', impact_analysis, "å½±å“åˆ†æç¼ºå°‘é£é™©è¯„ä¼°")
            self.assertIn('recommendations', impact_analysis, "å½±å“åˆ†æç¼ºå°‘å»ºè®®")
            
            # æµ‹è¯•2: é£é™©è¯„ä¼°
            risk_assessment = impact_analysis['risk_assessment']
            self.assertIn('overall_risk_level', risk_assessment, "é£é™©è¯„ä¼°ç¼ºå°‘æ€»ä½“é£é™©ç­‰çº§")
            self.assertIn('identified_risks', risk_assessment, "é£é™©è¯„ä¼°ç¼ºå°‘è¯†åˆ«çš„é£é™©")
            
            # æµ‹è¯•3: æ€§èƒ½é¢„æµ‹
            performance_impact = impact_analysis['performance_impact']
            self.assertIn('execution_time_change', performance_impact, "æ€§èƒ½å½±å“ç¼ºå°‘æ‰§è¡Œæ—¶é—´å˜åŒ–")
            self.assertIn('throughput_change', performance_impact, "æ€§èƒ½å½±å“ç¼ºå°‘ååé‡å˜åŒ–")
            
            # æµ‹è¯•4: å†²çªæ£€æµ‹
            conflicts = analyzer.detect_configuration_conflicts([current_config, new_config])
            self.assertIsInstance(conflicts, list, "å†²çªæ£€æµ‹ç»“æœæ ¼å¼é”™è¯¯")
            
            # æµ‹è¯•5: æ‰¹é‡åˆ†æ
            configs_to_analyze = [current_config, new_config]
            batch_analysis = analyzer.analyze_multiple_configs(configs_to_analyze)
            self.assertIsInstance(batch_analysis, dict, "æ‰¹é‡åˆ†æç»“æœæ ¼å¼é”™è¯¯")
            
            self.acceptance_result.add_test_result(
                "é…ç½®å½±å“åˆ†æå™¨", True,
                {
                    "impact_analysis_complete": True,
                    "risks_identified": len(risk_assessment.get('identified_risks', [])),
                    "conflicts_detected": len(conflicts)
                },
                "4.2"
            )
            
            print("  âœ… é…ç½®å½±å“åˆ†æå™¨æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "é…ç½®å½±å“åˆ†æå™¨", False,
                {"error": str(e)}, "4.2"
            )
            self.acceptance_result.add_issue("major", f"é…ç½®å½±å“åˆ†æå™¨å¼‚å¸¸: {e}", "4.2")
            print(f"  âŒ é…ç½®å½±å“åˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")


class TestPerformanceRequirements(FinalAcceptanceTest):
    """æ€§èƒ½éœ€æ±‚éªŒæ”¶æµ‹è¯•"""

    def test_system_performance_requirements(self):
        """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½éœ€æ±‚ - éœ€æ±‚5.1"""
        print("\n--- æµ‹è¯•ç³»ç»Ÿæ€§èƒ½éœ€æ±‚ ---")
        
        try:
            # åˆ›å»ºç»„ä»¶
            manager = IntelligentConfigManager(self.db_path)
            
            # æ€§èƒ½æµ‹è¯•1: é…ç½®ç®¡ç†å“åº”æ—¶é—´
            start_time = time.time()
            
            # æ‰¹é‡æ·»åŠ é…ç½®
            configs_added = 0
            for i in range(100):
                config = ImportTaskConfig(
                    task_id=f"perf_test_{i}",
                    name=f"æ€§èƒ½æµ‹è¯•ä»»åŠ¡{i}",
                    data_source="tongdaxin",
                    asset_type="stock",
                    data_type="kline",
                    symbols=[f"{i:06d}"],
                    frequency=DataFrequency.DAILY,
                    mode=ImportMode.BATCH
                )
                
                if manager.add_import_task(config):
                    configs_added += 1
            
            config_creation_time = time.time() - start_time
            avg_config_creation_time = config_creation_time / configs_added if configs_added > 0 else float('inf')
            
            # æ€§èƒ½æµ‹è¯•2: æŸ¥è¯¢å“åº”æ—¶é—´
            start_time = time.time()
            
            for i in range(50):
                manager.get_import_task(f"perf_test_{i}")
            
            query_time = time.time() - start_time
            avg_query_time = query_time / 50
            
            # æ€§èƒ½æµ‹è¯•3: ç»Ÿè®¡è®¡ç®—æ—¶é—´
            start_time = time.time()
            stats = manager.get_intelligent_statistics()
            stats_calculation_time = time.time() - start_time
            
            # æ€§èƒ½æµ‹è¯•4: å¹¶å‘æ“ä½œæ€§èƒ½
            def concurrent_operation(task_id):
                config = ImportTaskConfig(
                    task_id=f"concurrent_test_{task_id}",
                    name=f"å¹¶å‘æµ‹è¯•ä»»åŠ¡{task_id}",
                    data_source="tongdaxin",
                    asset_type="stock",
                    data_type="kline",
                    symbols=[f"{task_id:06d}"],
                    frequency=DataFrequency.DAILY,
                    mode=ImportMode.BATCH
                )
                return manager.add_import_task(config)
            
            start_time = time.time()
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = [executor.submit(concurrent_operation, i) for i in range(50)]
                concurrent_results = [f.result() for f in as_completed(futures)]
            
            concurrent_operation_time = time.time() - start_time
            successful_concurrent_ops = sum(concurrent_results)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.acceptance_result.add_performance_metric(
                "å¹³å‡é…ç½®åˆ›å»ºæ—¶é—´", avg_config_creation_time * 1000, 100, "ms"
            )
            self.acceptance_result.add_performance_metric(
                "å¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´", avg_query_time * 1000, 50, "ms"
            )
            self.acceptance_result.add_performance_metric(
                "ç»Ÿè®¡è®¡ç®—æ—¶é—´", stats_calculation_time * 1000, 500, "ms"
            )
            self.acceptance_result.add_performance_metric(
                "å¹¶å‘æ“ä½œå®Œæˆæ—¶é—´", concurrent_operation_time, 10, "s"
            )
            self.acceptance_result.add_performance_metric(
                "å¹¶å‘æ“ä½œæˆåŠŸç‡", successful_concurrent_ops / 50, 0.95, ""
            )
            
            # æ€§èƒ½æ–­è¨€
            self.assertLess(avg_config_creation_time, 0.1, "é…ç½®åˆ›å»ºæ—¶é—´è¿‡é•¿")
            self.assertLess(avg_query_time, 0.05, "æŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿")
            self.assertLess(stats_calculation_time, 0.5, "ç»Ÿè®¡è®¡ç®—æ—¶é—´è¿‡é•¿")
            self.assertGreater(successful_concurrent_ops / 50, 0.95, "å¹¶å‘æ“ä½œæˆåŠŸç‡è¿‡ä½")
            
            self.acceptance_result.add_test_result(
                "ç³»ç»Ÿæ€§èƒ½éœ€æ±‚", True,
                {
                    "configs_created": configs_added,
                    "avg_creation_time_ms": avg_config_creation_time * 1000,
                    "avg_query_time_ms": avg_query_time * 1000,
                    "concurrent_success_rate": successful_concurrent_ops / 50
                },
                "5.1"
            )
            
            print(f"  âœ… ç³»ç»Ÿæ€§èƒ½éœ€æ±‚æµ‹è¯•é€šè¿‡")
            print(f"    - å¹³å‡é…ç½®åˆ›å»ºæ—¶é—´: {avg_config_creation_time*1000:.2f}ms")
            print(f"    - å¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´: {avg_query_time*1000:.2f}ms")
            print(f"    - å¹¶å‘æ“ä½œæˆåŠŸç‡: {successful_concurrent_ops/50:.2%}")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "ç³»ç»Ÿæ€§èƒ½éœ€æ±‚", False,
                {"error": str(e)}, "5.1"
            )
            self.acceptance_result.add_issue("critical", f"ç³»ç»Ÿæ€§èƒ½éœ€æ±‚ä¸æ»¡è¶³: {e}", "5.1")
            print(f"  âŒ ç³»ç»Ÿæ€§èƒ½éœ€æ±‚æµ‹è¯•å¤±è´¥: {e}")
            raise

    def test_memory_usage_requirements(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨éœ€æ±‚ - éœ€æ±‚5.2"""
        print("\n--- æµ‹è¯•å†…å­˜ä½¿ç”¨éœ€æ±‚ ---")
        
        try:
            import psutil
            process = psutil.Process()
            
            # è®°å½•åˆå§‹å†…å­˜
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤§é‡ç»„ä»¶å’Œæ•°æ®
            components = []
            
            # åˆ›å»ºå¤šä¸ªé…ç½®ç®¡ç†å™¨å®ä¾‹
            for i in range(10):
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                manager = IntelligentConfigManager(temp_db.name)
                
                # æ·»åŠ å¤§é‡é…ç½®
                for j in range(50):
                    config = ImportTaskConfig(
                        task_id=f"memory_test_{i}_{j}",
                        name=f"å†…å­˜æµ‹è¯•ä»»åŠ¡{i}-{j}",
                        data_source="tongdaxin",
                        asset_type="stock",
                        data_type="kline",
                        symbols=[f"{j:06d}"],
                        frequency=DataFrequency.DAILY,
                        mode=ImportMode.BATCH
                    )
                    manager.add_import_task(config)
                
                components.append((manager, temp_db.name))
            
            # è®°å½•å³°å€¼å†…å­˜
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = peak_memory - initial_memory
            
            # æ¸…ç†ç»„ä»¶
            for manager, db_path in components:
                del manager
                try:
                    os.unlink(db_path)
                except:
                    pass
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # è®°å½•æ¸…ç†åå†…å­˜
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_recovered = peak_memory - final_memory
            
            # è®°å½•å†…å­˜æŒ‡æ ‡
            self.acceptance_result.add_performance_metric(
                "å†…å­˜ä½¿ç”¨å¢é•¿", memory_increase, 500, "MB"
            )
            self.acceptance_result.add_performance_metric(
                "å†…å­˜å›æ”¶ç‡", memory_recovered / memory_increase if memory_increase > 0 else 1, 0.8, ""
            )
            
            # å†…å­˜ä½¿ç”¨æ–­è¨€
            self.assertLess(memory_increase, 500, "å†…å­˜ä½¿ç”¨å¢é•¿è¿‡å¤š")
            self.assertGreater(memory_recovered / memory_increase if memory_increase > 0 else 1, 0.7, "å†…å­˜å›æ”¶ç‡è¿‡ä½")
            
            self.acceptance_result.add_test_result(
                "å†…å­˜ä½¿ç”¨éœ€æ±‚", True,
                {
                    "initial_memory_mb": initial_memory,
                    "peak_memory_mb": peak_memory,
                    "final_memory_mb": final_memory,
                    "memory_increase_mb": memory_increase,
                    "memory_recovery_rate": memory_recovered / memory_increase if memory_increase > 0 else 1
                },
                "5.2"
            )
            
            print(f"  âœ… å†…å­˜ä½¿ç”¨éœ€æ±‚æµ‹è¯•é€šè¿‡")
            print(f"    - å†…å­˜å¢é•¿: {memory_increase:.1f}MB")
            print(f"    - å†…å­˜å›æ”¶ç‡: {memory_recovered/memory_increase:.2%}" if memory_increase > 0 else "    - å†…å­˜å›æ”¶ç‡: 100%")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "å†…å­˜ä½¿ç”¨éœ€æ±‚", False,
                {"error": str(e)}, "5.2"
            )
            self.acceptance_result.add_issue("major", f"å†…å­˜ä½¿ç”¨éœ€æ±‚ä¸æ»¡è¶³: {e}", "5.2")
            print(f"  âŒ å†…å­˜ä½¿ç”¨éœ€æ±‚æµ‹è¯•å¤±è´¥: {e}")


class TestUserWorkflows(FinalAcceptanceTest):
    """ç”¨æˆ·å·¥ä½œæµç¨‹éªŒæ”¶æµ‹è¯•"""

    def test_complete_data_import_workflow(self):
        """æµ‹è¯•å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹ - éœ€æ±‚6.1"""
        print("\n--- æµ‹è¯•å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹ ---")
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´çš„ç”¨æˆ·å·¥ä½œæµç¨‹
            
            # æ­¥éª¤1: åˆ›å»ºé…ç½®ç®¡ç†å™¨
            manager = IntelligentConfigManager(self.db_path)
            
            # æ­¥éª¤2: åˆ›å»ºå¯¼å…¥ä»»åŠ¡é…ç½®
            config = ImportTaskConfig(
                task_id="workflow_test",
                name="å·¥ä½œæµç¨‹æµ‹è¯•ä»»åŠ¡",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001", "000002", "000300"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=4,
                batch_size=1000
            )
            
            # æ­¥éª¤3: æ·»åŠ ä»»åŠ¡
            success = manager.add_import_task(config)
            self.assertTrue(success, "æ·»åŠ å¯¼å…¥ä»»åŠ¡å¤±è´¥")
            
            # æ­¥éª¤4: è·å–æ™ºèƒ½æ¨è
            engine = ConfigRecommendationEngine(self.db_path)
            recommendations = engine.get_recommendations(
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols_count=len(config.symbols)
            )
            
            # æ­¥éª¤5: åˆ†æé…ç½®å½±å“
            analyzer = ConfigImpactAnalyzer(self.db_path)
            
            # åˆ›å»ºä¼˜åŒ–åçš„é…ç½®
            optimized_config = ImportTaskConfig(
                task_id="workflow_test",
                name="å·¥ä½œæµç¨‹æµ‹è¯•ä»»åŠ¡",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001", "000002", "000300"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=6,  # æ ¹æ®æ¨èè°ƒæ•´
                batch_size=1500  # æ ¹æ®æ¨èè°ƒæ•´
            )
            
            impact_analysis = analyzer.analyze_config_change(config, optimized_config)
            
            # æ­¥éª¤6: æ¨¡æ‹Ÿæ‰§è¡Œå’Œæ€§èƒ½åé¦ˆ
            execution_times = []
            success_rates = []
            
            for i in range(5):  # æ¨¡æ‹Ÿ5æ¬¡æ‰§è¡Œ
                # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´ï¼ˆåŸºäºé…ç½®å‚æ•°ï¼‰
                base_time = 60
                worker_factor = optimized_config.max_workers / 4
                batch_factor = optimized_config.batch_size / 1000
                
                execution_time = base_time / worker_factor * batch_factor + np.random.uniform(-10, 10)
                success_rate = 0.95 + np.random.uniform(-0.05, 0.05)
                
                execution_times.append(execution_time)
                success_rates.append(success_rate)
                
                # è®°å½•æ€§èƒ½åé¦ˆ
                manager.record_performance_feedback(
                    config=optimized_config,
                    execution_time=execution_time,
                    success_rate=success_rate,
                    error_rate=1 - success_rate,
                    throughput=len(config.symbols) * 1000 / execution_time
                )
            
            # æ­¥éª¤7: æ•°æ®å¼‚å¸¸æ£€æµ‹
            anomaly_config = AnomalyDetectionConfig(auto_repair_enabled=True)
            detector = DataAnomalyDetector(anomaly_config, self.db_path)
            
            # æ¨¡æ‹Ÿå¯¼å…¥çš„æ•°æ®
            imported_data = pd.DataFrame({
                'timestamp': pd.date_range('2024-01-01', periods=1000, freq='D'),
                'symbol': np.random.choice(['000001', '000002', '000300'], 1000),
                'price': np.random.normal(100, 10, 1000),
                'volume': np.random.randint(1000, 100000, 1000)
            })
            
            # æ£€æµ‹å¼‚å¸¸
            anomalies = detector.detect_anomalies(
                data=imported_data,
                data_source="workflow_test",
                symbol="MIXED",
                data_type="kline"
            )
            
            # æ­¥éª¤8: è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = manager.get_intelligent_statistics()
            
            # éªŒè¯å·¥ä½œæµç¨‹å®Œæ•´æ€§
            workflow_checks = [
                ("ä»»åŠ¡åˆ›å»º", success),
                ("æ¨èè·å–", len(recommendations) > 0),
                ("å½±å“åˆ†æ", 'performance_impact' in impact_analysis),
                ("æ€§èƒ½è®°å½•", len(execution_times) == 5),
                ("å¼‚å¸¸æ£€æµ‹", isinstance(anomalies, list)),
                ("ç»Ÿè®¡è·å–", isinstance(final_stats, dict))
            ]
            
            all_checks_passed = all(check[1] for check in workflow_checks)
            
            # è®¡ç®—å·¥ä½œæµç¨‹æŒ‡æ ‡
            avg_execution_time = np.mean(execution_times)
            avg_success_rate = np.mean(success_rates)
            
            self.acceptance_result.add_test_result(
                "å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹", all_checks_passed,
                {
                    "workflow_steps_completed": len([c for c in workflow_checks if c[1]]),
                    "total_workflow_steps": len(workflow_checks),
                    "avg_execution_time": avg_execution_time,
                    "avg_success_rate": avg_success_rate,
                    "anomalies_detected": len(anomalies),
                    "recommendations_received": len(recommendations)
                },
                "6.1"
            )
            
            if all_checks_passed:
                print(f"  âœ… å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
                print(f"    - å·¥ä½œæµç¨‹æ­¥éª¤: {len(workflow_checks)}/{len(workflow_checks)} å®Œæˆ")
                print(f"    - å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_execution_time:.2f}ç§’")
                print(f"    - å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2%}")
            else:
                failed_checks = [check[0] for check in workflow_checks if not check[1]]
                self.acceptance_result.add_issue("major", f"å·¥ä½œæµç¨‹æ­¥éª¤å¤±è´¥: {', '.join(failed_checks)}", "6.1")
                print(f"  âŒ å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥ï¼Œå¤±è´¥æ­¥éª¤: {', '.join(failed_checks)}")
            
            self.assertTrue(all_checks_passed, f"å·¥ä½œæµç¨‹æ£€æŸ¥å¤±è´¥: {[c[0] for c in workflow_checks if not c[1]]}")
            
        except Exception as e:
            self.acceptance_result.add_test_result(
                "å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹", False,
                {"error": str(e)}, "6.1"
            )
            self.acceptance_result.add_issue("critical", f"æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹å¼‚å¸¸: {e}", "6.1")
            print(f"  âŒ å®Œæ•´æ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            raise


def run_final_acceptance_tests():
    """è¿è¡Œæœ€ç»ˆéªŒæ”¶æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œæœ€ç»ˆç³»ç»Ÿé›†æˆå’Œç”¨æˆ·éªŒæ”¶æµ‹è¯•...")
    print("=" * 100)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestCoreSystemFunctionality,
        TestAIEnhancedFeatures,
        TestPerformanceRequirements,
        TestUserWorkflows
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(test_suite)
    
    return result.wasSuccessful(), len(result.failures), len(result.errors)


if __name__ == "__main__":
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    os.environ['TESTING'] = '1'
    
    # è¿è¡Œæœ€ç»ˆéªŒæ”¶æµ‹è¯•
    success, failures, errors = run_final_acceptance_tests()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    exit_code = 0 if success else 1
    exit(exit_code)
