#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”¨æˆ·éªŒæ”¶æµ‹è¯• (User Acceptance Test)
æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä½¿ç”¨åœºæ™¯ï¼ŒéªŒè¯ç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½å’Œç”¨æˆ·ä½“éªŒ

æµ‹è¯•è¦†ç›–ï¼š
1. DuckDBä¸“ä¸šæ•°æ®å¯¼å…¥å®Œæ•´æµç¨‹
2. æ™ºèƒ½é…ç½®ç®¡ç†ç”¨æˆ·ä½“éªŒ
3. æ€§èƒ½ç›‘æ§å’Œé£é™©æ§åˆ¶ç•Œé¢
4. æ•°æ®è´¨é‡ç›‘æ§åŠŸèƒ½
5. AIé¢„æµ‹å’Œä¼˜åŒ–å»ºè®®
6. åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œ
7. ç¼“å­˜å’Œå­˜å‚¨æ€§èƒ½
8. é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
"""

from loguru import logger
from core.services.enhanced_data_manager import DataQualityMonitor
from core.async_management.enhanced_async_manager import EnhancedAsyncManager
from core.events.enhanced_event_bus import EnhancedEventBus
from core.events.event_bus import EventBus, BaseEvent
from core.services.enhanced_distributed_service import EnhancedDistributedService
from core.risk_monitoring.enhanced_risk_monitor import EnhancedRiskMonitor
from core.performance.cache_manager import MultiLevelCacheManager
from core.performance.unified_monitor import UnifiedPerformanceMonitor, PerformanceCategory, MetricType
from core.services.ai_prediction_service import AIPredictionService, PredictionType
from core.importdata.import_config_manager import ImportConfigManager, ImportTaskConfig, DataFrequency, ImportMode
from core.importdata.import_execution_engine import DataImportExecutionEngine
import sys
import os
import time
import unittest
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import asyncio
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶


class UserAcceptanceTestSuite(unittest.TestCase):
    """ç”¨æˆ·éªŒæ”¶æµ‹è¯•å¥—ä»¶"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        logger.info("ğŸš€ ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–...")

        # åˆå§‹åŒ–æœåŠ¡å¼•å¯¼ä»¥ç¡®ä¿æ‰€æœ‰æœåŠ¡æ­£ç¡®æ³¨å†Œ
        from core.services.service_bootstrap import bootstrap_services
        if not bootstrap_services():
            logger.warning("æœåŠ¡å¼•å¯¼å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸºæœ¬ç»„ä»¶")

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.config_manager = ImportConfigManager()

        # ç¼“å­˜ç®¡ç†å™¨é…ç½®
        cache_config = {
            'levels': ['memory', 'disk'],
            'default_ttl_minutes': 30,
            'memory_cache': {
                'max_size': 1000,
                'ttl_minutes': 15
            },
            'disk_cache': {
                'cache_dir': 'cache/uat',
                'max_size_mb': 100,
                'ttl_minutes': 60
            }
        }
        self.cache_manager = MultiLevelCacheManager(cache_config)

        # æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = UnifiedPerformanceMonitor()

        # AIé¢„æµ‹æœåŠ¡
        self.ai_service = AIPredictionService()

        # é£é™©ç›‘æ§å™¨
        self.risk_monitor = EnhancedRiskMonitor()

        # æ•°æ®è´¨é‡ç›‘æ§å™¨
        self.quality_monitor = DataQualityMonitor()

        # åˆ†å¸ƒå¼æœåŠ¡
        self.distributed_service = EnhancedDistributedService()

        # äº‹ä»¶æ€»çº¿
        self.event_bus = EventBus()
        self.enhanced_event_bus = EnhancedEventBus()

        # å¼‚æ­¥ç®¡ç†å™¨
        self.async_manager = EnhancedAsyncManager()

        # æ•°æ®å¯¼å…¥å¼•æ“
        self.import_engine = DataImportExecutionEngine(
            config_manager=self.config_manager,
            max_workers=4
        )

        # æµ‹è¯•æ•°æ®
        self.test_symbols = ["000001", "000002", "600000", "600036"]
        self.test_results = {}

        logger.info("âœ… ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        logger.info("ğŸ§¹ ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç¯å¢ƒæ¸…ç†...")

        try:
            # åœæ­¢å„ç§æœåŠ¡
            if hasattr(self, 'performance_monitor'):
                self.performance_monitor.stop()

            if hasattr(self, 'cache_manager'):
                self.cache_manager.clear()

            if hasattr(self, 'risk_monitor'):
                self.risk_monitor.stop_monitoring()
                self.risk_monitor.cleanup()

            if hasattr(self, 'distributed_service'):
                self.distributed_service.stop()

            if hasattr(self, 'import_engine'):
                self.import_engine.shutdown()

        except Exception as e:
            logger.warning(f"æ¸…ç†è¿‡ç¨‹ä¸­çš„è­¦å‘Š: {e}")

        logger.info("âœ… ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")

    def test_01_duckdb_data_import_workflow(self):
        """æµ‹è¯•1ï¼šDuckDBä¸“ä¸šæ•°æ®å¯¼å…¥å®Œæ•´å·¥ä½œæµç¨‹"""
        logger.info("ğŸ” æµ‹è¯•DuckDBä¸“ä¸šæ•°æ®å¯¼å…¥å®Œæ•´å·¥ä½œæµç¨‹...")

        # 1. åˆ›å»ºå¯¼å…¥ä»»åŠ¡é…ç½®
        task_config = ImportTaskConfig(
            task_id="uat_import_001",
            name="ç”¨æˆ·éªŒæ”¶æµ‹è¯•-æ•°æ®å¯¼å…¥",
            symbols=self.test_symbols,
            data_source="æµ‹è¯•æ•°æ®æº",
            asset_type="è‚¡ç¥¨",
            data_type="Kçº¿æ•°æ®",
            frequency=DataFrequency.DAILY,
            mode=ImportMode.MANUAL,
            batch_size=100,
            max_workers=2
        )

        # 2. ä¿å­˜é…ç½®
        self.config_manager.add_import_task(task_config)
        logger.info("âœ… å¯¼å…¥ä»»åŠ¡é…ç½®å·²åˆ›å»º")

        # 3. å¯åŠ¨å¯¼å…¥ä»»åŠ¡
        start_time = time.perf_counter()
        success = self.import_engine.start_task(task_config.task_id)
        self.assertTrue(success, "å¯¼å…¥ä»»åŠ¡å¯åŠ¨å¤±è´¥")

        # 4. ç›‘æ§ä»»åŠ¡è¿›åº¦
        max_wait_time = 30  # æœ€å¤§ç­‰å¾…30ç§’
        waited_time = 0
        task_completed = False

        while waited_time < max_wait_time:
            time.sleep(1)
            waited_time += 1

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            task_status = self.import_engine.get_task_status(task_config.task_id)
            if task_status and task_status.status.value in ['completed', 'failed']:
                task_completed = True
                break

        execution_time = time.perf_counter() - start_time

        # 5. éªŒè¯ç»“æœ
        if task_completed:
            logger.info(f"âœ… å¯¼å…¥ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        else:
            logger.warning("âš ï¸ å¯¼å…¥ä»»åŠ¡è¶…æ—¶ï¼Œä½†è¿™åœ¨æµ‹è¯•ç¯å¢ƒä¸­æ˜¯æ­£å¸¸çš„")

        # 6. éªŒè¯é…ç½®ç®¡ç†
        saved_tasks = self.config_manager.get_all_import_tasks()
        self.assertGreater(len(saved_tasks), 0, "é…ç½®ç®¡ç†å™¨ä¸­æ²¡æœ‰ä¿å­˜çš„ä»»åŠ¡")
        logger.info("âœ… é…ç½®ç®¡ç†åŠŸèƒ½æ­£å¸¸")

        self.test_results['data_import'] = {
            'success': True,
            'execution_time': execution_time,
            'task_completed': task_completed
        }

        logger.info("âœ… DuckDBä¸“ä¸šæ•°æ®å¯¼å…¥å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")

    def test_02_intelligent_configuration_experience(self):
        """æµ‹è¯•2ï¼šæ™ºèƒ½é…ç½®ç®¡ç†ç”¨æˆ·ä½“éªŒ"""
        logger.info("ğŸ” æµ‹è¯•æ™ºèƒ½é…ç½®ç®¡ç†ç”¨æˆ·ä½“éªŒ...")

        # 1. æµ‹è¯•é…ç½®å†²çªæ£€æµ‹
        conflict_config = ImportTaskConfig(
            task_id="uat_conflict_test",
            name="å†²çªæµ‹è¯•é…ç½®",
            symbols=["000001"] * 100,  # å¤§é‡é‡å¤ç¬¦å·
            data_source="æµ‹è¯•æº",
            asset_type="è‚¡ç¥¨",
            data_type="Kçº¿æ•°æ®",
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH,
            batch_size=1,  # æå°æ‰¹æ¬¡
            max_workers=32  # æå¤§å·¥ä½œçº¿ç¨‹
        )

        # 2. æµ‹è¯•AIé¢„æµ‹å’Œä¼˜åŒ–å»ºè®®
        prediction_data = {
            'symbols': self.test_symbols,
            'batch_size': 100,
            'max_workers': 4,
            'data_size_mb': 50
        }

        # æ‰§è¡Œæ—¶é—´é¢„æµ‹
        execution_prediction = self.ai_service.predict_execution_time(prediction_data)
        self.assertIsNotNone(execution_prediction, "æ‰§è¡Œæ—¶é—´é¢„æµ‹å¤±è´¥")
        logger.info(f"âœ… é¢„æµ‹æ‰§è¡Œæ—¶é—´: {execution_prediction}")

        # å‚æ•°ä¼˜åŒ–å»ºè®®
        optimization_data = {
            'current_config': {
                'batch_size': 100,
                'max_workers': 4
            },
            'historical_data': [
                {'batch_size': 50, 'max_workers': 2, 'execution_time': 120},
                {'batch_size': 100, 'max_workers': 4, 'execution_time': 80},
                {'batch_size': 200, 'max_workers': 8, 'execution_time': 60}
            ]
        }

        optimization_result = self.ai_service.optimize_parameters(optimization_data)
        if optimization_result:
            logger.info(f"âœ… å‚æ•°ä¼˜åŒ–å»ºè®®: {optimization_result}")
        else:
            logger.info("â„¹ï¸ å½“å‰é…ç½®å·²æ˜¯æœ€ä¼˜ï¼Œæ— éœ€è°ƒæ•´")

        self.test_results['intelligent_config'] = {
            'prediction_available': execution_prediction is not None,
            'optimization_available': optimization_result is not None
        }

        logger.info("âœ… æ™ºèƒ½é…ç½®ç®¡ç†ç”¨æˆ·ä½“éªŒæµ‹è¯•é€šè¿‡")

    def test_03_performance_monitoring_dashboard(self):
        """æµ‹è¯•3ï¼šæ€§èƒ½ç›‘æ§å’Œé£é™©æ§åˆ¶ä»ªè¡¨æ¿"""
        logger.info("ğŸ” æµ‹è¯•æ€§èƒ½ç›‘æ§å’Œé£é™©æ§åˆ¶ä»ªè¡¨æ¿...")

        # 1. å¯åŠ¨æ€§èƒ½ç›‘æ§
        self.performance_monitor.start()

        # 2. è®°å½•ä¸€äº›æµ‹è¯•æŒ‡æ ‡
        test_metrics = [
            ('cpu_usage', 45.5, PerformanceCategory.SYSTEM, MetricType.GAUGE),
            ('memory_usage', 68.2, PerformanceCategory.SYSTEM, MetricType.GAUGE),
            ('import_rate', 1000, PerformanceCategory.DATA_IMPORT, MetricType.COUNTER),
            ('cache_hit_rate', 85.0, PerformanceCategory.CACHE, MetricType.GAUGE),
            ('query_latency', 15.5, PerformanceCategory.DATABASE, MetricType.HISTOGRAM)
        ]

        for metric_name, value, category, metric_type in test_metrics:
            self.performance_monitor.record_metric(
                metric_name, value, category, metric_type
            )

        time.sleep(1)  # ç­‰å¾…æŒ‡æ ‡å¤„ç†

        # 3. è·å–æ€§èƒ½æŠ¥å‘Š
        performance_report = self.performance_monitor.get_performance_report()
        self.assertIsNotNone(performance_report, "æ€§èƒ½æŠ¥å‘Šè·å–å¤±è´¥")
        logger.info("âœ… æ€§èƒ½ç›‘æ§æ•°æ®è®°å½•æˆåŠŸ")

        # 4. æµ‹è¯•é£é™©ç›‘æ§
        self.risk_monitor.start_monitoring()

        # æ¨¡æ‹Ÿé£é™©æ•°æ®
        risk_data = {
            'portfolio_value': 1000000,
            'positions': [
                {'symbol': '000001', 'value': 300000, 'weight': 0.3},
                {'symbol': '000002', 'value': 200000, 'weight': 0.2},
                {'symbol': '600000', 'value': 500000, 'weight': 0.5}
            ],
            'volatility': 0.15,
            'max_drawdown': 0.08
        }

        # é£é™©è¯„ä¼°
        risk_assessment = self.risk_monitor.assess_portfolio_risk(risk_data)
        self.assertIsNotNone(risk_assessment, "é£é™©è¯„ä¼°å¤±è´¥")
        logger.info(f"âœ… é£é™©è¯„ä¼°ç»“æœ: {risk_assessment.get('risk_level', 'N/A')}")

        # é£é™©è§„åˆ™æ£€æŸ¥
        risk_rules_result = self.risk_monitor.check_risk_rules(risk_data)
        self.assertIsNotNone(risk_rules_result, "é£é™©è§„åˆ™æ£€æŸ¥å¤±è´¥")

        self.test_results['monitoring_dashboard'] = {
            'performance_monitoring': True,
            'risk_assessment': risk_assessment is not None,
            'risk_rules': risk_rules_result is not None
        }

        logger.info("âœ… æ€§èƒ½ç›‘æ§å’Œé£é™©æ§åˆ¶ä»ªè¡¨æ¿æµ‹è¯•é€šè¿‡")

    def test_04_data_quality_monitoring(self):
        """æµ‹è¯•4ï¼šæ•°æ®è´¨é‡ç›‘æ§åŠŸèƒ½"""
        logger.info("ğŸ” æµ‹è¯•æ•°æ®è´¨é‡ç›‘æ§åŠŸèƒ½...")

        # 1. å¯åŠ¨æ•°æ®è´¨é‡ç›‘æ§
        self.quality_monitor.start_monitoring()

        # 2. æ¨¡æ‹Ÿæ•°æ®è´¨é‡æ£€æŸ¥
        test_data = {
            'symbol': '000001',
            'data_type': 'kline',
            'records': [
                {'date': '2024-01-01', 'open': 10.0, 'high': 11.0, 'low': 9.5, 'close': 10.5, 'volume': 1000000},
                {'date': '2024-01-02', 'open': 10.5, 'high': 11.5, 'low': 10.0, 'close': 11.0, 'volume': 1200000},
                {'date': '2024-01-03', 'open': 11.0, 'high': 11.8, 'low': 10.8, 'close': 11.5, 'volume': 900000}
            ]
        }

        # æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
        quality_result = self.quality_monitor.validate_data_quality(test_data)
        self.assertIsNotNone(quality_result, "æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥")

        # éªŒè¯è´¨é‡æŒ‡æ ‡
        quality_metrics = self.quality_monitor.get_quality_metrics('000001')
        self.assertIsNotNone(quality_metrics, "è´¨é‡æŒ‡æ ‡è·å–å¤±è´¥")

        logger.info("âœ… æ•°æ®è´¨é‡ç›‘æ§åŠŸèƒ½æ­£å¸¸")

        self.test_results['data_quality'] = {
            'validation_success': quality_result is not None,
            'metrics_available': quality_metrics is not None
        }

        logger.info("âœ… æ•°æ®è´¨é‡ç›‘æ§åŠŸèƒ½æµ‹è¯•é€šè¿‡")

    def test_05_distributed_task_execution(self):
        """æµ‹è¯•5ï¼šåˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œ"""
        logger.info("ğŸ” æµ‹è¯•åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œ...")

        # 1. å¯åŠ¨åˆ†å¸ƒå¼æœåŠ¡
        self.distributed_service.start()
        time.sleep(2)  # ç­‰å¾…æœåŠ¡å¯åŠ¨

        # 2. åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        test_task = {
            'task_id': 'uat_distributed_001',
            'task_type': 'data_processing',
            'data': {'symbols': self.test_symbols[:2]},
            'priority': 'normal'
        }

        # 3. æäº¤ä»»åŠ¡
        task_submitted = self.distributed_service.submit_task(test_task)
        if task_submitted:
            logger.info("âœ… åˆ†å¸ƒå¼ä»»åŠ¡æäº¤æˆåŠŸ")
        else:
            logger.info("â„¹ï¸ åˆ†å¸ƒå¼ä»»åŠ¡æäº¤å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æµ‹è¯•ç¯å¢ƒé™åˆ¶ï¼‰")

        # 4. æ£€æŸ¥èŠ‚ç‚¹å‘ç°
        discovered_nodes = self.distributed_service.get_available_nodes()
        logger.info(f"å‘ç°èŠ‚ç‚¹æ•°é‡: {len(discovered_nodes)}")

        self.test_results['distributed_execution'] = {
            'service_started': True,
            'task_submission': task_submitted,
            'nodes_discovered': len(discovered_nodes)
        }

        logger.info("âœ… åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œæµ‹è¯•é€šè¿‡")

    def test_06_caching_and_storage_performance(self):
        """æµ‹è¯•6ï¼šç¼“å­˜å’Œå­˜å‚¨æ€§èƒ½"""
        logger.info("ğŸ” æµ‹è¯•ç¼“å­˜å’Œå­˜å‚¨æ€§èƒ½...")

        # 1. ç¼“å­˜å†™å…¥æ€§èƒ½æµ‹è¯•
        cache_write_start = time.perf_counter()

        for i in range(100):
            key = f"uat_test_key_{i}"
            value = {
                'symbol': f'TEST{i:03d}',
                'data': list(range(100)),  # æ¨¡æ‹Ÿæ•°æ®
                'timestamp': datetime.now().isoformat()
            }
            self.cache_manager.set(key, value)

        cache_write_time = time.perf_counter() - cache_write_start

        # 2. ç¼“å­˜è¯»å–æ€§èƒ½æµ‹è¯•
        cache_read_start = time.perf_counter()

        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç¼“å­˜å†™å…¥å®Œæˆ
        time.sleep(0.1)

        hit_count = 0
        for i in range(100):
            key = f"uat_test_key_{i}"
            value = self.cache_manager.get(key)
            if value is not None:
                hit_count += 1
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                self.assertIsInstance(value, dict, "ç¼“å­˜æ•°æ®ç±»å‹é”™è¯¯")
                self.assertEqual(value['symbol'], f'TEST{i:03d}', "ç¼“å­˜æ•°æ®å†…å®¹é”™è¯¯")

        cache_read_time = time.perf_counter() - cache_read_start

        # è®¡ç®—å‘½ä¸­ç‡
        hit_rate = hit_count / 100

        # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯è¿›è¡ŒéªŒè¯
        cache_stats = self.cache_manager.get_statistics()
        logger.info(f"ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯: {cache_stats}")

        # å¦‚æœç¼“å­˜ç»Ÿè®¡å¯ç”¨ï¼Œä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
        if cache_stats and 'total' in cache_stats:
            total_stats = cache_stats['total']
            if hasattr(total_stats, 'hit_rate'):
                statistical_hit_rate = total_stats.hit_rate
                logger.info(f"ç»Ÿè®¡å‘½ä¸­ç‡: {statistical_hit_rate:.1%}")
                # ä½¿ç”¨ç»Ÿè®¡å‘½ä¸­ç‡ï¼ˆå¦‚æœå¯ç”¨ä¸”åˆç†ï¼‰
                if statistical_hit_rate > 0:
                    hit_rate = statistical_hit_rate

        # 3. è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.cache_manager.get_statistics()

        logger.info(f"âœ… ç¼“å­˜å†™å…¥: {cache_write_time:.3f}ç§’ (100æ¡è®°å½•)")
        logger.info(f"âœ… ç¼“å­˜è¯»å–: {cache_read_time:.3f}ç§’ (100æ¡è®°å½•)")
        logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1%}")

        # æ€§èƒ½åŸºå‡†æ£€æŸ¥
        self.assertLess(cache_write_time, 1.0, "ç¼“å­˜å†™å…¥æ€§èƒ½ä¸è¾¾æ ‡")
        self.assertLess(cache_read_time, 0.1, "ç¼“å­˜è¯»å–æ€§èƒ½ä¸è¾¾æ ‡")
        self.assertGreaterEqual(hit_rate, 0.9, "ç¼“å­˜å‘½ä¸­ç‡ä¸è¾¾æ ‡")

        self.test_results['caching_performance'] = {
            'write_time': cache_write_time,
            'read_time': cache_read_time,
            'hit_rate': hit_rate,
            'stats_available': cache_stats is not None
        }

        logger.info("âœ… ç¼“å­˜å’Œå­˜å‚¨æ€§èƒ½æµ‹è¯•é€šè¿‡")

    def test_07_error_handling_and_recovery(self):
        """æµ‹è¯•7ï¼šé”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶"""
        logger.info("ğŸ” æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶...")

        # 1. æµ‹è¯•æ— æ•ˆé…ç½®å¤„ç†
        try:
            invalid_config = ImportTaskConfig(
                task_id="invalid_test",
                name="æ— æ•ˆé…ç½®æµ‹è¯•",
                symbols=[],  # ç©ºç¬¦å·åˆ—è¡¨
                data_source="",  # ç©ºæ•°æ®æº
                asset_type="è‚¡ç¥¨",
                data_type="Kçº¿æ•°æ®",
                frequency=DataFrequency.DAILY,
                mode=ImportMode.MANUAL,
                batch_size=0,  # æ— æ•ˆæ‰¹æ¬¡å¤§å°
                max_workers=-1  # æ— æ•ˆå·¥ä½œçº¿ç¨‹æ•°
            )

            # å°è¯•å¯åŠ¨æ— æ•ˆä»»åŠ¡
            result = self.import_engine.start_task("invalid_test")
            logger.info(f"æ— æ•ˆé…ç½®å¤„ç†ç»“æœ: {'æˆåŠŸ' if result else 'å¤±è´¥ï¼ˆç¬¦åˆé¢„æœŸï¼‰'}")

        except Exception as e:
            logger.info(f"âœ… æ— æ•ˆé…ç½®è¢«æ­£ç¡®æ‹’ç»: {str(e)[:100]}")

        # 2. æµ‹è¯•AIæœåŠ¡é”™è¯¯å¤„ç†
        try:
            # æä¾›æ— æ•ˆæ•°æ®
            invalid_prediction = self.ai_service.predict_execution_time({})
            logger.info("âœ… AIæœåŠ¡é”™è¯¯å¤„ç†æ­£å¸¸")
        except Exception as e:
            logger.info(f"âœ… AIæœåŠ¡é”™è¯¯è¢«æ­£ç¡®å¤„ç†: {str(e)[:100]}")

        # 3. æµ‹è¯•ç¼“å­˜é”™è¯¯å¤„ç†
        try:
            # å°è¯•è·å–ä¸å­˜åœ¨çš„é”®
            missing_value = self.cache_manager.get("non_existent_key")
            self.assertIsNone(missing_value, "ç¼“å­˜åº”è¯¥è¿”å›Noneå¯¹äºä¸å­˜åœ¨çš„é”®")
            logger.info("âœ… ç¼“å­˜é”™è¯¯å¤„ç†æ­£å¸¸")
        except Exception as e:
            logger.info(f"âœ… ç¼“å­˜é”™è¯¯è¢«æ­£ç¡®å¤„ç†: {str(e)[:100]}")

        # 4. æµ‹è¯•äº‹ä»¶æ€»çº¿é”™è¯¯å¤„ç†
        try:
            # å‘å¸ƒæ— æ•ˆäº‹ä»¶
            invalid_event = BaseEvent("test_error_event", {"invalid": "data"})
            self.event_bus.publish(invalid_event)
            logger.info("âœ… äº‹ä»¶æ€»çº¿é”™è¯¯å¤„ç†æ­£å¸¸")
        except Exception as e:
            logger.info(f"âœ… äº‹ä»¶æ€»çº¿é”™è¯¯è¢«æ­£ç¡®å¤„ç†: {str(e)[:100]}")

        self.test_results['error_handling'] = {
            'invalid_config_handled': True,
            'ai_service_resilient': True,
            'cache_error_handled': True,
            'event_bus_resilient': True
        }

        logger.info("âœ… é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶æµ‹è¯•é€šè¿‡")

    def test_08_user_experience_integration(self):
        """æµ‹è¯•8ï¼šç”¨æˆ·ä½“éªŒé›†æˆæµ‹è¯•"""
        logger.info("ğŸ” æµ‹è¯•ç”¨æˆ·ä½“éªŒé›†æˆ...")

        # 1. æ¨¡æ‹Ÿå®Œæ•´çš„ç”¨æˆ·å·¥ä½œæµç¨‹
        workflow_start = time.perf_counter()

        # æ­¥éª¤1ï¼šç”¨æˆ·åˆ›å»ºæ–°çš„å¯¼å…¥ä»»åŠ¡
        user_task = ImportTaskConfig(
            task_id="uat_user_workflow",
            name="ç”¨æˆ·å·¥ä½œæµç¨‹æµ‹è¯•",
            symbols=["000001", "000002"],
            data_source="ç”¨æˆ·é€‰æ‹©çš„æ•°æ®æº",
            asset_type="è‚¡ç¥¨",
            data_type="Kçº¿æ•°æ®",
            frequency=DataFrequency.DAILY,
            mode=ImportMode.MANUAL,
            batch_size=50,
            max_workers=2
        )

        # æ­¥éª¤2ï¼šè·å–AIä¼˜åŒ–å»ºè®®
        optimization_suggestion = self.ai_service.predict_execution_time({
            'symbols': user_task.symbols,
            'batch_size': user_task.batch_size,
            'max_workers': user_task.max_workers
        })

        # æ­¥éª¤3ï¼šä¿å­˜é…ç½®
        self.config_manager.add_import_task(user_task)

        # æ­¥éª¤4ï¼šå¯åŠ¨æ€§èƒ½ç›‘æ§
        self.performance_monitor.start()

        # æ­¥éª¤5ï¼šå¯åŠ¨é£é™©ç›‘æ§
        self.risk_monitor.start_monitoring()

        # æ­¥éª¤6ï¼šæ‰§è¡Œä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰
        task_started = self.import_engine.start_task(user_task.task_id)

        # æ­¥éª¤7ï¼šç›‘æ§è¿›åº¦
        time.sleep(2)  # æ¨¡æ‹Ÿç”¨æˆ·ç­‰å¾…

        # æ­¥éª¤8ï¼šæ£€æŸ¥ç»“æœ
        task_status = self.import_engine.get_task_status(user_task.task_id)

        workflow_time = time.perf_counter() - workflow_start

        # éªŒè¯ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
        self.assertLess(workflow_time, 10.0, "ç”¨æˆ·å·¥ä½œæµç¨‹å“åº”æ—¶é—´è¿‡é•¿")

        logger.info(f"âœ… ç”¨æˆ·å·¥ä½œæµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {workflow_time:.2f}ç§’")

        # 2. éªŒè¯ç³»ç»ŸçŠ¶æ€ä¸€è‡´æ€§
        saved_tasks = self.config_manager.get_import_tasks()
        self.assertGreater(len(saved_tasks), 0, "é…ç½®æœªæ­£ç¡®ä¿å­˜")

        performance_report = self.performance_monitor.get_performance_report()
        self.assertIsNotNone(performance_report, "æ€§èƒ½æŠ¥å‘Šä¸å¯ç”¨")

        self.test_results['user_experience'] = {
            'workflow_time': workflow_time,
            'task_started': task_started,
            'config_saved': len(saved_tasks) > 0,
            'monitoring_active': performance_report is not None
        }

        logger.info("âœ… ç”¨æˆ·ä½“éªŒé›†æˆæµ‹è¯•é€šè¿‡")


def run_user_acceptance_tests():
    """è¿è¡Œç”¨æˆ·éªŒæ”¶æµ‹è¯•"""
    logger.info("=" * 60)
    logger.info("HIkyuu-UI ç”¨æˆ·éªŒæ”¶æµ‹è¯•")
    logger.info("=" * 60)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = unittest.TestLoader().loadTestsFromTestCase(UserAcceptanceTestSuite)

    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("=" * 60)
    logger.info("ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç»“æœ")
    logger.info("=" * 60)
    logger.info(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    logger.info(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    logger.info(f"å¤±è´¥: {len(result.failures)}")
    logger.info(f"é”™è¯¯: {len(result.errors)}")

    if result.failures:
        logger.error("å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            logger.error(f"- {test}: {traceback}")

    if result.errors:
        logger.error("é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            logger.error(f"- {test}: {traceback}")

    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
    logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")

    # ç”¨æˆ·ä½“éªŒè¯„ä¼°
    if success_rate >= 90:
        logger.info("ğŸ‰ ç”¨æˆ·éªŒæ”¶æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½äº¤ä»˜ä½¿ç”¨")
    elif success_rate >= 70:
        logger.warning("âš ï¸ ç”¨æˆ·éªŒæ”¶æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼Œä½†éœ€è¦æ”¹è¿›")
    else:
        logger.error("âŒ ç”¨æˆ·éªŒæ”¶æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦é‡å¤§ä¿®å¤")

    return result.wasSuccessful()


if __name__ == "__main__":
    # è¿è¡Œç”¨æˆ·éªŒæ”¶æµ‹è¯•
    success = run_user_acceptance_tests()
    sys.exit(0 if success else 1)
