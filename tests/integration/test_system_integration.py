#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç³»ç»Ÿé›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•

å»ºç«‹å…¨é¢çš„é›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶ï¼ŒéªŒè¯å„ç»„ä»¶é—´çš„åä½œå’Œå®Œæ•´çš„ç”¨æˆ·æµç¨‹ã€‚
æµ‹è¯•èŒƒå›´ï¼š
1. ç»„ä»¶é—´é›†æˆæµ‹è¯•
2. æ•°æ®æµç«¯åˆ°ç«¯æµ‹è¯•
3. ç”¨æˆ·å·¥ä½œæµç¨‹æµ‹è¯•
4. æ€§èƒ½é›†æˆæµ‹è¯•
5. é”™è¯¯æ¢å¤é›†æˆæµ‹è¯•
"""

import pytest
import unittest
from unittest.mock import Mock, patch, MagicMock
import tempfile
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import sqlite3
import threading
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import requests_mock

# å¯¼å…¥å¾…æµ‹è¯•çš„ç»„ä»¶
from core.importdata.intelligent_config_manager import (
    IntelligentConfigManager, ImportTaskConfig, DataFrequency, ImportMode,
    ConfigRecommendationType, ConfigOptimizationLevel
)
from core.ai.config_recommendation_engine import ConfigRecommendationEngine
from core.ai.config_impact_analyzer import ConfigImpactAnalyzer
from core.ui_integration.smart_data_integration import (
    SmartDataIntegration, UIIntegrationConfig, IntegrationMode
)
from core.ai.data_anomaly_detector import (
    DataAnomalyDetector, AnomalyDetectionConfig, AnomalyType
)


class TestSystemIntegration(unittest.TestCase):
    """ç³»ç»Ÿé›†æˆæµ‹è¯•åŸºç±»"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
        self.temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
        self.temp_db.close()
        self.db_path = self.temp_db.name
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
        self.test_data_dir = tempfile.mkdtemp()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.config_manager = IntelligentConfigManager(self.db_path)
        
        # Mockå¤–éƒ¨ä¾èµ–
        with patch('core.ai.config_recommendation_engine.AIPredictionService'), \
             patch('core.ai.config_impact_analyzer.AIPredictionService'):
            self.recommendation_engine = ConfigRecommendationEngine(self.db_path)
            self.impact_analyzer = ConfigImpactAnalyzer(self.db_path)
        
        # æ•°æ®é›†æˆç»„ä»¶
        integration_config = UIIntegrationConfig(
            enable_caching=True,
            cache_expiry_seconds=300,
            enable_predictive_loading=True,
            enable_adaptive_caching=True
        )
        
        with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
            self.data_integration = SmartDataIntegration(integration_config)
        
        # å¼‚å¸¸æ£€æµ‹å™¨
        anomaly_config = AnomalyDetectionConfig(
            auto_repair_enabled=True,
            enable_outlier_detection=True,
            enable_missing_data_detection=True
        )
        self.anomaly_detector = DataAnomalyDetector(anomaly_config, self.db_path)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        try:
            self.data_integration.close()
            os.unlink(self.db_path)
            import shutil
            shutil.rmtree(self.test_data_dir, ignore_errors=True)
        except:
            pass


class TestConfigurationWorkflow(TestSystemIntegration):
    """é…ç½®ç®¡ç†å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•"""

    def test_complete_configuration_lifecycle(self):
        """æµ‹è¯•å®Œæ•´çš„é…ç½®ç”Ÿå‘½å‘¨æœŸ"""
        print("\n=== æµ‹è¯•å®Œæ•´é…ç½®ç”Ÿå‘½å‘¨æœŸ ===")
        
        # 1. åˆ›å»ºå¯¼å…¥ä»»åŠ¡é…ç½®
        config = ImportTaskConfig(
            task_id="lifecycle_test_001",
            name="ç”Ÿå‘½å‘¨æœŸæµ‹è¯•ä»»åŠ¡",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001", "000002", "000300"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH,
            max_workers=4,
            batch_size=1000
        )
        
        # 2. æ·»åŠ ä»»åŠ¡åˆ°é…ç½®ç®¡ç†å™¨
        success = self.config_manager.add_import_task(config)
        self.assertTrue(success, "ä»»åŠ¡æ·»åŠ å¤±è´¥")
        print(f"âœ“ ä»»åŠ¡æ·»åŠ æˆåŠŸ: {config.task_id}")
        
        # 3. è®°å½•æ€§èƒ½åé¦ˆï¼ˆæ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œï¼‰
        performance_data = [
            (90.0, 0.95, 0.05, 1200.0),
            (85.0, 0.97, 0.03, 1300.0),
            (120.0, 0.85, 0.15, 800.0),
            (75.0, 0.98, 0.02, 1400.0)
        ]
        
        for exec_time, success_rate, error_rate, throughput in performance_data:
            self.config_manager.record_performance_feedback(
                config=config,
                execution_time=exec_time,
                success_rate=success_rate,
                error_rate=error_rate,
                throughput=throughput
            )
        
        print(f"âœ“ è®°å½•äº† {len(performance_data)} æ¡æ€§èƒ½åé¦ˆ")
        
        # 4. ç”Ÿæˆé…ç½®æ¨è
        recommendations = self.recommendation_engine.generate_recommendations_for_task(
            config.task_id, ConfigRecommendationType.PERFORMANCE
        )
        
        self.assertIsInstance(recommendations, list)
        print(f"âœ“ ç”Ÿæˆäº† {len(recommendations)} æ¡é…ç½®æ¨è")
        
        # 5. åˆ†æé…ç½®å˜æ›´å½±å“
        if recommendations:
            best_recommendation = recommendations[0]
            impact_report = self.impact_analyzer.analyze_impact(
                config, best_recommendation.recommended_changes
            )
            
            self.assertIsInstance(impact_report, dict)
            self.assertIn('overall_assessment', impact_report)
            print(f"âœ“ å®Œæˆé…ç½®å˜æ›´å½±å“åˆ†æ: {impact_report['overall_assessment']['status']}")
        
        # 6. æ£€æµ‹é…ç½®å†²çª
        conflicts = self.config_manager.detect_conflicts()
        self.assertIsInstance(conflicts, list)
        print(f"âœ“ æ£€æµ‹åˆ° {len(conflicts)} ä¸ªé…ç½®å†²çª")
        
        # 7. è·å–æ™ºèƒ½ç»Ÿè®¡ä¿¡æ¯
        stats = self.config_manager.get_intelligent_statistics()
        self.assertIsInstance(stats, dict)
        self.assertGreater(stats['total_tasks'], 0)
        self.assertGreater(stats['performance_history_count'], 0)
        print(f"âœ“ è·å–ç»Ÿè®¡ä¿¡æ¯: {stats['total_tasks']} ä¸ªä»»åŠ¡, {stats['performance_history_count']} æ¡æ€§èƒ½è®°å½•")

    def test_multi_task_configuration_management(self):
        """æµ‹è¯•å¤šä»»åŠ¡é…ç½®ç®¡ç†"""
        print("\n=== æµ‹è¯•å¤šä»»åŠ¡é…ç½®ç®¡ç† ===")
        
        # åˆ›å»ºå¤šä¸ªä¸åŒç±»å‹çš„ä»»åŠ¡
        tasks = [
            ImportTaskConfig(
                task_id=f"multi_task_{i:03d}",
                name=f"å¤šä»»åŠ¡æµ‹è¯•{i}",
                data_source="tongdaxin" if i % 2 == 0 else "akshare",
                asset_type="stock" if i % 3 == 0 else "index",
                data_type="kline",
                symbols=[f"{i:06d}"],
                frequency=DataFrequency.DAILY if i % 2 == 0 else DataFrequency.MINUTE,
                mode=ImportMode.BATCH if i % 3 == 0 else ImportMode.SCHEDULED,
                max_workers=2 + (i % 4),
                batch_size=500 + (i * 100)
            )
            for i in range(1, 11)  # åˆ›å»º10ä¸ªä»»åŠ¡
        ]
        
        # æ·»åŠ æ‰€æœ‰ä»»åŠ¡
        for task in tasks:
            success = self.config_manager.add_import_task(task)
            self.assertTrue(success, f"ä»»åŠ¡ {task.task_id} æ·»åŠ å¤±è´¥")
        
        print(f"âœ“ æˆåŠŸæ·»åŠ  {len(tasks)} ä¸ªä»»åŠ¡")
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡è®°å½•æ€§èƒ½æ•°æ®
        for task in tasks:
            # æ¨¡æ‹Ÿä¸åŒçš„æ€§èƒ½è¡¨ç°
            base_time = 60 + (hash(task.task_id) % 60)
            base_success = 0.9 + (hash(task.task_id) % 10) / 100
            
            self.config_manager.record_performance_feedback(
                config=task,
                execution_time=base_time,
                success_rate=min(1.0, base_success),
                error_rate=1.0 - min(1.0, base_success),
                throughput=1000 + (hash(task.task_id) % 500)
            )
        
        print("âœ“ ä¸ºæ‰€æœ‰ä»»åŠ¡è®°å½•äº†æ€§èƒ½æ•°æ®")
        
        # æ£€æµ‹ç³»ç»Ÿçº§å†²çª
        conflicts = self.config_manager.detect_conflicts()
        print(f"âœ“ æ£€æµ‹åˆ° {len(conflicts)} ä¸ªç³»ç»Ÿçº§å†²çª")
        
        # è·å–æ‰€æœ‰ä»»åŠ¡çš„æ¨è
        total_recommendations = 0
        for task in tasks[:5]:  # åªä¸ºå‰5ä¸ªä»»åŠ¡ç”Ÿæˆæ¨èï¼ˆèŠ‚çœæ—¶é—´ï¼‰
            recommendations = self.recommendation_engine.generate_recommendations_for_task(
                task.task_id, ConfigRecommendationType.BALANCED
            )
            total_recommendations += len(recommendations)
        
        print(f"âœ“ ç”Ÿæˆäº†æ€»è®¡ {total_recommendations} æ¡æ¨è")
        
        # éªŒè¯ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        stats = self.config_manager.get_intelligent_statistics()
        self.assertEqual(stats['total_tasks'], len(tasks))
        self.assertEqual(stats['active_tasks'], len(tasks))
        print(f"âœ“ ç³»ç»Ÿç»Ÿè®¡éªŒè¯é€šè¿‡: {stats['total_tasks']} ä¸ªä»»åŠ¡")


class TestDataProcessingWorkflow(TestSystemIntegration):
    """æ•°æ®å¤„ç†å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•"""

    @requests_mock.Mocker()
    def test_end_to_end_data_processing(self, m):
        """æµ‹è¯•ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹"""
        print("\n=== æµ‹è¯•ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹ ===")
        
        # 1. Mockå¤–éƒ¨æ•°æ®æºAPI
        mock_data = {
            'data': [
                {'symbol': '000001', 'date': '2024-01-01', 'open': 10.0, 'high': 10.5, 'low': 9.8, 'close': 10.2, 'volume': 1000000},
                {'symbol': '000001', 'date': '2024-01-02', 'open': 10.2, 'high': 10.8, 'low': 10.0, 'close': 10.6, 'volume': 1200000},
                {'symbol': '000001', 'date': '2024-01-03', 'open': 10.6, 'high': 11.0, 'low': 10.3, 'close': 10.8, 'volume': 1100000}
            ]
        }
        
        m.get('http://api.tongdaxin.com/stock/kline', json=mock_data)
        m.get('http://api.akshare.com/stock/kline', json=mock_data)
        
        # 2. é…ç½®æ•°æ®å¯¼å…¥ä»»åŠ¡
        config = ImportTaskConfig(
            task_id="e2e_data_test_001",
            name="ç«¯åˆ°ç«¯æ•°æ®æµ‹è¯•",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH
        )
        
        self.config_manager.add_import_task(config)
        print("âœ“ é…ç½®æ•°æ®å¯¼å…¥ä»»åŠ¡")
        
        # 3. é€šè¿‡æ•°æ®é›†æˆç»„ä»¶è·å–æ•°æ®
        with patch('core.ui_integration.smart_data_integration.requests.get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = mock_data
            mock_get.return_value = mock_response
            
            data_result = self.data_integration.check_data_for_widget(
                widget_type="stock_kline",
                symbol="000001",
                data_type="daily"
            )
            
            self.assertIsInstance(data_result, dict)
            print("âœ“ é€šè¿‡æ•°æ®é›†æˆç»„ä»¶è·å–æ•°æ®")
        
        # 4. å°†æ•°æ®è½¬æ¢ä¸ºDataFrameè¿›è¡Œå¼‚å¸¸æ£€æµ‹
        df_data = pd.DataFrame(mock_data['data'])
        df_data['date'] = pd.to_datetime(df_data['date'])
        
        # æ³¨å…¥ä¸€äº›å¼‚å¸¸æ•°æ®ç”¨äºæµ‹è¯•
        df_data.loc[1, 'high'] = 50.0  # å¼‚å¸¸é«˜ä»·
        df_data.loc[2, 'volume'] = np.nan  # ç¼ºå¤±äº¤æ˜“é‡
        
        # 5. æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        anomalies = self.anomaly_detector.detect_anomalies(
            data=df_data,
            data_source="tongdaxin",
            symbol="000001",
            data_type="kline"
        )
        
        self.assertGreater(len(anomalies), 0, "åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸")
        print(f"âœ“ æ£€æµ‹åˆ° {len(anomalies)} ä¸ªæ•°æ®å¼‚å¸¸")
        
        # 6. å°è¯•è‡ªåŠ¨ä¿®å¤å¼‚å¸¸
        repaired_count = 0
        for anomaly in anomalies:
            repair_result = self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
            if repair_result and repair_result.success:
                repaired_count += 1
        
        print(f"âœ“ æˆåŠŸä¿®å¤ {repaired_count} ä¸ªå¼‚å¸¸")
        
        # 7. è®°å½•å¤„ç†æ€§èƒ½å¹¶ç”Ÿæˆæ¨è
        processing_time = 45.0  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        success_rate = 0.95 if repaired_count > 0 else 0.85
        
        self.config_manager.record_performance_feedback(
            config=config,
            execution_time=processing_time,
            success_rate=success_rate,
            error_rate=1.0 - success_rate,
            throughput=len(df_data) / processing_time * 60  # æ¯åˆ†é’Ÿå¤„ç†è®°å½•æ•°
        )
        
        print("âœ“ è®°å½•å¤„ç†æ€§èƒ½åé¦ˆ")
        
        # 8. åŸºäºå¤„ç†ç»“æœç”Ÿæˆä¼˜åŒ–æ¨è
        recommendations = self.recommendation_engine.generate_recommendations_for_task(
            config.task_id, ConfigRecommendationType.RELIABILITY
        )
        
        print(f"âœ“ ç”Ÿæˆ {len(recommendations)} æ¡ä¼˜åŒ–æ¨è")
        
        # 9. éªŒè¯æ•´ä¸ªæµç¨‹çš„æ•°æ®ä¸€è‡´æ€§
        stats = self.config_manager.get_intelligent_statistics()
        anomaly_stats = self.anomaly_detector.get_anomaly_statistics()
        integration_stats = self.data_integration.get_statistics()
        
        self.assertGreater(stats['performance_history_count'], 0)
        self.assertGreater(anomaly_stats['total_anomalies'], 0)
        self.assertIsInstance(integration_stats, dict)
        
        print("âœ“ ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹éªŒè¯å®Œæˆ")

    def test_real_time_data_processing_simulation(self):
        """æµ‹è¯•å®æ—¶æ•°æ®å¤„ç†æ¨¡æ‹Ÿ"""
        print("\n=== æµ‹è¯•å®æ—¶æ•°æ®å¤„ç†æ¨¡æ‹Ÿ ===")
        
        # é…ç½®å®æ—¶æ•°æ®ä»»åŠ¡
        realtime_config = ImportTaskConfig(
            task_id="realtime_test_001",
            name="å®æ—¶æ•°æ®æµ‹è¯•",
            data_source="akshare",
            asset_type="stock",
            data_type="realtime",
            symbols=["000001", "000002", "000300"],
            frequency=DataFrequency.MINUTE,
            mode=ImportMode.REALTIME,
            max_workers=2,
            batch_size=100
        )
        
        self.config_manager.add_import_task(realtime_config)
        
        # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
        def simulate_realtime_data():
            """æ¨¡æ‹Ÿå®æ—¶æ•°æ®ç”Ÿæˆ"""
            base_price = 10.0
            for i in range(20):  # æ¨¡æ‹Ÿ20ä¸ªæ•°æ®ç‚¹
                # ç”Ÿæˆæ¨¡æ‹Ÿçš„å®æ—¶æ•°æ®
                price_change = np.random.normal(0, 0.1)
                current_price = max(0.1, base_price + price_change)
                
                data_point = pd.DataFrame({
                    'timestamp': [datetime.now()],
                    'symbol': ['000001'],
                    'price': [current_price],
                    'volume': [np.random.randint(1000, 10000)]
                })
                
                # æ£€æµ‹å¼‚å¸¸
                anomalies = self.anomaly_detector.detect_anomalies(
                    data=data_point,
                    data_source="akshare",
                    symbol="000001",
                    data_type="realtime"
                )
                
                # å¦‚æœæ£€æµ‹åˆ°å¼‚å¸¸ï¼Œå°è¯•ä¿®å¤
                if anomalies:
                    for anomaly in anomalies:
                        self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
                
                base_price = current_price
                time.sleep(0.1)  # æ¨¡æ‹Ÿå®æ—¶é—´éš”
        
        # åœ¨åå°è¿è¡Œå®æ—¶æ•°æ®æ¨¡æ‹Ÿ
        import threading
        simulation_thread = threading.Thread(target=simulate_realtime_data)
        simulation_thread.daemon = True
        simulation_thread.start()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ¨¡æ‹Ÿè¿è¡Œ
        time.sleep(3)
        
        # éªŒè¯å®æ—¶å¤„ç†ç»“æœ
        anomaly_stats = self.anomaly_detector.get_anomaly_statistics()
        print(f"âœ“ å®æ—¶å¤„ç†ä¸­æ£€æµ‹åˆ° {anomaly_stats['total_anomalies']} ä¸ªå¼‚å¸¸")
        
        # è®°å½•å®æ—¶å¤„ç†æ€§èƒ½
        self.config_manager.record_performance_feedback(
            config=realtime_config,
            execution_time=3.0,
            success_rate=0.98,
            error_rate=0.02,
            throughput=20 / 3.0  # æ¯ç§’å¤„ç†æ•°æ®ç‚¹æ•°
        )
        
        print("âœ“ å®æ—¶æ•°æ®å¤„ç†æ¨¡æ‹Ÿå®Œæˆ")


class TestPerformanceIntegration(TestSystemIntegration):
    """æ€§èƒ½é›†æˆæµ‹è¯•"""

    def test_high_load_scenario(self):
        """æµ‹è¯•é«˜è´Ÿè½½åœºæ™¯"""
        print("\n=== æµ‹è¯•é«˜è´Ÿè½½åœºæ™¯ ===")
        
        # åˆ›å»ºå¤§é‡ä»»åŠ¡é…ç½®
        num_tasks = 50
        tasks = []
        
        for i in range(num_tasks):
            config = ImportTaskConfig(
                task_id=f"load_test_{i:03d}",
                name=f"è´Ÿè½½æµ‹è¯•ä»»åŠ¡{i}",
                data_source="tongdaxin" if i % 2 == 0 else "akshare",
                asset_type="stock",
                data_type="kline",
                symbols=[f"{i:06d}"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH,
                max_workers=2,
                batch_size=1000
            )
            tasks.append(config)
        
        # å¹¶å‘æ·»åŠ ä»»åŠ¡
        def add_tasks_batch(task_batch):
            for task in task_batch:
                self.config_manager.add_import_task(task)
        
        # åˆ†æ‰¹å¹¶å‘æ·»åŠ 
        batch_size = 10
        threads = []
        
        start_time = time.time()
        
        for i in range(0, num_tasks, batch_size):
            batch = tasks[i:i + batch_size]
            thread = threading.Thread(target=add_tasks_batch, args=(batch,))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        add_time = time.time() - start_time
        print(f"âœ“ å¹¶å‘æ·»åŠ  {num_tasks} ä¸ªä»»åŠ¡è€—æ—¶: {add_time:.2f}ç§’")
        
        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½è¢«æ­£ç¡®æ·»åŠ 
        all_tasks = self.config_manager.get_all_import_tasks()
        self.assertEqual(len(all_tasks), num_tasks)
        
        # å¹¶å‘è®°å½•æ€§èƒ½æ•°æ®
        def record_performance_batch(task_batch):
            for task in task_batch:
                self.config_manager.record_performance_feedback(
                    config=task,
                    execution_time=np.random.uniform(30, 120),
                    success_rate=np.random.uniform(0.8, 1.0),
                    error_rate=np.random.uniform(0.0, 0.2),
                    throughput=np.random.uniform(500, 2000)
                )
        
        threads = []
        start_time = time.time()
        
        for i in range(0, num_tasks, batch_size):
            batch = tasks[i:i + batch_size]
            thread = threading.Thread(target=record_performance_batch, args=(batch,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        perf_time = time.time() - start_time
        print(f"âœ“ å¹¶å‘è®°å½• {num_tasks} ä¸ªä»»åŠ¡æ€§èƒ½æ•°æ®è€—æ—¶: {perf_time:.2f}ç§’")
        
        # éªŒè¯æ€§èƒ½æ•°æ®è®°å½•
        stats = self.config_manager.get_intelligent_statistics()
        self.assertEqual(stats['total_tasks'], num_tasks)
        self.assertEqual(stats['performance_history_count'], num_tasks)
        
        print(f"âœ“ é«˜è´Ÿè½½æµ‹è¯•å®Œæˆ: {num_tasks} ä¸ªä»»åŠ¡, æ€»è€—æ—¶: {add_time + perf_time:.2f}ç§’")

    def test_memory_usage_under_load(self):
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨"""
        print("\n=== æµ‹è¯•è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨ ===")
        
        import psutil
        import gc
        
        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # åˆ›å»ºå¤§é‡æ•°æ®è¿›è¡Œå¤„ç†
        large_datasets = []
        
        for i in range(10):
            # åˆ›å»ºå¤§å‹æ•°æ®é›†
            data = pd.DataFrame({
                'timestamp': pd.date_range('2024-01-01', periods=10000, freq='min'),
                'symbol': [f'TEST{i:03d}'] * 10000,
                'price': np.random.normal(100, 10, 10000),
                'volume': np.random.randint(1000, 100000, 10000),
                'high': np.random.normal(105, 10, 10000),
                'low': np.random.normal(95, 10, 10000)
            })
            
            # æ³¨å…¥å¼‚å¸¸æ•°æ®
            data.loc[np.random.choice(10000, 100, replace=False), 'price'] = np.nan
            data.loc[np.random.choice(10000, 50, replace=False), 'volume'] = 0
            
            large_datasets.append(data)
        
        # å¤„ç†æ‰€æœ‰æ•°æ®é›†
        total_anomalies = 0
        
        for i, data in enumerate(large_datasets):
            anomalies = self.anomaly_detector.detect_anomalies(
                data=data,
                data_source="memory_test",
                symbol=f"TEST{i:03d}",
                data_type="kline"
            )
            total_anomalies += len(anomalies)
            
            # å°è¯•ä¿®å¤éƒ¨åˆ†å¼‚å¸¸
            for anomaly in anomalies[:5]:  # åªä¿®å¤å‰5ä¸ªå¼‚å¸¸
                self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
        
        # è·å–å¤„ç†åå†…å­˜ä½¿ç”¨
        current_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = current_memory - initial_memory
        
        print(f"âœ“ å¤„ç†äº† {len(large_datasets)} ä¸ªå¤§å‹æ•°æ®é›†")
        print(f"âœ“ æ£€æµ‹åˆ°æ€»è®¡ {total_anomalies} ä¸ªå¼‚å¸¸")
        print(f"âœ“ å†…å­˜ä½¿ç”¨: åˆå§‹ {initial_memory:.1f}MB, å½“å‰ {current_memory:.1f}MB, å¢åŠ  {memory_increase:.1f}MB")
        
        # æ¸…ç†å†…å­˜
        large_datasets.clear()
        gc.collect()
        
        # éªŒè¯å†…å­˜å¢é•¿åœ¨åˆç†èŒƒå›´å†…ï¼ˆå°äº500MBï¼‰
        self.assertLess(memory_increase, 500, f"å†…å­˜å¢é•¿è¿‡å¤§: {memory_increase:.1f}MB")
        
        print("âœ“ å†…å­˜ä½¿ç”¨æµ‹è¯•é€šè¿‡")


class TestErrorRecoveryIntegration(TestSystemIntegration):
    """é”™è¯¯æ¢å¤é›†æˆæµ‹è¯•"""

    def test_database_connection_recovery(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ¢å¤"""
        print("\n=== æµ‹è¯•æ•°æ®åº“è¿æ¥æ¢å¤ ===")
        
        # æ·»åŠ ä¸€ä¸ªæ­£å¸¸ä»»åŠ¡
        config = ImportTaskConfig(
            task_id="db_recovery_test",
            name="æ•°æ®åº“æ¢å¤æµ‹è¯•",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH
        )
        
        success = self.config_manager.add_import_task(config)
        self.assertTrue(success)
        print("âœ“ æ­£å¸¸æ·»åŠ ä»»åŠ¡")
        
        # æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥é—®é¢˜ï¼ˆé€šè¿‡ä½¿ç”¨æ— æ•ˆè·¯å¾„ï¼‰
        original_db_path = self.config_manager.db_path
        self.config_manager.db_path = "/invalid/path/database.sqlite"
        
        # å°è¯•æ“ä½œï¼ˆåº”è¯¥ä¼˜é›…å¤„ç†é”™è¯¯ï¼‰
        try:
            stats = self.config_manager.get_intelligent_statistics()
            # å³ä½¿æ•°æ®åº“è¿æ¥æœ‰é—®é¢˜ï¼Œä¹Ÿåº”è¯¥è¿”å›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            self.assertIsInstance(stats, dict)
            print("âœ“ æ•°æ®åº“è¿æ¥é—®é¢˜æ—¶ä¼˜é›…é™çº§")
        except Exception as e:
            print(f"âœ“ æ•è·åˆ°é¢„æœŸçš„æ•°æ®åº“é”™è¯¯: {type(e).__name__}")
        
        # æ¢å¤æ•°æ®åº“è¿æ¥
        self.config_manager.db_path = original_db_path
        
        # éªŒè¯æ¢å¤ååŠŸèƒ½æ­£å¸¸
        stats = self.config_manager.get_intelligent_statistics()
        self.assertGreater(stats['total_tasks'], 0)
        print("âœ“ æ•°æ®åº“è¿æ¥æ¢å¤ååŠŸèƒ½æ­£å¸¸")

    def test_component_failure_isolation(self):
        """æµ‹è¯•ç»„ä»¶æ•…éšœéš”ç¦»"""
        print("\n=== æµ‹è¯•ç»„ä»¶æ•…éšœéš”ç¦» ===")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = ImportTaskConfig(
            task_id="isolation_test",
            name="æ•…éšœéš”ç¦»æµ‹è¯•",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.BATCH
        )
        
        self.config_manager.add_import_task(config)
        
        # æ¨¡æ‹Ÿæ¨èå¼•æ“æ•…éšœ
        with patch.object(self.recommendation_engine, 'generate_recommendations_for_task', 
                         side_effect=Exception("æ¨èå¼•æ“æ•…éšœ")):
            
            # é…ç½®ç®¡ç†å™¨åº”è¯¥ä»ç„¶èƒ½æ­£å¸¸å·¥ä½œ
            stats = self.config_manager.get_intelligent_statistics()
            self.assertIsInstance(stats, dict)
            print("âœ“ æ¨èå¼•æ“æ•…éšœæ—¶é…ç½®ç®¡ç†å™¨æ­£å¸¸å·¥ä½œ")
            
            # å…¶ä»–ç»„ä»¶ä¹Ÿåº”è¯¥æ­£å¸¸å·¥ä½œ
            test_data = pd.DataFrame({
                'price': [10.0, 11.0, 12.0],
                'volume': [1000, 1100, 1200]
            })
            
            anomalies = self.anomaly_detector.detect_anomalies(
                data=test_data,
                data_source="isolation_test",
                symbol="000001",
                data_type="kline"
            )
            
            self.assertIsInstance(anomalies, list)
            print("âœ“ æ¨èå¼•æ“æ•…éšœæ—¶å¼‚å¸¸æ£€æµ‹å™¨æ­£å¸¸å·¥ä½œ")
        
        # æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹å™¨æ•…éšœ
        with patch.object(self.anomaly_detector, 'detect_anomalies',
                         side_effect=Exception("å¼‚å¸¸æ£€æµ‹å™¨æ•…éšœ")):
            
            # é…ç½®ç®¡ç†å™¨å’Œæ¨èå¼•æ“åº”è¯¥ä»ç„¶èƒ½æ­£å¸¸å·¥ä½œ
            recommendations = self.recommendation_engine.generate_recommendations_for_task(
                config.task_id, ConfigRecommendationType.PERFORMANCE
            )
            
            self.assertIsInstance(recommendations, list)
            print("âœ“ å¼‚å¸¸æ£€æµ‹å™¨æ•…éšœæ—¶æ¨èå¼•æ“æ­£å¸¸å·¥ä½œ")
        
        print("âœ“ ç»„ä»¶æ•…éšœéš”ç¦»æµ‹è¯•å®Œæˆ")

    def test_data_corruption_handling(self):
        """æµ‹è¯•æ•°æ®æŸåå¤„ç†"""
        print("\n=== æµ‹è¯•æ•°æ®æŸåå¤„ç† ===")
        
        # åˆ›å»ºåŒ…å«å„ç§æŸåæ•°æ®çš„æµ‹è¯•æ•°æ®é›†
        corrupted_data = pd.DataFrame({
            'timestamp': ['2024-01-01', '2024-01-02', 'invalid_date', '2024-01-04', None],
            'symbol': ['000001', '000001', '000001', '', '000001'],
            'price': [10.0, -999.0, np.inf, 'not_a_number', 12.0],
            'volume': [1000, 0, -100, 1200, np.nan],
            'high': [10.5, 11.0, 11.5, 12.0, 12.5],
            'low': [9.5, 10.0, 10.5, 11.0, 11.5]
        })
        
        # å¼‚å¸¸æ£€æµ‹å™¨åº”è¯¥èƒ½å¤„ç†æŸåçš„æ•°æ®
        try:
            anomalies = self.anomaly_detector.detect_anomalies(
                data=corrupted_data,
                data_source="corruption_test",
                symbol="000001",
                data_type="kline"
            )
            
            self.assertIsInstance(anomalies, list)
            print(f"âœ“ æˆåŠŸå¤„ç†æŸåæ•°æ®ï¼Œæ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
            
            # å°è¯•ä¿®å¤æ£€æµ‹åˆ°çš„å¼‚å¸¸
            repaired_count = 0
            for anomaly in anomalies:
                repair_result = self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
                if repair_result and repair_result.success:
                    repaired_count += 1
            
            print(f"âœ“ æˆåŠŸä¿®å¤ {repaired_count} ä¸ªæ•°æ®å¼‚å¸¸")
            
        except Exception as e:
            # å³ä½¿å¤„ç†å¤±è´¥ï¼Œä¹Ÿåº”è¯¥æ˜¯å¯æ§çš„å¼‚å¸¸
            print(f"âœ“ ä¼˜é›…å¤„ç†æ•°æ®æŸåå¼‚å¸¸: {type(e).__name__}")
        
        print("âœ“ æ•°æ®æŸåå¤„ç†æµ‹è¯•å®Œæˆ")


class TestUserWorkflowIntegration(TestSystemIntegration):
    """ç”¨æˆ·å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•"""

    def test_typical_user_workflow(self):
        """æµ‹è¯•å…¸å‹ç”¨æˆ·å·¥ä½œæµç¨‹"""
        print("\n=== æµ‹è¯•å…¸å‹ç”¨æˆ·å·¥ä½œæµç¨‹ ===")
        
        # ç”¨æˆ·å·¥ä½œæµç¨‹ï¼š
        # 1. åˆ›å»ºæ•°æ®å¯¼å…¥ä»»åŠ¡
        # 2. é…ç½®ä»»åŠ¡å‚æ•°
        # 3. å¯åŠ¨ä»»åŠ¡å¹¶ç›‘æ§
        # 4. æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
        # 5. æ ¹æ®æ¨èä¼˜åŒ–é…ç½®
        # 6. å¤„ç†æ•°æ®è´¨é‡é—®é¢˜
        
        print("æ­¥éª¤1: ç”¨æˆ·åˆ›å»ºæ•°æ®å¯¼å…¥ä»»åŠ¡")
        user_config = ImportTaskConfig(
            task_id="user_workflow_001",
            name="ç”¨æˆ·è‚¡ç¥¨æ•°æ®å¯¼å…¥",
            data_source="tongdaxin",
            asset_type="stock",
            data_type="kline",
            symbols=["000001", "000002", "000300", "000858"],
            frequency=DataFrequency.DAILY,
            mode=ImportMode.SCHEDULED,
            schedule_cron="0 9 * * *",  # æ¯å¤©9ç‚¹æ‰§è¡Œ
            max_workers=4,
            batch_size=1000
        )
        
        success = self.config_manager.add_import_task(user_config)
        self.assertTrue(success)
        print("âœ“ ä»»åŠ¡åˆ›å»ºæˆåŠŸ")
        
        print("æ­¥éª¤2: æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œå’Œæ€§èƒ½ç›‘æ§")
        # æ¨¡æ‹Ÿå¤šæ¬¡ä»»åŠ¡æ‰§è¡Œ
        execution_results = [
            (120.0, 0.95, 0.05, 1200.0),  # ç¬¬ä¸€æ¬¡æ‰§è¡Œ
            (110.0, 0.97, 0.03, 1300.0),  # æ€§èƒ½æ”¹å–„
            (130.0, 0.92, 0.08, 1100.0),  # æ€§èƒ½ä¸‹é™
            (105.0, 0.98, 0.02, 1400.0),  # æœ€ä½³æ€§èƒ½
        ]
        
        for i, (exec_time, success_rate, error_rate, throughput) in enumerate(execution_results):
            self.config_manager.record_performance_feedback(
                config=user_config,
                execution_time=exec_time,
                success_rate=success_rate,
                error_rate=error_rate,
                throughput=throughput
            )
            print(f"  âœ“ è®°å½•ç¬¬{i+1}æ¬¡æ‰§è¡Œç»“æœ: {exec_time:.1f}s, æˆåŠŸç‡{success_rate:.1%}")
        
        print("æ­¥éª¤3: ç”¨æˆ·æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
        stats = self.config_manager.get_intelligent_statistics()
        print(f"  âœ“ ç³»ç»Ÿç»Ÿè®¡: {stats['total_tasks']}ä¸ªä»»åŠ¡, {stats['performance_history_count']}æ¡æ€§èƒ½è®°å½•")
        
        print("æ­¥éª¤4: ç³»ç»Ÿç”Ÿæˆä¼˜åŒ–æ¨è")
        recommendations = self.recommendation_engine.generate_recommendations_for_task(
            user_config.task_id, ConfigRecommendationType.PERFORMANCE
        )
        print(f"  âœ“ ç”Ÿæˆ{len(recommendations)}æ¡ä¼˜åŒ–æ¨è")
        
        if recommendations:
            best_recommendation = recommendations[0]
            print(f"  âœ“ æœ€ä½³æ¨è: {best_recommendation.description} (ç½®ä¿¡åº¦: {best_recommendation.confidence:.2f})")
            
            print("æ­¥éª¤5: ç”¨æˆ·åº”ç”¨æ¨èé…ç½®")
            # åˆ†æé…ç½®å˜æ›´å½±å“
            impact_report = self.impact_analyzer.analyze_impact(
                user_config, best_recommendation.recommended_changes
            )
            
            print(f"  âœ“ å½±å“åˆ†æ: {impact_report['overall_assessment']['status']}")
            
            if impact_report['overall_assessment']['status'] in ['safe', 'warning']:
                # åº”ç”¨æ¨èçš„é…ç½®å˜æ›´
                for key, value in best_recommendation.recommended_changes.items():
                    setattr(user_config, key, value)
                
                self.config_manager.update_import_task(user_config)
                print("  âœ“ é…ç½®æ›´æ–°æˆåŠŸ")
        
        print("æ­¥éª¤6: å¤„ç†æ•°æ®è´¨é‡é—®é¢˜")
        # æ¨¡æ‹Ÿå¯¼å…¥çš„æ•°æ®
        sample_data = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=100, freq='D'),
            'symbol': ['000001'] * 100,
            'open': np.random.normal(10, 1, 100),
            'high': np.random.normal(11, 1, 100),
            'low': np.random.normal(9, 1, 100),
            'close': np.random.normal(10, 1, 100),
            'volume': np.random.randint(100000, 1000000, 100)
        })
        
        # æ³¨å…¥ä¸€äº›æ•°æ®è´¨é‡é—®é¢˜
        sample_data.loc[10:15, 'close'] = np.nan  # ç¼ºå¤±æ•°æ®
        sample_data.loc[20, 'high'] = 50.0  # å¼‚å¸¸å€¼
        sample_data.loc[30:32] = sample_data.loc[29]  # é‡å¤æ•°æ®
        
        # æ£€æµ‹æ•°æ®è´¨é‡é—®é¢˜
        anomalies = self.anomaly_detector.detect_anomalies(
            data=sample_data,
            data_source="tongdaxin",
            symbol="000001",
            data_type="kline"
        )
        
        print(f"  âœ“ æ£€æµ‹åˆ°{len(anomalies)}ä¸ªæ•°æ®è´¨é‡é—®é¢˜")
        
        # è‡ªåŠ¨ä¿®å¤éƒ¨åˆ†é—®é¢˜
        auto_repaired = 0
        for anomaly in anomalies:
            repair_result = self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
            if repair_result and repair_result.success:
                auto_repaired += 1
        
        print(f"  âœ“ è‡ªåŠ¨ä¿®å¤{auto_repaired}ä¸ªé—®é¢˜")
        
        print("æ­¥éª¤7: ç”¨æˆ·æŸ¥çœ‹æœ€ç»ˆç»“æœ")
        final_stats = self.config_manager.get_intelligent_statistics()
        anomaly_stats = self.anomaly_detector.get_anomaly_statistics()
        
        print(f"  âœ“ æœ€ç»ˆç»Ÿè®¡: {final_stats['total_tasks']}ä¸ªä»»åŠ¡")
        print(f"  âœ“ æ•°æ®è´¨é‡: {anomaly_stats['total_anomalies']}ä¸ªå¼‚å¸¸, {anomaly_stats['resolved_anomalies']}ä¸ªå·²è§£å†³")
        
        print("âœ“ å…¸å‹ç”¨æˆ·å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆ")

    def test_power_user_advanced_workflow(self):
        """æµ‹è¯•é«˜çº§ç”¨æˆ·å·¥ä½œæµç¨‹"""
        print("\n=== æµ‹è¯•é«˜çº§ç”¨æˆ·å·¥ä½œæµç¨‹ ===")
        
        # é«˜çº§ç”¨æˆ·å·¥ä½œæµç¨‹ï¼š
        # 1. æ‰¹é‡åˆ›å»ºå¤šä¸ªå¤æ‚ä»»åŠ¡
        # 2. é…ç½®è‡ªåŠ¨ä¼˜åŒ–
        # 3. è®¾ç½®å†²çªæ£€æµ‹å’Œè§£å†³
        # 4. è‡ªå®šä¹‰æ¨èç­–ç•¥
        # 5. é«˜çº§æ•°æ®è´¨é‡ç›‘æ§
        
        print("æ­¥éª¤1: æ‰¹é‡åˆ›å»ºå¤æ‚ä»»åŠ¡é…ç½®")
        complex_tasks = []
        
        # åˆ›å»ºä¸åŒç±»å‹çš„å¤æ‚ä»»åŠ¡
        task_configs = [
            {
                'task_id': 'power_stock_daily',
                'name': 'è‚¡ç¥¨æ—¥çº¿æ•°æ®',
                'symbols': [f'{i:06d}' for i in range(1, 101)],  # 100åªè‚¡ç¥¨
                'frequency': DataFrequency.DAILY,
                'max_workers': 8,
                'batch_size': 2000
            },
            {
                'task_id': 'power_index_minute',
                'name': 'æŒ‡æ•°åˆ†é’Ÿæ•°æ®',
                'symbols': ['000001', '000300', '399001', '399006'],
                'frequency': DataFrequency.MINUTE,
                'max_workers': 4,
                'batch_size': 500
            },
            {
                'task_id': 'power_fund_nav',
                'name': 'åŸºé‡‘å‡€å€¼æ•°æ®',
                'symbols': [f'{i:06d}' for i in range(100001, 100051)],  # 50åªåŸºé‡‘
                'frequency': DataFrequency.DAILY,
                'max_workers': 2,
                'batch_size': 1000
            }
        ]
        
        for task_config in task_configs:
            config = ImportTaskConfig(
                task_id=task_config['task_id'],
                name=task_config['name'],
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=task_config['symbols'],
                frequency=task_config['frequency'],
                mode=ImportMode.SCHEDULED,
                schedule_cron="0 */6 * * *",  # æ¯6å°æ—¶æ‰§è¡Œ
                max_workers=task_config['max_workers'],
                batch_size=task_config['batch_size']
            )
            
            success = self.config_manager.add_import_task(config)
            self.assertTrue(success)
            complex_tasks.append(config)
        
        print(f"  âœ“ åˆ›å»º{len(complex_tasks)}ä¸ªå¤æ‚ä»»åŠ¡")
        
        print("æ­¥éª¤2: å¯ç”¨è‡ªåŠ¨é…ç½®ä¼˜åŒ–")
        self.config_manager.enable_auto_config(True)
        self.config_manager.set_auto_optimization_interval(1)  # 1å°æ—¶é—´éš”
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡è®°å½•ä¸åŒçš„æ€§èƒ½æ•°æ®
        for i, task in enumerate(complex_tasks):
            # æ¨¡æ‹Ÿä¸åŒçš„æ€§èƒ½è¡¨ç°
            performances = [
                (60 + i * 20, 0.9 + i * 0.02, 0.1 - i * 0.02, 1000 + i * 200),
                (55 + i * 18, 0.92 + i * 0.02, 0.08 - i * 0.02, 1100 + i * 200),
                (70 + i * 25, 0.88 + i * 0.02, 0.12 - i * 0.02, 900 + i * 200)
            ]
            
            for exec_time, success_rate, error_rate, throughput in performances:
                self.config_manager.record_performance_feedback(
                    config=task,
                    execution_time=exec_time,
                    success_rate=min(1.0, success_rate),
                    error_rate=max(0.0, error_rate),
                    throughput=throughput
                )
        
        print("  âœ“ è®°å½•å¤šæ ·åŒ–æ€§èƒ½æ•°æ®")
        
        print("æ­¥éª¤3: æ£€æµ‹å’Œè§£å†³é…ç½®å†²çª")
        conflicts = self.config_manager.detect_conflicts()
        print(f"  âœ“ æ£€æµ‹åˆ°{len(conflicts)}ä¸ªé…ç½®å†²çª")
        
        # å°è¯•è‡ªåŠ¨è§£å†³å†²çª
        resolved_conflicts = 0
        for conflict in conflicts:
            if conflict.auto_resolvable:
                # æ¨¡æ‹Ÿè‡ªåŠ¨è§£å†³å†²çª
                resolved_conflicts += 1
        
        print(f"  âœ“ è‡ªåŠ¨è§£å†³{resolved_conflicts}ä¸ªå†²çª")
        
        print("æ­¥éª¤4: ç”Ÿæˆå¤šç»´åº¦æ¨è")
        all_recommendations = []
        
        for task in complex_tasks:
            # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆä¸åŒç±»å‹çš„æ¨è
            for rec_type in [ConfigRecommendationType.PERFORMANCE, 
                           ConfigRecommendationType.RELIABILITY,
                           ConfigRecommendationType.COST]:
                recommendations = self.recommendation_engine.generate_recommendations_for_task(
                    task.task_id, rec_type
                )
                all_recommendations.extend(recommendations)
        
        print(f"  âœ“ ç”Ÿæˆ{len(all_recommendations)}æ¡å¤šç»´åº¦æ¨è")
        
        print("æ­¥éª¤5: é«˜çº§æ•°æ®è´¨é‡ç›‘æ§")
        # åˆ›å»ºå¤æ‚çš„æµ‹è¯•æ•°æ®é›†
        complex_data_issues = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=1000, freq='min'),
            'symbol': np.random.choice(['000001', '000002', '000300'], 1000),
            'price': np.concatenate([
                np.random.normal(10, 1, 800),  # æ­£å¸¸æ•°æ®
                np.random.normal(50, 5, 100),  # å¼‚å¸¸å€¼ç¾¤
                [np.nan] * 50,  # ç¼ºå¤±æ•°æ®
                np.random.normal(10, 1, 50)   # æ›´å¤šæ­£å¸¸æ•°æ®
            ]),
            'volume': np.concatenate([
                np.random.randint(1000, 10000, 900),
                [0] * 50,  # é›¶äº¤æ˜“é‡
                np.random.randint(1000, 10000, 50)
            ])
        })
        
        # æ·»åŠ é‡å¤æ•°æ®
        duplicate_indices = np.random.choice(1000, 100, replace=False)
        for idx in duplicate_indices:
            if idx < 999:
                complex_data_issues.loc[idx + 1] = complex_data_issues.loc[idx]
        
        # æ‰§è¡Œå…¨é¢çš„å¼‚å¸¸æ£€æµ‹
        comprehensive_anomalies = self.anomaly_detector.detect_anomalies(
            data=complex_data_issues,
            data_source="power_user_test",
            symbol="COMPLEX_DATA",
            data_type="kline"
        )
        
        print(f"  âœ“ æ£€æµ‹åˆ°{len(comprehensive_anomalies)}ä¸ªå¤æ‚æ•°æ®å¼‚å¸¸")
        
        # åˆ†ç±»å¤„ç†ä¸åŒç±»å‹çš„å¼‚å¸¸
        anomaly_types = {}
        for anomaly in comprehensive_anomalies:
            anomaly_type = anomaly.anomaly_type.value
            anomaly_types[anomaly_type] = anomaly_types.get(anomaly_type, 0) + 1
        
        print(f"  âœ“ å¼‚å¸¸ç±»å‹åˆ†å¸ƒ: {anomaly_types}")
        
        # æ‰¹é‡è‡ªåŠ¨ä¿®å¤
        batch_repair_results = []
        for anomaly in comprehensive_anomalies[:20]:  # åªå¤„ç†å‰20ä¸ªå¼‚å¸¸
            repair_result = self.anomaly_detector.auto_repair_anomaly(anomaly.anomaly_id)
            if repair_result:
                batch_repair_results.append(repair_result)
        
        successful_repairs = sum(1 for r in batch_repair_results if r.success)
        print(f"  âœ“ æ‰¹é‡ä¿®å¤ç»“æœ: {successful_repairs}/{len(batch_repair_results)} æˆåŠŸ")
        
        print("æ­¥éª¤6: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        final_stats = self.config_manager.get_intelligent_statistics()
        anomaly_stats = self.anomaly_detector.get_anomaly_statistics()
        
        print(f"  âœ“ ç³»ç»Ÿç»¼åˆç»Ÿè®¡:")
        print(f"    - æ€»ä»»åŠ¡æ•°: {final_stats['total_tasks']}")
        print(f"    - æ€§èƒ½è®°å½•: {final_stats['performance_history_count']}")
        print(f"    - æ¨èæ•°é‡: {final_stats['recommendation_count']}")
        print(f"    - å†²çªæ•°é‡: {final_stats['conflict_count']}")
        print(f"    - å¼‚å¸¸æ€»æ•°: {anomaly_stats['total_anomalies']}")
        print(f"    - å·²è§£å†³å¼‚å¸¸: {anomaly_stats['resolved_anomalies']}")
        
        print("âœ“ é«˜çº§ç”¨æˆ·å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆ")


def run_integration_tests():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œç³»ç»Ÿé›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•...")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æ‰€æœ‰æµ‹è¯•ç±»
    test_classes = [
        TestConfigurationWorkflow,
        TestDataProcessingWorkflow,
        TestPerformanceIntegration,
        TestErrorRecoveryIntegration,
        TestUserWorkflowIntegration
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("=" * 80)
    print(f"é›†æˆæµ‹è¯•å®Œæˆï¼")
    print(f"æˆåŠŸ: {'æ˜¯' if result.wasSuccessful() else 'å¦'}")
    print(f"å¤±è´¥æ•°: {len(result.failures)}")
    print(f"é”™è¯¯æ•°: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ å­˜åœ¨é›†æˆæµ‹è¯•å¤±è´¥æˆ–é”™è¯¯")
        
        if result.failures:
            print("\nå¤±è´¥çš„æµ‹è¯•:")
            for test, traceback in result.failures:
                print(f"  - {test}: {traceback.split('AssertionError:')[-1].strip()}")
        
        if result.errors:
            print("\né”™è¯¯çš„æµ‹è¯•:")
            for test, traceback in result.errors:
                print(f"  - {test}: {traceback.split('Exception:')[-1].strip()}")
    
    return result.wasSuccessful(), len(result.failures), len(result.errors)


if __name__ == "__main__":
    success, failures, errors = run_integration_tests()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    exit_code = 0 if success else 1
    exit(exit_code)
