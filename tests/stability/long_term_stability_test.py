#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é•¿æœŸè¿è¡Œç¨³å®šæ€§æµ‹è¯•

è¿›è¡Œé•¿æœŸè¿è¡Œç¨³å®šæ€§æµ‹è¯•å’Œå†…å­˜æ³„æ¼æ£€æµ‹ï¼ŒéªŒè¯ç³»ç»Ÿçš„é•¿æœŸå¯é æ€§å’Œèµ„æºç®¡ç†ã€‚
æµ‹è¯•åœºæ™¯åŒ…æ‹¬ï¼š
1. é•¿æ—¶é—´è¿ç»­è¿è¡Œæµ‹è¯•
2. å†…å­˜æ³„æ¼æ£€æµ‹
3. èµ„æºæ³„æ¼æ£€æµ‹
4. ç³»ç»Ÿç¨³å®šæ€§ç›‘æ§
5. å¼‚å¸¸æ¢å¤èƒ½åŠ›æµ‹è¯•
"""

import pytest
import unittest
import tempfile
import os
import time
import threading
import psutil
import gc
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from unittest.mock import patch, Mock
import sqlite3
import json
import signal
import sys
from pathlib import Path
import tracemalloc
import weakref

# å¯¼å…¥å¾…æµ‹è¯•çš„ç»„ä»¶
from core.importdata.intelligent_config_manager import (
    IntelligentConfigManager, ImportTaskConfig, DataFrequency, ImportMode
)
from core.ai.data_anomaly_detector import (
    DataAnomalyDetector, AnomalyDetectionConfig
)
from core.ui_integration.smart_data_integration import (
    SmartDataIntegration, UIIntegrationConfig
)


class LongTermStabilityTest(unittest.TestCase):
    """é•¿æœŸç¨³å®šæ€§æµ‹è¯•åŸºç±»"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.test_start_time = time.time()
        cls.process = psutil.Process()
        cls.initial_memory = cls.process.memory_info().rss / 1024 / 1024  # MB
        cls.initial_open_files = len(cls.process.open_files())
        cls.initial_threads = cls.process.num_threads()
        
        # å¯ç”¨å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ç”¨äºä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, cls._signal_handler)
        signal.signal(signal.SIGTERM, cls._signal_handler)
        
        cls.stop_flag = threading.Event()
        
        print(f"\n{'='*80}")
        print(f"å¼€å§‹é•¿æœŸç¨³å®šæ€§æµ‹è¯•")
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {cls.initial_memory:.1f} MB")
        print(f"åˆå§‹æ–‡ä»¶å¥æŸ„: {cls.initial_open_files}")
        print(f"åˆå§‹çº¿ç¨‹æ•°: {cls.initial_threads}")
        print(f"æµ‹è¯•è¿›ç¨‹PID: {os.getpid()}")
        print(f"{'='*80}")

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        final_memory = cls.process.memory_info().rss / 1024 / 1024  # MB
        final_open_files = len(cls.process.open_files())
        final_threads = cls.process.num_threads()
        test_duration = time.time() - cls.test_start_time
        
        # è·å–å†…å­˜è·Ÿè¸ªä¿¡æ¯
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        print(f"\n{'='*80}")
        print(f"é•¿æœŸç¨³å®šæ€§æµ‹è¯•å®Œæˆ")
        print(f"æµ‹è¯•æ€»è€—æ—¶: {test_duration:.2f} ç§’ ({test_duration/3600:.2f} å°æ—¶)")
        print(f"æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.1f} MB")
        print(f"å†…å­˜å¢é•¿: {final_memory - cls.initial_memory:.1f} MB")
        print(f"æœ€ç»ˆæ–‡ä»¶å¥æŸ„: {final_open_files}")
        print(f"æ–‡ä»¶å¥æŸ„å¢é•¿: {final_open_files - cls.initial_open_files}")
        print(f"æœ€ç»ˆçº¿ç¨‹æ•°: {final_threads}")
        print(f"çº¿ç¨‹æ•°å¢é•¿: {final_threads - cls.initial_threads}")
        print(f"å†…å­˜è·Ÿè¸ª - å½“å‰: {current / 1024 / 1024:.1f}MB, å³°å€¼: {peak / 1024 / 1024:.1f}MB")
        print(f"{'='*80}")

    @classmethod
    def _signal_handler(cls, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\næ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        cls.stop_flag.set()

    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„å‡†å¤‡"""
        self.temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
        self.temp_db.close()
        self.db_path = self.temp_db.name
        
        # è®°å½•æµ‹è¯•å¼€å§‹æ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µ
        self.test_start_memory = self.process.memory_info().rss / 1024 / 1024
        self.test_start_files = len(self.process.open_files())
        self.test_start_threads = self.process.num_threads()
        self.test_start_time = time.time()

    def tearDown(self):
        """æ¯ä¸ªæµ‹è¯•åçš„æ¸…ç†"""
        try:
            os.unlink(self.db_path)
        except:
            pass
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è®°å½•æµ‹è¯•ç»“æŸæ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µ
        test_end_memory = self.process.memory_info().rss / 1024 / 1024
        test_end_files = len(self.process.open_files())
        test_end_threads = self.process.num_threads()
        test_duration = time.time() - self.test_start_time
        
        memory_increase = test_end_memory - self.test_start_memory
        files_increase = test_end_files - self.test_start_files
        threads_increase = test_end_threads - self.test_start_threads
        
        print(f"  æµ‹è¯•è€—æ—¶: {test_duration:.2f}s")
        print(f"  å†…å­˜å¢é•¿: {memory_increase:.1f}MB")
        print(f"  æ–‡ä»¶å¥æŸ„å¢é•¿: {files_increase}")
        print(f"  çº¿ç¨‹æ•°å¢é•¿: {threads_increase}")


class TestMemoryLeakDetection(LongTermStabilityTest):
    """å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"""

    def test_config_manager_memory_leak(self):
        """æµ‹è¯•é…ç½®ç®¡ç†å™¨å†…å­˜æ³„æ¼"""
        print("\n--- æµ‹è¯•é…ç½®ç®¡ç†å™¨å†…å­˜æ³„æ¼ ---")
        
        # è®°å½•åˆå§‹å†…å­˜å¿«ç…§
        snapshot1 = tracemalloc.take_snapshot()
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå’Œé”€æ¯å¤šä¸ªé…ç½®ç®¡ç†å™¨å®ä¾‹
        num_iterations = 100
        managers_created = 0
        
        print(f"åˆ›å»ºå’Œé”€æ¯ {num_iterations} ä¸ªé…ç½®ç®¡ç†å™¨å®ä¾‹...")
        
        for iteration in range(num_iterations):
            if self.stop_flag.is_set():
                break
                
            try:
                # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                # åˆ›å»ºé…ç½®ç®¡ç†å™¨
                manager = IntelligentConfigManager(temp_db.name)
                managers_created += 1
                
                # æ·»åŠ ä¸€äº›ä»»åŠ¡
                for i in range(10):
                    config = ImportTaskConfig(
                        task_id=f"leak_test_{iteration}_{i}",
                        name=f"å†…å­˜æ³„æ¼æµ‹è¯•{iteration}-{i}",
                        data_source="tongdaxin",
                        asset_type="stock",
                        data_type="kline",
                        symbols=[f"{i:06d}"],
                        frequency=DataFrequency.DAILY,
                        mode=ImportMode.BATCH
                    )
                    manager.add_import_task(config)
                
                # è®°å½•ä¸€äº›æ€§èƒ½æ•°æ®
                for i in range(5):
                    manager.record_performance_feedback(
                        config=config,
                        execution_time=np.random.uniform(30, 120),
                        success_rate=np.random.uniform(0.8, 1.0),
                        error_rate=np.random.uniform(0.0, 0.2),
                        throughput=np.random.uniform(500, 2000)
                    )
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = manager.get_intelligent_statistics()
                
                # æ£€æµ‹å†²çª
                conflicts = manager.detect_conflicts()
                
                # æ˜¾å¼åˆ é™¤ç®¡ç†å™¨å¼•ç”¨
                del manager
                
                # æ¸…ç†æ•°æ®åº“æ–‡ä»¶
                try:
                    os.unlink(temp_db.name)
                except:
                    pass
                
                # æ¯10æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶å’Œå†…å­˜æ£€æŸ¥
                if (iteration + 1) % 10 == 0:
                    gc.collect()
                    current_memory = self.process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    
                    print(f"  è¿­ä»£ {iteration + 1}/{num_iterations}, å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
                    
                    # å¦‚æœå†…å­˜å¢é•¿è¿‡å¿«ï¼Œæå‰è­¦å‘Š
                    if memory_growth > 100:  # è¶…è¿‡100MB
                        print(f"  è­¦å‘Š: å†…å­˜å¢é•¿è¿‡å¿« ({memory_growth:.1f}MB)")
                
            except Exception as e:
                print(f"  è¿­ä»£ {iteration} å¤±è´¥: {e}")
        
        # æœ€ç»ˆåƒåœ¾å›æ”¶
        gc.collect()
        
        # è®°å½•æœ€ç»ˆå†…å­˜å¿«ç…§
        snapshot2 = tracemalloc.take_snapshot()
        final_memory = self.process.memory_info().rss / 1024 / 1024
        
        # åˆ†æå†…å­˜å·®å¼‚
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        
        print(f"å†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆ›å»ºç®¡ç†å™¨æ•°: {managers_created}")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"  å†…å­˜å¢é•¿: {final_memory - initial_memory:.1f}MB")
        print(f"  å¹³å‡æ¯ä¸ªå®ä¾‹å†…å­˜å¢é•¿: {(final_memory - initial_memory) / managers_created:.3f}MB")
        
        # æ˜¾ç¤ºå†…å­˜å¢é•¿æœ€å¤šçš„å‰5ä¸ªä½ç½®
        print("  å†…å­˜å¢é•¿æœ€å¤šçš„ä»£ç ä½ç½®:")
        for index, stat in enumerate(top_stats[:5]):
            print(f"    {index + 1}. {stat}")
        
        # å†…å­˜æ³„æ¼æ–­è¨€
        memory_growth = final_memory - initial_memory
        memory_per_instance = memory_growth / managers_created if managers_created > 0 else 0
        
        # å…è®¸æ¯ä¸ªå®ä¾‹æœ‰å°‘é‡å†…å­˜æ®‹ç•™ï¼ˆå°äº1MBï¼‰
        self.assertLess(memory_per_instance, 1.0, f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼: æ¯ä¸ªå®ä¾‹å¹³å‡å¢é•¿ {memory_per_instance:.3f}MB")
        self.assertLess(memory_growth, 50, f"æ€»å†…å­˜å¢é•¿è¿‡å¤§: {memory_growth:.1f}MB")

    def test_anomaly_detector_memory_leak(self):
        """æµ‹è¯•å¼‚å¸¸æ£€æµ‹å™¨å†…å­˜æ³„æ¼"""
        print("\n--- æµ‹è¯•å¼‚å¸¸æ£€æµ‹å™¨å†…å­˜æ³„æ¼ ---")
        
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå’Œé”€æ¯å¤šä¸ªå¼‚å¸¸æ£€æµ‹å™¨å®ä¾‹
        num_iterations = 50
        detectors_created = 0
        
        print(f"åˆ›å»ºå’Œé”€æ¯ {num_iterations} ä¸ªå¼‚å¸¸æ£€æµ‹å™¨å®ä¾‹...")
        
        for iteration in range(num_iterations):
            if self.stop_flag.is_set():
                break
                
            try:
                # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                # åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨
                config = AnomalyDetectionConfig(auto_repair_enabled=True)
                detector = DataAnomalyDetector(config, temp_db.name)
                detectors_created += 1
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_data = pd.DataFrame({
                    'timestamp': pd.date_range('2024-01-01', periods=1000, freq='min'),
                    'symbol': f'LEAK_TEST_{iteration}',
                    'price': np.concatenate([
                        np.random.normal(100, 10, 900),  # æ­£å¸¸æ•°æ®
                        [np.nan] * 50,  # ç¼ºå¤±æ•°æ®
                        np.random.normal(300, 50, 50)   # å¼‚å¸¸å€¼
                    ]),
                    'volume': np.random.randint(1000, 100000, 1000)
                })
                
                # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
                anomalies = detector.detect_anomalies(
                    data=test_data,
                    data_source="leak_test",
                    symbol=f"LEAK_TEST_{iteration}",
                    data_type="kline"
                )
                
                # å°è¯•è‡ªåŠ¨ä¿®å¤ä¸€äº›å¼‚å¸¸
                for anomaly in anomalies[:3]:  # åªä¿®å¤å‰3ä¸ªå¼‚å¸¸
                    detector.auto_repair_anomaly(anomaly.anomaly_id)
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = detector.get_anomaly_statistics()
                
                # æ¸…ç†å¼‚å¸¸è®°å½•
                detector.cleanup_old_records(days=0)  # æ¸…ç†æ‰€æœ‰è®°å½•
                
                # æ˜¾å¼åˆ é™¤æ£€æµ‹å™¨å¼•ç”¨
                del detector
                del test_data
                
                # æ¸…ç†æ•°æ®åº“æ–‡ä»¶
                try:
                    os.unlink(temp_db.name)
                except:
                    pass
                
                # æ¯10æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡å†…å­˜æ£€æŸ¥
                if (iteration + 1) % 10 == 0:
                    gc.collect()
                    current_memory = self.process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    
                    print(f"  è¿­ä»£ {iteration + 1}/{num_iterations}, å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
                
            except Exception as e:
                print(f"  è¿­ä»£ {iteration} å¤±è´¥: {e}")
        
        # æœ€ç»ˆåƒåœ¾å›æ”¶
        gc.collect()
        
        final_memory = self.process.memory_info().rss / 1024 / 1024
        
        print(f"å¼‚å¸¸æ£€æµ‹å™¨å†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆ›å»ºæ£€æµ‹å™¨æ•°: {detectors_created}")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"  å†…å­˜å¢é•¿: {final_memory - initial_memory:.1f}MB")
        print(f"  å¹³å‡æ¯ä¸ªå®ä¾‹å†…å­˜å¢é•¿: {(final_memory - initial_memory) / detectors_created:.3f}MB")
        
        # å†…å­˜æ³„æ¼æ–­è¨€
        memory_growth = final_memory - initial_memory
        memory_per_instance = memory_growth / detectors_created if detectors_created > 0 else 0
        
        self.assertLess(memory_per_instance, 2.0, f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼: æ¯ä¸ªå®ä¾‹å¹³å‡å¢é•¿ {memory_per_instance:.3f}MB")
        self.assertLess(memory_growth, 100, f"æ€»å†…å­˜å¢é•¿è¿‡å¤§: {memory_growth:.1f}MB")

    def test_data_integration_memory_leak(self):
        """æµ‹è¯•æ•°æ®é›†æˆç»„ä»¶å†…å­˜æ³„æ¼"""
        print("\n--- æµ‹è¯•æ•°æ®é›†æˆç»„ä»¶å†…å­˜æ³„æ¼ ---")
        
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå’Œé”€æ¯å¤šä¸ªæ•°æ®é›†æˆå®ä¾‹
        num_iterations = 30
        integrations_created = 0
        
        print(f"åˆ›å»ºå’Œé”€æ¯ {num_iterations} ä¸ªæ•°æ®é›†æˆå®ä¾‹...")
        
        for iteration in range(num_iterations):
            if self.stop_flag.is_set():
                break
                
            try:
                # åˆ›å»ºæ•°æ®é›†æˆå®ä¾‹
                config = UIIntegrationConfig(
                    enable_caching=True,
                    cache_expiry_seconds=300,
                    enable_predictive_loading=True,
                    enable_adaptive_caching=True
                )
                
                with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
                    integration = SmartDataIntegration(config)
                    integrations_created += 1
                
                # æ·»åŠ å¤§é‡ç¼“å­˜æ•°æ®
                for i in range(100):
                    cache_key = f"leak_test_{iteration}_{i}"
                    test_data = {
                        'symbol': f"TEST{i:06d}",
                        'data': [{'price': 10.0 + j * 0.1, 'volume': 1000 + j * 10} 
                                for j in range(50)]
                    }
                    integration._put_to_intelligent_cache(cache_key, test_data, "high", 300)
                
                # è®°å½•ä½¿ç”¨æ¨¡å¼
                for i in range(50):
                    integration._record_usage_pattern(
                        f"widget_{i % 5}", 
                        f"symbol_{i:06d}", 
                        "realtime" if i % 2 == 0 else "daily"
                    )
                
                # æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–
                integration.optimize_performance()
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = integration.get_statistics()
                
                # æ¸…ç†ç¼“å­˜
                integration.intelligent_cache.clear()
                
                # å…³é—­é›†æˆå®ä¾‹
                integration.close()
                
                # æ˜¾å¼åˆ é™¤å¼•ç”¨
                del integration
                
                # æ¯5æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡å†…å­˜æ£€æŸ¥
                if (iteration + 1) % 5 == 0:
                    gc.collect()
                    current_memory = self.process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    
                    print(f"  è¿­ä»£ {iteration + 1}/{num_iterations}, å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
                
            except Exception as e:
                print(f"  è¿­ä»£ {iteration} å¤±è´¥: {e}")
        
        # æœ€ç»ˆåƒåœ¾å›æ”¶
        gc.collect()
        
        final_memory = self.process.memory_info().rss / 1024 / 1024
        
        print(f"æ•°æ®é›†æˆå†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆ›å»ºé›†æˆå®ä¾‹æ•°: {integrations_created}")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"  å†…å­˜å¢é•¿: {final_memory - initial_memory:.1f}MB")
        print(f"  å¹³å‡æ¯ä¸ªå®ä¾‹å†…å­˜å¢é•¿: {(final_memory - initial_memory) / integrations_created:.3f}MB")
        
        # å†…å­˜æ³„æ¼æ–­è¨€
        memory_growth = final_memory - initial_memory
        memory_per_instance = memory_growth / integrations_created if integrations_created > 0 else 0
        
        self.assertLess(memory_per_instance, 3.0, f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼: æ¯ä¸ªå®ä¾‹å¹³å‡å¢é•¿ {memory_per_instance:.3f}MB")
        self.assertLess(memory_growth, 100, f"æ€»å†…å­˜å¢é•¿è¿‡å¤§: {memory_growth:.1f}MB")


class TestResourceLeakDetection(LongTermStabilityTest):
    """èµ„æºæ³„æ¼æ£€æµ‹æµ‹è¯•"""

    def test_file_handle_leak(self):
        """æµ‹è¯•æ–‡ä»¶å¥æŸ„æ³„æ¼"""
        print("\n--- æµ‹è¯•æ–‡ä»¶å¥æŸ„æ³„æ¼ ---")
        
        initial_files = len(self.process.open_files())
        
        # åˆ›å»ºå’Œé”€æ¯å¤šä¸ªç»„ä»¶å®ä¾‹ï¼Œç›‘æ§æ–‡ä»¶å¥æŸ„
        num_iterations = 100
        components_created = 0
        
        print(f"åˆ›å»ºå’Œé”€æ¯ {num_iterations} ä¸ªç»„ä»¶å®ä¾‹ï¼Œç›‘æ§æ–‡ä»¶å¥æŸ„...")
        
        for iteration in range(num_iterations):
            if self.stop_flag.is_set():
                break
                
            try:
                # åˆ›å»ºä¸´æ—¶æ•°æ®åº“æ–‡ä»¶
                temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.sqlite')
                temp_db.close()
                
                # åˆ›å»ºé…ç½®ç®¡ç†å™¨
                manager = IntelligentConfigManager(temp_db.name)
                
                # åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨
                anomaly_config = AnomalyDetectionConfig()
                detector = DataAnomalyDetector(anomaly_config, temp_db.name)
                
                components_created += 1
                
                # æ‰§è¡Œä¸€äº›æ“ä½œ
                config = ImportTaskConfig(
                    task_id=f"file_leak_test_{iteration}",
                    name=f"æ–‡ä»¶å¥æŸ„æ³„æ¼æµ‹è¯•{iteration}",
                    data_source="tongdaxin",
                    asset_type="stock",
                    data_type="kline",
                    symbols=["000001"],
                    frequency=DataFrequency.DAILY,
                    mode=ImportMode.BATCH
                )
                
                manager.add_import_task(config)
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶æ£€æµ‹å¼‚å¸¸
                test_data = pd.DataFrame({
                    'price': [10.0, np.nan, 12.0, 200.0, 11.0],
                    'volume': [1000, 1100, 1200, 1300, 1400]
                })
                
                anomalies = detector.detect_anomalies(
                    data=test_data,
                    data_source="file_leak_test",
                    symbol=f"TEST_{iteration}",
                    data_type="kline"
                )
                
                # æ˜¾å¼åˆ é™¤ç»„ä»¶å¼•ç”¨
                del manager
                del detector
                
                # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
                try:
                    os.unlink(temp_db.name)
                except:
                    pass
                
                # æ¯20æ¬¡è¿­ä»£æ£€æŸ¥æ–‡ä»¶å¥æŸ„
                if (iteration + 1) % 20 == 0:
                    gc.collect()
                    current_files = len(self.process.open_files())
                    files_growth = current_files - initial_files
                    
                    print(f"  è¿­ä»£ {iteration + 1}/{num_iterations}, æ–‡ä»¶å¥æŸ„å¢é•¿: {files_growth}")
                    
                    # å¦‚æœæ–‡ä»¶å¥æŸ„å¢é•¿è¿‡å¤šï¼Œæå‰è­¦å‘Š
                    if files_growth > 50:
                        print(f"  è­¦å‘Š: æ–‡ä»¶å¥æŸ„å¢é•¿è¿‡å¤š ({files_growth})")
                        
                        # æ˜¾ç¤ºå½“å‰æ‰“å¼€çš„æ–‡ä»¶
                        open_files = self.process.open_files()
                        print(f"  å½“å‰æ‰“å¼€æ–‡ä»¶æ•°: {len(open_files)}")
                        if len(open_files) > initial_files + 10:
                            print("  éƒ¨åˆ†æ‰“å¼€çš„æ–‡ä»¶:")
                            for i, file_info in enumerate(open_files[-10:]):  # æ˜¾ç¤ºæœ€å10ä¸ª
                                print(f"    {file_info.path}")
                
            except Exception as e:
                print(f"  è¿­ä»£ {iteration} å¤±è´¥: {e}")
        
        # æœ€ç»ˆæ£€æŸ¥
        gc.collect()
        final_files = len(self.process.open_files())
        
        print(f"æ–‡ä»¶å¥æŸ„æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆ›å»ºç»„ä»¶æ•°: {components_created}")
        print(f"  åˆå§‹æ–‡ä»¶å¥æŸ„: {initial_files}")
        print(f"  æœ€ç»ˆæ–‡ä»¶å¥æŸ„: {final_files}")
        print(f"  æ–‡ä»¶å¥æŸ„å¢é•¿: {final_files - initial_files}")
        print(f"  å¹³å‡æ¯ä¸ªç»„ä»¶æ–‡ä»¶å¥æŸ„å¢é•¿: {(final_files - initial_files) / components_created:.2f}")
        
        # æ–‡ä»¶å¥æŸ„æ³„æ¼æ–­è¨€
        files_growth = final_files - initial_files
        files_per_component = files_growth / components_created if components_created > 0 else 0
        
        self.assertLess(files_per_component, 0.1, f"å¯èƒ½å­˜åœ¨æ–‡ä»¶å¥æŸ„æ³„æ¼: æ¯ä¸ªç»„ä»¶å¹³å‡å¢é•¿ {files_per_component:.2f}")
        self.assertLess(files_growth, 10, f"æ€»æ–‡ä»¶å¥æŸ„å¢é•¿è¿‡å¤š: {files_growth}")

    def test_thread_leak(self):
        """æµ‹è¯•çº¿ç¨‹æ³„æ¼"""
        print("\n--- æµ‹è¯•çº¿ç¨‹æ³„æ¼ ---")
        
        initial_threads = self.process.num_threads()
        
        # åˆ›å»ºå’Œé”€æ¯å¤šä¸ªä½¿ç”¨çº¿ç¨‹çš„ç»„ä»¶
        num_iterations = 20
        components_created = 0
        
        print(f"åˆ›å»ºå’Œé”€æ¯ {num_iterations} ä¸ªä½¿ç”¨çº¿ç¨‹çš„ç»„ä»¶...")
        
        for iteration in range(num_iterations):
            if self.stop_flag.is_set():
                break
                
            try:
                # åˆ›å»ºæ•°æ®é›†æˆç»„ä»¶ï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰
                config = UIIntegrationConfig(
                    enable_caching=True,
                    enable_predictive_loading=True
                )
                
                with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor') as mock_executor:
                    # åˆ›å»ºçœŸå®çš„çº¿ç¨‹æ± æ¥æµ‹è¯•çº¿ç¨‹æ³„æ¼
                    real_executor = ThreadPoolExecutor(max_workers=4)
                    mock_executor.return_value = real_executor
                    
                    integration = SmartDataIntegration(config)
                    components_created += 1
                    
                    # æ‰§è¡Œä¸€äº›éœ€è¦çº¿ç¨‹çš„æ“ä½œ
                    futures = []
                    for i in range(10):
                        future = real_executor.submit(
                            lambda x: time.sleep(0.1) or x * x, i
                        )
                        futures.append(future)
                    
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    for future in futures:
                        future.result()
                    
                    # å…³é—­ç»„ä»¶
                    integration.close()
                    
                    # æ˜¾å¼å…³é—­çº¿ç¨‹æ± 
                    real_executor.shutdown(wait=True)
                    
                    # åˆ é™¤å¼•ç”¨
                    del integration
                    del real_executor
                
                # æ¯5æ¬¡è¿­ä»£æ£€æŸ¥çº¿ç¨‹æ•°
                if (iteration + 1) % 5 == 0:
                    gc.collect()
                    time.sleep(0.5)  # ç­‰å¾…çº¿ç¨‹å®Œå…¨æ¸…ç†
                    
                    current_threads = self.process.num_threads()
                    threads_growth = current_threads - initial_threads
                    
                    print(f"  è¿­ä»£ {iteration + 1}/{num_iterations}, çº¿ç¨‹æ•°å¢é•¿: {threads_growth}")
                
            except Exception as e:
                print(f"  è¿­ä»£ {iteration} å¤±è´¥: {e}")
        
        # æœ€ç»ˆæ£€æŸ¥
        gc.collect()
        time.sleep(1)  # ç­‰å¾…çº¿ç¨‹å®Œå…¨æ¸…ç†
        final_threads = self.process.num_threads()
        
        print(f"çº¿ç¨‹æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆ›å»ºç»„ä»¶æ•°: {components_created}")
        print(f"  åˆå§‹çº¿ç¨‹æ•°: {initial_threads}")
        print(f"  æœ€ç»ˆçº¿ç¨‹æ•°: {final_threads}")
        print(f"  çº¿ç¨‹æ•°å¢é•¿: {final_threads - initial_threads}")
        print(f"  å¹³å‡æ¯ä¸ªç»„ä»¶çº¿ç¨‹å¢é•¿: {(final_threads - initial_threads) / components_created:.2f}")
        
        # çº¿ç¨‹æ³„æ¼æ–­è¨€
        threads_growth = final_threads - initial_threads
        threads_per_component = threads_growth / components_created if components_created > 0 else 0
        
        self.assertLess(threads_per_component, 0.5, f"å¯èƒ½å­˜åœ¨çº¿ç¨‹æ³„æ¼: æ¯ä¸ªç»„ä»¶å¹³å‡å¢é•¿ {threads_per_component:.2f}")
        self.assertLess(threads_growth, 5, f"æ€»çº¿ç¨‹æ•°å¢é•¿è¿‡å¤š: {threads_growth}")


class TestLongRunningStability(LongTermStabilityTest):
    """é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""

    def test_continuous_operation_stability(self):
        """æµ‹è¯•è¿ç»­æ“ä½œç¨³å®šæ€§"""
        print("\n--- æµ‹è¯•è¿ç»­æ“ä½œç¨³å®šæ€§ ---")
        
        # æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰- å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
        test_duration = int(os.environ.get('STABILITY_TEST_DURATION', '300'))  # é»˜è®¤5åˆ†é’Ÿ
        print(f"è¿ç»­è¿è¡Œæµ‹è¯•æ—¶é•¿: {test_duration} ç§’ ({test_duration/60:.1f} åˆ†é’Ÿ)")
        
        # åˆ›å»ºç»„ä»¶
        manager = IntelligentConfigManager(self.db_path)
        
        anomaly_config = AnomalyDetectionConfig(auto_repair_enabled=True)
        detector = DataAnomalyDetector(anomaly_config, self.db_path)
        
        integration_config = UIIntegrationConfig(enable_caching=True)
        with patch('core.ui_integration.smart_data_integration.ThreadPoolExecutor'):
            integration = SmartDataIntegration(integration_config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        operations_completed = 0
        errors_encountered = 0
        memory_samples = []
        
        # è¿è¡ŒçŠ¶æ€
        start_time = time.time()
        last_report_time = start_time
        report_interval = 30  # æ¯30ç§’æŠ¥å‘Šä¸€æ¬¡
        
        try:
            print("å¼€å§‹è¿ç»­æ“ä½œæµ‹è¯•...")
            
            while time.time() - start_time < test_duration:
                if self.stop_flag.is_set():
                    print("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºæµ‹è¯•")
                    break
                
                try:
                    operation_start = time.time()
                    
                    # 1. é…ç½®ç®¡ç†æ“ä½œ
                    task_id = f"stability_test_{operations_completed}"
                    config = ImportTaskConfig(
                        task_id=task_id,
                        name=f"ç¨³å®šæ€§æµ‹è¯•ä»»åŠ¡{operations_completed}",
                        data_source="tongdaxin" if operations_completed % 2 == 0 else "akshare",
                        asset_type="stock",
                        data_type="kline",
                        symbols=[f"{(operations_completed % 1000):06d}"],
                        frequency=DataFrequency.DAILY,
                        mode=ImportMode.BATCH,
                        max_workers=2 + (operations_completed % 4),
                        batch_size=500 + (operations_completed % 500)
                    )
                    
                    manager.add_import_task(config)
                    
                    # è®°å½•æ€§èƒ½æ•°æ®
                    manager.record_performance_feedback(
                        config=config,
                        execution_time=np.random.uniform(30, 120),
                        success_rate=np.random.uniform(0.8, 1.0),
                        error_rate=np.random.uniform(0.0, 0.2),
                        throughput=np.random.uniform(500, 2000)
                    )
                    
                    # 2. å¼‚å¸¸æ£€æµ‹æ“ä½œ
                    if operations_completed % 5 == 0:  # æ¯5æ¬¡æ“ä½œæ‰§è¡Œä¸€æ¬¡å¼‚å¸¸æ£€æµ‹
                        test_data = pd.DataFrame({
                            'timestamp': pd.date_range('2024-01-01', periods=100, freq='min'),
                            'symbol': f'STABILITY_{operations_completed}',
                            'price': np.concatenate([
                                np.random.normal(100, 10, 90),
                                [np.nan] * 5,
                                np.random.normal(300, 50, 5)
                            ]),
                            'volume': np.random.randint(1000, 100000, 100)
                        })
                        
                        anomalies = detector.detect_anomalies(
                            data=test_data,
                            data_source="stability_test",
                            symbol=f"STABILITY_{operations_completed}",
                            data_type="kline"
                        )
                        
                        # å°è¯•ä¿®å¤ä¸€äº›å¼‚å¸¸
                        for anomaly in anomalies[:2]:
                            detector.auto_repair_anomaly(anomaly.anomaly_id)
                    
                    # 3. æ•°æ®é›†æˆæ“ä½œ
                    if operations_completed % 3 == 0:  # æ¯3æ¬¡æ“ä½œæ‰§è¡Œä¸€æ¬¡æ•°æ®é›†æˆ
                        cache_key = f"stability_test_{operations_completed}"
                        test_data = {
                            'symbol': f"STABILITY_{operations_completed:06d}",
                            'price': 10.0 + (operations_completed % 100) * 0.1,
                            'volume': 1000 + operations_completed * 10
                        }
                        
                        integration._put_to_intelligent_cache(cache_key, test_data, "high", 300)
                        cached_data = integration._get_from_intelligent_cache(cache_key)
                        
                        integration._record_usage_pattern(
                            "stability_widget", 
                            f"STABILITY_{operations_completed:06d}", 
                            "realtime"
                        )
                    
                    operations_completed += 1
                    
                    # å®šæœŸæ¸…ç†ä»¥é¿å…æ•°æ®ç§¯ç´¯è¿‡å¤š
                    if operations_completed % 100 == 0:
                        # æ¸…ç†æ—§çš„å¼‚å¸¸è®°å½•
                        detector.cleanup_old_records(days=0)
                        
                        # æ¸…ç†æ—§çš„ä»»åŠ¡ï¼ˆä¿ç•™æœ€è¿‘50ä¸ªï¼‰
                        all_tasks = manager.get_all_import_tasks()
                        if len(all_tasks) > 50:
                            task_ids = list(all_tasks.keys())
                            for task_id in task_ids[:-50]:  # åˆ é™¤é™¤æœ€å50ä¸ªå¤–çš„æ‰€æœ‰ä»»åŠ¡
                                manager.remove_import_task(task_id)
                        
                        # åƒåœ¾å›æ”¶
                        gc.collect()
                    
                    # è®°å½•å†…å­˜ä½¿ç”¨
                    current_memory = self.process.memory_info().rss / 1024 / 1024
                    memory_samples.append(current_memory)
                    
                    # å®šæœŸæŠ¥å‘Š
                    current_time = time.time()
                    if current_time - last_report_time >= report_interval:
                        elapsed_time = current_time - start_time
                        operations_per_second = operations_completed / elapsed_time
                        avg_memory = np.mean(memory_samples[-10:]) if memory_samples else 0
                        
                        print(f"  è¿è¡Œæ—¶é—´: {elapsed_time:.0f}s, å®Œæˆæ“ä½œ: {operations_completed}, "
                              f"é€Ÿåº¦: {operations_per_second:.1f}ops/s, å†…å­˜: {avg_memory:.1f}MB, "
                              f"é”™è¯¯: {errors_encountered}")
                        
                        last_report_time = current_time
                    
                    # æ§åˆ¶æ“ä½œé¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹
                    operation_time = time.time() - operation_start
                    if operation_time < 0.1:  # å¦‚æœæ“ä½œå¤ªå¿«ï¼Œç¨å¾®ç­‰å¾…
                        time.sleep(0.1 - operation_time)
                
                except Exception as e:
                    errors_encountered += 1
                    if errors_encountered <= 10:  # åªæ‰“å°å‰10ä¸ªé”™è¯¯
                        print(f"  æ“ä½œ {operations_completed} å‡ºé”™: {e}")
        
        finally:
            # æ¸…ç†èµ„æº
            try:
                integration.close()
            except:
                pass
        
        # æµ‹è¯•ç»“æœåˆ†æ
        total_time = time.time() - start_time
        operations_per_second = operations_completed / total_time if total_time > 0 else 0
        error_rate = errors_encountered / operations_completed if operations_completed > 0 else 0
        
        # å†…å­˜åˆ†æ
        if memory_samples:
            initial_memory = memory_samples[0]
            final_memory = memory_samples[-1]
            max_memory = max(memory_samples)
            avg_memory = np.mean(memory_samples)
            memory_growth = final_memory - initial_memory
        else:
            initial_memory = final_memory = max_memory = avg_memory = memory_growth = 0
        
        print(f"è¿ç»­æ“ä½œç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
        print(f"  æµ‹è¯•æ—¶é•¿: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"  å®Œæˆæ“ä½œæ•°: {operations_completed}")
        print(f"  é‡åˆ°é”™è¯¯æ•°: {errors_encountered}")
        print(f"  æ“ä½œé€Ÿåº¦: {operations_per_second:.2f} æ“ä½œ/ç§’")
        print(f"  é”™è¯¯ç‡: {error_rate:.4f} ({error_rate*100:.2f}%)")
        print(f"  å†…å­˜ä½¿ç”¨:")
        print(f"    åˆå§‹: {initial_memory:.1f}MB")
        print(f"    æœ€ç»ˆ: {final_memory:.1f}MB")
        print(f"    æœ€å¤§: {max_memory:.1f}MB")
        print(f"    å¹³å‡: {avg_memory:.1f}MB")
        print(f"    å¢é•¿: {memory_growth:.1f}MB")
        
        # ç¨³å®šæ€§æ–­è¨€
        self.assertGreater(operations_completed, test_duration / 2)  # è‡³å°‘æ¯2ç§’å®Œæˆä¸€ä¸ªæ“ä½œ
        self.assertLess(error_rate, 0.05)  # é”™è¯¯ç‡å°äº5%
        self.assertLess(memory_growth, 200)  # å†…å­˜å¢é•¿å°äº200MB
        self.assertGreater(operations_per_second, 0.5)  # è‡³å°‘æ¯ç§’0.5ä¸ªæ“ä½œ

    def test_exception_recovery(self):
        """æµ‹è¯•å¼‚å¸¸æ¢å¤èƒ½åŠ›"""
        print("\n--- æµ‹è¯•å¼‚å¸¸æ¢å¤èƒ½åŠ› ---")
        
        manager = IntelligentConfigManager(self.db_path)
        
        # æµ‹è¯•å„ç§å¼‚å¸¸æƒ…å†µä¸‹çš„æ¢å¤èƒ½åŠ›
        recovery_tests = [
            {
                'name': 'æ•°æ®åº“è¿æ¥å¼‚å¸¸',
                'exception_func': lambda: self._simulate_db_error(manager),
                'recovery_func': lambda: self._verify_db_recovery(manager)
            },
            {
                'name': 'å†…å­˜ä¸è¶³å¼‚å¸¸',
                'exception_func': lambda: self._simulate_memory_error(manager),
                'recovery_func': lambda: self._verify_memory_recovery(manager)
            },
            {
                'name': 'æ–‡ä»¶ç³»ç»Ÿå¼‚å¸¸',
                'exception_func': lambda: self._simulate_file_error(manager),
                'recovery_func': lambda: self._verify_file_recovery(manager)
            }
        ]
        
        recovery_results = []
        
        for test in recovery_tests:
            print(f"  æµ‹è¯• {test['name']}...")
            
            try:
                # è®°å½•å¼‚å¸¸å‰çŠ¶æ€
                initial_tasks = len(manager.get_all_import_tasks())
                
                # æ¨¡æ‹Ÿå¼‚å¸¸
                test['exception_func']()
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç³»ç»Ÿå¤„ç†å¼‚å¸¸
                time.sleep(1)
                
                # éªŒè¯æ¢å¤
                recovery_success = test['recovery_func']()
                
                # è®°å½•æ¢å¤åçŠ¶æ€
                final_tasks = len(manager.get_all_import_tasks())
                
                recovery_results.append({
                    'test_name': test['name'],
                    'recovery_success': recovery_success,
                    'initial_tasks': initial_tasks,
                    'final_tasks': final_tasks
                })
                
                print(f"    æ¢å¤ç»“æœ: {'æˆåŠŸ' if recovery_success else 'å¤±è´¥'}")
                
            except Exception as e:
                print(f"    æµ‹è¯•å¼‚å¸¸: {e}")
                recovery_results.append({
                    'test_name': test['name'],
                    'recovery_success': False,
                    'error': str(e)
                })
        
        # åˆ†ææ¢å¤ç»“æœ
        successful_recoveries = sum(1 for r in recovery_results if r.get('recovery_success', False))
        total_tests = len(recovery_results)
        
        print(f"å¼‚å¸¸æ¢å¤æµ‹è¯•ç»“æœ:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  æˆåŠŸæ¢å¤: {successful_recoveries}")
        print(f"  æ¢å¤æˆåŠŸç‡: {successful_recoveries/total_tests:.2%}")
        
        for result in recovery_results:
            status = "æˆåŠŸ" if result.get('recovery_success', False) else "å¤±è´¥"
            print(f"  {result['test_name']}: {status}")
            if 'error' in result:
                print(f"    é”™è¯¯: {result['error']}")
        
        # æ¢å¤èƒ½åŠ›æ–­è¨€
        self.assertGreaterEqual(successful_recoveries, total_tests * 0.7)  # è‡³å°‘70%æ¢å¤æˆåŠŸ

    def _simulate_db_error(self, manager):
        """æ¨¡æ‹Ÿæ•°æ®åº“é”™è¯¯"""
        # ä¸´æ—¶ä¿®æ”¹æ•°æ®åº“è·¯å¾„ä¸ºæ— æ•ˆè·¯å¾„
        original_path = manager.db_path
        manager.db_path = "/invalid/path/database.sqlite"
        
        try:
            # å°è¯•æ‰§è¡Œæ•°æ®åº“æ“ä½œ
            config = ImportTaskConfig(
                task_id="db_error_test",
                name="æ•°æ®åº“é”™è¯¯æµ‹è¯•",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH
            )
            manager.add_import_task(config)
        except:
            pass  # é¢„æœŸä¼šå‡ºé”™
        finally:
            # æ¢å¤æ­£ç¡®çš„æ•°æ®åº“è·¯å¾„
            manager.db_path = original_path

    def _verify_db_recovery(self, manager):
        """éªŒè¯æ•°æ®åº“æ¢å¤"""
        try:
            # å°è¯•æ­£å¸¸çš„æ•°æ®åº“æ“ä½œ
            config = ImportTaskConfig(
                task_id="db_recovery_test",
                name="æ•°æ®åº“æ¢å¤æµ‹è¯•",
                data_source="tongdaxin",
                asset_type="stock",
                data_type="kline",
                symbols=["000001"],
                frequency=DataFrequency.DAILY,
                mode=ImportMode.BATCH
            )
            
            success = manager.add_import_task(config)
            if success:
                # éªŒè¯ä»»åŠ¡ç¡®å®è¢«æ·»åŠ 
                retrieved = manager.get_import_task("db_recovery_test")
                return retrieved is not None
            
            return False
        except:
            return False

    def _simulate_memory_error(self, manager):
        """æ¨¡æ‹Ÿå†…å­˜ä¸è¶³é”™è¯¯"""
        # åˆ›å»ºå¤§é‡å¯¹è±¡æ¶ˆè€—å†…å­˜
        large_objects = []
        try:
            for i in range(1000):
                # åˆ›å»ºå¤§å‹DataFrame
                large_data = pd.DataFrame(np.random.rand(1000, 100))
                large_objects.append(large_data)
        except MemoryError:
            pass  # é¢„æœŸå¯èƒ½å‡ºç°å†…å­˜é”™è¯¯
        finally:
            # æ¸…ç†å¤§å¯¹è±¡
            del large_objects
            gc.collect()

    def _verify_memory_recovery(self, manager):
        """éªŒè¯å†…å­˜æ¢å¤"""
        try:
            # æ‰§è¡Œæ­£å¸¸æ“ä½œéªŒè¯ç³»ç»Ÿä»ç„¶å¯ç”¨
            stats = manager.get_intelligent_statistics()
            return isinstance(stats, dict) and 'total_tasks' in stats
        except:
            return False

    def _simulate_file_error(self, manager):
        """æ¨¡æ‹Ÿæ–‡ä»¶ç³»ç»Ÿé”™è¯¯"""
        # å°è¯•è®¿é—®ä¸å­˜åœ¨çš„æ–‡ä»¶
        try:
            with open("/nonexistent/path/file.txt", "r") as f:
                f.read()
        except:
            pass  # é¢„æœŸä¼šå‡ºé”™

    def _verify_file_recovery(self, manager):
        """éªŒè¯æ–‡ä»¶ç³»ç»Ÿæ¢å¤"""
        try:
            # éªŒè¯æ­£å¸¸çš„æ–‡ä»¶æ“ä½œ
            conflicts = manager.detect_conflicts()
            return isinstance(conflicts, list)
        except:
            return False


def run_long_term_stability_tests():
    """è¿è¡Œé•¿æœŸç¨³å®šæ€§æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œé•¿æœŸç¨³å®šæ€§æµ‹è¯•...")
    print("=" * 100)
    
    # æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ
    test_duration = int(os.environ.get('STABILITY_TEST_DURATION', '300'))
    print(f"ç¨³å®šæ€§æµ‹è¯•æŒç»­æ—¶é—´: {test_duration} ç§’ ({test_duration/60:.1f} åˆ†é’Ÿ)")
    print("æç¤º: å¯é€šè¿‡ç¯å¢ƒå˜é‡ STABILITY_TEST_DURATION è°ƒæ•´æµ‹è¯•æ—¶é•¿")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestMemoryLeakDetection,
        TestResourceLeakDetection,
        TestLongRunningStability
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(test_suite)
    
    print("=" * 100)
    print(f"é•¿æœŸç¨³å®šæ€§æµ‹è¯•å®Œæˆï¼")
    print(f"æˆåŠŸ: {'æ˜¯' if result.wasSuccessful() else 'å¦'}")
    print(f"å¤±è´¥æ•°: {len(result.failures)}")
    print(f"é”™è¯¯æ•°: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("ğŸ‰ æ‰€æœ‰é•¿æœŸç¨³å®šæ€§æµ‹è¯•é€šè¿‡ï¼")
        print("ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„é•¿æœŸè¿è¡Œç¨³å®šæ€§ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€‚")
    else:
        print("âŒ å­˜åœ¨ç¨³å®šæ€§æµ‹è¯•å¤±è´¥æˆ–é”™è¯¯")
        
        if result.failures:
            print("\nå¤±è´¥çš„æµ‹è¯•:")
            for test, traceback in result.failures:
                print(f"  - {test}")
        
        if result.errors:
            print("\né”™è¯¯çš„æµ‹è¯•:")
            for test, traceback in result.errors:
                print(f"  - {test}")
    
    return result.wasSuccessful(), len(result.failures), len(result.errors)


if __name__ == "__main__":
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    os.environ['TESTING'] = '1'
    
    # è¿è¡Œé•¿æœŸç¨³å®šæ€§æµ‹è¯•
    success, failures, errors = run_long_term_stability_tests()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    exit_code = 0 if success else 1
    exit(exit_code)
