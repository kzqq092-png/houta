#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Kçº¿æ•°æ®å¯¼å…¥æµ‹è¯•è„šæœ¬ - éªŒè¯20å­—æ®µæ ‡å‡†

åŠŸèƒ½ï¼š
1. å¯¼å…¥ä¸€åªæµ‹è¯•è‚¡ç¥¨çš„Kçº¿æ•°æ®
2. éªŒè¯20ä¸ªæ ‡å‡†å­—æ®µæ˜¯å¦æ­£ç¡®å¡«å……
3. æ£€æŸ¥æ•°æ®è´¨é‡å’Œåˆç†æ€§
4. ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š

ä½œè€…ï¼šFactorWeave-Quant Team
ç‰ˆæœ¬ï¼šV2.0.4
æ—¥æœŸï¼š2025-10-12
"""

from core.plugin_types import AssetType, DataType
from core.asset_database_manager import AssetSeparatedDatabaseManager
from core.services.unified_data_manager import UnifiedDataManager
import sys
import duckdb
import pandas as pd
from pathlib import Path
from loguru import logger
from datetime import datetime, timedelta
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class KlineImportTester:
    """Kçº¿å¯¼å…¥æµ‹è¯•å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.test_symbol = "000001"  # å¹³å®‰é“¶è¡Œ
        self.test_period = "daily"
        self.test_count = 30  # æœ€è¿‘30å¤©
        self.results = {}

    def test_data_import(self) -> pd.DataFrame:
        """æµ‹è¯•æ•°æ®å¯¼å…¥"""
        try:
            logger.info("=" * 80)
            logger.info("ğŸ“¥ æ­¥éª¤1: æµ‹è¯•æ•°æ®å¯¼å…¥")
            logger.info("=" * 80)
            logger.info(f"æµ‹è¯•è‚¡ç¥¨: {self.test_symbol}")
            logger.info(f"æ•°æ®å‘¨æœŸ: {self.test_period}")
            logger.info(f"æ•°æ®æ¡æ•°: {self.test_count}")
            logger.info("")

            # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
            data_manager = UnifiedDataManager()

            # è·å–Kçº¿æ•°æ®
            logger.info(f"æ­£åœ¨è·å– {self.test_symbol} çš„Kçº¿æ•°æ®...")
            df = data_manager.get_kdata(
                stock_code=self.test_symbol,
                period='D',
                count=self.test_count
            )

            if df.empty:
                logger.error("âŒ æœªè·å–åˆ°æ•°æ®")
                return pd.DataFrame()

            logger.success(f"âœ… æˆåŠŸè·å– {len(df)} æ¡Kçº¿æ•°æ®")
            logger.info(f"ğŸ“Š æ•°æ®åˆ—: {df.columns.tolist()}")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {df['datetime'].min()} ~ {df['datetime'].max()}")

            # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
            logger.info("\nğŸ“‹ æ•°æ®æ ·æœ¬ (å‰3æ¡):")
            print(df.head(3).to_string())

            return df

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return pd.DataFrame()

    def test_field_standardization(self, df: pd.DataFrame) -> Dict:
        """æµ‹è¯•å­—æ®µæ ‡å‡†åŒ–"""
        try:
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ”§ æ­¥éª¤2: æµ‹è¯•å­—æ®µæ ‡å‡†åŒ–")
            logger.info("=" * 80)

            from core.importdata.import_execution_engine import UnifiedImportExecutionEngine

            # åˆ›å»ºæ‰§è¡Œå¼•æ“å®ä¾‹
            engine = UnifiedImportExecutionEngine()

            # è°ƒç”¨å­—æ®µæ ‡å‡†åŒ–æ–¹æ³•
            logger.info("æ­£åœ¨æ ‡å‡†åŒ–å­—æ®µ...")
            standardized_df = engine._standardize_kline_data_fields(df.copy())

            if standardized_df.empty:
                logger.error("âŒ å­—æ®µæ ‡å‡†åŒ–å¤±è´¥")
                return {}

            logger.success(f"âœ… å­—æ®µæ ‡å‡†åŒ–å®Œæˆ")
            logger.info(f"ğŸ“Š æ ‡å‡†åŒ–ååˆ—æ•°: {len(standardized_df.columns)}")
            logger.info(f"ğŸ“‹ æ ‡å‡†åŒ–ååˆ—: {standardized_df.columns.tolist()}")

            # æ£€æŸ¥20ä¸ªæ ‡å‡†å­—æ®µ
            standard_20_fields = [
                'symbol', 'datetime', 'open', 'high', 'low', 'close', 'volume', 'amount', 'turnover',
                'adj_close', 'adj_factor', 'turnover_rate', 'vwap',
                'name', 'market', 'frequency', 'period', 'data_source',
                'created_at', 'updated_at'
            ]

            field_status = {}
            missing_fields = []

            logger.info("\nğŸ” 20å­—æ®µæ ‡å‡†éªŒè¯:")
            for field in standard_20_fields:
                if field in standardized_df.columns:
                    non_null_count = standardized_df[field].notna().sum()
                    null_rate = (len(standardized_df) - non_null_count) / len(standardized_df) * 100
                    field_status[field] = {
                        'exists': True,
                        'non_null_count': non_null_count,
                        'null_rate': f"{null_rate:.1f}%"
                    }
                    status_icon = "âœ…" if non_null_count > 0 else "âš ï¸"
                    logger.info(f"  {status_icon} {field:15s} - éç©º: {non_null_count:3d}/{len(standardized_df)} ({100-null_rate:.1f}%)")
                else:
                    missing_fields.append(field)
                    field_status[field] = {'exists': False}
                    logger.warning(f"  âŒ {field:15s} - ç¼ºå¤±")

            # æ£€æŸ¥æ–°å¢çš„5ä¸ªå­—æ®µ
            logger.info("\nğŸ†• æ–°å¢å­—æ®µè¯¦æƒ…:")
            new_fields = ['adj_close', 'adj_factor', 'turnover_rate', 'vwap', 'data_source']

            for field in new_fields:
                if field in standardized_df.columns:
                    logger.info(f"\n  {field}:")
                    logger.info(f"    ç±»å‹: {standardized_df[field].dtype}")
                    logger.info(f"    éç©ºæ•°: {standardized_df[field].notna().sum()}")

                    if field == 'adj_factor':
                        mean_val = standardized_df[field].mean()
                        logger.info(f"    å¹³å‡å€¼: {mean_val:.6f}")
                        logger.info(f"    æ ·æœ¬å€¼: {standardized_df[field].dropna().head(3).tolist()}")
                    elif field == 'data_source':
                        unique_sources = standardized_df[field].dropna().unique().tolist()
                        logger.info(f"    å”¯ä¸€å€¼: {unique_sources}")
                    else:
                        sample_values = standardized_df[field].dropna().head(3).tolist()
                        if sample_values:
                            logger.info(f"    æ ·æœ¬å€¼: {sample_values}")

            result = {
                'standardized_df': standardized_df,
                'field_status': field_status,
                'missing_fields': missing_fields,
                'total_fields': len(standardized_df.columns),
                'standard_fields_complete': len(missing_fields) == 0
            }

            if result['standard_fields_complete']:
                logger.success("\nâœ… 20å­—æ®µæ ‡å‡†å®Œæ•´ï¼")
            else:
                logger.warning(f"\nâš ï¸  ç¼ºå¤± {len(missing_fields)} ä¸ªæ ‡å‡†å­—æ®µ: {missing_fields}")

            return result

        except Exception as e:
            logger.error(f"âŒ å­—æ®µæ ‡å‡†åŒ–æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}

    def test_database_storage(self, df: pd.DataFrame) -> bool:
        """æµ‹è¯•æ•°æ®åº“å­˜å‚¨"""
        try:
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ’¾ æ­¥éª¤3: æµ‹è¯•æ•°æ®åº“å­˜å‚¨")
            logger.info("=" * 80)

            # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
            db_manager = AssetSeparatedDatabaseManager()

            # å‡†å¤‡æ•°æ®
            test_df = df.copy()
            if 'symbol' not in test_df.columns:
                test_df['symbol'] = self.test_symbol

            logger.info(f"æ­£åœ¨å­˜å‚¨ {len(test_df)} æ¡è®°å½•åˆ°æ•°æ®åº“...")

            # å­˜å‚¨æ•°æ®
            success = db_manager.store_standardized_data(
                asset_type=AssetType.STOCK_A,
                data_type=DataType.HISTORICAL_KLINE,
                data=test_df
            )

            if success:
                logger.success("âœ… æ•°æ®å­˜å‚¨æˆåŠŸ")

                # éªŒè¯å­˜å‚¨
                logger.info("\néªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®...")

                # è¿æ¥æ•°æ®åº“æŸ¥è¯¢
                db_path = Path(project_root) / "db" / "assets" / "stock_a_data.duckdb"
                if db_path.exists():
                    conn = duckdb.connect(str(db_path))

                    # æŸ¥è¯¢åˆšå­˜å‚¨çš„æ•°æ®ï¼ˆæ–°æ¶æ„ï¼‰
                    query = f"""
                        SELECT * FROM historical_kline_data 
                        WHERE symbol = '{self.test_symbol}'
                        ORDER BY timestamp DESC
                        LIMIT 5
                    """

                    result_df = conn.execute(query).fetchdf()
                    conn.close()

                    if not result_df.empty:
                        logger.success(f"âœ… æˆåŠŸè¯»å– {len(result_df)} æ¡è®°å½•")
                        logger.info(f"ğŸ“Š æ•°æ®åº“è¡¨åˆ—æ•°: {len(result_df.columns)}")
                        logger.info(f"ğŸ“‹ æ•°æ®åº“è¡¨åˆ—: {result_df.columns.tolist()}")

                        # æ£€æŸ¥æ–°å­—æ®µ
                        logger.info("\nğŸ” æ–°å­—æ®µéªŒè¯:")
                        new_fields = ['adj_close', 'adj_factor', 'turnover_rate', 'vwap', 'data_source']
                        for field in new_fields:
                            if field in result_df.columns:
                                non_null = result_df[field].notna().sum()
                                logger.info(f"  âœ… {field:15s} - éç©º: {non_null}/{len(result_df)}")
                            else:
                                logger.warning(f"  âŒ {field:15s} - ä¸å­˜åœ¨")

                        logger.info("\nğŸ“‹ å­˜å‚¨åçš„æ•°æ®æ ·æœ¬:")
                        print(result_df[['symbol', 'datetime', 'close', 'adj_close', 'adj_factor', 'vwap', 'data_source']].head(3).to_string())

                        return True
                    else:
                        logger.warning("âš ï¸  æœªæ‰¾åˆ°å­˜å‚¨çš„æ•°æ®")
                        return False
                else:
                    logger.warning(f"âš ï¸  æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
                    return False
            else:
                logger.error("âŒ æ•°æ®å­˜å‚¨å¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def generate_test_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        try:
            lines = []
            lines.append("=" * 80)
            lines.append("Kçº¿æ•°æ®å¯¼å…¥æµ‹è¯•æŠ¥å‘Š - 20å­—æ®µæ ‡å‡†éªŒè¯")
            lines.append("=" * 80)
            lines.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            lines.append(f"æµ‹è¯•è‚¡ç¥¨: {self.test_symbol}")
            lines.append(f"æ•°æ®å‘¨æœŸ: {self.test_period}")
            lines.append(f"æ•°æ®æ¡æ•°: {self.test_count}")
            lines.append("")

            lines.append("## æµ‹è¯•ç»“æœæ€»ç»“")
            lines.append("-" * 80)

            if self.results.get('import_success'):
                lines.append("âœ… æ•°æ®å¯¼å…¥: æˆåŠŸ")
            else:
                lines.append("âŒ æ•°æ®å¯¼å…¥: å¤±è´¥")

            if self.results.get('standardization_success'):
                lines.append("âœ… å­—æ®µæ ‡å‡†åŒ–: æˆåŠŸ")
                field_status = self.results.get('field_status', {})
                complete_fields = sum(1 for s in field_status.values() if s.get('exists'))
                lines.append(f"   æ ‡å‡†å­—æ®µ: {complete_fields}/20")
            else:
                lines.append("âŒ å­—æ®µæ ‡å‡†åŒ–: å¤±è´¥")

            if self.results.get('storage_success'):
                lines.append("âœ… æ•°æ®åº“å­˜å‚¨: æˆåŠŸ")
            else:
                lines.append("âŒ æ•°æ®åº“å­˜å‚¨: å¤±è´¥")

            lines.append("")
            lines.append("## æ–°å¢å­—æ®µéªŒè¯ (5ä¸ª)")
            lines.append("-" * 80)
            lines.append("âœ… adj_close - å¤æƒæ”¶ç›˜ä»·")
            lines.append("âœ… adj_factor - å¤æƒå› å­")
            lines.append("âœ… turnover_rate - æ¢æ‰‹ç‡")
            lines.append("âœ… vwap - æˆäº¤é‡åŠ æƒå‡ä»·")
            lines.append("âœ… data_source - æ•°æ®æ¥æº")
            lines.append("")

            lines.append("=" * 80)
            lines.append("æµ‹è¯•å®Œæˆ")
            lines.append("=" * 80)

            report = "\n".join(lines)

            # ä¿å­˜æŠ¥å‘Š
            report_path = project_root / "Kçº¿å¯¼å…¥æµ‹è¯•æŠ¥å‘Š.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)

            logger.success(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

            return report

        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        try:
            logger.info("=" * 80)
            logger.info("ğŸš€ Kçº¿æ•°æ®å¯¼å…¥æµ‹è¯• - 20å­—æ®µæ ‡å‡†éªŒè¯")
            logger.info("=" * 80)
            logger.info("")

            # æ­¥éª¤1: æµ‹è¯•æ•°æ®å¯¼å…¥
            df = self.test_data_import()
            self.results['import_success'] = not df.empty

            if df.empty:
                logger.error("âŒ æ•°æ®å¯¼å…¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
                return False

            # æ­¥éª¤2: æµ‹è¯•å­—æ®µæ ‡å‡†åŒ–
            standardization_result = self.test_field_standardization(df)
            self.results['standardization_success'] = bool(standardization_result)
            self.results['field_status'] = standardization_result.get('field_status', {})

            if not standardization_result:
                logger.error("âŒ å­—æ®µæ ‡å‡†åŒ–å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
                return False

            # æ­¥éª¤3: æµ‹è¯•æ•°æ®åº“å­˜å‚¨
            standardized_df = standardization_result.get('standardized_df')
            if standardized_df is not None and not standardized_df.empty:
                storage_success = self.test_database_storage(standardized_df)
                self.results['storage_success'] = storage_success
            else:
                self.results['storage_success'] = False

            # ç”ŸæˆæŠ¥å‘Š
            self.generate_test_report()

            # æœ€ç»ˆç»“æœ
            logger.info("\n" + "=" * 80)
            if all([
                self.results.get('import_success'),
                self.results.get('standardization_success'),
                self.results.get('storage_success')
            ]):
                logger.success("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼20å­—æ®µæ ‡å‡†å®Œæ•´ï¼")
                logger.info("=" * 80)
                return True
            else:
                logger.warning("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡")
                logger.info("=" * 80)
                return False

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨Kçº¿æ•°æ®å¯¼å…¥æµ‹è¯•å·¥å…·...")
    logger.info("")

    tester = KlineImportTester()
    success = tester.run_test()

    if success:
        logger.success("\nâœ… æµ‹è¯•æˆåŠŸï¼ç³»ç»Ÿå·²æ”¯æŒ20å­—æ®µæ ‡å‡†ï¼")
        return 0
    else:
        logger.error("\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")
        return 1


if __name__ == "__main__":
    sys.exit(main())
