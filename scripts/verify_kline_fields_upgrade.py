#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Kçº¿è¡¨å­—æ®µå‡çº§éªŒè¯è„šæœ¬

åŠŸèƒ½ï¼š
1. éªŒè¯æ•°æ®åº“è¡¨ç»“æ„ï¼ˆ20å­—æ®µï¼‰
2. æµ‹è¯•æ•°æ®å¯¼å…¥åŠŸèƒ½
3. æ£€æŸ¥æ–°å­—æ®µæ•°æ®è´¨é‡
4. ç”ŸæˆéªŒè¯æŠ¥å‘Š

ä½œè€…ï¼šFactorWeave-Quant Team
ç‰ˆæœ¬ï¼šV2.0.4
æ—¥æœŸï¼š2025-10-12
"""

import sys
import duckdb
import pandas as pd
from pathlib import Path
from loguru import logger
from datetime import datetime
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class KlineFieldsVerifier:
    """Kçº¿å­—æ®µå‡çº§éªŒè¯å™¨"""

    # æ ‡å‡†20å­—æ®µå®šä¹‰
    STANDARD_FIELDS = {
        # åŸºç¡€OHLCVå­—æ®µï¼ˆ9ä¸ªï¼‰
        'basic': ['symbol', 'datetime', 'open', 'high', 'low', 'close', 'volume', 'amount', 'turnover'],

        # å¤æƒæ•°æ®ï¼ˆ2ä¸ªï¼‰
        'adj': ['adj_close', 'adj_factor'],

        # æ‰©å±•äº¤æ˜“æ•°æ®ï¼ˆ2ä¸ªï¼‰
        'extended': ['turnover_rate', 'vwap'],

        # å…ƒæ•°æ®ï¼ˆ7ä¸ªï¼‰
        'metadata': ['name', 'market', 'frequency', 'period', 'data_source', 'created_at', 'updated_at']
    }

    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.db_path = project_root / "db" / "unified_kline_data.duckdb"
        self.conn = None
        self.verification_results = {}

    def connect_database(self) -> bool:
        """è¿æ¥æ•°æ®åº“"""
        try:
            if not self.db_path.exists():
                logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.db_path}")
                return False

            self.conn = duckdb.connect(str(self.db_path))
            logger.info(f"âœ… æˆåŠŸè¿æ¥æ•°æ®åº“: {self.db_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def get_table_list(self) -> List[str]:
        """è·å–Kçº¿è¡¨åˆ—è¡¨"""
        try:
            # ä½¿ç”¨DuckDBç‰¹å®šå‡½æ•°æŸ¥è¯¢è¡¨
            query = """
                SELECT table_name 
                FROM duckdb_tables()
                WHERE table_name LIKE '%kline%'
            """
            result = self.conn.execute(query).fetchall()
            tables = [row[0] for row in result]

            logger.info(f"ğŸ“Š å‘ç° {len(tables)} ä¸ªKçº¿è¡¨")
            for table in tables:
                logger.info(f"  - {table}")

            return tables

        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def verify_table_structure(self, table_name: str) -> Dict:
        """éªŒè¯è¡¨ç»“æ„"""
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ” éªŒè¯è¡¨ç»“æ„: {table_name}")
            logger.info(f"{'='*60}")

            # è·å–è¡¨åˆ—ä¿¡æ¯
            query = f"""
                SELECT column_name, data_type 
                FROM duckdb_columns()
                WHERE table_name = '{table_name}'
            """
            result = self.conn.execute(query).fetchall()

            columns = {row[0]: row[1] for row in result}
            total_columns = len(columns)

            logger.info(f"ğŸ“‹ è¡¨ {table_name} å…±æœ‰ {total_columns} åˆ—")

            # æ£€æŸ¥æ–°å­—æ®µ
            new_fields = ['adj_close', 'adj_factor', 'turnover_rate', 'vwap', 'data_source']
            missing_fields = []
            existing_fields = []

            for field in new_fields:
                if field in columns:
                    existing_fields.append(field)
                    logger.info(f"  âœ… {field}: {columns[field]}")
                else:
                    missing_fields.append(field)
                    logger.warning(f"  âŒ ç¼ºå¤±å­—æ®µ: {field}")

            # æ£€æŸ¥æ‰€æœ‰æ ‡å‡†å­—æ®µ
            all_standard_fields = []
            for category, fields in self.STANDARD_FIELDS.items():
                all_standard_fields.extend(fields)

            missing_standard = [f for f in all_standard_fields if f not in columns]

            # éªŒè¯ç»“æœ
            verification = {
                'table_name': table_name,
                'total_columns': total_columns,
                'new_fields_count': len(existing_fields),
                'new_fields_total': len(new_fields),
                'missing_new_fields': missing_fields,
                'missing_standard_fields': missing_standard,
                'is_complete': len(missing_fields) == 0,
                'columns': columns
            }

            if verification['is_complete']:
                logger.success(f"âœ… è¡¨ç»“æ„å®Œæ•´ï¼æ–°å­—æ®µ: {len(existing_fields)}/{len(new_fields)}")
            else:
                logger.warning(f"âš ï¸  è¡¨ç»“æ„ä¸å®Œæ•´ï¼ç¼ºå¤± {len(missing_fields)} ä¸ªæ–°å­—æ®µ")

            return verification

        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¡¨ç»“æ„å¤±è´¥: {e}")
            return {}

    def check_data_quality(self, table_name: str, limit: int = 100) -> Dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”¬ æ£€æŸ¥æ•°æ®è´¨é‡: {table_name}")
            logger.info(f"{'='*60}")

            # æŸ¥è¯¢æœ€æ–°æ•°æ®
            query = f"""
                SELECT 
                    symbol, datetime, close, 
                    adj_close, adj_factor, 
                    turnover_rate, vwap, 
                    data_source, volume, amount
                FROM {table_name}
                ORDER BY datetime DESC
                LIMIT {limit}
            """

            df = self.conn.execute(query).fetchdf()

            if df.empty:
                logger.warning("âš ï¸  è¡¨ä¸­æ²¡æœ‰æ•°æ®")
                return {'has_data': False}

            logger.info(f"ğŸ“Š æŸ¥è¯¢åˆ° {len(df)} æ¡æœ€æ–°è®°å½•")

            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality_report = {
                'has_data': True,
                'record_count': len(df),
                'date_range': {
                    'start': str(df['datetime'].min()),
                    'end': str(df['datetime'].max())
                },
                'symbols': df['symbol'].nunique(),
                'fields_quality': {}
            }

            # æ£€æŸ¥æ–°å­—æ®µçš„å¡«å……æƒ…å†µ
            new_fields_check = {
                'adj_close': {
                    'null_count': df['adj_close'].isna().sum(),
                    'null_rate': f"{df['adj_close'].isna().sum() / len(df) * 100:.2f}%",
                    'sample_values': df['adj_close'].dropna().head(5).tolist()
                },
                'adj_factor': {
                    'null_count': df['adj_factor'].isna().sum(),
                    'null_rate': f"{df['adj_factor'].isna().sum() / len(df) * 100:.2f}%",
                    'mean': float(df['adj_factor'].mean()) if not df['adj_factor'].isna().all() else 0,
                    'sample_values': df['adj_factor'].dropna().head(5).tolist()
                },
                'turnover_rate': {
                    'null_count': df['turnover_rate'].isna().sum(),
                    'null_rate': f"{df['turnover_rate'].isna().sum() / len(df) * 100:.2f}%",
                    'sample_values': df['turnover_rate'].dropna().head(5).tolist()
                },
                'vwap': {
                    'null_count': df['vwap'].isna().sum(),
                    'null_rate': f"{df['vwap'].isna().sum() / len(df) * 100:.2f}%",
                    'sample_values': df['vwap'].dropna().head(5).tolist()
                },
                'data_source': {
                    'null_count': df['data_source'].isna().sum(),
                    'null_rate': f"{df['data_source'].isna().sum() / len(df) * 100:.2f}%",
                    'unique_sources': df['data_source'].dropna().unique().tolist()
                }
            }

            quality_report['fields_quality'] = new_fields_check

            # æ‰“å°è´¨é‡æŠ¥å‘Š
            logger.info(f"\nğŸ“ˆ æ•°æ®è´¨é‡æŠ¥å‘Š:")
            logger.info(f"  è®°å½•æ•°: {quality_report['record_count']}")
            logger.info(f"  æ—¶é—´èŒƒå›´: {quality_report['date_range']['start']} ~ {quality_report['date_range']['end']}")
            logger.info(f"  è‚¡ç¥¨æ•°: {quality_report['symbols']}")

            logger.info(f"\nğŸ” æ–°å­—æ®µè´¨é‡:")
            for field, stats in new_fields_check.items():
                null_rate = stats['null_rate']
                logger.info(f"  {field}:")
                logger.info(f"    - ç©ºå€¼ç‡: {null_rate}")
                if 'mean' in stats:
                    logger.info(f"    - å¹³å‡å€¼: {stats['mean']:.4f}")
                if 'unique_sources' in stats and stats['unique_sources']:
                    logger.info(f"    - æ•°æ®æº: {', '.join(stats['unique_sources'])}")
                if stats['sample_values']:
                    logger.info(f"    - æ ·æœ¬å€¼: {stats['sample_values'][:3]}")

            # éªŒè¯æ•°æ®åˆç†æ€§
            logger.info(f"\nâœ… æ•°æ®åˆç†æ€§éªŒè¯:")

            # 1. å¤æƒä»·æ ¼åº”è¯¥æ¥è¿‘åŸä»·æ ¼
            if not df['adj_close'].isna().all():
                price_diff = (df['adj_close'] - df['close']).abs() / df['close'] * 100
                avg_diff = price_diff.mean()
                logger.info(f"  adj_close vs close å¹³å‡å·®å¼‚: {avg_diff:.2f}%")
                if avg_diff < 5:
                    logger.success(f"    âœ… å¤æƒä»·æ ¼åˆç†ï¼ˆå·®å¼‚<5%ï¼‰")
                else:
                    logger.warning(f"    âš ï¸  å¤æƒä»·æ ¼å·®å¼‚è¾ƒå¤§ï¼ˆ>{avg_diff:.2f}%ï¼‰")

            # 2. VWAPåº”è¯¥åœ¨OHLCèŒƒå›´å†…
            if not df['vwap'].isna().all():
                vwap_valid = ((df['vwap'] >= df['low']) & (df['vwap'] <= df['high'])).sum()
                vwap_valid_rate = vwap_valid / len(df[df['vwap'].notna()]) * 100 if len(df[df['vwap'].notna()]) > 0 else 0
                logger.info(f"  VWAPåˆç†æ€§: {vwap_valid_rate:.2f}% åœ¨[low, high]èŒƒå›´å†…")
                if vwap_valid_rate > 90:
                    logger.success(f"    âœ… VWAPè®¡ç®—åˆç†ï¼ˆ>90%ï¼‰")
                else:
                    logger.warning(f"    âš ï¸  VWAPå¯èƒ½æœ‰é—®é¢˜ï¼ˆ{vwap_valid_rate:.2f}%ï¼‰")

            # 3. æ¢æ‰‹ç‡åº”è¯¥åˆç†ï¼ˆä¸€èˆ¬<30%ï¼‰
            if not df['turnover_rate'].isna().all():
                high_turnover = (df['turnover_rate'] > 30).sum()
                high_turnover_rate = high_turnover / len(df[df['turnover_rate'].notna()]) * 100 if len(df[df['turnover_rate'].notna()]) > 0 else 0
                logger.info(f"  æ¢æ‰‹ç‡>30%çš„è®°å½•: {high_turnover_rate:.2f}%")
                if high_turnover_rate < 5:
                    logger.success(f"    âœ… æ¢æ‰‹ç‡åˆ†å¸ƒæ­£å¸¸ï¼ˆ<5%å¼‚å¸¸ï¼‰")
                else:
                    logger.warning(f"    âš ï¸  æ¢æ‰‹ç‡å¼‚å¸¸è®°å½•è¾ƒå¤šï¼ˆ{high_turnover_rate:.2f}%ï¼‰")

            return quality_report

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}

    def generate_report(self) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        try:
            report_lines = []
            report_lines.append("=" * 80)
            report_lines.append("Kçº¿è¡¨å­—æ®µå‡çº§éªŒè¯æŠ¥å‘Š")
            report_lines.append("=" * 80)
            report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append(f"æ•°æ®åº“: {self.db_path}")
            report_lines.append("")

            # è¡¨ç»“æ„éªŒè¯æ€»ç»“
            report_lines.append("## è¡¨ç»“æ„éªŒè¯")
            report_lines.append("-" * 80)

            for table_name, verification in self.verification_results.items():
                if 'structure' in verification:
                    struct = verification['structure']
                    status = "âœ… é€šè¿‡" if struct.get('is_complete') else "âŒ æœªé€šè¿‡"
                    report_lines.append(f"è¡¨å: {table_name}")
                    report_lines.append(f"  çŠ¶æ€: {status}")
                    report_lines.append(f"  æ€»åˆ—æ•°: {struct.get('total_columns', 0)}")
                    report_lines.append(f"  æ–°å­—æ®µ: {struct.get('new_fields_count', 0)}/{struct.get('new_fields_total', 5)}")

                    if struct.get('missing_new_fields'):
                        report_lines.append(f"  ç¼ºå¤±å­—æ®µ: {', '.join(struct['missing_new_fields'])}")
                    report_lines.append("")

            # æ•°æ®è´¨é‡éªŒè¯æ€»ç»“
            report_lines.append("## æ•°æ®è´¨é‡éªŒè¯")
            report_lines.append("-" * 80)

            for table_name, verification in self.verification_results.items():
                if 'quality' in verification:
                    quality = verification['quality']
                    if quality.get('has_data'):
                        report_lines.append(f"è¡¨å: {table_name}")
                        report_lines.append(f"  è®°å½•æ•°: {quality.get('record_count', 0)}")
                        report_lines.append(f"  æ—¶é—´èŒƒå›´: {quality['date_range']['start']} ~ {quality['date_range']['end']}")
                        report_lines.append(f"  è‚¡ç¥¨æ•°: {quality.get('symbols', 0)}")
                        report_lines.append("")

            report_lines.append("=" * 80)
            report_lines.append("éªŒè¯å®Œæˆ")
            report_lines.append("=" * 80)

            report_text = "\n".join(report_lines)

            # ä¿å­˜æŠ¥å‘Š
            report_path = project_root / "Kçº¿å­—æ®µå‡çº§éªŒè¯æŠ¥å‘Š.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_text)

            logger.success(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

            return report_text

        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def run_verification(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        try:
            logger.info("=" * 80)
            logger.info("Kçº¿è¡¨å­—æ®µå‡çº§éªŒè¯")
            logger.info("=" * 80)
            logger.info("")

            # 1. è¿æ¥æ•°æ®åº“
            if not self.connect_database():
                return False

            # 2. è·å–è¡¨åˆ—è¡¨
            tables = self.get_table_list()
            if not tables:
                logger.warning("âš ï¸  æœªå‘ç°Kçº¿è¡¨")
                return False

            # 3. éªŒè¯æ¯ä¸ªè¡¨
            for table_name in tables:
                # éªŒè¯è¡¨ç»“æ„
                structure_result = self.verify_table_structure(table_name)

                # æ£€æŸ¥æ•°æ®è´¨é‡
                quality_result = self.check_data_quality(table_name)

                # ä¿å­˜ç»“æœ
                self.verification_results[table_name] = {
                    'structure': structure_result,
                    'quality': quality_result
                }

            # 4. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report()

            logger.info("\n" + "=" * 80)
            logger.success("âœ… éªŒè¯å®Œæˆï¼")
            logger.info("=" * 80)

            return True

        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

        finally:
            if self.conn:
                self.conn.close()
                logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨Kçº¿å­—æ®µå‡çº§éªŒè¯å·¥å…·...")

    verifier = KlineFieldsVerifier()
    success = verifier.run_verification()

    if success:
        logger.success("ğŸ‰ éªŒè¯æˆåŠŸï¼")
        return 0
    else:
        logger.error("âŒ éªŒè¯å¤±è´¥ï¼")
        return 1


if __name__ == "__main__":
    sys.exit(main())
